AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py:155: UserWarning: You have specified a mini-batch size of 49152, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=2048 and n_envs=1)
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to ./logs/ppo_tensorboard/PPO 01.06.2024_18.02.45_1
Eval num_timesteps=2000, episode_reward=1089.46 +/- 353.81
Episode length: 246.60 +/- 70.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=724.46 +/- 177.85
Episode length: 193.00 +/- 26.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 193          |
|    mean_reward          | 724          |
| time/                   |              |
|    total_timesteps      | 4000         |
| train/                  |              |
|    approx_kl            | 0.0035846117 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.000369    |
|    learning_rate        | 0.001        |
|    loss                 | 4.93e+03     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0052      |
|    std                  | 1            |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=6000, episode_reward=948.73 +/- 246.21
Episode length: 245.20 +/- 51.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 949          |
| time/                   |              |
|    total_timesteps      | 6000         |
| train/                  |              |
|    approx_kl            | 0.0015821243 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.00787     |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+03     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1            |
|    value_loss           | 1.06e+04     |
------------------------------------------
Eval num_timesteps=8000, episode_reward=891.05 +/- 201.68
Episode length: 232.80 +/- 56.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 233           |
|    mean_reward          | 891           |
| time/                   |               |
|    total_timesteps      | 8000          |
| train/                  |               |
|    approx_kl            | 0.00033507068 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.68         |
|    explained_variance   | 0.0794        |
|    learning_rate        | 0.001         |
|    loss                 | 4.94e+03      |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.00013      |
|    std                  | 1             |
|    value_loss           | 1e+04         |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=1124.02 +/- 267.39
Episode length: 272.20 +/- 50.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 272           |
|    mean_reward          | 1.12e+03      |
| time/                   |               |
|    total_timesteps      | 10000         |
| train/                  |               |
|    approx_kl            | 0.00040567253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.69         |
|    explained_variance   | 0.0891        |
|    learning_rate        | 0.001         |
|    loss                 | 5.03e+03      |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.000751     |
|    std                  | 1             |
|    value_loss           | 1.02e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=1100.79 +/- 435.86
Episode length: 261.00 +/- 64.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | 1.1e+03      |
| time/                   |              |
|    total_timesteps      | 12000        |
| train/                  |              |
|    approx_kl            | 0.0005015762 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.0412       |
|    learning_rate        | 0.001        |
|    loss                 | 4.93e+03     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000692    |
|    std                  | 1.01         |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=14000, episode_reward=867.34 +/- 234.11
Episode length: 235.00 +/- 66.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 235           |
|    mean_reward          | 867           |
| time/                   |               |
|    total_timesteps      | 14000         |
| train/                  |               |
|    approx_kl            | 0.00023939766 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.71         |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 5.31e+03      |
|    n_updates            | 60            |
|    policy_gradient_loss | -0.000629     |
|    std                  | 1.01          |
|    value_loss           | 1.08e+04      |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=1020.11 +/- 190.03
Episode length: 270.60 +/- 45.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 1.02e+03     |
| time/                   |              |
|    total_timesteps      | 16000        |
| train/                  |              |
|    approx_kl            | 0.0006829013 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.001        |
|    loss                 | 5.81e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.02         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=1249.95 +/- 414.96
Episode length: 289.80 +/- 54.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 1.25e+03      |
| time/                   |               |
|    total_timesteps      | 18000         |
| train/                  |               |
|    approx_kl            | 0.00021337403 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.75         |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.001         |
|    loss                 | 4.53e+03      |
|    n_updates            | 80            |
|    policy_gradient_loss | 0.000135      |
|    std                  | 1.02          |
|    value_loss           | 9.17e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=968.15 +/- 393.60
Episode length: 248.40 +/- 52.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 968         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.000617596 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.76       |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.001       |
|    loss                 | 4.59e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00128    |
|    std                  | 1.02        |
|    value_loss           | 9.29e+03    |
-----------------------------------------
Eval num_timesteps=22000, episode_reward=1085.55 +/- 200.97
Episode length: 250.80 +/- 35.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.09e+03     |
| time/                   |              |
|    total_timesteps      | 22000        |
| train/                  |              |
|    approx_kl            | 0.0004542818 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.001        |
|    loss                 | 4.33e+03     |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000578    |
|    std                  | 1.03         |
|    value_loss           | 8.87e+03     |
------------------------------------------
Eval num_timesteps=24000, episode_reward=1174.08 +/- 398.17
Episode length: 265.40 +/- 55.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 265          |
|    mean_reward          | 1.17e+03     |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 0.0003413964 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.78        |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.001        |
|    loss                 | 5.03e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000657    |
|    std                  | 1.03         |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=907.74 +/- 237.62
Episode length: 250.80 +/- 55.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 908          |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 0.0005359298 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.001        |
|    loss                 | 4.42e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000974    |
|    std                  | 1.03         |
|    value_loss           | 8.93e+03     |
------------------------------------------
Eval num_timesteps=28000, episode_reward=1205.94 +/- 287.00
Episode length: 293.60 +/- 55.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | 1.21e+03      |
| time/                   |               |
|    total_timesteps      | 28000         |
| train/                  |               |
|    approx_kl            | 0.00030880028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.8          |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.001         |
|    loss                 | 4.58e+03      |
|    n_updates            | 130           |
|    policy_gradient_loss | -1.72e-05     |
|    std                  | 1.03          |
|    value_loss           | 9.27e+03      |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=1282.61 +/- 413.44
Episode length: 295.60 +/- 58.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0003041537 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.03         |
|    value_loss           | 9.42e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=32000, episode_reward=1958.29 +/- 731.27
Episode length: 416.00 +/- 112.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 1.96e+03      |
| time/                   |               |
|    total_timesteps      | 32000         |
| train/                  |               |
|    approx_kl            | 0.00052539876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.8          |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.001         |
|    loss                 | 4.67e+03      |
|    n_updates            | 150           |
|    policy_gradient_loss | -0.000691     |
|    std                  | 1.03          |
|    value_loss           | 9.45e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=34000, episode_reward=1395.46 +/- 453.96
Episode length: 326.40 +/- 44.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 326           |
|    mean_reward          | 1.4e+03       |
| time/                   |               |
|    total_timesteps      | 34000         |
| train/                  |               |
|    approx_kl            | 0.00040007275 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.81         |
|    explained_variance   | 0.0244        |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+03       |
|    n_updates            | 160           |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.04          |
|    value_loss           | 9.29e+03      |
-------------------------------------------
Eval num_timesteps=36000, episode_reward=1167.94 +/- 134.22
Episode length: 302.20 +/- 32.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 1.17e+03     |
| time/                   |              |
|    total_timesteps      | 36000        |
| train/                  |              |
|    approx_kl            | 0.0006948588 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.83        |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.001        |
|    loss                 | 4.77e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.04         |
|    value_loss           | 9.63e+03     |
------------------------------------------
Eval num_timesteps=38000, episode_reward=1605.36 +/- 710.58
Episode length: 373.00 +/- 121.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 373           |
|    mean_reward          | 1.61e+03      |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 0.00040085468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.85         |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+03      |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000409     |
|    std                  | 1.05          |
|    value_loss           | 8.59e+03      |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=1612.54 +/- 575.50
Episode length: 329.00 +/- 110.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 329           |
|    mean_reward          | 1.61e+03      |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | 0.00050796603 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.87         |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.001         |
|    loss                 | 4.56e+03      |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.00143      |
|    std                  | 1.05          |
|    value_loss           | 9.19e+03      |
-------------------------------------------
Eval num_timesteps=42000, episode_reward=1348.92 +/- 558.03
Episode length: 339.60 +/- 99.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 340           |
|    mean_reward          | 1.35e+03      |
| time/                   |               |
|    total_timesteps      | 42000         |
| train/                  |               |
|    approx_kl            | 0.00061924575 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.88         |
|    explained_variance   | 0.0681        |
|    learning_rate        | 0.001         |
|    loss                 | 4.14e+03      |
|    n_updates            | 200           |
|    policy_gradient_loss | -0.00114      |
|    std                  | 1.05          |
|    value_loss           | 8.35e+03      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=1532.09 +/- 147.53
Episode length: 354.20 +/- 43.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 354           |
|    mean_reward          | 1.53e+03      |
| time/                   |               |
|    total_timesteps      | 44000         |
| train/                  |               |
|    approx_kl            | 0.00034217586 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.89         |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 4.91e+03      |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.000272     |
|    std                  | 1.06          |
|    value_loss           | 9.98e+03      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=1110.68 +/- 479.73
Episode length: 250.40 +/- 56.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 250           |
|    mean_reward          | 1.11e+03      |
| time/                   |               |
|    total_timesteps      | 46000         |
| train/                  |               |
|    approx_kl            | 0.00021497277 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.89         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+03      |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.000722     |
|    std                  | 1.05          |
|    value_loss           | 8.57e+03      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=1309.28 +/- 301.92
Episode length: 259.20 +/- 41.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 259           |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 48000         |
| train/                  |               |
|    approx_kl            | 0.00024128982 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.89         |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.001         |
|    loss                 | 5.21e+03      |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.00063      |
|    std                  | 1.06          |
|    value_loss           | 1.05e+04      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=1423.07 +/- 273.63
Episode length: 281.40 +/- 37.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | 1.42e+03      |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00030832196 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.9          |
|    explained_variance   | 0.277         |
|    learning_rate        | 0.001         |
|    loss                 | 3.83e+03      |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000591     |
|    std                  | 1.06          |
|    value_loss           | 7.81e+03      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=1398.38 +/- 546.68
Episode length: 320.60 +/- 69.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | 1.4e+03      |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0004246512 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.91        |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.001        |
|    loss                 | 4.61e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.06         |
|    value_loss           | 9.42e+03     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=1486.31 +/- 300.85
Episode length: 305.20 +/- 64.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 1.49e+03     |
| time/                   |              |
|    total_timesteps      | 54000        |
| train/                  |              |
|    approx_kl            | 0.0005082743 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.001        |
|    loss                 | 3.98e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000876    |
|    std                  | 1.07         |
|    value_loss           | 8.06e+03     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=1707.99 +/- 456.27
Episode length: 398.40 +/- 107.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 398           |
|    mean_reward          | 1.71e+03      |
| time/                   |               |
|    total_timesteps      | 56000         |
| train/                  |               |
|    approx_kl            | 0.00033599947 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.94         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 5.31e+03      |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000898     |
|    std                  | 1.07          |
|    value_loss           | 1.07e+04      |
-------------------------------------------
Eval num_timesteps=58000, episode_reward=1308.35 +/- 568.75
Episode length: 365.40 +/- 184.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 58000        |
| train/                  |              |
|    approx_kl            | 0.0003122644 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.96        |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.001        |
|    loss                 | 4.5e+03      |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000655    |
|    std                  | 1.08         |
|    value_loss           | 9.09e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=1608.98 +/- 680.19
Episode length: 360.40 +/- 122.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 360          |
|    mean_reward          | 1.61e+03     |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0002481988 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.98        |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.001        |
|    loss                 | 4.14e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000436    |
|    std                  | 1.08         |
|    value_loss           | 8.44e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=1653.60 +/- 444.96
Episode length: 378.80 +/- 88.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 1.65e+03     |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0001627983 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.001        |
|    loss                 | 4.03e+03     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.000451    |
|    std                  | 1.08         |
|    value_loss           | 8.27e+03     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=1586.94 +/- 284.71
Episode length: 392.00 +/- 134.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 1.59e+03      |
| time/                   |               |
|    total_timesteps      | 64000         |
| train/                  |               |
|    approx_kl            | 0.00025219462 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6            |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.001         |
|    loss                 | 3.77e+03      |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000788     |
|    std                  | 1.09          |
|    value_loss           | 7.62e+03      |
-------------------------------------------
Eval num_timesteps=66000, episode_reward=1125.98 +/- 304.31
Episode length: 278.00 +/- 79.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 1.13e+03    |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.000284092 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.001       |
|    loss                 | 5.53e+03    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.000574   |
|    std                  | 1.09        |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=68000, episode_reward=1588.03 +/- 266.48
Episode length: 342.00 +/- 59.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 342           |
|    mean_reward          | 1.59e+03      |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 0.00021933942 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.03         |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.001         |
|    loss                 | 5.32e+03      |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000588     |
|    std                  | 1.09          |
|    value_loss           | 1.07e+04      |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=1482.97 +/- 725.66
Episode length: 359.00 +/- 204.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 359           |
|    mean_reward          | 1.48e+03      |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 0.00016964172 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.04         |
|    explained_variance   | 0.289         |
|    learning_rate        | 0.001         |
|    loss                 | 3.96e+03      |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000241     |
|    std                  | 1.1           |
|    value_loss           | 8.03e+03      |
-------------------------------------------
Eval num_timesteps=72000, episode_reward=1564.02 +/- 581.45
Episode length: 377.20 +/- 133.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 1.56e+03     |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0001378677 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.001        |
|    loss                 | 4.34e+03     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000417    |
|    std                  | 1.1          |
|    value_loss           | 8.81e+03     |
------------------------------------------
Eval num_timesteps=74000, episode_reward=1610.13 +/- 225.27
Episode length: 384.20 +/- 71.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 1.61e+03    |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 9.20858e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.001       |
|    loss                 | 3.94e+03    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.000337   |
|    std                  | 1.1         |
|    value_loss           | 8.08e+03    |
-----------------------------------------
Eval num_timesteps=76000, episode_reward=1547.10 +/- 462.28
Episode length: 368.60 +/- 157.03
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 369            |
|    mean_reward          | 1.55e+03       |
| time/                   |                |
|    total_timesteps      | 76000          |
| train/                  |                |
|    approx_kl            | 0.000113509275 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.06          |
|    explained_variance   | 0.324          |
|    learning_rate        | 0.001          |
|    loss                 | 4.25e+03       |
|    n_updates            | 370            |
|    policy_gradient_loss | 2.65e-05       |
|    std                  | 1.1            |
|    value_loss           | 8.64e+03       |
--------------------------------------------
Eval num_timesteps=78000, episode_reward=1234.59 +/- 119.17
Episode length: 351.20 +/- 48.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 5.280922e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.001        |
|    loss                 | 4.47e+03     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000187    |
|    std                  | 1.1          |
|    value_loss           | 9.1e+03      |
------------------------------------------
Eval num_timesteps=80000, episode_reward=1248.76 +/- 273.42
Episode length: 263.80 +/- 22.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 264           |
|    mean_reward          | 1.25e+03      |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00011587702 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 3.81e+03      |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000477     |
|    std                  | 1.1           |
|    value_loss           | 7.72e+03      |
-------------------------------------------
Eval num_timesteps=82000, episode_reward=1053.08 +/- 342.53
Episode length: 304.40 +/- 111.94
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 304            |
|    mean_reward          | 1.05e+03       |
| time/                   |                |
|    total_timesteps      | 82000          |
| train/                  |                |
|    approx_kl            | 0.000102559075 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.07          |
|    explained_variance   | 0.328          |
|    learning_rate        | 0.001          |
|    loss                 | 3.55e+03       |
|    n_updates            | 400            |
|    policy_gradient_loss | -0.000213      |
|    std                  | 1.1            |
|    value_loss           | 7.32e+03       |
--------------------------------------------
Eval num_timesteps=84000, episode_reward=1466.38 +/- 654.27
Episode length: 285.20 +/- 80.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 1.47e+03     |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 6.136749e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.001        |
|    loss                 | 4.23e+03     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000352    |
|    std                  | 1.1          |
|    value_loss           | 8.67e+03     |
------------------------------------------
Eval num_timesteps=86000, episode_reward=2035.20 +/- 540.42
Episode length: 435.80 +/- 112.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 436      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
New best mean reward!
Eval num_timesteps=88000, episode_reward=1491.88 +/- 628.95
Episode length: 323.00 +/- 113.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 1.49e+03      |
| time/                   |               |
|    total_timesteps      | 88000         |
| train/                  |               |
|    approx_kl            | 0.00019324574 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.478         |
|    learning_rate        | 0.001         |
|    loss                 | 4.17e+03      |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000736     |
|    std                  | 1.11          |
|    value_loss           | 8.63e+03      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=1289.38 +/- 385.95
Episode length: 280.00 +/- 70.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 90000         |
| train/                  |               |
|    approx_kl            | 0.00020019538 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.08         |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 3.07e+03      |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000609     |
|    std                  | 1.11          |
|    value_loss           | 6.31e+03      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=1622.93 +/- 663.12
Episode length: 367.20 +/- 114.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 367           |
|    mean_reward          | 1.62e+03      |
| time/                   |               |
|    total_timesteps      | 92000         |
| train/                  |               |
|    approx_kl            | 0.00012217561 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.08         |
|    explained_variance   | 0.477         |
|    learning_rate        | 0.001         |
|    loss                 | 4.38e+03      |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000151     |
|    std                  | 1.11          |
|    value_loss           | 9.06e+03      |
-------------------------------------------
Eval num_timesteps=94000, episode_reward=1279.66 +/- 445.63
Episode length: 336.60 +/- 65.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 94000        |
| train/                  |              |
|    approx_kl            | 3.737383e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.08        |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.001        |
|    loss                 | 3.97e+03     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000139    |
|    std                  | 1.11         |
|    value_loss           | 8.05e+03     |
------------------------------------------
Eval num_timesteps=96000, episode_reward=1596.98 +/- 506.88
Episode length: 357.60 +/- 83.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 358           |
|    mean_reward          | 1.6e+03       |
| time/                   |               |
|    total_timesteps      | 96000         |
| train/                  |               |
|    approx_kl            | 5.8786623e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.09         |
|    explained_variance   | 0.457         |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+03      |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000318     |
|    std                  | 1.11          |
|    value_loss           | 7.51e+03      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=1303.92 +/- 519.29
Episode length: 278.80 +/- 90.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 279           |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 98000         |
| train/                  |               |
|    approx_kl            | 0.00011595583 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.1          |
|    explained_variance   | 0.567         |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+03      |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000475     |
|    std                  | 1.11          |
|    value_loss           | 7.82e+03      |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=1567.22 +/- 516.10
Episode length: 429.80 +/- 117.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 9.853064e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.001        |
|    loss                 | 3.8e+03      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.0002      |
|    std                  | 1.11         |
|    value_loss           | 7.88e+03     |
------------------------------------------
Eval num_timesteps=102000, episode_reward=2287.17 +/- 1287.14
Episode length: 439.40 +/- 172.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 2.29e+03     |
| time/                   |              |
|    total_timesteps      | 102000       |
| train/                  |              |
|    approx_kl            | 9.306296e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.575        |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+03     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000218    |
|    std                  | 1.11         |
|    value_loss           | 7.78e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=104000, episode_reward=1121.86 +/- 192.70
Episode length: 304.40 +/- 53.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 1.12e+03      |
| time/                   |               |
|    total_timesteps      | 104000        |
| train/                  |               |
|    approx_kl            | 0.00012125794 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.1          |
|    explained_variance   | 0.545         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000608     |
|    std                  | 1.11          |
|    value_loss           | 7.57e+03      |
-------------------------------------------
Eval num_timesteps=106000, episode_reward=1823.12 +/- 192.39
Episode length: 401.60 +/- 118.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | 1.82e+03      |
| time/                   |               |
|    total_timesteps      | 106000        |
| train/                  |               |
|    approx_kl            | 0.00010809972 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.1          |
|    explained_variance   | 0.593         |
|    learning_rate        | 0.001         |
|    loss                 | 2.96e+03      |
|    n_updates            | 510           |
|    policy_gradient_loss | -0.000226     |
|    std                  | 1.11          |
|    value_loss           | 6.03e+03      |
-------------------------------------------
Eval num_timesteps=108000, episode_reward=1761.47 +/- 674.18
Episode length: 426.60 +/- 188.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 1.76e+03     |
| time/                   |              |
|    total_timesteps      | 108000       |
| train/                  |              |
|    approx_kl            | 6.272129e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.001        |
|    loss                 | 3.81e+03     |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000128    |
|    std                  | 1.11         |
|    value_loss           | 7.77e+03     |
------------------------------------------
Eval num_timesteps=110000, episode_reward=1394.63 +/- 323.85
Episode length: 335.20 +/- 70.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 335           |
|    mean_reward          | 1.39e+03      |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | 6.4896856e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.1          |
|    explained_variance   | 0.577         |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+03      |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.000255     |
|    std                  | 1.11          |
|    value_loss           | 7.43e+03      |
-------------------------------------------
Eval num_timesteps=112000, episode_reward=1713.22 +/- 466.38
Episode length: 408.40 +/- 162.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 1.71e+03      |
| time/                   |               |
|    total_timesteps      | 112000        |
| train/                  |               |
|    approx_kl            | 0.00012781183 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.11         |
|    explained_variance   | 0.584         |
|    learning_rate        | 0.001         |
|    loss                 | 3.59e+03      |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.00022      |
|    std                  | 1.12          |
|    value_loss           | 7.31e+03      |
-------------------------------------------
Eval num_timesteps=114000, episode_reward=1364.91 +/- 451.94
Episode length: 312.40 +/- 64.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | 1.36e+03      |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 8.3039195e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.11         |
|    explained_variance   | 0.551         |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+03      |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000222     |
|    std                  | 1.12          |
|    value_loss           | 7.27e+03      |
-------------------------------------------
Eval num_timesteps=116000, episode_reward=1451.94 +/- 456.93
Episode length: 425.80 +/- 178.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 1.45e+03    |
| time/                   |             |
|    total_timesteps      | 116000      |
| train/                  |             |
|    approx_kl            | 9.06348e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.001       |
|    loss                 | 4.35e+03    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000348   |
|    std                  | 1.12        |
|    value_loss           | 8.8e+03     |
-----------------------------------------
Eval num_timesteps=118000, episode_reward=2194.49 +/- 981.42
Episode length: 416.60 +/- 170.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 2.19e+03     |
| time/                   |              |
|    total_timesteps      | 118000       |
| train/                  |              |
|    approx_kl            | 0.0001700221 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.521        |
|    learning_rate        | 0.001        |
|    loss                 | 4.36e+03     |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.000689    |
|    std                  | 1.12         |
|    value_loss           | 8.89e+03     |
------------------------------------------
Eval num_timesteps=120000, episode_reward=1553.84 +/- 839.77
Episode length: 331.80 +/- 142.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 332           |
|    mean_reward          | 1.55e+03      |
| time/                   |               |
|    total_timesteps      | 120000        |
| train/                  |               |
|    approx_kl            | 0.00026643334 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 0.625         |
|    learning_rate        | 0.001         |
|    loss                 | 4.54e+03      |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.00095      |
|    std                  | 1.12          |
|    value_loss           | 9.23e+03      |
-------------------------------------------
Eval num_timesteps=122000, episode_reward=1748.23 +/- 364.70
Episode length: 358.20 +/- 62.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 358           |
|    mean_reward          | 1.75e+03      |
| time/                   |               |
|    total_timesteps      | 122000        |
| train/                  |               |
|    approx_kl            | 0.00028720318 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 0.468         |
|    learning_rate        | 0.001         |
|    loss                 | 3.72e+03      |
|    n_updates            | 590           |
|    policy_gradient_loss | -0.000492     |
|    std                  | 1.13          |
|    value_loss           | 7.64e+03      |
-------------------------------------------
Eval num_timesteps=124000, episode_reward=2449.37 +/- 1163.09
Episode length: 453.80 +/- 190.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 2.45e+03      |
| time/                   |               |
|    total_timesteps      | 124000        |
| train/                  |               |
|    approx_kl            | 0.00028916379 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.15         |
|    explained_variance   | 0.655         |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+03      |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000535     |
|    std                  | 1.13          |
|    value_loss           | 9.05e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=126000, episode_reward=2192.61 +/- 810.98
Episode length: 514.00 +/- 134.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 514           |
|    mean_reward          | 2.19e+03      |
| time/                   |               |
|    total_timesteps      | 126000        |
| train/                  |               |
|    approx_kl            | 0.00020071882 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0.562         |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+03      |
|    n_updates            | 610           |
|    policy_gradient_loss | -0.000834     |
|    std                  | 1.13          |
|    value_loss           | 8.21e+03      |
-------------------------------------------
Eval num_timesteps=128000, episode_reward=1795.75 +/- 395.24
Episode length: 385.60 +/- 61.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 1.8e+03       |
| time/                   |               |
|    total_timesteps      | 128000        |
| train/                  |               |
|    approx_kl            | 0.00017493856 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.17         |
|    explained_variance   | 0.601         |
|    learning_rate        | 0.001         |
|    loss                 | 4.05e+03      |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.000463     |
|    std                  | 1.13          |
|    value_loss           | 8.28e+03      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=1511.60 +/- 441.14
Episode length: 344.00 +/- 116.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 344           |
|    mean_reward          | 1.51e+03      |
| time/                   |               |
|    total_timesteps      | 130000        |
| train/                  |               |
|    approx_kl            | 0.00032546424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0.554         |
|    learning_rate        | 0.001         |
|    loss                 | 3.57e+03      |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.000923     |
|    std                  | 1.13          |
|    value_loss           | 7.21e+03      |
-------------------------------------------
Eval num_timesteps=132000, episode_reward=2389.57 +/- 315.29
Episode length: 419.40 +/- 66.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 2.39e+03      |
| time/                   |               |
|    total_timesteps      | 132000        |
| train/                  |               |
|    approx_kl            | 0.00015877304 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0.592         |
|    learning_rate        | 0.001         |
|    loss                 | 3.82e+03      |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000496     |
|    std                  | 1.13          |
|    value_loss           | 7.76e+03      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=1974.11 +/- 1125.69
Episode length: 374.00 +/- 193.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 374           |
|    mean_reward          | 1.97e+03      |
| time/                   |               |
|    total_timesteps      | 134000        |
| train/                  |               |
|    approx_kl            | 0.00011323328 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.17         |
|    explained_variance   | 0.686         |
|    learning_rate        | 0.001         |
|    loss                 | 4.79e+03      |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.13          |
|    value_loss           | 9.7e+03       |
-------------------------------------------
Eval num_timesteps=136000, episode_reward=2025.19 +/- 807.13
Episode length: 446.00 +/- 169.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 136000        |
| train/                  |               |
|    approx_kl            | 0.00022966616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.18         |
|    explained_variance   | 0.605         |
|    learning_rate        | 0.001         |
|    loss                 | 4.34e+03      |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.000904     |
|    std                  | 1.14          |
|    value_loss           | 8.84e+03      |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=2123.09 +/- 733.21
Episode length: 432.80 +/- 123.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 433           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 138000        |
| train/                  |               |
|    approx_kl            | 0.00020338196 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.19         |
|    explained_variance   | 0.526         |
|    learning_rate        | 0.001         |
|    loss                 | 3.75e+03      |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000574     |
|    std                  | 1.14          |
|    value_loss           | 7.59e+03      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=1855.37 +/- 675.59
Episode length: 403.00 +/- 121.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 403          |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0004139584 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.19        |
|    explained_variance   | 0.627        |
|    learning_rate        | 0.001        |
|    loss                 | 3.32e+03     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.000875    |
|    std                  | 1.14         |
|    value_loss           | 6.77e+03     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=1469.72 +/- 295.88
Episode length: 347.20 +/- 70.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 1.47e+03     |
| time/                   |              |
|    total_timesteps      | 142000       |
| train/                  |              |
|    approx_kl            | 0.0002127077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.19        |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.001        |
|    loss                 | 4.11e+03     |
|    n_updates            | 690          |
|    policy_gradient_loss | 5.58e-05     |
|    std                  | 1.14         |
|    value_loss           | 8.35e+03     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=1762.25 +/- 750.98
Episode length: 386.40 +/- 136.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 1.76e+03      |
| time/                   |               |
|    total_timesteps      | 144000        |
| train/                  |               |
|    approx_kl            | 7.4555195e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.2          |
|    explained_variance   | 0.632         |
|    learning_rate        | 0.001         |
|    loss                 | 3.7e+03       |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000172     |
|    std                  | 1.14          |
|    value_loss           | 7.52e+03      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=1524.38 +/- 660.23
Episode length: 378.20 +/- 135.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 378           |
|    mean_reward          | 1.52e+03      |
| time/                   |               |
|    total_timesteps      | 146000        |
| train/                  |               |
|    approx_kl            | 0.00013582327 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.2          |
|    explained_variance   | 0.662         |
|    learning_rate        | 0.001         |
|    loss                 | 3.01e+03      |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000748     |
|    std                  | 1.14          |
|    value_loss           | 6.11e+03      |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=1865.97 +/- 645.94
Episode length: 509.20 +/- 188.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 509           |
|    mean_reward          | 1.87e+03      |
| time/                   |               |
|    total_timesteps      | 148000        |
| train/                  |               |
|    approx_kl            | 0.00048153388 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.21         |
|    explained_variance   | 0.55          |
|    learning_rate        | 0.001         |
|    loss                 | 2.99e+03      |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.00147      |
|    std                  | 1.15          |
|    value_loss           | 6.08e+03      |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=1978.08 +/- 571.10
Episode length: 428.60 +/- 110.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 1.98e+03     |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0005791482 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.603        |
|    learning_rate        | 0.001        |
|    loss                 | 3.55e+03     |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.15         |
|    value_loss           | 7.38e+03     |
------------------------------------------
Eval num_timesteps=152000, episode_reward=1768.82 +/- 277.59
Episode length: 464.00 +/- 159.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 1.77e+03      |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 0.00018364016 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.23         |
|    explained_variance   | 0.7           |
|    learning_rate        | 0.001         |
|    loss                 | 4.12e+03      |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.000525     |
|    std                  | 1.15          |
|    value_loss           | 8.3e+03       |
-------------------------------------------
Eval num_timesteps=154000, episode_reward=2299.20 +/- 299.12
Episode length: 582.40 +/- 142.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 582           |
|    mean_reward          | 2.3e+03       |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 0.00019579686 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.24         |
|    explained_variance   | 0.691         |
|    learning_rate        | 0.001         |
|    loss                 | 4.34e+03      |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.000647     |
|    std                  | 1.15          |
|    value_loss           | 8.83e+03      |
-------------------------------------------
Eval num_timesteps=156000, episode_reward=2043.08 +/- 728.25
Episode length: 512.20 +/- 114.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 512           |
|    mean_reward          | 2.04e+03      |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 0.00050931575 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.24         |
|    explained_variance   | 0.677         |
|    learning_rate        | 0.001         |
|    loss                 | 2.9e+03       |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.00121      |
|    std                  | 1.15          |
|    value_loss           | 5.94e+03      |
-------------------------------------------
Eval num_timesteps=158000, episode_reward=2157.60 +/- 233.89
Episode length: 423.20 +/- 92.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 423           |
|    mean_reward          | 2.16e+03      |
| time/                   |               |
|    total_timesteps      | 158000        |
| train/                  |               |
|    approx_kl            | 0.00036518223 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.25         |
|    explained_variance   | 0.618         |
|    learning_rate        | 0.001         |
|    loss                 | 2.71e+03      |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000261     |
|    std                  | 1.16          |
|    value_loss           | 5.54e+03      |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=1666.49 +/- 341.80
Episode length: 484.00 +/- 101.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 484           |
|    mean_reward          | 1.67e+03      |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 9.7597134e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.25         |
|    explained_variance   | 0.562         |
|    learning_rate        | 0.001         |
|    loss                 | 3.2e+03       |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.000319     |
|    std                  | 1.16          |
|    value_loss           | 6.52e+03      |
-------------------------------------------
Eval num_timesteps=162000, episode_reward=1774.55 +/- 547.55
Episode length: 400.20 +/- 99.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 1.77e+03      |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 7.4596406e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.26         |
|    explained_variance   | 0.74          |
|    learning_rate        | 0.001         |
|    loss                 | 3.44e+03      |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.000329     |
|    std                  | 1.16          |
|    value_loss           | 6.98e+03      |
-------------------------------------------
Eval num_timesteps=164000, episode_reward=1369.77 +/- 604.56
Episode length: 368.40 +/- 148.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 368           |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 0.00016019377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.26         |
|    explained_variance   | 0.754         |
|    learning_rate        | 0.001         |
|    loss                 | 3.21e+03      |
|    n_updates            | 800           |
|    policy_gradient_loss | -0.000516     |
|    std                  | 1.16          |
|    value_loss           | 6.51e+03      |
-------------------------------------------
Eval num_timesteps=166000, episode_reward=2116.39 +/- 437.28
Episode length: 527.20 +/- 90.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 527           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 0.00021680049 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.27         |
|    explained_variance   | 0.709         |
|    learning_rate        | 0.001         |
|    loss                 | 3.78e+03      |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.000442     |
|    std                  | 1.16          |
|    value_loss           | 7.71e+03      |
-------------------------------------------
Eval num_timesteps=168000, episode_reward=2202.97 +/- 585.86
Episode length: 470.00 +/- 121.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 2.2e+03       |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 0.00026203887 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.27         |
|    explained_variance   | 0.686         |
|    learning_rate        | 0.001         |
|    loss                 | 2.43e+03      |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.000683     |
|    std                  | 1.16          |
|    value_loss           | 4.98e+03      |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=2306.73 +/- 1019.92
Episode length: 567.60 +/- 155.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 568           |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 170000        |
| train/                  |               |
|    approx_kl            | 0.00017268368 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.27         |
|    explained_variance   | 0.755         |
|    learning_rate        | 0.001         |
|    loss                 | 2.32e+03      |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.000397     |
|    std                  | 1.16          |
|    value_loss           | 4.71e+03      |
-------------------------------------------
Eval num_timesteps=172000, episode_reward=1886.88 +/- 331.86
Episode length: 457.80 +/- 115.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=174000, episode_reward=1720.78 +/- 500.15
Episode length: 465.60 +/- 95.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 1.72e+03      |
| time/                   |               |
|    total_timesteps      | 174000        |
| train/                  |               |
|    approx_kl            | 0.00011464479 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.29         |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 6.03e+03      |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000368     |
|    std                  | 1.17          |
|    value_loss           | 1.29e+04      |
-------------------------------------------
Eval num_timesteps=176000, episode_reward=2379.83 +/- 1128.10
Episode length: 517.40 +/- 243.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 2.38e+03     |
| time/                   |              |
|    total_timesteps      | 176000       |
| train/                  |              |
|    approx_kl            | 9.870849e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.29        |
|    explained_variance   | 0.729        |
|    learning_rate        | 0.001        |
|    loss                 | 2.35e+03     |
|    n_updates            | 850          |
|    policy_gradient_loss | -5.43e-05    |
|    std                  | 1.17         |
|    value_loss           | 5.92e+03     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=2049.51 +/- 1007.90
Episode length: 469.00 +/- 191.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 2.05e+03     |
| time/                   |              |
|    total_timesteps      | 178000       |
| train/                  |              |
|    approx_kl            | 3.084648e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.586        |
|    learning_rate        | 0.001        |
|    loss                 | 3.01e+03     |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.000115    |
|    std                  | 1.17         |
|    value_loss           | 6.23e+03     |
------------------------------------------
Eval num_timesteps=180000, episode_reward=1778.37 +/- 876.61
Episode length: 408.20 +/- 110.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 1.78e+03      |
| time/                   |               |
|    total_timesteps      | 180000        |
| train/                  |               |
|    approx_kl            | 0.00011690377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.3          |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.001         |
|    loss                 | 2.56e+03      |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000434     |
|    std                  | 1.17          |
|    value_loss           | 5.4e+03       |
-------------------------------------------
Eval num_timesteps=182000, episode_reward=2130.75 +/- 587.58
Episode length: 592.80 +/- 91.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 593           |
|    mean_reward          | 2.13e+03      |
| time/                   |               |
|    total_timesteps      | 182000        |
| train/                  |               |
|    approx_kl            | 0.00040915157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.31         |
|    explained_variance   | 0.803         |
|    learning_rate        | 0.001         |
|    loss                 | 2.6e+03       |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.00154      |
|    std                  | 1.17          |
|    value_loss           | 5.27e+03      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=1793.73 +/- 454.30
Episode length: 436.00 +/- 86.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 1.79e+03      |
| time/                   |               |
|    total_timesteps      | 184000        |
| train/                  |               |
|    approx_kl            | 0.00041608076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.32         |
|    explained_variance   | 0.749         |
|    learning_rate        | 0.001         |
|    loss                 | 2.91e+03      |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.000745     |
|    std                  | 1.18          |
|    value_loss           | 5.92e+03      |
-------------------------------------------
Eval num_timesteps=186000, episode_reward=2076.18 +/- 883.45
Episode length: 466.20 +/- 110.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 2.08e+03      |
| time/                   |               |
|    total_timesteps      | 186000        |
| train/                  |               |
|    approx_kl            | 0.00028961102 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.32         |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.001         |
|    loss                 | 2.33e+03      |
|    n_updates            | 900           |
|    policy_gradient_loss | -5.65e-05     |
|    std                  | 1.18          |
|    value_loss           | 4.94e+03      |
-------------------------------------------
Eval num_timesteps=188000, episode_reward=1601.78 +/- 665.00
Episode length: 366.60 +/- 106.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 367          |
|    mean_reward          | 1.6e+03      |
| time/                   |              |
|    total_timesteps      | 188000       |
| train/                  |              |
|    approx_kl            | 0.0003837726 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+04     |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000927    |
|    std                  | 1.18         |
|    value_loss           | 3.02e+04     |
------------------------------------------
Eval num_timesteps=190000, episode_reward=1469.37 +/- 357.53
Episode length: 431.60 +/- 89.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 1.47e+03    |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.000441856 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.001       |
|    loss                 | 2.6e+03     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00034    |
|    std                  | 1.18        |
|    value_loss           | 5.35e+03    |
-----------------------------------------
Eval num_timesteps=192000, episode_reward=1706.71 +/- 275.77
Episode length: 462.80 +/- 100.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 463           |
|    mean_reward          | 1.71e+03      |
| time/                   |               |
|    total_timesteps      | 192000        |
| train/                  |               |
|    approx_kl            | 0.00082462875 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.34         |
|    explained_variance   | 0.55          |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.00148      |
|    std                  | 1.18          |
|    value_loss           | 3.84e+03      |
-------------------------------------------
Eval num_timesteps=194000, episode_reward=1740.36 +/- 469.45
Episode length: 471.40 +/- 73.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 471           |
|    mean_reward          | 1.74e+03      |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 0.00066532963 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.34         |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+03      |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.000139     |
|    std                  | 1.18          |
|    value_loss           | 5.21e+03      |
-------------------------------------------
Eval num_timesteps=196000, episode_reward=1465.21 +/- 418.60
Episode length: 425.80 +/- 50.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | 1.47e+03     |
| time/                   |              |
|    total_timesteps      | 196000       |
| train/                  |              |
|    approx_kl            | 0.0008134915 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.34        |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+03     |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.18         |
|    value_loss           | 6.66e+03     |
------------------------------------------
Eval num_timesteps=198000, episode_reward=2648.88 +/- 1143.34
Episode length: 504.80 +/- 123.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 2.65e+03     |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 0.0004354304 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.35        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 2.74e+03     |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00099     |
|    std                  | 1.19         |
|    value_loss           | 5.66e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=200000, episode_reward=1307.02 +/- 183.13
Episode length: 472.00 +/- 127.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0005623596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.36        |
|    explained_variance   | 0.689        |
|    learning_rate        | 0.001        |
|    loss                 | 3.06e+03     |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.19         |
|    value_loss           | 6.25e+03     |
------------------------------------------
Eval num_timesteps=202000, episode_reward=1375.42 +/- 166.80
Episode length: 454.40 +/- 83.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 1.38e+03     |
| time/                   |              |
|    total_timesteps      | 202000       |
| train/                  |              |
|    approx_kl            | 0.0003691082 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.37        |
|    explained_variance   | 0.681        |
|    learning_rate        | 0.001        |
|    loss                 | 2.43e+03     |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.000791    |
|    std                  | 1.19         |
|    value_loss           | 5.05e+03     |
------------------------------------------
Eval num_timesteps=204000, episode_reward=1525.83 +/- 371.50
Episode length: 384.00 +/- 55.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | 1.53e+03      |
| time/                   |               |
|    total_timesteps      | 204000        |
| train/                  |               |
|    approx_kl            | 0.00011462113 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.37         |
|    explained_variance   | 0.764         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 990           |
|    policy_gradient_loss | 0.000204      |
|    std                  | 1.19          |
|    value_loss           | 3.73e+03      |
-------------------------------------------
Eval num_timesteps=206000, episode_reward=1319.76 +/- 139.21
Episode length: 455.40 +/- 161.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 1.32e+03     |
| time/                   |              |
|    total_timesteps      | 206000       |
| train/                  |              |
|    approx_kl            | 9.263749e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.37        |
|    explained_variance   | 0.741        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.000369    |
|    std                  | 1.19         |
|    value_loss           | 4.24e+03     |
------------------------------------------
Eval num_timesteps=208000, episode_reward=1721.54 +/- 477.12
Episode length: 427.40 +/- 116.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 1.72e+03      |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 0.00013797134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.37         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+03      |
|    n_updates            | 1010          |
|    policy_gradient_loss | -0.000564     |
|    std                  | 1.19          |
|    value_loss           | 4.6e+03       |
-------------------------------------------
Eval num_timesteps=210000, episode_reward=2123.65 +/- 874.74
Episode length: 631.60 +/- 146.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 632           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 0.00012255862 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.38         |
|    explained_variance   | 0.754         |
|    learning_rate        | 0.001         |
|    loss                 | 2.14e+03      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -3.32e-05     |
|    std                  | 1.2           |
|    value_loss           | 4.38e+03      |
-------------------------------------------
Eval num_timesteps=212000, episode_reward=1536.45 +/- 555.19
Episode length: 434.20 +/- 77.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 1.54e+03      |
| time/                   |               |
|    total_timesteps      | 212000        |
| train/                  |               |
|    approx_kl            | 0.00016022648 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.39         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+03      |
|    n_updates            | 1030          |
|    policy_gradient_loss | -0.000758     |
|    std                  | 1.2           |
|    value_loss           | 3.95e+03      |
-------------------------------------------
Eval num_timesteps=214000, episode_reward=1593.50 +/- 587.09
Episode length: 541.60 +/- 144.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 542           |
|    mean_reward          | 1.59e+03      |
| time/                   |               |
|    total_timesteps      | 214000        |
| train/                  |               |
|    approx_kl            | 0.00063440995 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.4          |
|    explained_variance   | 0.788         |
|    learning_rate        | 0.001         |
|    loss                 | 1.98e+03      |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.00118      |
|    std                  | 1.2           |
|    value_loss           | 4.03e+03      |
-------------------------------------------
Eval num_timesteps=216000, episode_reward=1501.61 +/- 599.71
Episode length: 380.00 +/- 156.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 1.5e+03      |
| time/                   |              |
|    total_timesteps      | 216000       |
| train/                  |              |
|    approx_kl            | 0.0003675183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.4         |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 3.53e+03     |
|    n_updates            | 1050         |
|    policy_gradient_loss | 0.000192     |
|    std                  | 1.2          |
|    value_loss           | 7.29e+03     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=1245.08 +/- 248.50
Episode length: 437.20 +/- 69.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 437           |
|    mean_reward          | 1.25e+03      |
| time/                   |               |
|    total_timesteps      | 218000        |
| train/                  |               |
|    approx_kl            | 0.00021016042 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.4          |
|    explained_variance   | 0.666         |
|    loss                 | 3.34e+03      |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00038      |
|    std                  | 1.2           |
|    value_loss           | 6.79e+03      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=2041.57 +/- 491.78
Episode length: 496.00 +/- 121.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 496        |
|    mean_reward          | 2.04e+03   |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.00046799 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.41      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.001      |
|    loss                 | 1.57e+03   |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0011    |
|    std                  | 1.21       |
|    value_loss           | 3.22e+03   |
----------------------------------------
Eval num_timesteps=222000, episode_reward=2439.33 +/- 1299.83
Episode length: 538.40 +/- 78.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 222000       |
| train/                  |              |
|    approx_kl            | 0.0001920789 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.42        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 2.03e+03     |
|    n_updates            | 1080         |
|    policy_gradient_loss | 7.29e-06     |
|    std                  | 1.21         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=1397.52 +/- 292.00
Episode length: 445.40 +/- 35.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 1.4e+03      |
| time/                   |              |
|    total_timesteps      | 224000       |
| train/                  |              |
|    approx_kl            | 8.934477e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 0.746        |
|    learning_rate        | 0.001        |
|    loss                 | 2.25e+03     |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.000275    |
|    std                  | 1.21         |
|    value_loss           | 4.66e+03     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=1805.19 +/- 288.25
Episode length: 506.40 +/- 91.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 506           |
|    mean_reward          | 1.81e+03      |
| time/                   |               |
|    total_timesteps      | 226000        |
| train/                  |               |
|    approx_kl            | 0.00013526349 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 2.41e+03      |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.00038      |
|    std                  | 1.21          |
|    value_loss           | 4.9e+03       |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=1911.99 +/- 946.31
Episode length: 500.80 +/- 254.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 501           |
|    mean_reward          | 1.91e+03      |
| time/                   |               |
|    total_timesteps      | 228000        |
| train/                  |               |
|    approx_kl            | 0.00030275027 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.552         |
|    learning_rate        | 0.001         |
|    loss                 | 2.79e+03      |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.000616     |
|    std                  | 1.21          |
|    value_loss           | 5.76e+03      |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=1764.20 +/- 567.99
Episode length: 486.20 +/- 96.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 1.76e+03      |
| time/                   |               |
|    total_timesteps      | 230000        |
| train/                  |               |
|    approx_kl            | 0.00010768819 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.573         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+03      |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.000176     |
|    std                  | 1.21          |
|    value_loss           | 5.61e+03      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=1794.04 +/- 427.26
Episode length: 532.20 +/- 146.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 532          |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 232000       |
| train/                  |              |
|    approx_kl            | 6.592189e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.001        |
|    loss                 | 9.66e+03     |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000244    |
|    std                  | 1.21         |
|    value_loss           | 2.03e+04     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=2187.42 +/- 1086.73
Episode length: 481.40 +/- 139.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 2.19e+03      |
| time/                   |               |
|    total_timesteps      | 234000        |
| train/                  |               |
|    approx_kl            | 4.2352767e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.649         |
|    learning_rate        | 0.001         |
|    loss                 | 2.87e+03      |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000213     |
|    std                  | 1.21          |
|    value_loss           | 5.82e+03      |
-------------------------------------------
Eval num_timesteps=236000, episode_reward=1692.29 +/- 651.75
Episode length: 484.20 +/- 162.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 484           |
|    mean_reward          | 1.69e+03      |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 0.00020467895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.44         |
|    explained_variance   | 0.816         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000952     |
|    std                  | 1.21          |
|    value_loss           | 3.87e+03      |
-------------------------------------------
Eval num_timesteps=238000, episode_reward=1871.22 +/- 686.84
Episode length: 637.00 +/- 239.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0005521136 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+03     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.21         |
|    value_loss           | 4.76e+03     |
------------------------------------------
Eval num_timesteps=240000, episode_reward=1766.05 +/- 316.77
Episode length: 556.60 +/- 139.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 557           |
|    mean_reward          | 1.77e+03      |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 0.00021905568 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.815         |
|    learning_rate        | 0.001         |
|    loss                 | 2.28e+03      |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.000438     |
|    std                  | 1.21          |
|    value_loss           | 4.76e+03      |
-------------------------------------------
Eval num_timesteps=242000, episode_reward=1955.62 +/- 629.35
Episode length: 570.40 +/- 173.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 570           |
|    mean_reward          | 1.96e+03      |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00011291026 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.44         |
|    explained_variance   | 0.781         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+03       |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000296     |
|    std                  | 1.21          |
|    value_loss           | 4.81e+03      |
-------------------------------------------
Eval num_timesteps=244000, episode_reward=1922.57 +/- 553.73
Episode length: 523.20 +/- 102.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 523           |
|    mean_reward          | 1.92e+03      |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 0.00019970222 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.44         |
|    explained_variance   | 0.777         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.000692     |
|    std                  | 1.21          |
|    value_loss           | 5.17e+03      |
-------------------------------------------
Eval num_timesteps=246000, episode_reward=1812.89 +/- 299.86
Episode length: 528.80 +/- 111.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 529           |
|    mean_reward          | 1.81e+03      |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 0.00071217294 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.45         |
|    explained_variance   | 0.827         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+03      |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.0014       |
|    std                  | 1.22          |
|    value_loss           | 3.56e+03      |
-------------------------------------------
Eval num_timesteps=248000, episode_reward=1543.11 +/- 217.47
Episode length: 554.00 +/- 36.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 554          |
|    mean_reward          | 1.54e+03     |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0017754268 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+03      |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 1.23         |
|    value_loss           | 3.1e+03      |
------------------------------------------
Eval num_timesteps=250000, episode_reward=1837.56 +/- 535.09
Episode length: 515.00 +/- 107.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0017294774 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+03     |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 1.24         |
|    value_loss           | 3.03e+03     |
------------------------------------------
Eval num_timesteps=252000, episode_reward=1666.48 +/- 918.66
Episode length: 445.80 +/- 180.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 1.67e+03     |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0008196533 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.53        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.000193    |
|    std                  | 1.24         |
|    value_loss           | 2.74e+03     |
------------------------------------------
Eval num_timesteps=254000, episode_reward=1663.10 +/- 291.73
Episode length: 553.00 +/- 120.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 553           |
|    mean_reward          | 1.66e+03      |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 0.00016501886 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.53         |
|    explained_variance   | 0.601         |
|    learning_rate        | 0.001         |
|    loss                 | 3.01e+03      |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000159     |
|    std                  | 1.24          |
|    value_loss           | 6.26e+03      |
-------------------------------------------
Eval num_timesteps=256000, episode_reward=1792.28 +/- 340.85
Episode length: 699.80 +/- 159.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=258000, episode_reward=1654.17 +/- 424.51
Episode length: 606.20 +/- 91.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 606           |
|    mean_reward          | 1.65e+03      |
| time/                   |               |
|    total_timesteps      | 258000        |
| train/                  |               |
|    approx_kl            | 0.00051474356 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.53         |
|    explained_variance   | 0.658         |
|    learning_rate        | 0.001         |
|    loss                 | 3.06e+03      |
|    n_updates            | 1250          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 1.24          |
|    value_loss           | 6.43e+03      |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=2485.07 +/- 751.25
Episode length: 687.80 +/- 145.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 688           |
|    mean_reward          | 2.49e+03      |
| time/                   |               |
|    total_timesteps      | 260000        |
| train/                  |               |
|    approx_kl            | 0.00048105558 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.54         |
|    explained_variance   | 0.782         |
|    learning_rate        | 0.001         |
|    loss                 | 2.32e+03      |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000888     |
|    std                  | 1.25          |
|    value_loss           | 4.8e+03       |
-------------------------------------------
Eval num_timesteps=262000, episode_reward=2889.29 +/- 387.10
Episode length: 618.00 +/- 84.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 618           |
|    mean_reward          | 2.89e+03      |
| time/                   |               |
|    total_timesteps      | 262000        |
| train/                  |               |
|    approx_kl            | 0.00036337436 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.55         |
|    explained_variance   | 0.741         |
|    learning_rate        | 0.001         |
|    loss                 | 2.65e+03      |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.00072      |
|    std                  | 1.25          |
|    value_loss           | 5.44e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=264000, episode_reward=2986.46 +/- 949.35
Episode length: 700.00 +/- 161.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 700          |
|    mean_reward          | 2.99e+03     |
| time/                   |              |
|    total_timesteps      | 264000       |
| train/                  |              |
|    approx_kl            | 0.0008717157 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.55        |
|    explained_variance   | 0.578        |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+03     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.25         |
|    value_loss           | 6.62e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=266000, episode_reward=3099.77 +/- 622.89
Episode length: 821.80 +/- 115.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 822          |
|    mean_reward          | 3.1e+03      |
| time/                   |              |
|    total_timesteps      | 266000       |
| train/                  |              |
|    approx_kl            | 0.0020482785 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.58        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.26         |
|    value_loss           | 2.39e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=268000, episode_reward=2462.44 +/- 223.66
Episode length: 584.60 +/- 124.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 585          |
|    mean_reward          | 2.46e+03     |
| time/                   |              |
|    total_timesteps      | 268000       |
| train/                  |              |
|    approx_kl            | 0.0012508647 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.001        |
|    loss                 | 3.49e+03     |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.26         |
|    value_loss           | 7.11e+03     |
------------------------------------------
Eval num_timesteps=270000, episode_reward=3704.19 +/- 640.31
Episode length: 788.00 +/- 183.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 788          |
|    mean_reward          | 3.7e+03      |
| time/                   |              |
|    total_timesteps      | 270000       |
| train/                  |              |
|    approx_kl            | 0.0019060627 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.61        |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.001        |
|    loss                 | 3e+03        |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 1.27         |
|    value_loss           | 6.13e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=272000, episode_reward=2275.81 +/- 433.13
Episode length: 555.60 +/- 197.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 556          |
|    mean_reward          | 2.28e+03     |
| time/                   |              |
|    total_timesteps      | 272000       |
| train/                  |              |
|    approx_kl            | 0.0020466358 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.63        |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.001        |
|    loss                 | 2.83e+03     |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.27         |
|    value_loss           | 5.76e+03     |
------------------------------------------
Eval num_timesteps=274000, episode_reward=2692.54 +/- 762.87
Episode length: 605.20 +/- 177.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 605         |
|    mean_reward          | 2.69e+03    |
| time/                   |             |
|    total_timesteps      | 274000      |
| train/                  |             |
|    approx_kl            | 0.001016895 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.64       |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.001       |
|    loss                 | 2.85e+03    |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00105    |
|    std                  | 1.27        |
|    value_loss           | 5.98e+03    |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=2178.05 +/- 540.25
Episode length: 678.60 +/- 83.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 679           |
|    mean_reward          | 2.18e+03      |
| time/                   |               |
|    total_timesteps      | 276000        |
| train/                  |               |
|    approx_kl            | 0.00061310845 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.64         |
|    explained_variance   | 0.375         |
|    learning_rate        | 0.001         |
|    loss                 | 2.53e+03      |
|    n_updates            | 1340          |
|    policy_gradient_loss | 0.000109      |
|    std                  | 1.27          |
|    value_loss           | 5.43e+03      |
-------------------------------------------
Eval num_timesteps=278000, episode_reward=1954.16 +/- 511.56
Episode length: 528.80 +/- 86.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 529           |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | 0.00018951096 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.65         |
|    explained_variance   | 0.661         |
|    learning_rate        | 0.001         |
|    loss                 | 2.99e+03      |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000416     |
|    std                  | 1.28          |
|    value_loss           | 6.19e+03      |
-------------------------------------------
Eval num_timesteps=280000, episode_reward=2088.42 +/- 234.86
Episode length: 475.00 +/- 105.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 2.09e+03      |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00019732025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.66         |
|    explained_variance   | 0.513         |
|    learning_rate        | 0.001         |
|    loss                 | 3.05e+03      |
|    n_updates            | 1360          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 1.28          |
|    value_loss           | 6.3e+03       |
-------------------------------------------
Eval num_timesteps=282000, episode_reward=2925.63 +/- 864.62
Episode length: 641.00 +/- 109.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 641          |
|    mean_reward          | 2.93e+03     |
| time/                   |              |
|    total_timesteps      | 282000       |
| train/                  |              |
|    approx_kl            | 0.0008032421 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.67        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+03     |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.29         |
|    value_loss           | 3.12e+03     |
------------------------------------------
Eval num_timesteps=284000, episode_reward=2002.39 +/- 702.56
Episode length: 595.00 +/- 66.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 595          |
|    mean_reward          | 2e+03        |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 0.0009870506 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.676        |
|    learning_rate        | 0.001        |
|    loss                 | 2.29e+03     |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.000878    |
|    std                  | 1.3          |
|    value_loss           | 4.82e+03     |
------------------------------------------
Eval num_timesteps=286000, episode_reward=3200.92 +/- 734.79
Episode length: 835.80 +/- 132.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 836          |
|    mean_reward          | 3.2e+03      |
| time/                   |              |
|    total_timesteps      | 286000       |
| train/                  |              |
|    approx_kl            | 0.0005472394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.001        |
|    loss                 | 2.14e+03     |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.000641    |
|    std                  | 1.3          |
|    value_loss           | 4.42e+03     |
------------------------------------------
Eval num_timesteps=288000, episode_reward=2613.34 +/- 463.87
Episode length: 885.00 +/- 265.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 885          |
|    mean_reward          | 2.61e+03     |
| time/                   |              |
|    total_timesteps      | 288000       |
| train/                  |              |
|    approx_kl            | 0.0011004524 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.001        |
|    loss                 | 7.28e+03     |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.3          |
|    value_loss           | 1.56e+04     |
------------------------------------------
Eval num_timesteps=290000, episode_reward=3876.96 +/- 884.75
Episode length: 780.40 +/- 60.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 780           |
|    mean_reward          | 3.88e+03      |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 0.00062037166 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7          |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.001         |
|    loss                 | 6.35e+03      |
|    n_updates            | 1410          |
|    policy_gradient_loss | 0.000864      |
|    std                  | 1.29          |
|    value_loss           | 1.37e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=292000, episode_reward=1970.06 +/- 307.69
Episode length: 520.60 +/- 78.47
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 521            |
|    mean_reward          | 1.97e+03       |
| time/                   |                |
|    total_timesteps      | 292000         |
| train/                  |                |
|    approx_kl            | 0.000104524865 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.7           |
|    explained_variance   | 0.79           |
|    learning_rate        | 0.001          |
|    loss                 | 2.79e+03       |
|    n_updates            | 1420           |
|    policy_gradient_loss | -5.38e-05      |
|    std                  | 1.29           |
|    value_loss           | 5.85e+03       |
--------------------------------------------
Eval num_timesteps=294000, episode_reward=2391.58 +/- 780.67
Episode length: 672.40 +/- 222.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 672           |
|    mean_reward          | 2.39e+03      |
| time/                   |               |
|    total_timesteps      | 294000        |
| train/                  |               |
|    approx_kl            | 0.00022945064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7          |
|    explained_variance   | 0.407         |
|    learning_rate        | 0.001         |
|    loss                 | 2.91e+03      |
|    n_updates            | 1430          |
|    policy_gradient_loss | -0.000988     |
|    std                  | 1.29          |
|    value_loss           | 6.1e+03       |
-------------------------------------------
Eval num_timesteps=296000, episode_reward=2605.12 +/- 905.62
Episode length: 637.20 +/- 193.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 2.61e+03     |
| time/                   |              |
|    total_timesteps      | 296000       |
| train/                  |              |
|    approx_kl            | 0.0006897721 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.29         |
|    value_loss           | 5.35e+03     |
------------------------------------------
Eval num_timesteps=298000, episode_reward=2335.32 +/- 403.87
Episode length: 613.20 +/- 108.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 613           |
|    mean_reward          | 2.34e+03      |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 0.00056032254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7          |
|    explained_variance   | 0.0526        |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+03      |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000204     |
|    std                  | 1.29          |
|    value_loss           | 5.17e+03      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=4231.99 +/- 1109.16
Episode length: 873.40 +/- 280.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 873          |
|    mean_reward          | 4.23e+03     |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0003486237 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.793        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+03     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.000748    |
|    std                  | 1.29         |
|    value_loss           | 4.86e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=302000, episode_reward=2685.83 +/- 1499.33
Episode length: 705.00 +/- 143.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 705          |
|    mean_reward          | 2.69e+03     |
| time/                   |              |
|    total_timesteps      | 302000       |
| train/                  |              |
|    approx_kl            | 8.733143e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 3.17e+03     |
|    n_updates            | 1470         |
|    policy_gradient_loss | 9.59e-05     |
|    std                  | 1.29         |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=2665.69 +/- 821.05
Episode length: 704.80 +/- 208.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 705           |
|    mean_reward          | 2.67e+03      |
| time/                   |               |
|    total_timesteps      | 304000        |
| train/                  |               |
|    approx_kl            | 0.00025615448 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7          |
|    explained_variance   | 0.692         |
|    learning_rate        | 0.001         |
|    loss                 | 2.57e+03      |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.000699     |
|    std                  | 1.29          |
|    value_loss           | 5.34e+03      |
-------------------------------------------
Eval num_timesteps=306000, episode_reward=1259.96 +/- 2019.17
Episode length: 558.40 +/- 104.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 558           |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 306000        |
| train/                  |               |
|    approx_kl            | 0.00038932057 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.71         |
|    explained_variance   | 0.669         |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+03      |
|    n_updates            | 1490          |
|    policy_gradient_loss | -0.000112     |
|    std                  | 1.3           |
|    value_loss           | 6.53e+03      |
-------------------------------------------
Eval num_timesteps=308000, episode_reward=3005.98 +/- 1354.18
Episode length: 719.00 +/- 211.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 719           |
|    mean_reward          | 3.01e+03      |
| time/                   |               |
|    total_timesteps      | 308000        |
| train/                  |               |
|    approx_kl            | 0.00019277184 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.71         |
|    explained_variance   | 0.525         |
|    learning_rate        | 0.001         |
|    loss                 | 3.74e+03      |
|    n_updates            | 1500          |
|    policy_gradient_loss | -0.00036      |
|    std                  | 1.3           |
|    value_loss           | 7.8e+03       |
-------------------------------------------
Eval num_timesteps=310000, episode_reward=2509.58 +/- 537.60
Episode length: 622.20 +/- 51.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 622          |
|    mean_reward          | 2.51e+03     |
| time/                   |              |
|    total_timesteps      | 310000       |
| train/                  |              |
|    approx_kl            | 0.0004194641 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.751        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+03     |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.3          |
|    value_loss           | 4.54e+03     |
------------------------------------------
Eval num_timesteps=312000, episode_reward=2140.88 +/- 1044.10
Episode length: 628.80 +/- 206.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 629          |
|    mean_reward          | 2.14e+03     |
| time/                   |              |
|    total_timesteps      | 312000       |
| train/                  |              |
|    approx_kl            | 0.0003271636 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 2.78e+03     |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000468    |
|    std                  | 1.3          |
|    value_loss           | 5.74e+03     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=3704.94 +/- 1114.35
Episode length: 785.20 +/- 155.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 785          |
|    mean_reward          | 3.7e+03      |
| time/                   |              |
|    total_timesteps      | 314000       |
| train/                  |              |
|    approx_kl            | 0.0006494168 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.001        |
|    loss                 | 2.12e+03     |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.3          |
|    value_loss           | 4.42e+03     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=1909.30 +/- 328.16
Episode length: 530.60 +/- 169.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 531          |
|    mean_reward          | 1.91e+03     |
| time/                   |              |
|    total_timesteps      | 316000       |
| train/                  |              |
|    approx_kl            | 0.0006977394 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+03     |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.3          |
|    value_loss           | 6.59e+03     |
------------------------------------------
Eval num_timesteps=318000, episode_reward=2271.22 +/- 419.51
Episode length: 635.00 +/- 125.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 635           |
|    mean_reward          | 2.27e+03      |
| time/                   |               |
|    total_timesteps      | 318000        |
| train/                  |               |
|    approx_kl            | 0.00094774005 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+03      |
|    n_updates            | 1550          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 1.31          |
|    value_loss           | 7.31e+03      |
-------------------------------------------
Eval num_timesteps=320000, episode_reward=2588.86 +/- 565.47
Episode length: 578.80 +/- 196.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 579           |
|    mean_reward          | 2.59e+03      |
| time/                   |               |
|    total_timesteps      | 320000        |
| train/                  |               |
|    approx_kl            | 0.00015735836 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 3.2e+03       |
|    n_updates            | 1560          |
|    policy_gradient_loss | -7.53e-06     |
|    std                  | 1.31          |
|    value_loss           | 6.62e+03      |
-------------------------------------------
Eval num_timesteps=322000, episode_reward=2779.06 +/- 782.08
Episode length: 732.40 +/- 136.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 732           |
|    mean_reward          | 2.78e+03      |
| time/                   |               |
|    total_timesteps      | 322000        |
| train/                  |               |
|    approx_kl            | 0.00014224084 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.001         |
|    loss                 | 2.53e+03      |
|    n_updates            | 1570          |
|    policy_gradient_loss | -0.000496     |
|    std                  | 1.31          |
|    value_loss           | 5.41e+03      |
-------------------------------------------
Eval num_timesteps=324000, episode_reward=2346.72 +/- 226.16
Episode length: 672.40 +/- 90.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 672          |
|    mean_reward          | 2.35e+03     |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 0.0003999424 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.776        |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+03     |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.000826    |
|    std                  | 1.31         |
|    value_loss           | 5.01e+03     |
------------------------------------------
Eval num_timesteps=326000, episode_reward=2479.01 +/- 742.35
Episode length: 747.80 +/- 200.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 748           |
|    mean_reward          | 2.48e+03      |
| time/                   |               |
|    total_timesteps      | 326000        |
| train/                  |               |
|    approx_kl            | 0.00060099363 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0.824         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+03      |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.000748     |
|    std                  | 1.31          |
|    value_loss           | 6.44e+03      |
-------------------------------------------
Eval num_timesteps=328000, episode_reward=3151.53 +/- 719.27
Episode length: 726.60 +/- 91.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 727          |
|    mean_reward          | 3.15e+03     |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0005966326 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+03     |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.000626    |
|    std                  | 1.31         |
|    value_loss           | 6.42e+03     |
------------------------------------------
Eval num_timesteps=330000, episode_reward=2494.38 +/- 799.09
Episode length: 589.00 +/- 132.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 589          |
|    mean_reward          | 2.49e+03     |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0006661328 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 1.88e+03     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.31         |
|    value_loss           | 4.03e+03     |
------------------------------------------
Eval num_timesteps=332000, episode_reward=2396.99 +/- 626.52
Episode length: 720.60 +/- 208.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 721           |
|    mean_reward          | 2.4e+03       |
| time/                   |               |
|    total_timesteps      | 332000        |
| train/                  |               |
|    approx_kl            | 0.00040031862 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.76         |
|    explained_variance   | 0.383         |
|    learning_rate        | 0.001         |
|    loss                 | 7.6e+03       |
|    n_updates            | 1620          |
|    policy_gradient_loss | -6.15e-05     |
|    std                  | 1.31          |
|    value_loss           | 1.62e+04      |
-------------------------------------------
Eval num_timesteps=334000, episode_reward=2832.81 +/- 1041.83
Episode length: 562.20 +/- 146.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 562          |
|    mean_reward          | 2.83e+03     |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 6.452802e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 3.28e+03     |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.000289    |
|    std                  | 1.31         |
|    value_loss           | 7.04e+03     |
------------------------------------------
Eval num_timesteps=336000, episode_reward=2724.11 +/- 467.62
Episode length: 655.00 +/- 53.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 655           |
|    mean_reward          | 2.72e+03      |
| time/                   |               |
|    total_timesteps      | 336000        |
| train/                  |               |
|    approx_kl            | 0.00014966488 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.77         |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.001         |
|    loss                 | 3.66e+03      |
|    n_updates            | 1640          |
|    policy_gradient_loss | -0.000524     |
|    std                  | 1.31          |
|    value_loss           | 7.42e+03      |
-------------------------------------------
Eval num_timesteps=338000, episode_reward=2876.87 +/- 884.61
Episode length: 667.60 +/- 60.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 668           |
|    mean_reward          | 2.88e+03      |
| time/                   |               |
|    total_timesteps      | 338000        |
| train/                  |               |
|    approx_kl            | 0.00040238124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.77         |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+03      |
|    n_updates            | 1650          |
|    policy_gradient_loss | -0.000884     |
|    std                  | 1.32          |
|    value_loss           | 4.73e+03      |
-------------------------------------------
Eval num_timesteps=340000, episode_reward=3155.55 +/- 1109.28
Episode length: 617.80 +/- 168.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | 3.16e+03     |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0011118789 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 2.87e+03     |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 1.32         |
|    value_loss           | 5.95e+03     |
------------------------------------------
Eval num_timesteps=342000, episode_reward=3364.25 +/- 423.22
Episode length: 654.20 +/- 69.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 3.36e+03 |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=344000, episode_reward=3237.90 +/- 631.83
Episode length: 737.00 +/- 109.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 737          |
|    mean_reward          | 3.24e+03     |
| time/                   |              |
|    total_timesteps      | 344000       |
| train/                  |              |
|    approx_kl            | 0.0009919308 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 3.18e+03     |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.000942    |
|    std                  | 1.31         |
|    value_loss           | 6.47e+03     |
------------------------------------------
Eval num_timesteps=346000, episode_reward=3050.05 +/- 874.96
Episode length: 774.80 +/- 239.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 775          |
|    mean_reward          | 3.05e+03     |
| time/                   |              |
|    total_timesteps      | 346000       |
| train/                  |              |
|    approx_kl            | 0.0003194093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 3.13e+03     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.000205    |
|    std                  | 1.31         |
|    value_loss           | 6.49e+03     |
------------------------------------------
Eval num_timesteps=348000, episode_reward=4088.02 +/- 1071.51
Episode length: 814.60 +/- 159.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | 4.09e+03     |
| time/                   |              |
|    total_timesteps      | 348000       |
| train/                  |              |
|    approx_kl            | 0.0009612314 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.001        |
|    loss                 | 2.75e+03     |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.32         |
|    value_loss           | 5.56e+03     |
------------------------------------------
Eval num_timesteps=350000, episode_reward=3412.74 +/- 1085.74
Episode length: 717.40 +/- 64.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 717           |
|    mean_reward          | 3.41e+03      |
| time/                   |               |
|    total_timesteps      | 350000        |
| train/                  |               |
|    approx_kl            | 0.00069113664 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.78         |
|    explained_variance   | 0.55          |
|    learning_rate        | 0.001         |
|    loss                 | 2.04e+03      |
|    n_updates            | 1700          |
|    policy_gradient_loss | -0.000456     |
|    std                  | 1.32          |
|    value_loss           | 4.23e+03      |
-------------------------------------------
Eval num_timesteps=352000, episode_reward=3024.44 +/- 809.37
Episode length: 656.20 +/- 121.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 656           |
|    mean_reward          | 3.02e+03      |
| time/                   |               |
|    total_timesteps      | 352000        |
| train/                  |               |
|    approx_kl            | 0.00017139607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.8          |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+03      |
|    n_updates            | 1710          |
|    policy_gradient_loss | -7.64e-05     |
|    std                  | 1.32          |
|    value_loss           | 6.91e+03      |
-------------------------------------------
Eval num_timesteps=354000, episode_reward=3231.47 +/- 1208.66
Episode length: 800.60 +/- 226.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 801           |
|    mean_reward          | 3.23e+03      |
| time/                   |               |
|    total_timesteps      | 354000        |
| train/                  |               |
|    approx_kl            | 0.00016583348 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.8          |
|    explained_variance   | 0.712         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+03      |
|    n_updates            | 1720          |
|    policy_gradient_loss | -0.000268     |
|    std                  | 1.33          |
|    value_loss           | 4.58e+03      |
-------------------------------------------
Eval num_timesteps=356000, episode_reward=2543.38 +/- 528.38
Episode length: 664.60 +/- 94.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 665          |
|    mean_reward          | 2.54e+03     |
| time/                   |              |
|    total_timesteps      | 356000       |
| train/                  |              |
|    approx_kl            | 9.829097e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 5.61e+03     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.000392    |
|    std                  | 1.33         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=358000, episode_reward=2713.36 +/- 1673.88
Episode length: 758.80 +/- 212.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 759         |
|    mean_reward          | 2.71e+03    |
| time/                   |             |
|    total_timesteps      | 358000      |
| train/                  |             |
|    approx_kl            | 9.67565e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.81       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.001       |
|    loss                 | 3.13e+03    |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.000334   |
|    std                  | 1.33        |
|    value_loss           | 6.59e+03    |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=2618.77 +/- 793.74
Episode length: 650.40 +/- 84.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 650          |
|    mean_reward          | 2.62e+03     |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0003223029 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.81        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 2.58e+03     |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.000688    |
|    std                  | 1.33         |
|    value_loss           | 5.25e+03     |
------------------------------------------
Eval num_timesteps=362000, episode_reward=2609.47 +/- 823.40
Episode length: 717.80 +/- 281.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 718           |
|    mean_reward          | 2.61e+03      |
| time/                   |               |
|    total_timesteps      | 362000        |
| train/                  |               |
|    approx_kl            | 0.00074685924 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.82         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.99e+03      |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.00104      |
|    std                  | 1.33          |
|    value_loss           | 4.1e+03       |
-------------------------------------------
Eval num_timesteps=364000, episode_reward=2252.66 +/- 337.31
Episode length: 507.00 +/- 77.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 507           |
|    mean_reward          | 2.25e+03      |
| time/                   |               |
|    total_timesteps      | 364000        |
| train/                  |               |
|    approx_kl            | 0.00047655616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.83         |
|    explained_variance   | 0.669         |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+03      |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.34          |
|    value_loss           | 3.77e+03      |
-------------------------------------------
Eval num_timesteps=366000, episode_reward=2133.49 +/- 642.45
Episode length: 471.40 +/- 97.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 471           |
|    mean_reward          | 2.13e+03      |
| time/                   |               |
|    total_timesteps      | 366000        |
| train/                  |               |
|    approx_kl            | 0.00018961602 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.84         |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.001         |
|    loss                 | 2.45e+03      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.000396     |
|    std                  | 1.34          |
|    value_loss           | 5.21e+03      |
-------------------------------------------
Eval num_timesteps=368000, episode_reward=2976.78 +/- 308.70
Episode length: 565.20 +/- 40.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 565           |
|    mean_reward          | 2.98e+03      |
| time/                   |               |
|    total_timesteps      | 368000        |
| train/                  |               |
|    approx_kl            | 0.00013922277 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.84         |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 1790          |
|    policy_gradient_loss | -0.000261     |
|    std                  | 1.34          |
|    value_loss           | 5.07e+03      |
-------------------------------------------
Eval num_timesteps=370000, episode_reward=2597.99 +/- 932.86
Episode length: 544.60 +/- 143.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 545           |
|    mean_reward          | 2.6e+03       |
| time/                   |               |
|    total_timesteps      | 370000        |
| train/                  |               |
|    approx_kl            | 0.00049002457 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.84         |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.001         |
|    loss                 | 2.21e+03      |
|    n_updates            | 1800          |
|    policy_gradient_loss | -0.00106      |
|    std                  | 1.34          |
|    value_loss           | 4.59e+03      |
-------------------------------------------
Eval num_timesteps=372000, episode_reward=3158.95 +/- 564.36
Episode length: 601.60 +/- 56.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 602         |
|    mean_reward          | 3.16e+03    |
| time/                   |             |
|    total_timesteps      | 372000      |
| train/                  |             |
|    approx_kl            | 0.000835389 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 2.5e+03     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 1.34        |
|    value_loss           | 5.13e+03    |
-----------------------------------------
Eval num_timesteps=374000, episode_reward=2666.91 +/- 501.90
Episode length: 539.20 +/- 91.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 539          |
|    mean_reward          | 2.67e+03     |
| time/                   |              |
|    total_timesteps      | 374000       |
| train/                  |              |
|    approx_kl            | 0.0005262444 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 2.08e+03     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.000547    |
|    std                  | 1.34         |
|    value_loss           | 4.34e+03     |
------------------------------------------
Eval num_timesteps=376000, episode_reward=2804.41 +/- 659.59
Episode length: 595.00 +/- 108.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 595          |
|    mean_reward          | 2.8e+03      |
| time/                   |              |
|    total_timesteps      | 376000       |
| train/                  |              |
|    approx_kl            | 0.0004247641 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.85        |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.001        |
|    loss                 | 2.43e+03     |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.34         |
|    value_loss           | 4.98e+03     |
------------------------------------------
Eval num_timesteps=378000, episode_reward=2464.32 +/- 322.80
Episode length: 537.60 +/- 34.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 538           |
|    mean_reward          | 2.46e+03      |
| time/                   |               |
|    total_timesteps      | 378000        |
| train/                  |               |
|    approx_kl            | 0.00036452615 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.001         |
|    loss                 | 3.35e+03      |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 1.35          |
|    value_loss           | 6.82e+03      |
-------------------------------------------
Eval num_timesteps=380000, episode_reward=2544.01 +/- 330.81
Episode length: 515.80 +/- 37.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 516           |
|    mean_reward          | 2.54e+03      |
| time/                   |               |
|    total_timesteps      | 380000        |
| train/                  |               |
|    approx_kl            | 0.00017838625 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.001         |
|    loss                 | 2.38e+03      |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.000283     |
|    std                  | 1.35          |
|    value_loss           | 4.9e+03       |
-------------------------------------------
Eval num_timesteps=382000, episode_reward=3109.40 +/- 548.25
Episode length: 578.00 +/- 29.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 578           |
|    mean_reward          | 3.11e+03      |
| time/                   |               |
|    total_timesteps      | 382000        |
| train/                  |               |
|    approx_kl            | 0.00015186344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+03      |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 1.35          |
|    value_loss           | 5.49e+03      |
-------------------------------------------
Eval num_timesteps=384000, episode_reward=2309.72 +/- 560.21
Episode length: 530.60 +/- 87.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 531           |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 384000        |
| train/                  |               |
|    approx_kl            | 0.00040854307 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+03      |
|    n_updates            | 1870          |
|    policy_gradient_loss | -0.000907     |
|    std                  | 1.35          |
|    value_loss           | 5.15e+03      |
-------------------------------------------
Eval num_timesteps=386000, episode_reward=1056.05 +/- 1785.03
Episode length: 498.40 +/- 130.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 1.06e+03     |
| time/                   |              |
|    total_timesteps      | 386000       |
| train/                  |              |
|    approx_kl            | 0.0007684252 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.001        |
|    loss                 | 2.87e+03     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 1.35         |
|    value_loss           | 5.86e+03     |
------------------------------------------
Eval num_timesteps=388000, episode_reward=2407.43 +/- 1199.60
Episode length: 508.20 +/- 132.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 2.41e+03     |
| time/                   |              |
|    total_timesteps      | 388000       |
| train/                  |              |
|    approx_kl            | 0.0003583334 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+03     |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.000441    |
|    std                  | 1.35         |
|    value_loss           | 3.53e+03     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=2103.18 +/- 984.35
Episode length: 481.00 +/- 206.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 2.1e+03       |
| time/                   |               |
|    total_timesteps      | 390000        |
| train/                  |               |
|    approx_kl            | 0.00048821187 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.88         |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+03      |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.0012       |
|    std                  | 1.35          |
|    value_loss           | 3.65e+03      |
-------------------------------------------
Eval num_timesteps=392000, episode_reward=3141.84 +/- 772.18
Episode length: 583.00 +/- 63.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 583           |
|    mean_reward          | 3.14e+03      |
| time/                   |               |
|    total_timesteps      | 392000        |
| train/                  |               |
|    approx_kl            | 0.00061460095 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.88         |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+03      |
|    n_updates            | 1910          |
|    policy_gradient_loss | -0.000373     |
|    std                  | 1.35          |
|    value_loss           | 4.21e+03      |
-------------------------------------------
Eval num_timesteps=394000, episode_reward=2584.67 +/- 617.59
Episode length: 538.80 +/- 111.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 539           |
|    mean_reward          | 2.58e+03      |
| time/                   |               |
|    total_timesteps      | 394000        |
| train/                  |               |
|    approx_kl            | 0.00019097555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.88         |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.001         |
|    loss                 | 2.63e+03      |
|    n_updates            | 1920          |
|    policy_gradient_loss | -7.95e-05     |
|    std                  | 1.35          |
|    value_loss           | 5.47e+03      |
-------------------------------------------
Eval num_timesteps=396000, episode_reward=2201.68 +/- 506.24
Episode length: 513.20 +/- 98.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 513           |
|    mean_reward          | 2.2e+03       |
| time/                   |               |
|    total_timesteps      | 396000        |
| train/                  |               |
|    approx_kl            | 0.00011407625 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.89         |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 2.12e+03      |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.000295     |
|    std                  | 1.36          |
|    value_loss           | 4.38e+03      |
-------------------------------------------
Eval num_timesteps=398000, episode_reward=2893.48 +/- 455.53
Episode length: 645.80 +/- 96.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 646           |
|    mean_reward          | 2.89e+03      |
| time/                   |               |
|    total_timesteps      | 398000        |
| train/                  |               |
|    approx_kl            | 0.00022590577 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.9          |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.001         |
|    loss                 | 3.25e+03      |
|    n_updates            | 1940          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 1.36          |
|    value_loss           | 6.65e+03      |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=3544.26 +/- 1179.33
Episode length: 622.60 +/- 142.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 623           |
|    mean_reward          | 3.54e+03      |
| time/                   |               |
|    total_timesteps      | 400000        |
| train/                  |               |
|    approx_kl            | 0.00035104167 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.92         |
|    explained_variance   | 0.946         |
|    learning_rate        | 0.001         |
|    loss                 | 2.47e+03      |
|    n_updates            | 1950          |
|    policy_gradient_loss | -0.00114      |
|    std                  | 1.37          |
|    value_loss           | 5.07e+03      |
-------------------------------------------
Eval num_timesteps=402000, episode_reward=2499.49 +/- 743.57
Episode length: 558.60 +/- 139.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 559           |
|    mean_reward          | 2.5e+03       |
| time/                   |               |
|    total_timesteps      | 402000        |
| train/                  |               |
|    approx_kl            | 0.00033156027 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.94         |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.001         |
|    loss                 | 2.6e+03       |
|    n_updates            | 1960          |
|    policy_gradient_loss | -0.000316     |
|    std                  | 1.38          |
|    value_loss           | 5.33e+03      |
-------------------------------------------
Eval num_timesteps=404000, episode_reward=2527.76 +/- 394.01
Episode length: 538.00 +/- 47.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 2.53e+03     |
| time/                   |              |
|    total_timesteps      | 404000       |
| train/                  |              |
|    approx_kl            | 0.0002561395 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+03     |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.000639    |
|    std                  | 1.38         |
|    value_loss           | 5.09e+03     |
------------------------------------------
Eval num_timesteps=406000, episode_reward=3453.69 +/- 994.77
Episode length: 678.20 +/- 104.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 678           |
|    mean_reward          | 3.45e+03      |
| time/                   |               |
|    total_timesteps      | 406000        |
| train/                  |               |
|    approx_kl            | 0.00021333332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.97         |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 2.61e+03      |
|    n_updates            | 1980          |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.38          |
|    value_loss           | 5.34e+03      |
-------------------------------------------
Eval num_timesteps=408000, episode_reward=2377.27 +/- 767.70
Episode length: 489.00 +/- 96.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 2.38e+03      |
| time/                   |               |
|    total_timesteps      | 408000        |
| train/                  |               |
|    approx_kl            | 0.00013719304 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.97         |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.001         |
|    loss                 | 2.64e+03      |
|    n_updates            | 1990          |
|    policy_gradient_loss | -0.000144     |
|    std                  | 1.39          |
|    value_loss           | 5.43e+03      |
-------------------------------------------
Eval num_timesteps=410000, episode_reward=2501.42 +/- 423.03
Episode length: 543.20 +/- 124.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 543           |
|    mean_reward          | 2.5e+03       |
| time/                   |               |
|    total_timesteps      | 410000        |
| train/                  |               |
|    approx_kl            | 0.00020610302 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.98         |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.001         |
|    loss                 | 2.34e+03      |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.000775     |
|    std                  | 1.39          |
|    value_loss           | 4.78e+03      |
-------------------------------------------
Eval num_timesteps=412000, episode_reward=2720.00 +/- 556.68
Episode length: 524.60 +/- 77.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | 2.72e+03      |
| time/                   |               |
|    total_timesteps      | 412000        |
| train/                  |               |
|    approx_kl            | 0.00025321715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.98         |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+03      |
|    n_updates            | 2010          |
|    policy_gradient_loss | -0.000314     |
|    std                  | 1.39          |
|    value_loss           | 3.07e+03      |
-------------------------------------------
Eval num_timesteps=414000, episode_reward=1887.77 +/- 282.33
Episode length: 373.80 +/- 71.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 374           |
|    mean_reward          | 1.89e+03      |
| time/                   |               |
|    total_timesteps      | 414000        |
| train/                  |               |
|    approx_kl            | 0.00023109675 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.99         |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+03      |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.000597     |
|    std                  | 1.39          |
|    value_loss           | 5.62e+03      |
-------------------------------------------
Eval num_timesteps=416000, episode_reward=2518.46 +/- 306.05
Episode length: 562.20 +/- 31.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 562           |
|    mean_reward          | 2.52e+03      |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | 0.00020930704 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.99         |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+03      |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.000413     |
|    std                  | 1.39          |
|    value_loss           | 3.21e+03      |
-------------------------------------------
Eval num_timesteps=418000, episode_reward=2378.95 +/- 380.87
Episode length: 511.80 +/- 130.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 512          |
|    mean_reward          | 2.38e+03     |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0001666123 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.99        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+03     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.000283    |
|    std                  | 1.39         |
|    value_loss           | 5.13e+03     |
------------------------------------------
Eval num_timesteps=420000, episode_reward=2118.01 +/- 276.84
Episode length: 462.40 +/- 63.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 420000        |
| train/                  |               |
|    approx_kl            | 0.00019534095 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.99         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+03      |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.000616     |
|    std                  | 1.39          |
|    value_loss           | 4.43e+03      |
-------------------------------------------
Eval num_timesteps=422000, episode_reward=2560.24 +/- 598.63
Episode length: 481.00 +/- 109.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 2.56e+03      |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 0.00014714879 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7            |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+03      |
|    n_updates            | 2060          |
|    policy_gradient_loss | 2.42e-05      |
|    std                  | 1.4           |
|    value_loss           | 3.67e+03      |
-------------------------------------------
Eval num_timesteps=424000, episode_reward=1955.91 +/- 250.24
Episode length: 446.60 +/- 38.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 1.96e+03     |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 6.676596e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 2.13e+03     |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.000126    |
|    std                  | 1.4          |
|    value_loss           | 4.48e+03     |
------------------------------------------
Eval num_timesteps=426000, episode_reward=1945.11 +/- 480.20
Episode length: 448.60 +/- 80.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 426000        |
| train/                  |               |
|    approx_kl            | 0.00012019879 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.02         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 2.05e+03      |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.000398     |
|    std                  | 1.4           |
|    value_loss           | 4.2e+03       |
-------------------------------------------
Eval num_timesteps=428000, episode_reward=1897.85 +/- 491.29
Episode length: 470.60 +/- 204.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2155.17 +/- 816.91
Episode length: 515.60 +/- 122.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 516           |
|    mean_reward          | 2.16e+03      |
| time/                   |               |
|    total_timesteps      | 430000        |
| train/                  |               |
|    approx_kl            | 0.00019850407 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.02         |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+03      |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.000438     |
|    std                  | 1.4           |
|    value_loss           | 5.46e+03      |
-------------------------------------------
Eval num_timesteps=432000, episode_reward=2852.21 +/- 600.84
Episode length: 636.20 +/- 129.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 636           |
|    mean_reward          | 2.85e+03      |
| time/                   |               |
|    total_timesteps      | 432000        |
| train/                  |               |
|    approx_kl            | 0.00021040262 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.03         |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.001         |
|    loss                 | 2.47e+03      |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.000544     |
|    std                  | 1.41          |
|    value_loss           | 5.07e+03      |
-------------------------------------------
Eval num_timesteps=434000, episode_reward=2291.94 +/- 766.24
Episode length: 491.60 +/- 57.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 492           |
|    mean_reward          | 2.29e+03      |
| time/                   |               |
|    total_timesteps      | 434000        |
| train/                  |               |
|    approx_kl            | 0.00032585915 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.04         |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.001         |
|    loss                 | 2.7e+03       |
|    n_updates            | 2110          |
|    policy_gradient_loss | -0.000726     |
|    std                  | 1.41          |
|    value_loss           | 5.51e+03      |
-------------------------------------------
Eval num_timesteps=436000, episode_reward=2028.40 +/- 478.67
Episode length: 386.80 +/- 122.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 387           |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 436000        |
| train/                  |               |
|    approx_kl            | 0.00020943422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.07         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+03      |
|    n_updates            | 2120          |
|    policy_gradient_loss | -0.000478     |
|    std                  | 1.42          |
|    value_loss           | 3.4e+03       |
-------------------------------------------
Eval num_timesteps=438000, episode_reward=2437.33 +/- 575.11
Episode length: 693.40 +/- 112.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 693           |
|    mean_reward          | 2.44e+03      |
| time/                   |               |
|    total_timesteps      | 438000        |
| train/                  |               |
|    approx_kl            | 8.7997236e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.08         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 2130          |
|    policy_gradient_loss | -6.77e-05     |
|    std                  | 1.42          |
|    value_loss           | 3.06e+03      |
-------------------------------------------
Eval num_timesteps=440000, episode_reward=2321.78 +/- 777.05
Episode length: 507.40 +/- 71.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 507           |
|    mean_reward          | 2.32e+03      |
| time/                   |               |
|    total_timesteps      | 440000        |
| train/                  |               |
|    approx_kl            | 0.00015074096 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.09         |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+03      |
|    n_updates            | 2140          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 1.43          |
|    value_loss           | 2.65e+03      |
-------------------------------------------
Eval num_timesteps=442000, episode_reward=1917.60 +/- 806.20
Episode length: 457.60 +/- 104.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 1.92e+03     |
| time/                   |              |
|    total_timesteps      | 442000       |
| train/                  |              |
|    approx_kl            | 0.0002798382 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.1         |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+03     |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000642    |
|    std                  | 1.43         |
|    value_loss           | 3.01e+03     |
------------------------------------------
Eval num_timesteps=444000, episode_reward=1370.31 +/- 275.00
Episode length: 304.20 +/- 83.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 444000        |
| train/                  |               |
|    approx_kl            | 0.00015616871 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.11         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 2160          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 1.44          |
|    value_loss           | 3.31e+03      |
-------------------------------------------
Eval num_timesteps=446000, episode_reward=2099.70 +/- 707.53
Episode length: 441.00 +/- 125.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | 2.1e+03       |
| time/                   |               |
|    total_timesteps      | 446000        |
| train/                  |               |
|    approx_kl            | 0.00018795885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.12         |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.001         |
|    loss                 | 2.12e+03      |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.000244     |
|    std                  | 1.44          |
|    value_loss           | 4.37e+03      |
-------------------------------------------
Eval num_timesteps=448000, episode_reward=2478.72 +/- 536.34
Episode length: 594.20 +/- 98.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 594           |
|    mean_reward          | 2.48e+03      |
| time/                   |               |
|    total_timesteps      | 448000        |
| train/                  |               |
|    approx_kl            | 0.00020540066 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.13         |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 2180          |
|    policy_gradient_loss | -0.000598     |
|    std                  | 1.44          |
|    value_loss           | 3.75e+03      |
-------------------------------------------
Eval num_timesteps=450000, episode_reward=1783.85 +/- 501.65
Episode length: 410.00 +/- 114.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 1.78e+03      |
| time/                   |               |
|    total_timesteps      | 450000        |
| train/                  |               |
|    approx_kl            | 0.00011644224 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.13         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+03      |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000171     |
|    std                  | 1.44          |
|    value_loss           | 3.75e+03      |
-------------------------------------------
Eval num_timesteps=452000, episode_reward=2269.42 +/- 473.09
Episode length: 573.60 +/- 124.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 574          |
|    mean_reward          | 2.27e+03     |
| time/                   |              |
|    total_timesteps      | 452000       |
| train/                  |              |
|    approx_kl            | 0.0002479699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.14        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 2.03e+03     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000693    |
|    std                  | 1.45         |
|    value_loss           | 4.15e+03     |
------------------------------------------
Eval num_timesteps=454000, episode_reward=2239.49 +/- 775.15
Episode length: 518.20 +/- 164.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 518           |
|    mean_reward          | 2.24e+03      |
| time/                   |               |
|    total_timesteps      | 454000        |
| train/                  |               |
|    approx_kl            | 0.00061908027 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.15         |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.001         |
|    loss                 | 1.49e+03      |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.001        |
|    std                  | 1.45          |
|    value_loss           | 3.14e+03      |
-------------------------------------------
Eval num_timesteps=456000, episode_reward=2324.20 +/- 528.24
Episode length: 482.20 +/- 79.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 2.32e+03      |
| time/                   |               |
|    total_timesteps      | 456000        |
| train/                  |               |
|    approx_kl            | 0.00061289663 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.16         |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 2.38e+03      |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.45          |
|    value_loss           | 4.87e+03      |
-------------------------------------------
Eval num_timesteps=458000, episode_reward=1999.62 +/- 865.34
Episode length: 485.00 +/- 82.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 2e+03         |
| time/                   |               |
|    total_timesteps      | 458000        |
| train/                  |               |
|    approx_kl            | 0.00014790494 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.15         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 1.98e+03      |
|    n_updates            | 2230          |
|    policy_gradient_loss | 0.000206      |
|    std                  | 1.45          |
|    value_loss           | 4.07e+03      |
-------------------------------------------
Eval num_timesteps=460000, episode_reward=1667.08 +/- 398.02
Episode length: 361.20 +/- 78.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 361           |
|    mean_reward          | 1.67e+03      |
| time/                   |               |
|    total_timesteps      | 460000        |
| train/                  |               |
|    approx_kl            | 0.00030716398 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.16         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+03      |
|    n_updates            | 2240          |
|    policy_gradient_loss | -0.000741     |
|    std                  | 1.45          |
|    value_loss           | 4.51e+03      |
-------------------------------------------
Eval num_timesteps=462000, episode_reward=2242.50 +/- 975.92
Episode length: 430.00 +/- 172.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 2.24e+03      |
| time/                   |               |
|    total_timesteps      | 462000        |
| train/                  |               |
|    approx_kl            | 0.00032194547 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.17         |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+03      |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000714     |
|    std                  | 1.46          |
|    value_loss           | 3.77e+03      |
-------------------------------------------
Eval num_timesteps=464000, episode_reward=1785.44 +/- 360.02
Episode length: 444.40 +/- 98.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 1.79e+03      |
| time/                   |               |
|    total_timesteps      | 464000        |
| train/                  |               |
|    approx_kl            | 0.00021177204 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.19         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+03      |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.000413     |
|    std                  | 1.46          |
|    value_loss           | 3.6e+03       |
-------------------------------------------
Eval num_timesteps=466000, episode_reward=3349.07 +/- 1993.40
Episode length: 509.80 +/- 104.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 510          |
|    mean_reward          | 3.35e+03     |
| time/                   |              |
|    total_timesteps      | 466000       |
| train/                  |              |
|    approx_kl            | 0.0004602814 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.19        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 1.56e+03     |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.000949    |
|    std                  | 1.46         |
|    value_loss           | 3.2e+03      |
------------------------------------------
Eval num_timesteps=468000, episode_reward=2244.23 +/- 402.26
Episode length: 488.60 +/- 58.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 2.24e+03      |
| time/                   |               |
|    total_timesteps      | 468000        |
| train/                  |               |
|    approx_kl            | 0.00033277005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.2          |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 2.53e+03      |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.000634     |
|    std                  | 1.47          |
|    value_loss           | 5.17e+03      |
-------------------------------------------
Eval num_timesteps=470000, episode_reward=2522.55 +/- 682.98
Episode length: 503.00 +/- 126.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 2.52e+03     |
| time/                   |              |
|    total_timesteps      | 470000       |
| train/                  |              |
|    approx_kl            | 0.0005218562 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+03     |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.000962    |
|    std                  | 1.47         |
|    value_loss           | 4.85e+03     |
------------------------------------------
Eval num_timesteps=472000, episode_reward=1552.26 +/- 270.64
Episode length: 421.80 +/- 48.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 1.55e+03      |
| time/                   |               |
|    total_timesteps      | 472000        |
| train/                  |               |
|    approx_kl            | 0.00023402408 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.23         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 2300          |
|    policy_gradient_loss | -0.000373     |
|    std                  | 1.48          |
|    value_loss           | 3.24e+03      |
-------------------------------------------
Eval num_timesteps=474000, episode_reward=2123.62 +/- 645.87
Episode length: 478.60 +/- 138.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 479           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 474000        |
| train/                  |               |
|    approx_kl            | 0.00014487011 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.24         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000217     |
|    std                  | 1.48          |
|    value_loss           | 2.51e+03      |
-------------------------------------------
Eval num_timesteps=476000, episode_reward=2149.52 +/- 683.65
Episode length: 514.20 +/- 48.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 514           |
|    mean_reward          | 2.15e+03      |
| time/                   |               |
|    total_timesteps      | 476000        |
| train/                  |               |
|    approx_kl            | 0.00048281468 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.25         |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.00128      |
|    std                  | 1.48          |
|    value_loss           | 5.02e+03      |
-------------------------------------------
Eval num_timesteps=478000, episode_reward=2630.95 +/- 505.04
Episode length: 502.00 +/- 86.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 502           |
|    mean_reward          | 2.63e+03      |
| time/                   |               |
|    total_timesteps      | 478000        |
| train/                  |               |
|    approx_kl            | 0.00037137914 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.26         |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+03      |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.000179     |
|    std                  | 1.49          |
|    value_loss           | 3.76e+03      |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=1855.10 +/- 618.90
Episode length: 401.20 +/- 130.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 1.86e+03      |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 0.00010698149 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.26         |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+03      |
|    n_updates            | 2340          |
|    policy_gradient_loss | -0.000292     |
|    std                  | 1.49          |
|    value_loss           | 3.49e+03      |
-------------------------------------------
Eval num_timesteps=482000, episode_reward=2336.85 +/- 762.78
Episode length: 492.20 +/- 121.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 492           |
|    mean_reward          | 2.34e+03      |
| time/                   |               |
|    total_timesteps      | 482000        |
| train/                  |               |
|    approx_kl            | 0.00046105898 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.27         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+03      |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.000685     |
|    std                  | 1.49          |
|    value_loss           | 5.06e+03      |
-------------------------------------------
Eval num_timesteps=484000, episode_reward=1975.78 +/- 721.64
Episode length: 509.00 +/- 103.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 509          |
|    mean_reward          | 1.98e+03     |
| time/                   |              |
|    total_timesteps      | 484000       |
| train/                  |              |
|    approx_kl            | 0.0008383799 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+03     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.000859    |
|    std                  | 1.5          |
|    value_loss           | 4e+03        |
------------------------------------------
Eval num_timesteps=486000, episode_reward=2481.85 +/- 720.00
Episode length: 548.00 +/- 92.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 548           |
|    mean_reward          | 2.48e+03      |
| time/                   |               |
|    total_timesteps      | 486000        |
| train/                  |               |
|    approx_kl            | 0.00065684086 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.29         |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.001         |
|    loss                 | 2.66e+03      |
|    n_updates            | 2370          |
|    policy_gradient_loss | -0.000816     |
|    std                  | 1.5           |
|    value_loss           | 5.46e+03      |
-------------------------------------------
Eval num_timesteps=488000, episode_reward=2274.76 +/- 827.38
Episode length: 550.20 +/- 165.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 550           |
|    mean_reward          | 2.27e+03      |
| time/                   |               |
|    total_timesteps      | 488000        |
| train/                  |               |
|    approx_kl            | 0.00031785824 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.3          |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 2380          |
|    policy_gradient_loss | -0.000346     |
|    std                  | 1.51          |
|    value_loss           | 3.78e+03      |
-------------------------------------------
Eval num_timesteps=490000, episode_reward=2363.07 +/- 813.72
Episode length: 480.20 +/- 142.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 2.36e+03     |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0003101991 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.001        |
|    loss                 | 6.17e+03     |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.000681    |
|    std                  | 1.5          |
|    value_loss           | 1.28e+04     |
------------------------------------------
Eval num_timesteps=492000, episode_reward=2726.40 +/- 1220.28
Episode length: 516.00 +/- 174.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 516           |
|    mean_reward          | 2.73e+03      |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 0.00034142175 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.3          |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.001         |
|    loss                 | 2.33e+03      |
|    n_updates            | 2400          |
|    policy_gradient_loss | -0.000653     |
|    std                  | 1.5           |
|    value_loss           | 4.8e+03       |
-------------------------------------------
Eval num_timesteps=494000, episode_reward=3460.71 +/- 1140.01
Episode length: 544.80 +/- 105.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 545           |
|    mean_reward          | 3.46e+03      |
| time/                   |               |
|    total_timesteps      | 494000        |
| train/                  |               |
|    approx_kl            | 0.00043979922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.31         |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+03      |
|    n_updates            | 2410          |
|    policy_gradient_loss | -0.000511     |
|    std                  | 1.51          |
|    value_loss           | 3.92e+03      |
-------------------------------------------
Eval num_timesteps=496000, episode_reward=2601.09 +/- 458.69
Episode length: 529.20 +/- 74.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 529           |
|    mean_reward          | 2.6e+03       |
| time/                   |               |
|    total_timesteps      | 496000        |
| train/                  |               |
|    approx_kl            | 0.00043115072 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.32         |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+03      |
|    n_updates            | 2420          |
|    policy_gradient_loss | -0.000585     |
|    std                  | 1.52          |
|    value_loss           | 3.65e+03      |
-------------------------------------------
Eval num_timesteps=498000, episode_reward=1818.20 +/- 527.90
Episode length: 448.20 +/- 87.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 448           |
|    mean_reward          | 1.82e+03      |
| time/                   |               |
|    total_timesteps      | 498000        |
| train/                  |               |
|    approx_kl            | 0.00026828423 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.34         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+03      |
|    n_updates            | 2430          |
|    policy_gradient_loss | -0.000499     |
|    std                  | 1.52          |
|    value_loss           | 3.53e+03      |
-------------------------------------------
Eval num_timesteps=500000, episode_reward=1983.57 +/- 709.94
Episode length: 456.20 +/- 130.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 456           |
|    mean_reward          | 1.98e+03      |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 0.00023421965 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.35         |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+03      |
|    n_updates            | 2440          |
|    policy_gradient_loss | -7.39e-05     |
|    std                  | 1.52          |
|    value_loss           | 3.68e+03      |
-------------------------------------------
Eval num_timesteps=502000, episode_reward=2641.10 +/- 622.33
Episode length: 546.00 +/- 84.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 2.64e+03     |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0004042743 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+03     |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.52         |
|    value_loss           | 3.83e+03     |
------------------------------------------
Eval num_timesteps=504000, episode_reward=2366.57 +/- 805.18
Episode length: 526.80 +/- 73.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 527          |
|    mean_reward          | 2.37e+03     |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0004888091 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+03     |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.000792    |
|    std                  | 1.52         |
|    value_loss           | 5.41e+03     |
------------------------------------------
Eval num_timesteps=506000, episode_reward=1649.95 +/- 498.84
Episode length: 376.80 +/- 78.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 1.65e+03     |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0004308294 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 1.31e+03     |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.000827    |
|    std                  | 1.52         |
|    value_loss           | 2.69e+03     |
------------------------------------------
Eval num_timesteps=508000, episode_reward=2380.40 +/- 324.37
Episode length: 519.60 +/- 40.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 520           |
|    mean_reward          | 2.38e+03      |
| time/                   |               |
|    total_timesteps      | 508000        |
| train/                  |               |
|    approx_kl            | 0.00038849236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.35         |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 2480          |
|    policy_gradient_loss | -0.000412     |
|    std                  | 1.52          |
|    value_loss           | 3.81e+03      |
-------------------------------------------
Eval num_timesteps=510000, episode_reward=1732.19 +/- 558.44
Episode length: 401.80 +/- 116.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0004226913 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+03     |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.000741    |
|    std                  | 1.52         |
|    value_loss           | 3.24e+03     |
------------------------------------------
Eval num_timesteps=512000, episode_reward=2442.47 +/- 961.68
Episode length: 478.00 +/- 130.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=514000, episode_reward=1745.92 +/- 409.46
Episode length: 407.20 +/- 84.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 1.75e+03      |
| time/                   |               |
|    total_timesteps      | 514000        |
| train/                  |               |
|    approx_kl            | 0.00064733886 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.35         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+03      |
|    n_updates            | 2500          |
|    policy_gradient_loss | -0.000603     |
|    std                  | 1.52          |
|    value_loss           | 3.53e+03      |
-------------------------------------------
Eval num_timesteps=516000, episode_reward=1680.35 +/- 267.93
Episode length: 410.40 +/- 89.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 1.68e+03      |
| time/                   |               |
|    total_timesteps      | 516000        |
| train/                  |               |
|    approx_kl            | 0.00017362376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.36         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 2510          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 1.53          |
|    value_loss           | 2.81e+03      |
-------------------------------------------
Eval num_timesteps=518000, episode_reward=1619.90 +/- 467.58
Episode length: 343.20 +/- 67.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | 1.62e+03     |
| time/                   |              |
|    total_timesteps      | 518000       |
| train/                  |              |
|    approx_kl            | 0.0004041014 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.37        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+03     |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.000906    |
|    std                  | 1.53         |
|    value_loss           | 2.19e+03     |
------------------------------------------
Eval num_timesteps=520000, episode_reward=2359.18 +/- 865.79
Episode length: 460.00 +/- 129.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 2.36e+03     |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0015994123 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+03     |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 1.52         |
|    value_loss           | 3.37e+03     |
------------------------------------------
Eval num_timesteps=522000, episode_reward=2127.92 +/- 762.31
Episode length: 419.80 +/- 103.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 2.13e+03     |
| time/                   |              |
|    total_timesteps      | 522000       |
| train/                  |              |
|    approx_kl            | 0.0010411876 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 2.19e+03     |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.000162    |
|    std                  | 1.52         |
|    value_loss           | 4.48e+03     |
------------------------------------------
Eval num_timesteps=524000, episode_reward=2001.85 +/- 478.10
Episode length: 407.20 +/- 72.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 2e+03        |
| time/                   |              |
|    total_timesteps      | 524000       |
| train/                  |              |
|    approx_kl            | 0.0001331967 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.000172    |
|    std                  | 1.52         |
|    value_loss           | 2.81e+03     |
------------------------------------------
Eval num_timesteps=526000, episode_reward=1858.59 +/- 735.35
Episode length: 450.80 +/- 107.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 451           |
|    mean_reward          | 1.86e+03      |
| time/                   |               |
|    total_timesteps      | 526000        |
| train/                  |               |
|    approx_kl            | 0.00010018665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.34         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+03      |
|    n_updates            | 2560          |
|    policy_gradient_loss | -0.000123     |
|    std                  | 1.52          |
|    value_loss           | 2.65e+03      |
-------------------------------------------
Eval num_timesteps=528000, episode_reward=2064.13 +/- 447.83
Episode length: 498.00 +/- 102.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | 2.06e+03      |
| time/                   |               |
|    total_timesteps      | 528000        |
| train/                  |               |
|    approx_kl            | 0.00018377326 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.35         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+03      |
|    n_updates            | 2570          |
|    policy_gradient_loss | -0.000616     |
|    std                  | 1.52          |
|    value_loss           | 3.77e+03      |
-------------------------------------------
Eval num_timesteps=530000, episode_reward=1727.53 +/- 109.78
Episode length: 453.00 +/- 59.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 1.73e+03      |
| time/                   |               |
|    total_timesteps      | 530000        |
| train/                  |               |
|    approx_kl            | 0.00042138237 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.36         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+03      |
|    n_updates            | 2580          |
|    policy_gradient_loss | -0.000728     |
|    std                  | 1.53          |
|    value_loss           | 2.76e+03      |
-------------------------------------------
Eval num_timesteps=532000, episode_reward=2262.72 +/- 627.55
Episode length: 473.60 +/- 59.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | 2.26e+03      |
| time/                   |               |
|    total_timesteps      | 532000        |
| train/                  |               |
|    approx_kl            | 0.00023801898 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.36         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 4.8e+03       |
|    n_updates            | 2590          |
|    policy_gradient_loss | -0.000154     |
|    std                  | 1.53          |
|    value_loss           | 1.05e+04      |
-------------------------------------------
Eval num_timesteps=534000, episode_reward=2683.37 +/- 794.66
Episode length: 524.80 +/- 78.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 2.68e+03     |
| time/                   |              |
|    total_timesteps      | 534000       |
| train/                  |              |
|    approx_kl            | 0.0001371557 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+03     |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.000517    |
|    std                  | 1.53         |
|    value_loss           | 2.52e+03     |
------------------------------------------
Eval num_timesteps=536000, episode_reward=1693.28 +/- 334.57
Episode length: 443.40 +/- 111.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 1.69e+03      |
| time/                   |               |
|    total_timesteps      | 536000        |
| train/                  |               |
|    approx_kl            | 0.00019547346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.37         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+03      |
|    n_updates            | 2610          |
|    policy_gradient_loss | -0.000497     |
|    std                  | 1.53          |
|    value_loss           | 2.17e+03      |
-------------------------------------------
Eval num_timesteps=538000, episode_reward=1869.21 +/- 427.07
Episode length: 455.80 +/- 121.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 456           |
|    mean_reward          | 1.87e+03      |
| time/                   |               |
|    total_timesteps      | 538000        |
| train/                  |               |
|    approx_kl            | 0.00014002345 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.38         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+03      |
|    n_updates            | 2620          |
|    policy_gradient_loss | -0.000131     |
|    std                  | 1.54          |
|    value_loss           | 2.63e+03      |
-------------------------------------------
Eval num_timesteps=540000, episode_reward=1835.10 +/- 425.26
Episode length: 451.00 +/- 39.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 451           |
|    mean_reward          | 1.84e+03      |
| time/                   |               |
|    total_timesteps      | 540000        |
| train/                  |               |
|    approx_kl            | 0.00017206674 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.39         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 2630          |
|    policy_gradient_loss | -0.000236     |
|    std                  | 1.54          |
|    value_loss           | 2.52e+03      |
-------------------------------------------
Eval num_timesteps=542000, episode_reward=2061.94 +/- 521.14
Episode length: 476.60 +/- 104.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 477           |
|    mean_reward          | 2.06e+03      |
| time/                   |               |
|    total_timesteps      | 542000        |
| train/                  |               |
|    approx_kl            | 0.00019136077 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.4          |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+03      |
|    n_updates            | 2640          |
|    policy_gradient_loss | -0.000366     |
|    std                  | 1.54          |
|    value_loss           | 3.13e+03      |
-------------------------------------------
Eval num_timesteps=544000, episode_reward=2082.70 +/- 590.43
Episode length: 487.40 +/- 194.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 2.08e+03      |
| time/                   |               |
|    total_timesteps      | 544000        |
| train/                  |               |
|    approx_kl            | 0.00042492794 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.41         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 2650          |
|    policy_gradient_loss | -0.000742     |
|    std                  | 1.55          |
|    value_loss           | 2.81e+03      |
-------------------------------------------
Eval num_timesteps=546000, episode_reward=1876.51 +/- 428.83
Episode length: 405.40 +/- 69.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 1.88e+03    |
| time/                   |             |
|    total_timesteps      | 546000      |
| train/                  |             |
|    approx_kl            | 0.001609151 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00111    |
|    std                  | 1.55        |
|    value_loss           | 2.5e+03     |
-----------------------------------------
Eval num_timesteps=548000, episode_reward=1531.29 +/- 278.83
Episode length: 322.80 +/- 79.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | 1.53e+03     |
| time/                   |              |
|    total_timesteps      | 548000       |
| train/                  |              |
|    approx_kl            | 0.0008864403 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.41        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -7.28e-05    |
|    std                  | 1.54         |
|    value_loss           | 2.75e+03     |
------------------------------------------
Eval num_timesteps=550000, episode_reward=1786.82 +/- 372.46
Episode length: 411.40 +/- 59.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 1.79e+03      |
| time/                   |               |
|    total_timesteps      | 550000        |
| train/                  |               |
|    approx_kl            | 0.00015017146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.4          |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+03      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.000276     |
|    std                  | 1.54          |
|    value_loss           | 2.63e+03      |
-------------------------------------------
Eval num_timesteps=552000, episode_reward=1811.47 +/- 508.30
Episode length: 356.80 +/- 83.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 357           |
|    mean_reward          | 1.81e+03      |
| time/                   |               |
|    total_timesteps      | 552000        |
| train/                  |               |
|    approx_kl            | 0.00019494459 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.4          |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+03      |
|    n_updates            | 2690          |
|    policy_gradient_loss | -0.000528     |
|    std                  | 1.54          |
|    value_loss           | 3.93e+03      |
-------------------------------------------
Eval num_timesteps=554000, episode_reward=1949.51 +/- 446.29
Episode length: 387.40 +/- 63.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 387           |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 554000        |
| train/                  |               |
|    approx_kl            | 0.00053058716 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.4          |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+03      |
|    n_updates            | 2700          |
|    policy_gradient_loss | -0.00108      |
|    std                  | 1.54          |
|    value_loss           | 2.98e+03      |
-------------------------------------------
Eval num_timesteps=556000, episode_reward=1940.04 +/- 204.02
Episode length: 426.60 +/- 45.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 1.94e+03      |
| time/                   |               |
|    total_timesteps      | 556000        |
| train/                  |               |
|    approx_kl            | 0.00044885682 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.41         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 965           |
|    n_updates            | 2710          |
|    policy_gradient_loss | -0.000348     |
|    std                  | 1.55          |
|    value_loss           | 1.99e+03      |
-------------------------------------------
Eval num_timesteps=558000, episode_reward=1938.94 +/- 344.28
Episode length: 428.60 +/- 41.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 1.94e+03      |
| time/                   |               |
|    total_timesteps      | 558000        |
| train/                  |               |
|    approx_kl            | 0.00038021224 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.42         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+03      |
|    n_updates            | 2720          |
|    policy_gradient_loss | -0.000771     |
|    std                  | 1.55          |
|    value_loss           | 2.51e+03      |
-------------------------------------------
Eval num_timesteps=560000, episode_reward=1950.46 +/- 554.10
Episode length: 410.60 +/- 53.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 560000        |
| train/                  |               |
|    approx_kl            | 0.00027063803 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.43         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+03      |
|    n_updates            | 2730          |
|    policy_gradient_loss | -0.000334     |
|    std                  | 1.55          |
|    value_loss           | 2.54e+03      |
-------------------------------------------
Eval num_timesteps=562000, episode_reward=1951.01 +/- 462.24
Episode length: 412.80 +/- 70.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 413           |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 562000        |
| train/                  |               |
|    approx_kl            | 0.00026470717 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.44         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 2740          |
|    policy_gradient_loss | -6.77e-05     |
|    std                  | 1.56          |
|    value_loss           | 3.8e+03       |
-------------------------------------------
Eval num_timesteps=564000, episode_reward=1934.60 +/- 266.75
Episode length: 436.00 +/- 34.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 1.93e+03     |
| time/                   |              |
|    total_timesteps      | 564000       |
| train/                  |              |
|    approx_kl            | 0.0002426304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+03     |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.000572    |
|    std                  | 1.56         |
|    value_loss           | 3.46e+03     |
------------------------------------------
Eval num_timesteps=566000, episode_reward=2265.00 +/- 574.24
Episode length: 463.40 +/- 83.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 463           |
|    mean_reward          | 2.26e+03      |
| time/                   |               |
|    total_timesteps      | 566000        |
| train/                  |               |
|    approx_kl            | 0.00015083695 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.47         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 2760          |
|    policy_gradient_loss | -8.11e-05     |
|    std                  | 1.57          |
|    value_loss           | 3.36e+03      |
-------------------------------------------
Eval num_timesteps=568000, episode_reward=1579.64 +/- 340.23
Episode length: 382.40 +/- 49.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 1.58e+03     |
| time/                   |              |
|    total_timesteps      | 568000       |
| train/                  |              |
|    approx_kl            | 9.742862e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+03     |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.000277    |
|    std                  | 1.57         |
|    value_loss           | 3.35e+03     |
------------------------------------------
Eval num_timesteps=570000, episode_reward=1777.62 +/- 312.58
Episode length: 375.40 +/- 47.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 375           |
|    mean_reward          | 1.78e+03      |
| time/                   |               |
|    total_timesteps      | 570000        |
| train/                  |               |
|    approx_kl            | 0.00020870927 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.48         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 2780          |
|    policy_gradient_loss | -0.000713     |
|    std                  | 1.57          |
|    value_loss           | 2.42e+03      |
-------------------------------------------
Eval num_timesteps=572000, episode_reward=2296.57 +/- 495.64
Episode length: 469.00 +/- 54.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | 2.3e+03       |
| time/                   |               |
|    total_timesteps      | 572000        |
| train/                  |               |
|    approx_kl            | 0.00048368468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.49         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+03      |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.000915     |
|    std                  | 1.58          |
|    value_loss           | 3.67e+03      |
-------------------------------------------
Eval num_timesteps=574000, episode_reward=2210.60 +/- 470.96
Episode length: 416.80 +/- 110.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 2.21e+03      |
| time/                   |               |
|    total_timesteps      | 574000        |
| train/                  |               |
|    approx_kl            | 0.00029244865 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.5          |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+03      |
|    n_updates            | 2800          |
|    policy_gradient_loss | -0.000329     |
|    std                  | 1.58          |
|    value_loss           | 3.71e+03      |
-------------------------------------------
Eval num_timesteps=576000, episode_reward=1960.34 +/- 586.36
Episode length: 438.40 +/- 59.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 1.96e+03      |
| time/                   |               |
|    total_timesteps      | 576000        |
| train/                  |               |
|    approx_kl            | 0.00022970952 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.51         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+03      |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.000439     |
|    std                  | 1.58          |
|    value_loss           | 3.96e+03      |
-------------------------------------------
Eval num_timesteps=578000, episode_reward=2023.99 +/- 387.33
Episode length: 402.00 +/- 70.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | 2.02e+03      |
| time/                   |               |
|    total_timesteps      | 578000        |
| train/                  |               |
|    approx_kl            | 0.00021763355 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.52         |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+03      |
|    n_updates            | 2820          |
|    policy_gradient_loss | -0.000402     |
|    std                  | 1.59          |
|    value_loss           | 2.55e+03      |
-------------------------------------------
Eval num_timesteps=580000, episode_reward=1844.81 +/- 606.24
Episode length: 388.40 +/- 93.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 1.84e+03      |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00035692562 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.53         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.74e+03      |
|    n_updates            | 2830          |
|    policy_gradient_loss | -0.000675     |
|    std                  | 1.6           |
|    value_loss           | 3.58e+03      |
-------------------------------------------
Eval num_timesteps=582000, episode_reward=1599.22 +/- 391.88
Episode length: 335.40 +/- 81.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | 1.6e+03      |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0006152338 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.56        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 954          |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.000903    |
|    std                  | 1.61         |
|    value_loss           | 1.96e+03     |
------------------------------------------
Eval num_timesteps=584000, episode_reward=1839.02 +/- 488.40
Episode length: 366.00 +/- 84.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0012154556 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+03     |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.62         |
|    value_loss           | 2.92e+03     |
------------------------------------------
Eval num_timesteps=586000, episode_reward=1842.65 +/- 387.51
Episode length: 415.20 +/- 74.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0021759784 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+03      |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.62         |
|    value_loss           | 3.09e+03     |
------------------------------------------
Eval num_timesteps=588000, episode_reward=2171.19 +/- 495.11
Episode length: 497.80 +/- 85.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 2.17e+03     |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0018565598 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+03     |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.62         |
|    value_loss           | 3.26e+03     |
------------------------------------------
Eval num_timesteps=590000, episode_reward=1702.28 +/- 522.19
Episode length: 400.20 +/- 86.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 1.7e+03       |
| time/                   |               |
|    total_timesteps      | 590000        |
| train/                  |               |
|    approx_kl            | 0.00017463099 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.61         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 783           |
|    n_updates            | 2880          |
|    policy_gradient_loss | 1.09e-05      |
|    std                  | 1.62          |
|    value_loss           | 1.62e+03      |
-------------------------------------------
Eval num_timesteps=592000, episode_reward=1827.44 +/- 671.18
Episode length: 467.60 +/- 145.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 468           |
|    mean_reward          | 1.83e+03      |
| time/                   |               |
|    total_timesteps      | 592000        |
| train/                  |               |
|    approx_kl            | 0.00020185599 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.61         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 2890          |
|    policy_gradient_loss | -0.000416     |
|    std                  | 1.63          |
|    value_loss           | 3.81e+03      |
-------------------------------------------
Eval num_timesteps=594000, episode_reward=2886.18 +/- 1474.14
Episode length: 474.20 +/- 33.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | 2.89e+03      |
| time/                   |               |
|    total_timesteps      | 594000        |
| train/                  |               |
|    approx_kl            | 0.00072470953 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.63         |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.001         |
|    loss                 | 691           |
|    n_updates            | 2900          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 1.64          |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=596000, episode_reward=2060.97 +/- 656.14
Episode length: 486.20 +/- 58.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 2.06e+03     |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0008834724 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.000649    |
|    std                  | 1.65         |
|    value_loss           | 2.3e+03      |
------------------------------------------
Eval num_timesteps=598000, episode_reward=1868.75 +/- 448.29
Episode length: 454.80 +/- 116.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 455      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
Eval num_timesteps=600000, episode_reward=1991.73 +/- 686.03
Episode length: 437.60 +/- 143.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 1.99e+03      |
| time/                   |               |
|    total_timesteps      | 600000        |
| train/                  |               |
|    approx_kl            | 0.00067329034 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.69         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 2920          |
|    policy_gradient_loss | -0.000791     |
|    std                  | 1.66          |
|    value_loss           | 2.94e+03      |
-------------------------------------------
Eval num_timesteps=602000, episode_reward=2298.90 +/- 326.85
Episode length: 542.80 +/- 33.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 543          |
|    mean_reward          | 2.3e+03      |
| time/                   |              |
|    total_timesteps      | 602000       |
| train/                  |              |
|    approx_kl            | 0.0008043413 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 697          |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.67         |
|    value_loss           | 1.45e+03     |
------------------------------------------
Eval num_timesteps=604000, episode_reward=2431.23 +/- 602.85
Episode length: 501.40 +/- 91.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 501           |
|    mean_reward          | 2.43e+03      |
| time/                   |               |
|    total_timesteps      | 604000        |
| train/                  |               |
|    approx_kl            | 0.00065127306 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.73         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 2940          |
|    policy_gradient_loss | -0.000567     |
|    std                  | 1.68          |
|    value_loss           | 3e+03         |
-------------------------------------------
Eval num_timesteps=606000, episode_reward=1867.89 +/- 666.44
Episode length: 439.40 +/- 130.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 1.87e+03      |
| time/                   |               |
|    total_timesteps      | 606000        |
| train/                  |               |
|    approx_kl            | 0.00024205848 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.74         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+03      |
|    n_updates            | 2950          |
|    policy_gradient_loss | -0.00031      |
|    std                  | 1.68          |
|    value_loss           | 2.58e+03      |
-------------------------------------------
Eval num_timesteps=608000, episode_reward=2397.86 +/- 772.73
Episode length: 566.60 +/- 88.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 567           |
|    mean_reward          | 2.4e+03       |
| time/                   |               |
|    total_timesteps      | 608000        |
| train/                  |               |
|    approx_kl            | 0.00022834694 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.75         |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 2960          |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.68          |
|    value_loss           | 2.68e+03      |
-------------------------------------------
Eval num_timesteps=610000, episode_reward=2204.94 +/- 211.81
Episode length: 505.40 +/- 14.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | 2.2e+03     |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.001045888 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+03    |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00126    |
|    std                  | 1.69        |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=2161.40 +/- 500.12
Episode length: 540.40 +/- 155.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | 2.16e+03     |
| time/                   |              |
|    total_timesteps      | 612000       |
| train/                  |              |
|    approx_kl            | 0.0005478321 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+03     |
|    n_updates            | 2980         |
|    policy_gradient_loss | -7.38e-05    |
|    std                  | 1.69         |
|    value_loss           | 2.13e+03     |
------------------------------------------
Eval num_timesteps=614000, episode_reward=3041.55 +/- 2252.34
Episode length: 480.20 +/- 139.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 3.04e+03     |
| time/                   |              |
|    total_timesteps      | 614000       |
| train/                  |              |
|    approx_kl            | 0.0005124857 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+03      |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.7          |
|    value_loss           | 2.88e+03     |
------------------------------------------
Eval num_timesteps=616000, episode_reward=2531.97 +/- 712.94
Episode length: 563.00 +/- 77.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 563          |
|    mean_reward          | 2.53e+03     |
| time/                   |              |
|    total_timesteps      | 616000       |
| train/                  |              |
|    approx_kl            | 0.0004304891 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.0004      |
|    std                  | 1.71         |
|    value_loss           | 3.13e+03     |
------------------------------------------
Eval num_timesteps=618000, episode_reward=2284.75 +/- 330.67
Episode length: 554.20 +/- 162.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 554           |
|    mean_reward          | 2.28e+03      |
| time/                   |               |
|    total_timesteps      | 618000        |
| train/                  |               |
|    approx_kl            | 0.00018098205 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.82         |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+03      |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.000261     |
|    std                  | 1.72          |
|    value_loss           | 2.8e+03       |
-------------------------------------------
Eval num_timesteps=620000, episode_reward=2144.97 +/- 765.47
Episode length: 476.20 +/- 119.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 2.14e+03      |
| time/                   |               |
|    total_timesteps      | 620000        |
| train/                  |               |
|    approx_kl            | 0.00027666937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.84         |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 3020          |
|    policy_gradient_loss | -0.000483     |
|    std                  | 1.72          |
|    value_loss           | 3.57e+03      |
-------------------------------------------
Eval num_timesteps=622000, episode_reward=2161.03 +/- 655.06
Episode length: 613.60 +/- 100.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 614           |
|    mean_reward          | 2.16e+03      |
| time/                   |               |
|    total_timesteps      | 622000        |
| train/                  |               |
|    approx_kl            | 0.00038600006 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.84         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+03      |
|    n_updates            | 3030          |
|    policy_gradient_loss | -0.000739     |
|    std                  | 1.72          |
|    value_loss           | 4.55e+03      |
-------------------------------------------
Eval num_timesteps=624000, episode_reward=2671.47 +/- 504.32
Episode length: 643.20 +/- 134.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 643           |
|    mean_reward          | 2.67e+03      |
| time/                   |               |
|    total_timesteps      | 624000        |
| train/                  |               |
|    approx_kl            | 0.00019938362 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.86         |
|    explained_variance   | 0.805         |
|    learning_rate        | 0.001         |
|    loss                 | 3.12e+03      |
|    n_updates            | 3040          |
|    policy_gradient_loss | -0.000306     |
|    std                  | 1.73          |
|    value_loss           | 7.15e+03      |
-------------------------------------------
Eval num_timesteps=626000, episode_reward=2178.97 +/- 1459.36
Episode length: 686.60 +/- 176.95
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 687            |
|    mean_reward          | 2.18e+03       |
| time/                   |                |
|    total_timesteps      | 626000         |
| train/                  |                |
|    approx_kl            | 0.000116224954 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.87          |
|    explained_variance   | 0.75           |
|    learning_rate        | 0.001          |
|    loss                 | 4.43e+03       |
|    n_updates            | 3050           |
|    policy_gradient_loss | 1.22e-05       |
|    std                  | 1.73           |
|    value_loss           | 9.08e+03       |
--------------------------------------------
Eval num_timesteps=628000, episode_reward=2084.49 +/- 716.02
Episode length: 496.80 +/- 90.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 2.08e+03     |
| time/                   |              |
|    total_timesteps      | 628000       |
| train/                  |              |
|    approx_kl            | 0.0002684412 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.87        |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+03     |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.000737    |
|    std                  | 1.74         |
|    value_loss           | 3.39e+03     |
------------------------------------------
Eval num_timesteps=630000, episode_reward=2601.78 +/- 982.85
Episode length: 623.80 +/- 118.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 624           |
|    mean_reward          | 2.6e+03       |
| time/                   |               |
|    total_timesteps      | 630000        |
| train/                  |               |
|    approx_kl            | 0.00030736683 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.88         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+03      |
|    n_updates            | 3070          |
|    policy_gradient_loss | -0.000377     |
|    std                  | 1.74          |
|    value_loss           | 3.13e+03      |
-------------------------------------------
Eval num_timesteps=632000, episode_reward=2775.24 +/- 826.58
Episode length: 650.60 +/- 109.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 651           |
|    mean_reward          | 2.78e+03      |
| time/                   |               |
|    total_timesteps      | 632000        |
| train/                  |               |
|    approx_kl            | 0.00016956669 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.88         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+03      |
|    n_updates            | 3080          |
|    policy_gradient_loss | -0.000418     |
|    std                  | 1.74          |
|    value_loss           | 3.72e+03      |
-------------------------------------------
Eval num_timesteps=634000, episode_reward=1932.68 +/- 878.66
Episode length: 493.60 +/- 185.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 494           |
|    mean_reward          | 1.93e+03      |
| time/                   |               |
|    total_timesteps      | 634000        |
| train/                  |               |
|    approx_kl            | 0.00018407326 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.89         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+03      |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.000665     |
|    std                  | 1.74          |
|    value_loss           | 3.52e+03      |
-------------------------------------------
Eval num_timesteps=636000, episode_reward=3322.53 +/- 834.58
Episode length: 853.60 +/- 198.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 854           |
|    mean_reward          | 3.32e+03      |
| time/                   |               |
|    total_timesteps      | 636000        |
| train/                  |               |
|    approx_kl            | 0.00014447194 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.89         |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+03      |
|    n_updates            | 3100          |
|    policy_gradient_loss | -0.000333     |
|    std                  | 1.74          |
|    value_loss           | 3.52e+03      |
-------------------------------------------
Eval num_timesteps=638000, episode_reward=2552.27 +/- 533.68
Episode length: 747.80 +/- 215.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 748           |
|    mean_reward          | 2.55e+03      |
| time/                   |               |
|    total_timesteps      | 638000        |
| train/                  |               |
|    approx_kl            | 0.00021276693 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.9          |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+03      |
|    n_updates            | 3110          |
|    policy_gradient_loss | -0.000412     |
|    std                  | 1.75          |
|    value_loss           | 2.82e+03      |
-------------------------------------------
Eval num_timesteps=640000, episode_reward=877.60 +/- 2203.81
Episode length: 572.60 +/- 109.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 573           |
|    mean_reward          | 878           |
| time/                   |               |
|    total_timesteps      | 640000        |
| train/                  |               |
|    approx_kl            | 0.00013823327 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.9          |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+03      |
|    n_updates            | 3120          |
|    policy_gradient_loss | -0.000262     |
|    std                  | 1.75          |
|    value_loss           | 2.98e+03      |
-------------------------------------------
Eval num_timesteps=642000, episode_reward=1992.36 +/- 1428.30
Episode length: 610.60 +/- 109.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 611          |
|    mean_reward          | 1.99e+03     |
| time/                   |              |
|    total_timesteps      | 642000       |
| train/                  |              |
|    approx_kl            | 7.718263e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.91        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+03     |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.000101    |
|    std                  | 1.75         |
|    value_loss           | 2.72e+03     |
------------------------------------------
Eval num_timesteps=644000, episode_reward=1878.73 +/- 351.24
Episode length: 570.00 +/- 145.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 570           |
|    mean_reward          | 1.88e+03      |
| time/                   |               |
|    total_timesteps      | 644000        |
| train/                  |               |
|    approx_kl            | 3.4884142e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.91         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+03      |
|    n_updates            | 3140          |
|    policy_gradient_loss | -7.23e-05     |
|    std                  | 1.75          |
|    value_loss           | 2.91e+03      |
-------------------------------------------
Eval num_timesteps=646000, episode_reward=2602.92 +/- 912.18
Episode length: 631.80 +/- 174.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 632           |
|    mean_reward          | 2.6e+03       |
| time/                   |               |
|    total_timesteps      | 646000        |
| train/                  |               |
|    approx_kl            | 3.6928745e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.92         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 3150          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.75          |
|    value_loss           | 2.71e+03      |
-------------------------------------------
Eval num_timesteps=648000, episode_reward=2308.31 +/- 529.56
Episode length: 581.00 +/- 153.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 581           |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 648000        |
| train/                  |               |
|    approx_kl            | 7.3120435e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.92         |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+03      |
|    n_updates            | 3160          |
|    policy_gradient_loss | -0.000303     |
|    std                  | 1.75          |
|    value_loss           | 3.45e+03      |
-------------------------------------------
Eval num_timesteps=650000, episode_reward=1418.68 +/- 446.27
Episode length: 598.00 +/- 272.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 598           |
|    mean_reward          | 1.42e+03      |
| time/                   |               |
|    total_timesteps      | 650000        |
| train/                  |               |
|    approx_kl            | 0.00024111188 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.92         |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+03      |
|    n_updates            | 3170          |
|    policy_gradient_loss | -0.000658     |
|    std                  | 1.76          |
|    value_loss           | 4.47e+03      |
-------------------------------------------
Eval num_timesteps=652000, episode_reward=2558.04 +/- 435.65
Episode length: 593.40 +/- 84.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 593           |
|    mean_reward          | 2.56e+03      |
| time/                   |               |
|    total_timesteps      | 652000        |
| train/                  |               |
|    approx_kl            | 0.00027702216 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.93         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 3180          |
|    policy_gradient_loss | -0.000508     |
|    std                  | 1.77          |
|    value_loss           | 2.9e+03       |
-------------------------------------------
Eval num_timesteps=654000, episode_reward=2433.33 +/- 735.70
Episode length: 526.20 +/- 97.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 526           |
|    mean_reward          | 2.43e+03      |
| time/                   |               |
|    total_timesteps      | 654000        |
| train/                  |               |
|    approx_kl            | 0.00017414609 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.95         |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+03      |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.00059      |
|    std                  | 1.77          |
|    value_loss           | 2.92e+03      |
-------------------------------------------
Eval num_timesteps=656000, episode_reward=2025.42 +/- 605.40
Episode length: 465.20 +/- 129.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 465           |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 656000        |
| train/                  |               |
|    approx_kl            | 0.00015774718 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.97         |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 3.87e+03      |
|    n_updates            | 3200          |
|    policy_gradient_loss | -0.000357     |
|    std                  | 1.78          |
|    value_loss           | 7.96e+03      |
-------------------------------------------
Eval num_timesteps=658000, episode_reward=2578.54 +/- 786.03
Episode length: 646.60 +/- 79.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 647           |
|    mean_reward          | 2.58e+03      |
| time/                   |               |
|    total_timesteps      | 658000        |
| train/                  |               |
|    approx_kl            | 0.00021428551 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.99         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 3210          |
|    policy_gradient_loss | -0.000596     |
|    std                  | 1.79          |
|    value_loss           | 2.51e+03      |
-------------------------------------------
Eval num_timesteps=660000, episode_reward=2397.05 +/- 501.98
Episode length: 502.40 +/- 78.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 502           |
|    mean_reward          | 2.4e+03       |
| time/                   |               |
|    total_timesteps      | 660000        |
| train/                  |               |
|    approx_kl            | 0.00046966554 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 3220          |
|    policy_gradient_loss | -0.000493     |
|    std                  | 1.79          |
|    value_loss           | 2.23e+03      |
-------------------------------------------
Eval num_timesteps=662000, episode_reward=1645.74 +/- 1153.78
Episode length: 522.20 +/- 97.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | 1.65e+03     |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0002037157 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 801          |
|    n_updates            | 3230         |
|    policy_gradient_loss | -0.000141    |
|    std                  | 1.79         |
|    value_loss           | 1.76e+03     |
------------------------------------------
Eval num_timesteps=664000, episode_reward=2115.50 +/- 734.40
Episode length: 445.20 +/- 148.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 445           |
|    mean_reward          | 2.12e+03      |
| time/                   |               |
|    total_timesteps      | 664000        |
| train/                  |               |
|    approx_kl            | 0.00010725067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 3240          |
|    policy_gradient_loss | -0.00016      |
|    std                  | 1.79          |
|    value_loss           | 4.04e+03      |
-------------------------------------------
Eval num_timesteps=666000, episode_reward=1133.17 +/- 1460.75
Episode length: 512.20 +/- 95.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 512           |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 666000        |
| train/                  |               |
|    approx_kl            | 0.00012545206 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+03      |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.000425     |
|    std                  | 1.79          |
|    value_loss           | 4.51e+03      |
-------------------------------------------
Eval num_timesteps=668000, episode_reward=2027.37 +/- 1641.39
Episode length: 597.00 +/- 35.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 597           |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 668000        |
| train/                  |               |
|    approx_kl            | 0.00034768396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 3260          |
|    policy_gradient_loss | -0.000661     |
|    std                  | 1.79          |
|    value_loss           | 2.44e+03      |
-------------------------------------------
Eval num_timesteps=670000, episode_reward=1727.83 +/- 1807.57
Episode length: 483.20 +/- 189.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 1.73e+03      |
| time/                   |               |
|    total_timesteps      | 670000        |
| train/                  |               |
|    approx_kl            | 0.00018759217 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 794           |
|    n_updates            | 3270          |
|    policy_gradient_loss | 0.000107      |
|    std                  | 1.79          |
|    value_loss           | 1.79e+03      |
-------------------------------------------
Eval num_timesteps=672000, episode_reward=2001.71 +/- 163.89
Episode length: 472.60 +/- 39.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 473           |
|    mean_reward          | 2e+03         |
| time/                   |               |
|    total_timesteps      | 672000        |
| train/                  |               |
|    approx_kl            | 3.1602016e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+03      |
|    n_updates            | 3280          |
|    policy_gradient_loss | -8.7e-05      |
|    std                  | 1.79          |
|    value_loss           | 3.42e+03      |
-------------------------------------------
Eval num_timesteps=674000, episode_reward=2309.79 +/- 848.75
Episode length: 436.40 +/- 140.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 674000        |
| train/                  |               |
|    approx_kl            | 0.00012490831 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8            |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+03      |
|    n_updates            | 3290          |
|    policy_gradient_loss | -0.000369     |
|    std                  | 1.79          |
|    value_loss           | 3.42e+03      |
-------------------------------------------
Eval num_timesteps=676000, episode_reward=2951.00 +/- 1192.57
Episode length: 608.60 +/- 168.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 609           |
|    mean_reward          | 2.95e+03      |
| time/                   |               |
|    total_timesteps      | 676000        |
| train/                  |               |
|    approx_kl            | 0.00015222767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+03      |
|    n_updates            | 3300          |
|    policy_gradient_loss | -0.000102     |
|    std                  | 1.8           |
|    value_loss           | 2.92e+03      |
-------------------------------------------
Eval num_timesteps=678000, episode_reward=2455.44 +/- 615.39
Episode length: 493.80 +/- 60.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | 2.46e+03     |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 9.554395e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.000337    |
|    std                  | 1.79         |
|    value_loss           | 3.38e+03     |
------------------------------------------
Eval num_timesteps=680000, episode_reward=1049.99 +/- 1996.81
Episode length: 599.00 +/- 82.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 599          |
|    mean_reward          | 1.05e+03     |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0001030702 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.694        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+04     |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.000294    |
|    std                  | 1.79         |
|    value_loss           | 2.95e+04     |
------------------------------------------
Eval num_timesteps=682000, episode_reward=1937.69 +/- 518.05
Episode length: 547.80 +/- 39.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 548           |
|    mean_reward          | 1.94e+03      |
| time/                   |               |
|    total_timesteps      | 682000        |
| train/                  |               |
|    approx_kl            | 2.4502893e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 975           |
|    n_updates            | 3330          |
|    policy_gradient_loss | -6.66e-05     |
|    std                  | 1.79          |
|    value_loss           | 2.11e+03      |
-------------------------------------------
Eval num_timesteps=684000, episode_reward=2342.69 +/- 189.38
Episode length: 565.00 +/- 97.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=686000, episode_reward=2140.32 +/- 591.12
Episode length: 549.80 +/- 49.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 550           |
|    mean_reward          | 2.14e+03      |
| time/                   |               |
|    total_timesteps      | 686000        |
| train/                  |               |
|    approx_kl            | 2.8908311e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+03      |
|    n_updates            | 3340          |
|    policy_gradient_loss | -0.000233     |
|    std                  | 1.79          |
|    value_loss           | 3.51e+03      |
-------------------------------------------
Eval num_timesteps=688000, episode_reward=2643.72 +/- 539.81
Episode length: 521.40 +/- 84.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 521           |
|    mean_reward          | 2.64e+03      |
| time/                   |               |
|    total_timesteps      | 688000        |
| train/                  |               |
|    approx_kl            | 0.00010262657 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+03      |
|    n_updates            | 3350          |
|    policy_gradient_loss | -0.000381     |
|    std                  | 1.79          |
|    value_loss           | 2.98e+03      |
-------------------------------------------
Eval num_timesteps=690000, episode_reward=1729.64 +/- 1017.20
Episode length: 467.20 +/- 86.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 1.73e+03      |
| time/                   |               |
|    total_timesteps      | 690000        |
| train/                  |               |
|    approx_kl            | 0.00014176196 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.01         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 6.26e+03      |
|    n_updates            | 3360          |
|    policy_gradient_loss | -0.000425     |
|    std                  | 1.8           |
|    value_loss           | 1.32e+04      |
-------------------------------------------
Eval num_timesteps=692000, episode_reward=2322.44 +/- 1074.22
Episode length: 523.20 +/- 121.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 523           |
|    mean_reward          | 2.32e+03      |
| time/                   |               |
|    total_timesteps      | 692000        |
| train/                  |               |
|    approx_kl            | 0.00015885249 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.02         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 6.21e+03      |
|    n_updates            | 3370          |
|    policy_gradient_loss | -0.000271     |
|    std                  | 1.8           |
|    value_loss           | 1.31e+04      |
-------------------------------------------
Eval num_timesteps=694000, episode_reward=2095.07 +/- 581.28
Episode length: 468.60 +/- 87.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 2.1e+03      |
| time/                   |              |
|    total_timesteps      | 694000       |
| train/                  |              |
|    approx_kl            | 7.680876e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+03     |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.00025     |
|    std                  | 1.8          |
|    value_loss           | 2.45e+03     |
------------------------------------------
Eval num_timesteps=696000, episode_reward=2434.66 +/- 480.11
Episode length: 562.00 +/- 55.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 562          |
|    mean_reward          | 2.43e+03     |
| time/                   |              |
|    total_timesteps      | 696000       |
| train/                  |              |
|    approx_kl            | 5.155173e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.001        |
|    loss                 | 4.8e+03      |
|    n_updates            | 3390         |
|    policy_gradient_loss | 1.92e-05     |
|    std                  | 1.8          |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=698000, episode_reward=2064.66 +/- 402.98
Episode length: 516.40 +/- 30.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 2.06e+03     |
| time/                   |              |
|    total_timesteps      | 698000       |
| train/                  |              |
|    approx_kl            | 2.554385e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+03     |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.8          |
|    value_loss           | 4.32e+03     |
------------------------------------------
Eval num_timesteps=700000, episode_reward=2236.13 +/- 1039.76
Episode length: 575.00 +/- 69.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 575           |
|    mean_reward          | 2.24e+03      |
| time/                   |               |
|    total_timesteps      | 700000        |
| train/                  |               |
|    approx_kl            | 8.5138774e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+03      |
|    n_updates            | 3410          |
|    policy_gradient_loss | -0.000294     |
|    std                  | 1.81          |
|    value_loss           | 2.49e+03      |
-------------------------------------------
Eval num_timesteps=702000, episode_reward=2684.12 +/- 290.02
Episode length: 562.40 +/- 59.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 562           |
|    mean_reward          | 2.68e+03      |
| time/                   |               |
|    total_timesteps      | 702000        |
| train/                  |               |
|    approx_kl            | 0.00010984714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.001         |
|    loss                 | 4.47e+03      |
|    n_updates            | 3420          |
|    policy_gradient_loss | -0.000246     |
|    std                  | 1.81          |
|    value_loss           | 9.62e+03      |
-------------------------------------------
Eval num_timesteps=704000, episode_reward=2648.40 +/- 462.09
Episode length: 540.80 +/- 57.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 541           |
|    mean_reward          | 2.65e+03      |
| time/                   |               |
|    total_timesteps      | 704000        |
| train/                  |               |
|    approx_kl            | 3.9603066e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 3.68e+03      |
|    n_updates            | 3430          |
|    policy_gradient_loss | -0.000198     |
|    std                  | 1.81          |
|    value_loss           | 8.04e+03      |
-------------------------------------------
Eval num_timesteps=706000, episode_reward=2158.32 +/- 553.41
Episode length: 445.60 +/- 114.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 2.16e+03      |
| time/                   |               |
|    total_timesteps      | 706000        |
| train/                  |               |
|    approx_kl            | 0.00015982543 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+03      |
|    n_updates            | 3440          |
|    policy_gradient_loss | -0.000593     |
|    std                  | 1.81          |
|    value_loss           | 2.36e+03      |
-------------------------------------------
Eval num_timesteps=708000, episode_reward=2314.78 +/- 790.86
Episode length: 512.20 +/- 54.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 512           |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 708000        |
| train/                  |               |
|    approx_kl            | 0.00018098133 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.000198     |
|    std                  | 1.81          |
|    value_loss           | 3.22e+03      |
-------------------------------------------
Eval num_timesteps=710000, episode_reward=2238.75 +/- 454.49
Episode length: 497.00 +/- 90.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 2.24e+03     |
| time/                   |              |
|    total_timesteps      | 710000       |
| train/                  |              |
|    approx_kl            | 7.938259e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+03      |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.000278    |
|    std                  | 1.81         |
|    value_loss           | 3.11e+03     |
------------------------------------------
Eval num_timesteps=712000, episode_reward=2652.56 +/- 859.74
Episode length: 513.20 +/- 136.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 513           |
|    mean_reward          | 2.65e+03      |
| time/                   |               |
|    total_timesteps      | 712000        |
| train/                  |               |
|    approx_kl            | 7.5060816e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.001         |
|    loss                 | 5.79e+03      |
|    n_updates            | 3470          |
|    policy_gradient_loss | 4.63e-06      |
|    std                  | 1.8           |
|    value_loss           | 1.27e+04      |
-------------------------------------------
Eval num_timesteps=714000, episode_reward=3094.43 +/- 536.47
Episode length: 608.40 +/- 42.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 608          |
|    mean_reward          | 3.09e+03     |
| time/                   |              |
|    total_timesteps      | 714000       |
| train/                  |              |
|    approx_kl            | 3.733643e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 1.79e+03     |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.000251    |
|    std                  | 1.8          |
|    value_loss           | 3.94e+03     |
------------------------------------------
Eval num_timesteps=716000, episode_reward=2554.15 +/- 373.47
Episode length: 513.60 +/- 69.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 514           |
|    mean_reward          | 2.55e+03      |
| time/                   |               |
|    total_timesteps      | 716000        |
| train/                  |               |
|    approx_kl            | 4.7897047e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+03      |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.000107     |
|    std                  | 1.8           |
|    value_loss           | 3.8e+03       |
-------------------------------------------
Eval num_timesteps=718000, episode_reward=1912.35 +/- 1425.37
Episode length: 555.60 +/- 138.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 556           |
|    mean_reward          | 1.91e+03      |
| time/                   |               |
|    total_timesteps      | 718000        |
| train/                  |               |
|    approx_kl            | 7.2120194e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 843           |
|    n_updates            | 3500          |
|    policy_gradient_loss | -0.000209     |
|    std                  | 1.81          |
|    value_loss           | 1.95e+03      |
-------------------------------------------
Eval num_timesteps=720000, episode_reward=2772.89 +/- 537.75
Episode length: 535.40 +/- 79.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 535         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.000133895 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 1.19e+03    |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.000448   |
|    std                  | 1.81        |
|    value_loss           | 2.55e+03    |
-----------------------------------------
Eval num_timesteps=722000, episode_reward=2912.92 +/- 815.86
Episode length: 579.20 +/- 82.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 579           |
|    mean_reward          | 2.91e+03      |
| time/                   |               |
|    total_timesteps      | 722000        |
| train/                  |               |
|    approx_kl            | 0.00013540662 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.05         |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 3.91e+03      |
|    n_updates            | 3520          |
|    policy_gradient_loss | -0.000203     |
|    std                  | 1.81          |
|    value_loss           | 8.24e+03      |
-------------------------------------------
Eval num_timesteps=724000, episode_reward=2931.24 +/- 967.19
Episode length: 551.00 +/- 127.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 551           |
|    mean_reward          | 2.93e+03      |
| time/                   |               |
|    total_timesteps      | 724000        |
| train/                  |               |
|    approx_kl            | 3.1218427e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.05         |
|    explained_variance   | 0.738         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 3530          |
|    policy_gradient_loss | 0.000107      |
|    std                  | 1.82          |
|    value_loss           | 3.31e+04      |
-------------------------------------------
Eval num_timesteps=726000, episode_reward=2367.36 +/- 623.98
Episode length: 481.80 +/- 59.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 2.37e+03     |
| time/                   |              |
|    total_timesteps      | 726000       |
| train/                  |              |
|    approx_kl            | 1.896004e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+03     |
|    n_updates            | 3540         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.82         |
|    value_loss           | 5.12e+03     |
------------------------------------------
Eval num_timesteps=728000, episode_reward=2274.74 +/- 559.46
Episode length: 510.20 +/- 52.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 510          |
|    mean_reward          | 2.27e+03     |
| time/                   |              |
|    total_timesteps      | 728000       |
| train/                  |              |
|    approx_kl            | 8.628506e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+03     |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00032     |
|    std                  | 1.82         |
|    value_loss           | 3.56e+03     |
------------------------------------------
Eval num_timesteps=730000, episode_reward=2372.62 +/- 822.90
Episode length: 498.00 +/- 146.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | 2.37e+03      |
| time/                   |               |
|    total_timesteps      | 730000        |
| train/                  |               |
|    approx_kl            | 0.00020666965 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.07         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 972           |
|    n_updates            | 3560          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 1.82          |
|    value_loss           | 2.13e+03      |
-------------------------------------------
Eval num_timesteps=732000, episode_reward=1721.98 +/- 971.01
Episode length: 661.20 +/- 94.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 661          |
|    mean_reward          | 1.72e+03     |
| time/                   |              |
|    total_timesteps      | 732000       |
| train/                  |              |
|    approx_kl            | 0.0008380539 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+03     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.83         |
|    value_loss           | 2.83e+03     |
------------------------------------------
Eval num_timesteps=734000, episode_reward=2931.61 +/- 658.50
Episode length: 623.80 +/- 121.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 624          |
|    mean_reward          | 2.93e+03     |
| time/                   |              |
|    total_timesteps      | 734000       |
| train/                  |              |
|    approx_kl            | 0.0005017831 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 755          |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.000419    |
|    std                  | 1.83         |
|    value_loss           | 1.7e+03      |
------------------------------------------
Eval num_timesteps=736000, episode_reward=2397.18 +/- 939.51
Episode length: 556.60 +/- 111.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 557           |
|    mean_reward          | 2.4e+03       |
| time/                   |               |
|    total_timesteps      | 736000        |
| train/                  |               |
|    approx_kl            | 0.00021029363 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.09         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 877           |
|    n_updates            | 3590          |
|    policy_gradient_loss | -0.000513     |
|    std                  | 1.83          |
|    value_loss           | 2.04e+03      |
-------------------------------------------
Eval num_timesteps=738000, episode_reward=3559.79 +/- 1940.69
Episode length: 572.40 +/- 181.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 572           |
|    mean_reward          | 3.56e+03      |
| time/                   |               |
|    total_timesteps      | 738000        |
| train/                  |               |
|    approx_kl            | 0.00017287015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.1          |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 826           |
|    n_updates            | 3600          |
|    policy_gradient_loss | -0.000263     |
|    std                  | 1.84          |
|    value_loss           | 1.87e+03      |
-------------------------------------------
Eval num_timesteps=740000, episode_reward=2441.28 +/- 907.44
Episode length: 604.20 +/- 109.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 604          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 740000       |
| train/                  |              |
|    approx_kl            | 0.0009543549 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.1         |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 3610         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 1.84         |
|    value_loss           | 2.36e+03     |
------------------------------------------
Eval num_timesteps=742000, episode_reward=2400.72 +/- 834.36
Episode length: 825.00 +/- 203.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 825          |
|    mean_reward          | 2.4e+03      |
| time/                   |              |
|    total_timesteps      | 742000       |
| train/                  |              |
|    approx_kl            | 0.0013641543 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.11        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 986          |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.84         |
|    value_loss           | 2.23e+03     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=2522.85 +/- 585.94
Episode length: 789.40 +/- 245.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 789           |
|    mean_reward          | 2.52e+03      |
| time/                   |               |
|    total_timesteps      | 744000        |
| train/                  |               |
|    approx_kl            | 0.00025962238 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.11         |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+03       |
|    n_updates            | 3630          |
|    policy_gradient_loss | 7.13e-06      |
|    std                  | 1.84          |
|    value_loss           | 3.07e+03      |
-------------------------------------------
Eval num_timesteps=746000, episode_reward=3438.11 +/- 1543.30
Episode length: 893.80 +/- 269.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 894           |
|    mean_reward          | 3.44e+03      |
| time/                   |               |
|    total_timesteps      | 746000        |
| train/                  |               |
|    approx_kl            | 0.00016233406 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.12         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+03      |
|    n_updates            | 3640          |
|    policy_gradient_loss | -0.000431     |
|    std                  | 1.85          |
|    value_loss           | 2.21e+03      |
-------------------------------------------
Eval num_timesteps=748000, episode_reward=3389.39 +/- 802.76
Episode length: 913.80 +/- 269.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 914          |
|    mean_reward          | 3.39e+03     |
| time/                   |              |
|    total_timesteps      | 748000       |
| train/                  |              |
|    approx_kl            | 9.772985e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+03     |
|    n_updates            | 3650         |
|    policy_gradient_loss | -1.56e-05    |
|    std                  | 1.85         |
|    value_loss           | 3.63e+03     |
------------------------------------------
Eval num_timesteps=750000, episode_reward=2824.29 +/- 1023.73
Episode length: 623.20 +/- 187.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 623          |
|    mean_reward          | 2.82e+03     |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 6.975501e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+03     |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000251    |
|    std                  | 1.85         |
|    value_loss           | 4.96e+03     |
------------------------------------------
Eval num_timesteps=752000, episode_reward=2658.51 +/- 913.25
Episode length: 642.00 +/- 137.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 642           |
|    mean_reward          | 2.66e+03      |
| time/                   |               |
|    total_timesteps      | 752000        |
| train/                  |               |
|    approx_kl            | 0.00016115181 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.14         |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 394           |
|    n_updates            | 3670          |
|    policy_gradient_loss | -0.000718     |
|    std                  | 1.85          |
|    value_loss           | 1.05e+03      |
-------------------------------------------
Eval num_timesteps=754000, episode_reward=2345.01 +/- 1375.46
Episode length: 712.20 +/- 291.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 712           |
|    mean_reward          | 2.35e+03      |
| time/                   |               |
|    total_timesteps      | 754000        |
| train/                  |               |
|    approx_kl            | 0.00011873929 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.14         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 985           |
|    n_updates            | 3680          |
|    policy_gradient_loss | 2.74e-05      |
|    std                  | 1.86          |
|    value_loss           | 2.13e+03      |
-------------------------------------------
Eval num_timesteps=756000, episode_reward=3032.06 +/- 1330.67
Episode length: 718.60 +/- 302.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 719          |
|    mean_reward          | 3.03e+03     |
| time/                   |              |
|    total_timesteps      | 756000       |
| train/                  |              |
|    approx_kl            | 8.651879e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 736          |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.000523    |
|    std                  | 1.86         |
|    value_loss           | 1.63e+03     |
------------------------------------------
Eval num_timesteps=758000, episode_reward=2438.47 +/- 675.56
Episode length: 671.20 +/- 114.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 671          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 758000       |
| train/                  |              |
|    approx_kl            | 0.0001382867 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 731          |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.000593    |
|    std                  | 1.86         |
|    value_loss           | 1.81e+03     |
------------------------------------------
Eval num_timesteps=760000, episode_reward=2758.43 +/- 749.09
Episode length: 812.40 +/- 104.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | 2.76e+03     |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0001060816 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.16        |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.001        |
|    loss                 | 4.76e+03     |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.000117    |
|    std                  | 1.87         |
|    value_loss           | 1.08e+04     |
------------------------------------------
Eval num_timesteps=762000, episode_reward=1461.33 +/- 299.70
Episode length: 425.00 +/- 130.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 1.46e+03     |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 9.466274e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.17        |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+03     |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000114    |
|    std                  | 1.87         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=764000, episode_reward=2865.76 +/- 1589.62
Episode length: 731.80 +/- 287.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 732           |
|    mean_reward          | 2.87e+03      |
| time/                   |               |
|    total_timesteps      | 764000        |
| train/                  |               |
|    approx_kl            | 4.0650164e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.18         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 605           |
|    n_updates            | 3730          |
|    policy_gradient_loss | -0.000315     |
|    std                  | 1.87          |
|    value_loss           | 1.7e+03       |
-------------------------------------------
Eval num_timesteps=766000, episode_reward=1867.65 +/- 487.75
Episode length: 594.60 +/- 250.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 595         |
|    mean_reward          | 1.87e+03    |
| time/                   |             |
|    total_timesteps      | 766000      |
| train/                  |             |
|    approx_kl            | 6.88741e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.18       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 731         |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00031    |
|    std                  | 1.87        |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=768000, episode_reward=2000.32 +/- 513.75
Episode length: 499.80 +/- 104.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=1848.45 +/- 1139.88
Episode length: 774.00 +/- 373.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 774          |
|    mean_reward          | 1.85e+03     |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 6.541677e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 768          |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.000252    |
|    std                  | 1.87         |
|    value_loss           | 1.85e+03     |
------------------------------------------
Eval num_timesteps=772000, episode_reward=4483.55 +/- 3406.72
Episode length: 865.20 +/- 333.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 865          |
|    mean_reward          | 4.48e+03     |
| time/                   |              |
|    total_timesteps      | 772000       |
| train/                  |              |
|    approx_kl            | 0.0001130655 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 4.59e+03     |
|    n_updates            | 3760         |
|    policy_gradient_loss | -0.000112    |
|    std                  | 1.88         |
|    value_loss           | 9.84e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=774000, episode_reward=3112.00 +/- 1308.66
Episode length: 773.60 +/- 171.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 774           |
|    mean_reward          | 3.11e+03      |
| time/                   |               |
|    total_timesteps      | 774000        |
| train/                  |               |
|    approx_kl            | 7.0355774e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.19         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+03       |
|    n_updates            | 3770          |
|    policy_gradient_loss | -7.44e-05     |
|    std                  | 1.88          |
|    value_loss           | 3.33e+03      |
-------------------------------------------
Eval num_timesteps=776000, episode_reward=2706.27 +/- 2877.22
Episode length: 957.40 +/- 289.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 957          |
|    mean_reward          | 2.71e+03     |
| time/                   |              |
|    total_timesteps      | 776000       |
| train/                  |              |
|    approx_kl            | 4.966781e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+03     |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.000209    |
|    std                  | 1.88         |
|    value_loss           | 3.21e+03     |
------------------------------------------
Eval num_timesteps=778000, episode_reward=2774.61 +/- 1445.50
Episode length: 915.40 +/- 299.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 915          |
|    mean_reward          | 2.77e+03     |
| time/                   |              |
|    total_timesteps      | 778000       |
| train/                  |              |
|    approx_kl            | 9.129589e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 566          |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.000329    |
|    std                  | 1.88         |
|    value_loss           | 1.36e+03     |
------------------------------------------
Eval num_timesteps=780000, episode_reward=2563.11 +/- 628.25
Episode length: 900.80 +/- 209.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 901          |
|    mean_reward          | 2.56e+03     |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0001169882 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 477          |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000399    |
|    std                  | 1.88         |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=782000, episode_reward=2798.90 +/- 751.67
Episode length: 797.00 +/- 166.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 797          |
|    mean_reward          | 2.8e+03      |
| time/                   |              |
|    total_timesteps      | 782000       |
| train/                  |              |
|    approx_kl            | 0.0001668217 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.659        |
|    learning_rate        | 0.001        |
|    loss                 | 4.68e+03     |
|    n_updates            | 3810         |
|    policy_gradient_loss | -3.56e-05    |
|    std                  | 1.88         |
|    value_loss           | 9.77e+03     |
------------------------------------------
Eval num_timesteps=784000, episode_reward=3896.54 +/- 3371.83
Episode length: 822.20 +/- 105.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 822          |
|    mean_reward          | 3.9e+03      |
| time/                   |              |
|    total_timesteps      | 784000       |
| train/                  |              |
|    approx_kl            | 2.522496e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.001        |
|    loss                 | 9.1e+03      |
|    n_updates            | 3820         |
|    policy_gradient_loss | 1.01e-05     |
|    std                  | 1.88         |
|    value_loss           | 2.63e+04     |
------------------------------------------
Eval num_timesteps=786000, episode_reward=2735.25 +/- 576.07
Episode length: 828.80 +/- 310.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 829         |
|    mean_reward          | 2.74e+03    |
| time/                   |             |
|    total_timesteps      | 786000      |
| train/                  |             |
|    approx_kl            | 9.81192e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.001       |
|    loss                 | 1.77e+03    |
|    n_updates            | 3830        |
|    policy_gradient_loss | -8.4e-05    |
|    std                  | 1.88        |
|    value_loss           | 4.41e+03    |
-----------------------------------------
Eval num_timesteps=788000, episode_reward=1004.61 +/- 2161.30
Episode length: 752.20 +/- 292.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 752           |
|    mean_reward          | 1e+03         |
| time/                   |               |
|    total_timesteps      | 788000        |
| train/                  |               |
|    approx_kl            | 2.0280306e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.2          |
|    explained_variance   | 0.759         |
|    learning_rate        | 0.001         |
|    loss                 | 8.2e+03       |
|    n_updates            | 3840          |
|    policy_gradient_loss | -5.83e-05     |
|    std                  | 1.88          |
|    value_loss           | 1.71e+04      |
-------------------------------------------
Eval num_timesteps=790000, episode_reward=2824.10 +/- 1446.78
Episode length: 906.80 +/- 350.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 907           |
|    mean_reward          | 2.82e+03      |
| time/                   |               |
|    total_timesteps      | 790000        |
| train/                  |               |
|    approx_kl            | 4.4528017e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.2          |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 796           |
|    n_updates            | 3850          |
|    policy_gradient_loss | -0.000284     |
|    std                  | 1.88          |
|    value_loss           | 1.74e+03      |
-------------------------------------------
Eval num_timesteps=792000, episode_reward=3124.72 +/- 1219.59
Episode length: 891.40 +/- 361.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 891           |
|    mean_reward          | 3.12e+03      |
| time/                   |               |
|    total_timesteps      | 792000        |
| train/                  |               |
|    approx_kl            | 2.3252767e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.2          |
|    explained_variance   | 0.834         |
|    learning_rate        | 0.001         |
|    loss                 | 6.77e+03      |
|    n_updates            | 3860          |
|    policy_gradient_loss | -5.18e-05     |
|    std                  | 1.88          |
|    value_loss           | 1.54e+04      |
-------------------------------------------
Eval num_timesteps=794000, episode_reward=2302.94 +/- 541.69
Episode length: 726.40 +/- 188.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 726           |
|    mean_reward          | 2.3e+03       |
| time/                   |               |
|    total_timesteps      | 794000        |
| train/                  |               |
|    approx_kl            | 4.3935142e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.2          |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 501           |
|    n_updates            | 3870          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.88          |
|    value_loss           | 1.19e+03      |
-------------------------------------------
Eval num_timesteps=796000, episode_reward=2440.21 +/- 676.43
Episode length: 758.80 +/- 265.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 759          |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 796000       |
| train/                  |              |
|    approx_kl            | 9.747219e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+03     |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00019     |
|    std                  | 1.89         |
|    value_loss           | 4.1e+03      |
------------------------------------------
Eval num_timesteps=798000, episode_reward=2659.88 +/- 1325.65
Episode length: 642.80 +/- 195.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 643          |
|    mean_reward          | 2.66e+03     |
| time/                   |              |
|    total_timesteps      | 798000       |
| train/                  |              |
|    approx_kl            | 3.696882e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+03     |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.000165    |
|    std                  | 1.89         |
|    value_loss           | 2.99e+03     |
------------------------------------------
Eval num_timesteps=800000, episode_reward=3151.69 +/- 1266.85
Episode length: 837.60 +/- 207.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 838          |
|    mean_reward          | 3.15e+03     |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 8.590464e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.23        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+03      |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.000207    |
|    std                  | 1.9          |
|    value_loss           | 3.6e+03      |
------------------------------------------
Eval num_timesteps=802000, episode_reward=2960.78 +/- 1184.47
Episode length: 778.60 +/- 330.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 779           |
|    mean_reward          | 2.96e+03      |
| time/                   |               |
|    total_timesteps      | 802000        |
| train/                  |               |
|    approx_kl            | 6.3433516e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.24         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+03      |
|    n_updates            | 3910          |
|    policy_gradient_loss | -0.000123     |
|    std                  | 1.9           |
|    value_loss           | 9.32e+03      |
-------------------------------------------
Eval num_timesteps=804000, episode_reward=2617.41 +/- 510.71
Episode length: 795.20 +/- 332.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 795           |
|    mean_reward          | 2.62e+03      |
| time/                   |               |
|    total_timesteps      | 804000        |
| train/                  |               |
|    approx_kl            | 4.7499983e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.24         |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 3920          |
|    policy_gradient_loss | -0.000217     |
|    std                  | 1.91          |
|    value_loss           | 4.09e+03      |
-------------------------------------------
Eval num_timesteps=806000, episode_reward=3201.08 +/- 1311.16
Episode length: 875.80 +/- 147.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 876           |
|    mean_reward          | 3.2e+03       |
| time/                   |               |
|    total_timesteps      | 806000        |
| train/                  |               |
|    approx_kl            | 3.2140815e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.25         |
|    explained_variance   | 0.622         |
|    learning_rate        | 0.001         |
|    loss                 | 6.37e+03      |
|    n_updates            | 3930          |
|    policy_gradient_loss | 1.02e-05      |
|    std                  | 1.91          |
|    value_loss           | 1.57e+04      |
-------------------------------------------
Eval num_timesteps=808000, episode_reward=2868.04 +/- 930.95
Episode length: 793.40 +/- 143.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 793           |
|    mean_reward          | 2.87e+03      |
| time/                   |               |
|    total_timesteps      | 808000        |
| train/                  |               |
|    approx_kl            | 2.5227113e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.26         |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 4.78e+03      |
|    n_updates            | 3940          |
|    policy_gradient_loss | -0.0002       |
|    std                  | 1.91          |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=810000, episode_reward=3186.43 +/- 1385.11
Episode length: 803.80 +/- 187.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 804           |
|    mean_reward          | 3.19e+03      |
| time/                   |               |
|    total_timesteps      | 810000        |
| train/                  |               |
|    approx_kl            | 2.2892928e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 8.74e+03      |
|    n_updates            | 3950          |
|    policy_gradient_loss | -1.4e-06      |
|    std                  | 1.92          |
|    value_loss           | 1.93e+04      |
-------------------------------------------
Eval num_timesteps=812000, episode_reward=3105.93 +/- 1118.15
Episode length: 751.60 +/- 228.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 752           |
|    mean_reward          | 3.11e+03      |
| time/                   |               |
|    total_timesteps      | 812000        |
| train/                  |               |
|    approx_kl            | 2.2593042e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.602         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+04      |
|    n_updates            | 3960          |
|    policy_gradient_loss | -0.000171     |
|    std                  | 1.92          |
|    value_loss           | 2.2e+04       |
-------------------------------------------
Eval num_timesteps=814000, episode_reward=3050.97 +/- 2547.46
Episode length: 851.60 +/- 351.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 852           |
|    mean_reward          | 3.05e+03      |
| time/                   |               |
|    total_timesteps      | 814000        |
| train/                  |               |
|    approx_kl            | 0.00015381176 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 842           |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.000715     |
|    std                  | 1.92          |
|    value_loss           | 2.03e+03      |
-------------------------------------------
Eval num_timesteps=816000, episode_reward=4673.56 +/- 3810.40
Episode length: 865.00 +/- 306.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 865          |
|    mean_reward          | 4.67e+03     |
| time/                   |              |
|    total_timesteps      | 816000       |
| train/                  |              |
|    approx_kl            | 0.0003060501 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 8.59e+03     |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.000243    |
|    std                  | 1.93         |
|    value_loss           | 1.76e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=818000, episode_reward=3448.28 +/- 1448.30
Episode length: 954.40 +/- 229.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 954           |
|    mean_reward          | 3.45e+03      |
| time/                   |               |
|    total_timesteps      | 818000        |
| train/                  |               |
|    approx_kl            | 0.00014585545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.3          |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 347           |
|    n_updates            | 3990          |
|    policy_gradient_loss | -0.000761     |
|    std                  | 1.93          |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=820000, episode_reward=3988.70 +/- 3781.21
Episode length: 745.20 +/- 152.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 745           |
|    mean_reward          | 3.99e+03      |
| time/                   |               |
|    total_timesteps      | 820000        |
| train/                  |               |
|    approx_kl            | 0.00011835608 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 560           |
|    n_updates            | 4000          |
|    policy_gradient_loss | -0.000422     |
|    std                  | 1.94          |
|    value_loss           | 1.44e+03      |
-------------------------------------------
Eval num_timesteps=822000, episode_reward=3086.54 +/- 2139.74
Episode length: 735.20 +/- 265.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 735          |
|    mean_reward          | 3.09e+03     |
| time/                   |              |
|    total_timesteps      | 822000       |
| train/                  |              |
|    approx_kl            | 5.459602e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.31        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 2.4e+03      |
|    n_updates            | 4010         |
|    policy_gradient_loss | -7.43e-05    |
|    std                  | 1.94         |
|    value_loss           | 5.33e+03     |
------------------------------------------
Eval num_timesteps=824000, episode_reward=2730.95 +/- 523.83
Episode length: 805.60 +/- 118.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 806           |
|    mean_reward          | 2.73e+03      |
| time/                   |               |
|    total_timesteps      | 824000        |
| train/                  |               |
|    approx_kl            | 4.3037347e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+03      |
|    n_updates            | 4020          |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.94          |
|    value_loss           | 2.32e+03      |
-------------------------------------------
Eval num_timesteps=826000, episode_reward=3456.24 +/- 1799.75
Episode length: 761.40 +/- 147.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 761           |
|    mean_reward          | 3.46e+03      |
| time/                   |               |
|    total_timesteps      | 826000        |
| train/                  |               |
|    approx_kl            | 5.5078795e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 901           |
|    n_updates            | 4030          |
|    policy_gradient_loss | -0.000335     |
|    std                  | 1.94          |
|    value_loss           | 2.03e+03      |
-------------------------------------------
Eval num_timesteps=828000, episode_reward=2530.42 +/- 797.28
Episode length: 813.00 +/- 172.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 813           |
|    mean_reward          | 2.53e+03      |
| time/                   |               |
|    total_timesteps      | 828000        |
| train/                  |               |
|    approx_kl            | 6.1014696e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 736           |
|    n_updates            | 4040          |
|    policy_gradient_loss | -0.000119     |
|    std                  | 1.94          |
|    value_loss           | 1.63e+03      |
-------------------------------------------
Eval num_timesteps=830000, episode_reward=2464.04 +/- 1095.55
Episode length: 660.00 +/- 138.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 660           |
|    mean_reward          | 2.46e+03      |
| time/                   |               |
|    total_timesteps      | 830000        |
| train/                  |               |
|    approx_kl            | 5.0161703e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.852         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 4050          |
|    policy_gradient_loss | -0.000219     |
|    std                  | 1.94          |
|    value_loss           | 3.64e+03      |
-------------------------------------------
Eval num_timesteps=832000, episode_reward=2716.23 +/- 2585.87
Episode length: 672.00 +/- 354.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 672           |
|    mean_reward          | 2.72e+03      |
| time/                   |               |
|    total_timesteps      | 832000        |
| train/                  |               |
|    approx_kl            | 3.1973177e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.001         |
|    loss                 | 3.61e+03      |
|    n_updates            | 4060          |
|    policy_gradient_loss | 3.35e-05      |
|    std                  | 1.94          |
|    value_loss           | 9.55e+03      |
-------------------------------------------
Eval num_timesteps=834000, episode_reward=1831.16 +/- 1734.02
Episode length: 815.40 +/- 183.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 815           |
|    mean_reward          | 1.83e+03      |
| time/                   |               |
|    total_timesteps      | 834000        |
| train/                  |               |
|    approx_kl            | 6.8215013e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.31         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 642           |
|    n_updates            | 4070          |
|    policy_gradient_loss | -6.82e-05     |
|    std                  | 1.94          |
|    value_loss           | 1.66e+03      |
-------------------------------------------
Eval num_timesteps=836000, episode_reward=4054.17 +/- 3800.34
Episode length: 736.20 +/- 263.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 736           |
|    mean_reward          | 4.05e+03      |
| time/                   |               |
|    total_timesteps      | 836000        |
| train/                  |               |
|    approx_kl            | 3.5054545e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.32         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 429           |
|    n_updates            | 4080          |
|    policy_gradient_loss | -0.00024      |
|    std                  | 1.94          |
|    value_loss           | 974           |
-------------------------------------------
Eval num_timesteps=838000, episode_reward=2721.46 +/- 617.30
Episode length: 755.40 +/- 167.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 755           |
|    mean_reward          | 2.72e+03      |
| time/                   |               |
|    total_timesteps      | 838000        |
| train/                  |               |
|    approx_kl            | 0.00016136939 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.32         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 593           |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.000397     |
|    std                  | 1.95          |
|    value_loss           | 1.63e+03      |
-------------------------------------------
Eval num_timesteps=840000, episode_reward=3679.67 +/- 2862.97
Episode length: 705.00 +/- 151.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 705          |
|    mean_reward          | 3.68e+03     |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 9.676223e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.33        |
|    explained_variance   | 0.759        |
|    learning_rate        | 0.001        |
|    loss                 | 4.91e+03     |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.000358    |
|    std                  | 1.95         |
|    value_loss           | 1.34e+04     |
------------------------------------------
Eval num_timesteps=842000, episode_reward=3520.00 +/- 2754.51
Episode length: 605.20 +/- 224.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 605           |
|    mean_reward          | 3.52e+03      |
| time/                   |               |
|    total_timesteps      | 842000        |
| train/                  |               |
|    approx_kl            | 2.1570246e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.34         |
|    explained_variance   | 0.856         |
|    learning_rate        | 0.001         |
|    loss                 | 2.13e+03      |
|    n_updates            | 4110          |
|    policy_gradient_loss | 2.05e-06      |
|    std                  | 1.95          |
|    value_loss           | 4.78e+03      |
-------------------------------------------
Eval num_timesteps=844000, episode_reward=3613.37 +/- 1642.01
Episode length: 824.00 +/- 181.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 824         |
|    mean_reward          | 3.61e+03    |
| time/                   |             |
|    total_timesteps      | 844000      |
| train/                  |             |
|    approx_kl            | 4.04756e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 852         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.000213   |
|    std                  | 1.96        |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=846000, episode_reward=2589.83 +/- 1291.77
Episode length: 558.80 +/- 167.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 559           |
|    mean_reward          | 2.59e+03      |
| time/                   |               |
|    total_timesteps      | 846000        |
| train/                  |               |
|    approx_kl            | 0.00015201193 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 684           |
|    n_updates            | 4130          |
|    policy_gradient_loss | -0.000638     |
|    std                  | 1.96          |
|    value_loss           | 1.6e+03       |
-------------------------------------------
Eval num_timesteps=848000, episode_reward=4922.86 +/- 4065.51
Episode length: 829.00 +/- 240.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 829           |
|    mean_reward          | 4.92e+03      |
| time/                   |               |
|    total_timesteps      | 848000        |
| train/                  |               |
|    approx_kl            | 0.00013497003 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.37         |
|    explained_variance   | 0.52          |
|    learning_rate        | 0.001         |
|    loss                 | 7.44e+03      |
|    n_updates            | 4140          |
|    policy_gradient_loss | 0.000277      |
|    std                  | 1.97          |
|    value_loss           | 1.97e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=850000, episode_reward=2511.92 +/- 1218.08
Episode length: 694.60 +/- 249.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 695           |
|    mean_reward          | 2.51e+03      |
| time/                   |               |
|    total_timesteps      | 850000        |
| train/                  |               |
|    approx_kl            | 2.3535482e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.38         |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+03      |
|    n_updates            | 4150          |
|    policy_gradient_loss | -7e-05        |
|    std                  | 1.97          |
|    value_loss           | 4.37e+03      |
-------------------------------------------
Eval num_timesteps=852000, episode_reward=3259.08 +/- 3573.36
Episode length: 588.80 +/- 333.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 589          |
|    mean_reward          | 3.26e+03     |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 3.706754e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.38        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 490          |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.000256    |
|    std                  | 1.97         |
|    value_loss           | 1.24e+03     |
------------------------------------------
Eval num_timesteps=854000, episode_reward=2880.72 +/- 1356.26
Episode length: 810.80 +/- 361.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
Eval num_timesteps=856000, episode_reward=3658.02 +/- 2746.12
Episode length: 880.60 +/- 353.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 881          |
|    mean_reward          | 3.66e+03     |
| time/                   |              |
|    total_timesteps      | 856000       |
| train/                  |              |
|    approx_kl            | 5.350841e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.39        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 459          |
|    n_updates            | 4170         |
|    policy_gradient_loss | -8.85e-05    |
|    std                  | 1.98         |
|    value_loss           | 1.2e+03      |
------------------------------------------
Eval num_timesteps=858000, episode_reward=2814.14 +/- 578.16
Episode length: 717.60 +/- 81.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 718          |
|    mean_reward          | 2.81e+03     |
| time/                   |              |
|    total_timesteps      | 858000       |
| train/                  |              |
|    approx_kl            | 7.323595e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 382          |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.000514    |
|    std                  | 1.98         |
|    value_loss           | 874          |
------------------------------------------
Eval num_timesteps=860000, episode_reward=2605.56 +/- 1645.90
Episode length: 797.00 +/- 410.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 797          |
|    mean_reward          | 2.61e+03     |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 6.248127e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.41        |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.001        |
|    loss                 | 8.26e+03     |
|    n_updates            | 4190         |
|    policy_gradient_loss | 5.28e-05     |
|    std                  | 1.99         |
|    value_loss           | 2.15e+04     |
------------------------------------------
Eval num_timesteps=862000, episode_reward=5421.05 +/- 4474.83
Episode length: 898.80 +/- 274.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 899           |
|    mean_reward          | 5.42e+03      |
| time/                   |               |
|    total_timesteps      | 862000        |
| train/                  |               |
|    approx_kl            | 5.9534766e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.41         |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.001         |
|    loss                 | 946           |
|    n_updates            | 4200          |
|    policy_gradient_loss | -0.000403     |
|    std                  | 1.99          |
|    value_loss           | 2.71e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=864000, episode_reward=2535.16 +/- 1030.25
Episode length: 659.40 +/- 175.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 659          |
|    mean_reward          | 2.54e+03     |
| time/                   |              |
|    total_timesteps      | 864000       |
| train/                  |              |
|    approx_kl            | 0.0001122813 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.41        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 463          |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.000173    |
|    std                  | 1.99         |
|    value_loss           | 1.44e+03     |
------------------------------------------
Eval num_timesteps=866000, episode_reward=2053.94 +/- 588.83
Episode length: 639.40 +/- 161.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 639          |
|    mean_reward          | 2.05e+03     |
| time/                   |              |
|    total_timesteps      | 866000       |
| train/                  |              |
|    approx_kl            | 0.0005538229 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 461          |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2            |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=868000, episode_reward=2461.09 +/- 544.99
Episode length: 845.60 +/- 310.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 846          |
|    mean_reward          | 2.46e+03     |
| time/                   |              |
|    total_timesteps      | 868000       |
| train/                  |              |
|    approx_kl            | 0.0007098562 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 856          |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00022     |
|    std                  | 2.01         |
|    value_loss           | 1.83e+03     |
------------------------------------------
Eval num_timesteps=870000, episode_reward=1559.10 +/- 519.48
Episode length: 638.00 +/- 168.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 638           |
|    mean_reward          | 1.56e+03      |
| time/                   |               |
|    total_timesteps      | 870000        |
| train/                  |               |
|    approx_kl            | 0.00046796416 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.46         |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 3.29e+03      |
|    n_updates            | 4240          |
|    policy_gradient_loss | -0.000731     |
|    std                  | 2.01          |
|    value_loss           | 6.71e+03      |
-------------------------------------------
Eval num_timesteps=872000, episode_reward=2679.63 +/- 911.21
Episode length: 885.00 +/- 229.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 885           |
|    mean_reward          | 2.68e+03      |
| time/                   |               |
|    total_timesteps      | 872000        |
| train/                  |               |
|    approx_kl            | 0.00029848053 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.47         |
|    explained_variance   | 0.701         |
|    learning_rate        | 0.001         |
|    loss                 | 4.56e+03      |
|    n_updates            | 4250          |
|    policy_gradient_loss | -0.000111     |
|    std                  | 2.02          |
|    value_loss           | 1.38e+04      |
-------------------------------------------
Eval num_timesteps=874000, episode_reward=3207.81 +/- 2166.97
Episode length: 639.40 +/- 204.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 639           |
|    mean_reward          | 3.21e+03      |
| time/                   |               |
|    total_timesteps      | 874000        |
| train/                  |               |
|    approx_kl            | 0.00016269067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.47         |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.001         |
|    loss                 | 390           |
|    n_updates            | 4260          |
|    policy_gradient_loss | -0.000581     |
|    std                  | 2.02          |
|    value_loss           | 892           |
-------------------------------------------
Eval num_timesteps=876000, episode_reward=3187.92 +/- 1453.29
Episode length: 719.80 +/- 110.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 720           |
|    mean_reward          | 3.19e+03      |
| time/                   |               |
|    total_timesteps      | 876000        |
| train/                  |               |
|    approx_kl            | 0.00014501219 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.47         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 445           |
|    n_updates            | 4270          |
|    policy_gradient_loss | -5.78e-05     |
|    std                  | 2.02          |
|    value_loss           | 1.05e+03      |
-------------------------------------------
Eval num_timesteps=878000, episode_reward=4581.07 +/- 3496.76
Episode length: 819.80 +/- 261.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 820          |
|    mean_reward          | 4.58e+03     |
| time/                   |              |
|    total_timesteps      | 878000       |
| train/                  |              |
|    approx_kl            | 6.389525e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.48        |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.001        |
|    loss                 | 6.59e+03     |
|    n_updates            | 4280         |
|    policy_gradient_loss | -0.000177    |
|    std                  | 2.02         |
|    value_loss           | 1.76e+04     |
------------------------------------------
Eval num_timesteps=880000, episode_reward=3679.15 +/- 4168.84
Episode length: 568.40 +/- 162.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 568           |
|    mean_reward          | 3.68e+03      |
| time/                   |               |
|    total_timesteps      | 880000        |
| train/                  |               |
|    approx_kl            | 4.2877946e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.001         |
|    loss                 | 3.06e+03      |
|    n_updates            | 4290          |
|    policy_gradient_loss | -0.000192     |
|    std                  | 2.02          |
|    value_loss           | 6.95e+03      |
-------------------------------------------
Eval num_timesteps=882000, episode_reward=2567.91 +/- 576.80
Episode length: 788.00 +/- 196.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 788           |
|    mean_reward          | 2.57e+03      |
| time/                   |               |
|    total_timesteps      | 882000        |
| train/                  |               |
|    approx_kl            | 3.6714977e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.631         |
|    learning_rate        | 0.001         |
|    loss                 | 7.56e+03      |
|    n_updates            | 4300          |
|    policy_gradient_loss | -0.000224     |
|    std                  | 2.02          |
|    value_loss           | 2e+04         |
-------------------------------------------
Eval num_timesteps=884000, episode_reward=3924.30 +/- 3433.27
Episode length: 732.20 +/- 103.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 732           |
|    mean_reward          | 3.92e+03      |
| time/                   |               |
|    total_timesteps      | 884000        |
| train/                  |               |
|    approx_kl            | 3.5402598e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.864         |
|    learning_rate        | 0.001         |
|    loss                 | 5.35e+03      |
|    n_updates            | 4310          |
|    policy_gradient_loss | -0.000289     |
|    std                  | 2.02          |
|    value_loss           | 1.25e+04      |
-------------------------------------------
Eval num_timesteps=886000, episode_reward=3431.19 +/- 2110.24
Episode length: 626.20 +/- 98.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 626           |
|    mean_reward          | 3.43e+03      |
| time/                   |               |
|    total_timesteps      | 886000        |
| train/                  |               |
|    approx_kl            | 4.4837856e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 2e+03         |
|    n_updates            | 4320          |
|    policy_gradient_loss | 9.89e-05      |
|    std                  | 2.02          |
|    value_loss           | 4.95e+03      |
-------------------------------------------
Eval num_timesteps=888000, episode_reward=2306.97 +/- 1764.82
Episode length: 1008.00 +/- 453.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.01e+03      |
|    mean_reward          | 2.31e+03      |
| time/                   |               |
|    total_timesteps      | 888000        |
| train/                  |               |
|    approx_kl            | 2.3838336e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+03      |
|    n_updates            | 4330          |
|    policy_gradient_loss | -0.000236     |
|    std                  | 2.02          |
|    value_loss           | 9.89e+03      |
-------------------------------------------
Eval num_timesteps=890000, episode_reward=4294.38 +/- 3733.48
Episode length: 804.60 +/- 276.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 805           |
|    mean_reward          | 4.29e+03      |
| time/                   |               |
|    total_timesteps      | 890000        |
| train/                  |               |
|    approx_kl            | 4.0450686e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.48         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 936           |
|    n_updates            | 4340          |
|    policy_gradient_loss | -0.000172     |
|    std                  | 2.02          |
|    value_loss           | 2.75e+03      |
-------------------------------------------
Eval num_timesteps=892000, episode_reward=1673.51 +/- 359.14
Episode length: 543.40 +/- 101.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 543          |
|    mean_reward          | 1.67e+03     |
| time/                   |              |
|    total_timesteps      | 892000       |
| train/                  |              |
|    approx_kl            | 3.784956e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.49        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 609          |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.000242    |
|    std                  | 2.02         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=894000, episode_reward=2016.27 +/- 188.38
Episode length: 737.00 +/- 261.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 737          |
|    mean_reward          | 2.02e+03     |
| time/                   |              |
|    total_timesteps      | 894000       |
| train/                  |              |
|    approx_kl            | 8.410934e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.49        |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.001        |
|    loss                 | 3.78e+03     |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.000247    |
|    std                  | 2.03         |
|    value_loss           | 8.65e+03     |
------------------------------------------
Eval num_timesteps=896000, episode_reward=1436.33 +/- 1421.55
Episode length: 646.80 +/- 126.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 647          |
|    mean_reward          | 1.44e+03     |
| time/                   |              |
|    total_timesteps      | 896000       |
| train/                  |              |
|    approx_kl            | 2.419384e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+03     |
|    n_updates            | 4370         |
|    policy_gradient_loss | -9.96e-05    |
|    std                  | 2.03         |
|    value_loss           | 4.16e+03     |
------------------------------------------
Eval num_timesteps=898000, episode_reward=1842.80 +/- 291.65
Episode length: 580.80 +/- 21.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 581           |
|    mean_reward          | 1.84e+03      |
| time/                   |               |
|    total_timesteps      | 898000        |
| train/                  |               |
|    approx_kl            | 2.8057286e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.001         |
|    loss                 | 661           |
|    n_updates            | 4380          |
|    policy_gradient_loss | -0.000162     |
|    std                  | 2.03          |
|    value_loss           | 2.05e+03      |
-------------------------------------------
Eval num_timesteps=900000, episode_reward=2143.29 +/- 831.85
Episode length: 722.20 +/- 216.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 722           |
|    mean_reward          | 2.14e+03      |
| time/                   |               |
|    total_timesteps      | 900000        |
| train/                  |               |
|    approx_kl            | 1.1771976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.838         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+03       |
|    n_updates            | 4390          |
|    policy_gradient_loss | -7.82e-05     |
|    std                  | 2.03          |
|    value_loss           | 5.35e+03      |
-------------------------------------------
Eval num_timesteps=902000, episode_reward=2354.13 +/- 329.31
Episode length: 758.20 +/- 163.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 758           |
|    mean_reward          | 2.35e+03      |
| time/                   |               |
|    total_timesteps      | 902000        |
| train/                  |               |
|    approx_kl            | 0.00010021642 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.001         |
|    loss                 | 497           |
|    n_updates            | 4400          |
|    policy_gradient_loss | -0.000483     |
|    std                  | 2.03          |
|    value_loss           | 1.15e+03      |
-------------------------------------------
Eval num_timesteps=904000, episode_reward=7511.24 +/- 4303.12
Episode length: 1105.60 +/- 274.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.11e+03      |
|    mean_reward          | 7.51e+03      |
| time/                   |               |
|    total_timesteps      | 904000        |
| train/                  |               |
|    approx_kl            | 0.00017814492 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 837           |
|    n_updates            | 4410          |
|    policy_gradient_loss | -0.000372     |
|    std                  | 2.03          |
|    value_loss           | 2.13e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=906000, episode_reward=2954.63 +/- 1174.87
Episode length: 732.60 +/- 154.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 733          |
|    mean_reward          | 2.95e+03     |
| time/                   |              |
|    total_timesteps      | 906000       |
| train/                  |              |
|    approx_kl            | 6.423282e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 951          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.000276    |
|    std                  | 2.04         |
|    value_loss           | 4.06e+03     |
------------------------------------------
Eval num_timesteps=908000, episode_reward=2266.58 +/- 1054.01
Episode length: 651.80 +/- 158.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 652           |
|    mean_reward          | 2.27e+03      |
| time/                   |               |
|    total_timesteps      | 908000        |
| train/                  |               |
|    approx_kl            | 5.6932913e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.001         |
|    loss                 | 1.52e+03      |
|    n_updates            | 4430          |
|    policy_gradient_loss | -0.000247     |
|    std                  | 2.03          |
|    value_loss           | 3.39e+03      |
-------------------------------------------
Eval num_timesteps=910000, episode_reward=5773.18 +/- 3729.48
Episode length: 778.40 +/- 114.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 778         |
|    mean_reward          | 5.77e+03    |
| time/                   |             |
|    total_timesteps      | 910000      |
| train/                  |             |
|    approx_kl            | 7.50976e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.001       |
|    loss                 | 3.58e+03    |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.000259   |
|    std                  | 2.03        |
|    value_loss           | 7.81e+03    |
-----------------------------------------
Eval num_timesteps=912000, episode_reward=2803.69 +/- 761.55
Episode length: 871.40 +/- 156.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 871           |
|    mean_reward          | 2.8e+03       |
| time/                   |               |
|    total_timesteps      | 912000        |
| train/                  |               |
|    approx_kl            | 3.3741497e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.478         |
|    learning_rate        | 0.001         |
|    loss                 | 7.73e+03      |
|    n_updates            | 4450          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.03          |
|    value_loss           | 1.87e+04      |
-------------------------------------------
Eval num_timesteps=914000, episode_reward=3125.26 +/- 2328.14
Episode length: 870.40 +/- 423.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 870           |
|    mean_reward          | 3.13e+03      |
| time/                   |               |
|    total_timesteps      | 914000        |
| train/                  |               |
|    approx_kl            | 1.2534525e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.457         |
|    learning_rate        | 0.001         |
|    loss                 | 8.95e+03      |
|    n_updates            | 4460          |
|    policy_gradient_loss | 5.9e-05       |
|    std                  | 2.03          |
|    value_loss           | 2.22e+04      |
-------------------------------------------
Eval num_timesteps=916000, episode_reward=3004.05 +/- 859.62
Episode length: 757.00 +/- 211.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 757           |
|    mean_reward          | 3e+03         |
| time/                   |               |
|    total_timesteps      | 916000        |
| train/                  |               |
|    approx_kl            | 1.8613413e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.001         |
|    loss                 | 794           |
|    n_updates            | 4470          |
|    policy_gradient_loss | -0.000173     |
|    std                  | 2.04          |
|    value_loss           | 2.36e+03      |
-------------------------------------------
Eval num_timesteps=918000, episode_reward=1847.57 +/- 517.42
Episode length: 652.20 +/- 112.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 652           |
|    mean_reward          | 1.85e+03      |
| time/                   |               |
|    total_timesteps      | 918000        |
| train/                  |               |
|    approx_kl            | 2.9740826e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 513           |
|    n_updates            | 4480          |
|    policy_gradient_loss | -0.000273     |
|    std                  | 2.03          |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=920000, episode_reward=2191.90 +/- 1007.27
Episode length: 729.20 +/- 253.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 729           |
|    mean_reward          | 2.19e+03      |
| time/                   |               |
|    total_timesteps      | 920000        |
| train/                  |               |
|    approx_kl            | 0.00012674622 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.895         |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+03      |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000491     |
|    std                  | 2.03          |
|    value_loss           | 3.72e+03      |
-------------------------------------------
Eval num_timesteps=922000, episode_reward=4293.89 +/- 3749.81
Episode length: 929.60 +/- 214.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 930           |
|    mean_reward          | 4.29e+03      |
| time/                   |               |
|    total_timesteps      | 922000        |
| train/                  |               |
|    approx_kl            | 0.00011046033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 862           |
|    n_updates            | 4500          |
|    policy_gradient_loss | -0.000238     |
|    std                  | 2.03          |
|    value_loss           | 2.04e+03      |
-------------------------------------------
Eval num_timesteps=924000, episode_reward=3134.41 +/- 1471.74
Episode length: 703.80 +/- 261.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 704           |
|    mean_reward          | 3.13e+03      |
| time/                   |               |
|    total_timesteps      | 924000        |
| train/                  |               |
|    approx_kl            | 5.0448958e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 758           |
|    n_updates            | 4510          |
|    policy_gradient_loss | -0.000198     |
|    std                  | 2.03          |
|    value_loss           | 1.92e+03      |
-------------------------------------------
Eval num_timesteps=926000, episode_reward=5096.05 +/- 3002.84
Episode length: 947.20 +/- 288.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 947           |
|    mean_reward          | 5.1e+03       |
| time/                   |               |
|    total_timesteps      | 926000        |
| train/                  |               |
|    approx_kl            | 0.00023364945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 342           |
|    n_updates            | 4520          |
|    policy_gradient_loss | -0.000867     |
|    std                  | 2.03          |
|    value_loss           | 819           |
-------------------------------------------
Eval num_timesteps=928000, episode_reward=2414.86 +/- 802.44
Episode length: 721.00 +/- 275.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 721           |
|    mean_reward          | 2.41e+03      |
| time/                   |               |
|    total_timesteps      | 928000        |
| train/                  |               |
|    approx_kl            | 0.00031850956 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 749           |
|    n_updates            | 4530          |
|    policy_gradient_loss | -0.000549     |
|    std                  | 2.03          |
|    value_loss           | 1.66e+03      |
-------------------------------------------
Eval num_timesteps=930000, episode_reward=3309.51 +/- 1288.87
Episode length: 985.00 +/- 340.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 985           |
|    mean_reward          | 3.31e+03      |
| time/                   |               |
|    total_timesteps      | 930000        |
| train/                  |               |
|    approx_kl            | 0.00021546215 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+03      |
|    n_updates            | 4540          |
|    policy_gradient_loss | -0.000176     |
|    std                  | 2.03          |
|    value_loss           | 2.83e+03      |
-------------------------------------------
Eval num_timesteps=932000, episode_reward=4089.66 +/- 4032.91
Episode length: 820.00 +/- 380.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 820           |
|    mean_reward          | 4.09e+03      |
| time/                   |               |
|    total_timesteps      | 932000        |
| train/                  |               |
|    approx_kl            | 8.1382226e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 4550          |
|    policy_gradient_loss | -0.000291     |
|    std                  | 2.03          |
|    value_loss           | 2.53e+03      |
-------------------------------------------
Eval num_timesteps=934000, episode_reward=2641.04 +/- 643.56
Episode length: 836.60 +/- 240.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 837           |
|    mean_reward          | 2.64e+03      |
| time/                   |               |
|    total_timesteps      | 934000        |
| train/                  |               |
|    approx_kl            | 4.9231952e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+04      |
|    n_updates            | 4560          |
|    policy_gradient_loss | -3.15e-06     |
|    std                  | 2.03          |
|    value_loss           | 4.86e+04      |
-------------------------------------------
Eval num_timesteps=936000, episode_reward=3350.43 +/- 1181.66
Episode length: 807.20 +/- 166.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 807           |
|    mean_reward          | 3.35e+03      |
| time/                   |               |
|    total_timesteps      | 936000        |
| train/                  |               |
|    approx_kl            | 2.2662512e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.861         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+03       |
|    n_updates            | 4570          |
|    policy_gradient_loss | -0.000247     |
|    std                  | 2.03          |
|    value_loss           | 4.34e+03      |
-------------------------------------------
Eval num_timesteps=938000, episode_reward=1666.11 +/- 223.73
Episode length: 624.80 +/- 74.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 625           |
|    mean_reward          | 1.67e+03      |
| time/                   |               |
|    total_timesteps      | 938000        |
| train/                  |               |
|    approx_kl            | 4.8644695e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.802         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 4580          |
|    policy_gradient_loss | -0.000105     |
|    std                  | 2.03          |
|    value_loss           | 2.99e+03      |
-------------------------------------------
Eval num_timesteps=940000, episode_reward=3730.32 +/- 1981.59
Episode length: 817.20 +/- 372.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 3.73e+03 |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=942000, episode_reward=3851.38 +/- 2781.22
Episode length: 915.80 +/- 321.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 916          |
|    mean_reward          | 3.85e+03     |
| time/                   |              |
|    total_timesteps      | 942000       |
| train/                  |              |
|    approx_kl            | 4.579025e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+03     |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.000316    |
|    std                  | 2.03         |
|    value_loss           | 3.05e+03     |
------------------------------------------
Eval num_timesteps=944000, episode_reward=5018.56 +/- 3339.67
Episode length: 767.80 +/- 244.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 5.02e+03    |
| time/                   |             |
|    total_timesteps      | 944000      |
| train/                  |             |
|    approx_kl            | 9.49729e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 767         |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.000454   |
|    std                  | 2.04        |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=946000, episode_reward=2274.80 +/- 586.75
Episode length: 744.80 +/- 122.63
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 745            |
|    mean_reward          | 2.27e+03       |
| time/                   |                |
|    total_timesteps      | 946000         |
| train/                  |                |
|    approx_kl            | 0.000103017286 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.52          |
|    explained_variance   | 0.0715         |
|    learning_rate        | 0.001          |
|    loss                 | 1.27e+04       |
|    n_updates            | 4610           |
|    policy_gradient_loss | -0.000268      |
|    std                  | 2.04           |
|    value_loss           | 3.06e+04       |
--------------------------------------------
Eval num_timesteps=948000, episode_reward=2130.59 +/- 627.16
Episode length: 662.20 +/- 138.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 662          |
|    mean_reward          | 2.13e+03     |
| time/                   |              |
|    total_timesteps      | 948000       |
| train/                  |              |
|    approx_kl            | 7.104495e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+03     |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.000473    |
|    std                  | 2.04         |
|    value_loss           | 2.52e+03     |
------------------------------------------
Eval num_timesteps=950000, episode_reward=1835.14 +/- 709.50
Episode length: 642.40 +/- 129.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 642          |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 950000       |
| train/                  |              |
|    approx_kl            | 9.323319e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+03     |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.000266    |
|    std                  | 2.04         |
|    value_loss           | 3.74e+03     |
------------------------------------------
Eval num_timesteps=952000, episode_reward=2749.80 +/- 1567.56
Episode length: 818.80 +/- 243.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 819          |
|    mean_reward          | 2.75e+03     |
| time/                   |              |
|    total_timesteps      | 952000       |
| train/                  |              |
|    approx_kl            | 8.891552e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+03      |
|    n_updates            | 4640         |
|    policy_gradient_loss | -0.000298    |
|    std                  | 2.04         |
|    value_loss           | 2.61e+03     |
------------------------------------------
Eval num_timesteps=954000, episode_reward=5680.03 +/- 4136.49
Episode length: 1056.80 +/- 391.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.06e+03      |
|    mean_reward          | 5.68e+03      |
| time/                   |               |
|    total_timesteps      | 954000        |
| train/                  |               |
|    approx_kl            | 0.00011980138 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.53         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 3.61e+03      |
|    n_updates            | 4650          |
|    policy_gradient_loss | -8.35e-05     |
|    std                  | 2.05          |
|    value_loss           | 7.81e+03      |
-------------------------------------------
Eval num_timesteps=956000, episode_reward=2850.88 +/- 1424.69
Episode length: 824.20 +/- 419.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 824           |
|    mean_reward          | 2.85e+03      |
| time/                   |               |
|    total_timesteps      | 956000        |
| train/                  |               |
|    approx_kl            | 0.00010022073 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.54         |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 478           |
|    n_updates            | 4660          |
|    policy_gradient_loss | -0.000535     |
|    std                  | 2.05          |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=958000, episode_reward=1645.35 +/- 411.44
Episode length: 570.80 +/- 92.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 571           |
|    mean_reward          | 1.65e+03      |
| time/                   |               |
|    total_timesteps      | 958000        |
| train/                  |               |
|    approx_kl            | 0.00033720763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 484           |
|    n_updates            | 4670          |
|    policy_gradient_loss | -0.000682     |
|    std                  | 2.05          |
|    value_loss           | 1.18e+03      |
-------------------------------------------
Eval num_timesteps=960000, episode_reward=3716.12 +/- 3768.55
Episode length: 577.60 +/- 114.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 578          |
|    mean_reward          | 3.72e+03     |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0002107718 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 3.65e+03     |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.000421    |
|    std                  | 2.06         |
|    value_loss           | 7.78e+03     |
------------------------------------------
Eval num_timesteps=962000, episode_reward=2256.48 +/- 717.23
Episode length: 682.00 +/- 189.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 682           |
|    mean_reward          | 2.26e+03      |
| time/                   |               |
|    total_timesteps      | 962000        |
| train/                  |               |
|    approx_kl            | 0.00017360339 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.001         |
|    loss                 | 709           |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.00045      |
|    std                  | 2.06          |
|    value_loss           | 2.53e+03      |
-------------------------------------------
Eval num_timesteps=964000, episode_reward=2186.18 +/- 692.62
Episode length: 586.60 +/- 96.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 587          |
|    mean_reward          | 2.19e+03     |
| time/                   |              |
|    total_timesteps      | 964000       |
| train/                  |              |
|    approx_kl            | 0.0006191664 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 412          |
|    n_updates            | 4700         |
|    policy_gradient_loss | -0.000955    |
|    std                  | 2.05         |
|    value_loss           | 918          |
------------------------------------------
Eval num_timesteps=966000, episode_reward=2271.91 +/- 283.37
Episode length: 649.00 +/- 174.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 649           |
|    mean_reward          | 2.27e+03      |
| time/                   |               |
|    total_timesteps      | 966000        |
| train/                  |               |
|    approx_kl            | 0.00030929683 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.758         |
|    learning_rate        | 0.001         |
|    loss                 | 6.4e+03       |
|    n_updates            | 4710          |
|    policy_gradient_loss | -0.000197     |
|    std                  | 2.05          |
|    value_loss           | 1.43e+04      |
-------------------------------------------
Eval num_timesteps=968000, episode_reward=1931.69 +/- 585.98
Episode length: 577.80 +/- 128.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 578          |
|    mean_reward          | 1.93e+03     |
| time/                   |              |
|    total_timesteps      | 968000       |
| train/                  |              |
|    approx_kl            | 9.419408e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 762          |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.000303    |
|    std                  | 2.06         |
|    value_loss           | 1.77e+03     |
------------------------------------------
Eval num_timesteps=970000, episode_reward=2089.00 +/- 595.64
Episode length: 533.00 +/- 86.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 533          |
|    mean_reward          | 2.09e+03     |
| time/                   |              |
|    total_timesteps      | 970000       |
| train/                  |              |
|    approx_kl            | 9.292454e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+03     |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.000216    |
|    std                  | 2.06         |
|    value_loss           | 4.91e+03     |
------------------------------------------
Eval num_timesteps=972000, episode_reward=2065.18 +/- 729.33
Episode length: 618.00 +/- 131.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 618           |
|    mean_reward          | 2.07e+03      |
| time/                   |               |
|    total_timesteps      | 972000        |
| train/                  |               |
|    approx_kl            | 4.7866808e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+03      |
|    n_updates            | 4740          |
|    policy_gradient_loss | -0.000138     |
|    std                  | 2.06          |
|    value_loss           | 5.47e+03      |
-------------------------------------------
Eval num_timesteps=974000, episode_reward=2052.52 +/- 1128.04
Episode length: 546.60 +/- 85.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 547           |
|    mean_reward          | 2.05e+03      |
| time/                   |               |
|    total_timesteps      | 974000        |
| train/                  |               |
|    approx_kl            | 2.9174611e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 4750          |
|    policy_gradient_loss | -5.86e-06     |
|    std                  | 2.06          |
|    value_loss           | 3.49e+03      |
-------------------------------------------
Eval num_timesteps=976000, episode_reward=2873.22 +/- 775.89
Episode length: 691.40 +/- 133.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 691           |
|    mean_reward          | 2.87e+03      |
| time/                   |               |
|    total_timesteps      | 976000        |
| train/                  |               |
|    approx_kl            | 7.0595095e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.57         |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+03      |
|    n_updates            | 4760          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 2.06          |
|    value_loss           | 2.86e+03      |
-------------------------------------------
Eval num_timesteps=978000, episode_reward=2564.53 +/- 801.52
Episode length: 569.00 +/- 89.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 569           |
|    mean_reward          | 2.56e+03      |
| time/                   |               |
|    total_timesteps      | 978000        |
| train/                  |               |
|    approx_kl            | 0.00017798491 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.57         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 615           |
|    n_updates            | 4770          |
|    policy_gradient_loss | -0.00047      |
|    std                  | 2.06          |
|    value_loss           | 1.32e+03      |
-------------------------------------------
Eval num_timesteps=980000, episode_reward=3029.36 +/- 2400.23
Episode length: 578.00 +/- 87.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 578           |
|    mean_reward          | 3.03e+03      |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 0.00011445154 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.57         |
|    explained_variance   | 0.815         |
|    learning_rate        | 0.001         |
|    loss                 | 4.4e+03       |
|    n_updates            | 4780          |
|    policy_gradient_loss | 8.93e-06      |
|    std                  | 2.06          |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=982000, episode_reward=1859.85 +/- 877.80
Episode length: 516.20 +/- 120.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 982000       |
| train/                  |              |
|    approx_kl            | 3.694356e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 2.5e+03      |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000233    |
|    std                  | 2.06         |
|    value_loss           | 5.43e+03     |
------------------------------------------
Eval num_timesteps=984000, episode_reward=1893.72 +/- 537.76
Episode length: 520.40 +/- 57.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 520           |
|    mean_reward          | 1.89e+03      |
| time/                   |               |
|    total_timesteps      | 984000        |
| train/                  |               |
|    approx_kl            | 0.00055746274 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.001         |
|    loss                 | 552           |
|    n_updates            | 4800          |
|    policy_gradient_loss | -0.0011       |
|    std                  | 2.06          |
|    value_loss           | 1.15e+03      |
-------------------------------------------
Eval num_timesteps=986000, episode_reward=2890.63 +/- 1636.12
Episode length: 705.20 +/- 233.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 705          |
|    mean_reward          | 2.89e+03     |
| time/                   |              |
|    total_timesteps      | 986000       |
| train/                  |              |
|    approx_kl            | 0.0007126664 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.000383    |
|    std                  | 2.06         |
|    value_loss           | 3.59e+03     |
------------------------------------------
Eval num_timesteps=988000, episode_reward=3855.39 +/- 3934.81
Episode length: 682.00 +/- 91.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 682           |
|    mean_reward          | 3.86e+03      |
| time/                   |               |
|    total_timesteps      | 988000        |
| train/                  |               |
|    approx_kl            | 8.3649036e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 810           |
|    n_updates            | 4820          |
|    policy_gradient_loss | 5.94e-05      |
|    std                  | 2.06          |
|    value_loss           | 2.04e+03      |
-------------------------------------------
Eval num_timesteps=990000, episode_reward=1916.80 +/- 360.46
Episode length: 519.00 +/- 98.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 519          |
|    mean_reward          | 1.92e+03     |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 8.541776e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.001        |
|    loss                 | 4.18e+03     |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000426    |
|    std                  | 2.06         |
|    value_loss           | 1.07e+04     |
------------------------------------------
Eval num_timesteps=992000, episode_reward=5401.71 +/- 3494.08
Episode length: 677.80 +/- 126.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 678         |
|    mean_reward          | 5.4e+03     |
| time/                   |             |
|    total_timesteps      | 992000      |
| train/                  |             |
|    approx_kl            | 7.04533e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.55       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 1.92e+03    |
|    n_updates            | 4840        |
|    policy_gradient_loss | -7.66e-05   |
|    std                  | 2.06        |
|    value_loss           | 4.26e+03    |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=3927.79 +/- 2683.51
Episode length: 715.20 +/- 115.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 715           |
|    mean_reward          | 3.93e+03      |
| time/                   |               |
|    total_timesteps      | 994000        |
| train/                  |               |
|    approx_kl            | 2.3964152e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.001         |
|    loss                 | 505           |
|    n_updates            | 4850          |
|    policy_gradient_loss | -0.000195     |
|    std                  | 2.06          |
|    value_loss           | 1.14e+03      |
-------------------------------------------
Eval num_timesteps=996000, episode_reward=2301.04 +/- 1093.95
Episode length: 587.60 +/- 115.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 588          |
|    mean_reward          | 2.3e+03      |
| time/                   |              |
|    total_timesteps      | 996000       |
| train/                  |              |
|    approx_kl            | 0.0001490585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 796          |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.000666    |
|    std                  | 2.06         |
|    value_loss           | 1.73e+03     |
------------------------------------------
Eval num_timesteps=998000, episode_reward=3224.72 +/- 2150.85
Episode length: 589.40 +/- 166.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 589           |
|    mean_reward          | 3.22e+03      |
| time/                   |               |
|    total_timesteps      | 998000        |
| train/                  |               |
|    approx_kl            | 0.00014177576 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+03      |
|    n_updates            | 4870          |
|    policy_gradient_loss | -0.00024      |
|    std                  | 2.06          |
|    value_loss           | 3.21e+03      |
-------------------------------------------
Eval num_timesteps=1000000, episode_reward=2848.88 +/- 1537.13
Episode length: 719.40 +/- 261.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 719           |
|    mean_reward          | 2.85e+03      |
| time/                   |               |
|    total_timesteps      | 1000000       |
| train/                  |               |
|    approx_kl            | 0.00033623172 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.001         |
|    loss                 | 328           |
|    n_updates            | 4880          |
|    policy_gradient_loss | -0.000841     |
|    std                  | 2.06          |
|    value_loss           | 718           |
-------------------------------------------
Eval num_timesteps=1002000, episode_reward=3209.21 +/- 705.67
Episode length: 721.60 +/- 137.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 722         |
|    mean_reward          | 3.21e+03    |
| time/                   |             |
|    total_timesteps      | 1002000     |
| train/                  |             |
|    approx_kl            | 0.001089134 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.56       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 355         |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00126    |
|    std                  | 2.07        |
|    value_loss           | 775         |
-----------------------------------------
Eval num_timesteps=1004000, episode_reward=3966.86 +/- 679.43
Episode length: 972.60 +/- 214.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 973           |
|    mean_reward          | 3.97e+03      |
| time/                   |               |
|    total_timesteps      | 1004000       |
| train/                  |               |
|    approx_kl            | 0.00044312785 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.58         |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+03      |
|    n_updates            | 4900          |
|    policy_gradient_loss | -0.000478     |
|    std                  | 2.07          |
|    value_loss           | 6.71e+03      |
-------------------------------------------
Eval num_timesteps=1006000, episode_reward=3393.27 +/- 2490.06
Episode length: 791.00 +/- 217.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 791           |
|    mean_reward          | 3.39e+03      |
| time/                   |               |
|    total_timesteps      | 1006000       |
| train/                  |               |
|    approx_kl            | 8.8508124e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.58         |
|    explained_variance   | 0.548         |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+04      |
|    n_updates            | 4910          |
|    policy_gradient_loss | -4.6e-05      |
|    std                  | 2.07          |
|    value_loss           | 3.3e+04       |
-------------------------------------------
Eval num_timesteps=1008000, episode_reward=2854.79 +/- 913.57
Episode length: 818.40 +/- 223.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | 2.85e+03     |
| time/                   |              |
|    total_timesteps      | 1008000      |
| train/                  |              |
|    approx_kl            | 2.272916e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 798          |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.000123    |
|    std                  | 2.08         |
|    value_loss           | 2.18e+03     |
------------------------------------------
Eval num_timesteps=1010000, episode_reward=1450.20 +/- 410.99
Episode length: 551.20 +/- 170.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 551           |
|    mean_reward          | 1.45e+03      |
| time/                   |               |
|    total_timesteps      | 1010000       |
| train/                  |               |
|    approx_kl            | 0.00018674185 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.59         |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 281           |
|    n_updates            | 4930          |
|    policy_gradient_loss | -0.000918     |
|    std                  | 2.08          |
|    value_loss           | 725           |
-------------------------------------------
Eval num_timesteps=1012000, episode_reward=3168.98 +/- 1015.06
Episode length: 843.40 +/- 292.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 843          |
|    mean_reward          | 3.17e+03     |
| time/                   |              |
|    total_timesteps      | 1012000      |
| train/                  |              |
|    approx_kl            | 0.0003104925 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+03     |
|    n_updates            | 4940         |
|    policy_gradient_loss | 0.000238     |
|    std                  | 2.08         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=1014000, episode_reward=3960.17 +/- 4255.33
Episode length: 815.20 +/- 231.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 815           |
|    mean_reward          | 3.96e+03      |
| time/                   |               |
|    total_timesteps      | 1014000       |
| train/                  |               |
|    approx_kl            | 4.0462182e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.6          |
|    explained_variance   | 0.271         |
|    learning_rate        | 0.001         |
|    loss                 | 1.84e+04      |
|    n_updates            | 4950          |
|    policy_gradient_loss | -3e-05        |
|    std                  | 2.08          |
|    value_loss           | 4.74e+04      |
-------------------------------------------
Eval num_timesteps=1016000, episode_reward=3553.06 +/- 1822.83
Episode length: 976.80 +/- 444.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 977           |
|    mean_reward          | 3.55e+03      |
| time/                   |               |
|    total_timesteps      | 1016000       |
| train/                  |               |
|    approx_kl            | 0.00010193375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.6          |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+03      |
|    n_updates            | 4960          |
|    policy_gradient_loss | -0.000437     |
|    std                  | 2.08          |
|    value_loss           | 2.77e+03      |
-------------------------------------------
Eval num_timesteps=1018000, episode_reward=3238.27 +/- 791.50
Episode length: 926.80 +/- 116.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 927           |
|    mean_reward          | 3.24e+03      |
| time/                   |               |
|    total_timesteps      | 1018000       |
| train/                  |               |
|    approx_kl            | 0.00012820092 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.61         |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 4970          |
|    policy_gradient_loss | -0.000282     |
|    std                  | 2.09          |
|    value_loss           | 3.35e+03      |
-------------------------------------------
Eval num_timesteps=1020000, episode_reward=4129.21 +/- 1406.49
Episode length: 1117.60 +/- 287.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.12e+03      |
|    mean_reward          | 4.13e+03      |
| time/                   |               |
|    total_timesteps      | 1020000       |
| train/                  |               |
|    approx_kl            | 0.00012140814 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.61         |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.001         |
|    loss                 | 428           |
|    n_updates            | 4980          |
|    policy_gradient_loss | -0.000556     |
|    std                  | 2.09          |
|    value_loss           | 1.17e+03      |
-------------------------------------------
Eval num_timesteps=1022000, episode_reward=3495.25 +/- 1449.80
Episode length: 1037.20 +/- 217.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.04e+03      |
|    mean_reward          | 3.5e+03       |
| time/                   |               |
|    total_timesteps      | 1022000       |
| train/                  |               |
|    approx_kl            | 0.00018143066 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.6          |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 680           |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.000513     |
|    std                  | 2.08          |
|    value_loss           | 1.61e+03      |
-------------------------------------------
Eval num_timesteps=1024000, episode_reward=3066.58 +/- 1387.12
Episode length: 911.20 +/- 378.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1026000, episode_reward=2947.29 +/- 1098.87
Episode length: 872.80 +/- 270.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 873           |
|    mean_reward          | 2.95e+03      |
| time/                   |               |
|    total_timesteps      | 1026000       |
| train/                  |               |
|    approx_kl            | 0.00013316132 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.59         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 655           |
|    n_updates            | 5000          |
|    policy_gradient_loss | -0.000453     |
|    std                  | 2.07          |
|    value_loss           | 2.02e+03      |
-------------------------------------------
Eval num_timesteps=1028000, episode_reward=2718.89 +/- 1240.81
Episode length: 1020.00 +/- 326.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.02e+03     |
|    mean_reward          | 2.72e+03     |
| time/                   |              |
|    total_timesteps      | 1028000      |
| train/                  |              |
|    approx_kl            | 9.978202e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+03     |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.000344    |
|    std                  | 2.08         |
|    value_loss           | 4.91e+03     |
------------------------------------------
Eval num_timesteps=1030000, episode_reward=2142.28 +/- 937.73
Episode length: 807.40 +/- 397.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 807           |
|    mean_reward          | 2.14e+03      |
| time/                   |               |
|    total_timesteps      | 1030000       |
| train/                  |               |
|    approx_kl            | 0.00013184114 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.59         |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+03      |
|    n_updates            | 5020          |
|    policy_gradient_loss | -9.11e-05     |
|    std                  | 2.08          |
|    value_loss           | 2.16e+03      |
-------------------------------------------
Eval num_timesteps=1032000, episode_reward=3415.13 +/- 3600.59
Episode length: 823.80 +/- 462.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 824           |
|    mean_reward          | 3.42e+03      |
| time/                   |               |
|    total_timesteps      | 1032000       |
| train/                  |               |
|    approx_kl            | 5.9810904e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.6          |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+03      |
|    n_updates            | 5030          |
|    policy_gradient_loss | -0.000185     |
|    std                  | 2.08          |
|    value_loss           | 2.94e+03      |
-------------------------------------------
Eval num_timesteps=1034000, episode_reward=2971.73 +/- 1403.01
Episode length: 954.20 +/- 262.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 954          |
|    mean_reward          | 2.97e+03     |
| time/                   |              |
|    total_timesteps      | 1034000      |
| train/                  |              |
|    approx_kl            | 9.384408e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.6         |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 780          |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.0004      |
|    std                  | 2.09         |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=1036000, episode_reward=2526.49 +/- 4254.59
Episode length: 752.60 +/- 305.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 753          |
|    mean_reward          | 2.53e+03     |
| time/                   |              |
|    total_timesteps      | 1036000      |
| train/                  |              |
|    approx_kl            | 0.0002495428 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.61        |
|    explained_variance   | 0.833        |
|    learning_rate        | 0.001        |
|    loss                 | 3.63e+03     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.000521    |
|    std                  | 2.09         |
|    value_loss           | 7.62e+03     |
------------------------------------------
Eval num_timesteps=1038000, episode_reward=3231.89 +/- 1311.38
Episode length: 870.40 +/- 121.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 870           |
|    mean_reward          | 3.23e+03      |
| time/                   |               |
|    total_timesteps      | 1038000       |
| train/                  |               |
|    approx_kl            | 0.00042254225 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 491           |
|    n_updates            | 5060          |
|    policy_gradient_loss | -0.000762     |
|    std                  | 2.1           |
|    value_loss           | 1.32e+03      |
-------------------------------------------
Eval num_timesteps=1040000, episode_reward=2683.14 +/- 1478.08
Episode length: 1088.00 +/- 238.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.09e+03      |
|    mean_reward          | 2.68e+03      |
| time/                   |               |
|    total_timesteps      | 1040000       |
| train/                  |               |
|    approx_kl            | 0.00018459899 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+04       |
|    n_updates            | 5070          |
|    policy_gradient_loss | -0.000245     |
|    std                  | 2.1           |
|    value_loss           | 3.79e+04      |
-------------------------------------------
Eval num_timesteps=1042000, episode_reward=3104.74 +/- 1595.53
Episode length: 1049.40 +/- 289.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.05e+03     |
|    mean_reward          | 3.1e+03      |
| time/                   |              |
|    total_timesteps      | 1042000      |
| train/                  |              |
|    approx_kl            | 4.396116e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.63        |
|    explained_variance   | 0.815        |
|    learning_rate        | 0.001        |
|    loss                 | 4.26e+03     |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.000261    |
|    std                  | 2.1          |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=1044000, episode_reward=2562.66 +/- 2679.21
Episode length: 942.40 +/- 247.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 942          |
|    mean_reward          | 2.56e+03     |
| time/                   |              |
|    total_timesteps      | 1044000      |
| train/                  |              |
|    approx_kl            | 4.420997e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.63        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+03     |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000338    |
|    std                  | 2.1          |
|    value_loss           | 4.51e+03     |
------------------------------------------
Eval num_timesteps=1046000, episode_reward=3396.91 +/- 1671.47
Episode length: 984.60 +/- 327.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 985           |
|    mean_reward          | 3.4e+03       |
| time/                   |               |
|    total_timesteps      | 1046000       |
| train/                  |               |
|    approx_kl            | 5.2090385e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.706         |
|    learning_rate        | 0.001         |
|    loss                 | 7.48e+03      |
|    n_updates            | 5100          |
|    policy_gradient_loss | -0.000167     |
|    std                  | 2.1           |
|    value_loss           | 2.15e+04      |
-------------------------------------------
Eval num_timesteps=1048000, episode_reward=2318.33 +/- 476.34
Episode length: 1147.80 +/- 458.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.15e+03      |
|    mean_reward          | 2.32e+03      |
| time/                   |               |
|    total_timesteps      | 1048000       |
| train/                  |               |
|    approx_kl            | 1.7813174e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.615         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 5110          |
|    policy_gradient_loss | -0.000125     |
|    std                  | 2.1           |
|    value_loss           | 3.3e+04       |
-------------------------------------------
Eval num_timesteps=1050000, episode_reward=3939.58 +/- 1364.09
Episode length: 1208.20 +/- 179.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.21e+03      |
|    mean_reward          | 3.94e+03      |
| time/                   |               |
|    total_timesteps      | 1050000       |
| train/                  |               |
|    approx_kl            | 1.3294804e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 5.1e+03       |
|    n_updates            | 5120          |
|    policy_gradient_loss | -0.000164     |
|    std                  | 2.1           |
|    value_loss           | 1.36e+04      |
-------------------------------------------
Eval num_timesteps=1052000, episode_reward=2073.45 +/- 1870.39
Episode length: 954.80 +/- 300.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 955           |
|    mean_reward          | 2.07e+03      |
| time/                   |               |
|    total_timesteps      | 1052000       |
| train/                  |               |
|    approx_kl            | 2.5813672e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.604         |
|    learning_rate        | 0.001         |
|    loss                 | 4.2e+03       |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000247     |
|    std                  | 2.1           |
|    value_loss           | 9.73e+03      |
-------------------------------------------
Eval num_timesteps=1054000, episode_reward=5316.86 +/- 2987.12
Episode length: 1091.00 +/- 246.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.09e+03      |
|    mean_reward          | 5.32e+03      |
| time/                   |               |
|    total_timesteps      | 1054000       |
| train/                  |               |
|    approx_kl            | 3.3846474e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+03      |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000265     |
|    std                  | 2.1           |
|    value_loss           | 3.98e+03      |
-------------------------------------------
Eval num_timesteps=1056000, episode_reward=2479.17 +/- 1012.06
Episode length: 891.20 +/- 226.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 891          |
|    mean_reward          | 2.48e+03     |
| time/                   |              |
|    total_timesteps      | 1056000      |
| train/                  |              |
|    approx_kl            | 3.241445e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.63        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 810          |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 2.1          |
|    value_loss           | 2.5e+03      |
------------------------------------------
Eval num_timesteps=1058000, episode_reward=2888.26 +/- 1085.01
Episode length: 903.80 +/- 377.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 904           |
|    mean_reward          | 2.89e+03      |
| time/                   |               |
|    total_timesteps      | 1058000       |
| train/                  |               |
|    approx_kl            | 2.4378125e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+03      |
|    n_updates            | 5160          |
|    policy_gradient_loss | 9.71e-05      |
|    std                  | 2.1           |
|    value_loss           | 8.43e+03      |
-------------------------------------------
Eval num_timesteps=1060000, episode_reward=4895.09 +/- 3114.49
Episode length: 980.20 +/- 310.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 980           |
|    mean_reward          | 4.9e+03       |
| time/                   |               |
|    total_timesteps      | 1060000       |
| train/                  |               |
|    approx_kl            | 1.1118769e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.861         |
|    learning_rate        | 0.001         |
|    loss                 | 3.55e+03      |
|    n_updates            | 5170          |
|    policy_gradient_loss | -9.17e-05     |
|    std                  | 2.1           |
|    value_loss           | 8.71e+03      |
-------------------------------------------
Eval num_timesteps=1062000, episode_reward=1127.48 +/- 2665.24
Episode length: 1010.00 +/- 185.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.01e+03      |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 1062000       |
| train/                  |               |
|    approx_kl            | 0.00010525499 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 526           |
|    n_updates            | 5180          |
|    policy_gradient_loss | -0.000539     |
|    std                  | 2.1           |
|    value_loss           | 1.53e+03      |
-------------------------------------------
Eval num_timesteps=1064000, episode_reward=4157.14 +/- 1504.84
Episode length: 964.00 +/- 265.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 964           |
|    mean_reward          | 4.16e+03      |
| time/                   |               |
|    total_timesteps      | 1064000       |
| train/                  |               |
|    approx_kl            | 0.00045843938 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.65         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 802           |
|    n_updates            | 5190          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 2.12          |
|    value_loss           | 1.84e+03      |
-------------------------------------------
Eval num_timesteps=1066000, episode_reward=1910.07 +/- 1503.67
Episode length: 870.60 +/- 231.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 871          |
|    mean_reward          | 1.91e+03     |
| time/                   |              |
|    total_timesteps      | 1066000      |
| train/                  |              |
|    approx_kl            | 0.0003164778 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.67        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 595          |
|    n_updates            | 5200         |
|    policy_gradient_loss | -0.00038     |
|    std                  | 2.12         |
|    value_loss           | 1.42e+03     |
------------------------------------------
Eval num_timesteps=1068000, episode_reward=1721.28 +/- 430.26
Episode length: 702.80 +/- 167.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 703           |
|    mean_reward          | 1.72e+03      |
| time/                   |               |
|    total_timesteps      | 1068000       |
| train/                  |               |
|    approx_kl            | 0.00012215081 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.68         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 513           |
|    n_updates            | 5210          |
|    policy_gradient_loss | -0.000339     |
|    std                  | 2.13          |
|    value_loss           | 1.58e+03      |
-------------------------------------------
Eval num_timesteps=1070000, episode_reward=2244.03 +/- 299.56
Episode length: 900.80 +/- 201.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 901           |
|    mean_reward          | 2.24e+03      |
| time/                   |               |
|    total_timesteps      | 1070000       |
| train/                  |               |
|    approx_kl            | 6.0493243e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.7          |
|    explained_variance   | 0.548         |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+03      |
|    n_updates            | 5220          |
|    policy_gradient_loss | 0.000234      |
|    std                  | 2.14          |
|    value_loss           | 1.53e+04      |
-------------------------------------------
Eval num_timesteps=1072000, episode_reward=3752.33 +/- 2657.30
Episode length: 1014.80 +/- 330.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.01e+03      |
|    mean_reward          | 3.75e+03      |
| time/                   |               |
|    total_timesteps      | 1072000       |
| train/                  |               |
|    approx_kl            | 1.1521246e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.7          |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.001         |
|    loss                 | 1e+03         |
|    n_updates            | 5230          |
|    policy_gradient_loss | -5.92e-05     |
|    std                  | 2.14          |
|    value_loss           | 2.47e+03      |
-------------------------------------------
Eval num_timesteps=1074000, episode_reward=3356.04 +/- 1649.05
Episode length: 768.60 +/- 161.85
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 769            |
|    mean_reward          | 3.36e+03       |
| time/                   |                |
|    total_timesteps      | 1074000        |
| train/                  |                |
|    approx_kl            | 1.30139815e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.71          |
|    explained_variance   | 0.821          |
|    learning_rate        | 0.001          |
|    loss                 | 4.07e+03       |
|    n_updates            | 5240           |
|    policy_gradient_loss | -0.000108      |
|    std                  | 2.14           |
|    value_loss           | 8.92e+03       |
--------------------------------------------
Eval num_timesteps=1076000, episode_reward=2901.11 +/- 692.88
Episode length: 814.80 +/- 122.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 815           |
|    mean_reward          | 2.9e+03       |
| time/                   |               |
|    total_timesteps      | 1076000       |
| train/                  |               |
|    approx_kl            | 1.8566614e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.71         |
|    explained_variance   | 0.817         |
|    learning_rate        | 0.001         |
|    loss                 | 5.69e+03      |
|    n_updates            | 5250          |
|    policy_gradient_loss | -0.000145     |
|    std                  | 2.14          |
|    value_loss           | 1.47e+04      |
-------------------------------------------
Eval num_timesteps=1078000, episode_reward=3006.97 +/- 462.92
Episode length: 904.00 +/- 257.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 904           |
|    mean_reward          | 3.01e+03      |
| time/                   |               |
|    total_timesteps      | 1078000       |
| train/                  |               |
|    approx_kl            | 5.2883173e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.71         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 370           |
|    n_updates            | 5260          |
|    policy_gradient_loss | -0.00029      |
|    std                  | 2.14          |
|    value_loss           | 955           |
-------------------------------------------
Eval num_timesteps=1080000, episode_reward=4174.82 +/- 2090.33
Episode length: 1095.80 +/- 359.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.1e+03       |
|    mean_reward          | 4.17e+03      |
| time/                   |               |
|    total_timesteps      | 1080000       |
| train/                  |               |
|    approx_kl            | 0.00019503015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.71         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 790           |
|    n_updates            | 5270          |
|    policy_gradient_loss | -0.000618     |
|    std                  | 2.14          |
|    value_loss           | 2.2e+03       |
-------------------------------------------
Eval num_timesteps=1082000, episode_reward=4090.58 +/- 4083.24
Episode length: 1152.00 +/- 172.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.15e+03      |
|    mean_reward          | 4.09e+03      |
| time/                   |               |
|    total_timesteps      | 1082000       |
| train/                  |               |
|    approx_kl            | 0.00012307728 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.72         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 828           |
|    n_updates            | 5280          |
|    policy_gradient_loss | 2.28e-05      |
|    std                  | 2.14          |
|    value_loss           | 2.49e+03      |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=2460.60 +/- 839.10
Episode length: 826.80 +/- 293.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 827          |
|    mean_reward          | 2.46e+03     |
| time/                   |              |
|    total_timesteps      | 1084000      |
| train/                  |              |
|    approx_kl            | 4.149601e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 597          |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.000241    |
|    std                  | 2.15         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=1086000, episode_reward=3329.49 +/- 2297.73
Episode length: 1220.60 +/- 373.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.22e+03     |
|    mean_reward          | 3.33e+03     |
| time/                   |              |
|    total_timesteps      | 1086000      |
| train/                  |              |
|    approx_kl            | 9.335598e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 427          |
|    n_updates            | 5300         |
|    policy_gradient_loss | -0.000376    |
|    std                  | 2.15         |
|    value_loss           | 1.22e+03     |
------------------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 679, in <module>
    elif args.run_type == "test":
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 417, in run_full
    callback=[eval_callback,
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 223, in collect_rollouts
    rollout_buffer.add(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\buffers.py", line 469, in add
    self.observations[self.pos] = np.array(obs)
FloatingPointError: underflow encountered in cast