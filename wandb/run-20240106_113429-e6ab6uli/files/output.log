AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO 01.06.2024_11.35.12_1
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-190.08 +/- 54.63
Episode length: 221.80 +/- 71.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-220.42 +/- 23.28
Episode length: 190.60 +/- 28.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-203.66 +/- 23.20
Episode length: 217.00 +/- 39.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
Eval num_timesteps=7968, episode_reward=-210.34 +/- 48.33
Episode length: 214.20 +/- 60.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-193.98 +/- 25.53
Episode length: 229.80 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-204.85 +/- 37.95
Episode length: 202.40 +/- 40.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-198.32 +/- 34.32
Episode length: 221.60 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Eval num_timesteps=15936, episode_reward=-218.77 +/- 28.32
Episode length: 210.40 +/- 31.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 15936    |
---------------------------------
Eval num_timesteps=17928, episode_reward=-228.13 +/- 21.56
Episode length: 196.20 +/- 32.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 17928    |
---------------------------------
Eval num_timesteps=19920, episode_reward=-200.24 +/- 36.23
Episode length: 209.60 +/- 56.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 19920    |
---------------------------------
Eval num_timesteps=21912, episode_reward=-167.81 +/- 69.39
Episode length: 246.80 +/- 74.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 21912    |
---------------------------------
New best mean reward!
Eval num_timesteps=23904, episode_reward=-220.56 +/- 16.43
Episode length: 207.40 +/- 61.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 23904    |
---------------------------------
Eval num_timesteps=25896, episode_reward=-203.70 +/- 22.66
Episode length: 229.60 +/- 44.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 25896    |
---------------------------------
Eval num_timesteps=27888, episode_reward=-174.36 +/- 71.30
Episode length: 238.40 +/- 62.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 27888    |
---------------------------------
Eval num_timesteps=29880, episode_reward=-207.50 +/- 72.39
Episode length: 209.20 +/- 80.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 29880    |
---------------------------------
Eval num_timesteps=31872, episode_reward=-207.76 +/- 41.60
Episode length: 235.20 +/- 47.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 31872    |
---------------------------------
Eval num_timesteps=33864, episode_reward=-243.82 +/- 20.96
Episode length: 182.80 +/- 43.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 33864    |
---------------------------------
Eval num_timesteps=35856, episode_reward=-199.04 +/- 37.89
Episode length: 243.20 +/- 56.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 35856    |
---------------------------------
Eval num_timesteps=37848, episode_reward=-203.05 +/- 70.82
Episode length: 216.80 +/- 64.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 37848    |
---------------------------------
Eval num_timesteps=39840, episode_reward=-184.88 +/- 42.23
Episode length: 248.80 +/- 85.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 39840    |
---------------------------------
Eval num_timesteps=41832, episode_reward=-209.22 +/- 17.95
Episode length: 220.80 +/- 30.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 41832    |
---------------------------------
Eval num_timesteps=43824, episode_reward=-192.37 +/- 40.45
Episode length: 215.00 +/- 52.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 43824    |
---------------------------------
Eval num_timesteps=45816, episode_reward=-233.50 +/- 23.22
Episode length: 193.80 +/- 37.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 45816    |
---------------------------------
Eval num_timesteps=47808, episode_reward=-229.55 +/- 22.95
Episode length: 206.40 +/- 37.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 47808    |
---------------------------------
Eval num_timesteps=49800, episode_reward=-186.38 +/- 42.15
Episode length: 219.80 +/- 41.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 49800       |
| train/                  |             |
|    approx_kl            | 0.003881611 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.72       |
|    explained_variance   | -7.84e-05   |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00229    |
|    std                  | 1.02        |
|    value_loss           | 2.61e+03    |
-----------------------------------------
Eval num_timesteps=51792, episode_reward=-215.10 +/- 36.15
Episode length: 230.60 +/- 38.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 51792    |
---------------------------------
Eval num_timesteps=53784, episode_reward=-199.24 +/- 38.41
Episode length: 213.00 +/- 43.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 53784    |
---------------------------------
Eval num_timesteps=55776, episode_reward=-198.04 +/- 60.17
Episode length: 211.60 +/- 57.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 55776    |
---------------------------------
Eval num_timesteps=57768, episode_reward=-189.66 +/- 40.51
Episode length: 227.00 +/- 38.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 57768    |
---------------------------------
Eval num_timesteps=59760, episode_reward=-215.11 +/- 15.32
Episode length: 211.60 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 59760    |
---------------------------------
Eval num_timesteps=61752, episode_reward=-204.03 +/- 40.86
Episode length: 214.20 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 61752    |
---------------------------------
Eval num_timesteps=63744, episode_reward=-222.51 +/- 12.43
Episode length: 197.60 +/- 22.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 63744    |
---------------------------------
Eval num_timesteps=65736, episode_reward=-186.24 +/- 38.08
Episode length: 231.40 +/- 41.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 65736    |
---------------------------------
Eval num_timesteps=67728, episode_reward=-195.21 +/- 52.02
Episode length: 196.60 +/- 38.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 67728    |
---------------------------------
Eval num_timesteps=69720, episode_reward=-206.16 +/- 49.50
Episode length: 217.20 +/- 75.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 69720    |
---------------------------------
Eval num_timesteps=71712, episode_reward=-199.14 +/- 30.43
Episode length: 221.60 +/- 38.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 71712    |
---------------------------------
Eval num_timesteps=73704, episode_reward=-218.33 +/- 30.35
Episode length: 203.40 +/- 29.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 73704    |
---------------------------------
Eval num_timesteps=75696, episode_reward=-197.31 +/- 40.43
Episode length: 233.00 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 75696    |
---------------------------------
Eval num_timesteps=77688, episode_reward=-216.02 +/- 39.81
Episode length: 219.80 +/- 44.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 77688    |
---------------------------------
Eval num_timesteps=79680, episode_reward=-199.60 +/- 47.52
Episode length: 216.80 +/- 53.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 79680    |
---------------------------------
Eval num_timesteps=81672, episode_reward=-194.45 +/- 42.31
Episode length: 209.40 +/- 47.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 81672    |
---------------------------------
Eval num_timesteps=83664, episode_reward=-191.27 +/- 32.05
Episode length: 225.40 +/- 54.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 83664    |
---------------------------------
Eval num_timesteps=85656, episode_reward=-206.45 +/- 20.37
Episode length: 210.20 +/- 37.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 85656    |
---------------------------------
Eval num_timesteps=87648, episode_reward=-202.96 +/- 29.41
Episode length: 203.40 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 87648    |
---------------------------------
Eval num_timesteps=89640, episode_reward=-159.13 +/- 74.90
Episode length: 268.80 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 89640    |
---------------------------------
New best mean reward!
Eval num_timesteps=91632, episode_reward=-207.09 +/- 26.66
Episode length: 208.80 +/- 30.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 91632    |
---------------------------------
Eval num_timesteps=93624, episode_reward=-220.35 +/- 46.03
Episode length: 218.00 +/- 68.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 93624    |
---------------------------------
Eval num_timesteps=95616, episode_reward=-208.07 +/- 25.86
Episode length: 184.40 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 95616    |
---------------------------------
Eval num_timesteps=97608, episode_reward=-208.36 +/- 23.95
Episode length: 212.20 +/- 49.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 97608    |
---------------------------------
Eval num_timesteps=99600, episode_reward=-198.54 +/- 47.49
Episode length: 227.80 +/- 81.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | -199        |
| time/                   |             |
|    total_timesteps      | 99600       |
| train/                  |             |
|    approx_kl            | 0.003278856 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.78       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.001       |
|    loss                 | 852         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00127    |
|    std                  | 1.03        |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=101592, episode_reward=-175.90 +/- 32.54
Episode length: 248.00 +/- 74.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 101592   |
---------------------------------
Eval num_timesteps=103584, episode_reward=-144.82 +/- 22.10
Episode length: 316.80 +/- 65.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 103584   |
---------------------------------
New best mean reward!
Eval num_timesteps=105576, episode_reward=-175.98 +/- 64.46
Episode length: 263.20 +/- 97.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 105576   |
---------------------------------
Eval num_timesteps=107568, episode_reward=-188.96 +/- 44.01
Episode length: 272.40 +/- 105.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 107568   |
---------------------------------
Eval num_timesteps=109560, episode_reward=-215.65 +/- 9.99
Episode length: 203.60 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 109560   |
---------------------------------
Eval num_timesteps=111552, episode_reward=-204.43 +/- 34.87
Episode length: 249.40 +/- 63.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 111552   |
---------------------------------
Eval num_timesteps=113544, episode_reward=-153.00 +/- 35.33
Episode length: 264.20 +/- 48.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 113544   |
---------------------------------
Eval num_timesteps=115536, episode_reward=-178.95 +/- 62.42
Episode length: 254.80 +/- 71.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 115536   |
---------------------------------
Eval num_timesteps=117528, episode_reward=-186.49 +/- 85.35
Episode length: 233.80 +/- 80.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 117528   |
---------------------------------
Eval num_timesteps=119520, episode_reward=-155.59 +/- 74.58
Episode length: 307.80 +/- 128.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 119520   |
---------------------------------
Eval num_timesteps=121512, episode_reward=-164.38 +/- 55.74
Episode length: 264.20 +/- 56.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 121512   |
---------------------------------
Eval num_timesteps=123504, episode_reward=-182.44 +/- 38.31
Episode length: 253.00 +/- 45.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 123504   |
---------------------------------
Eval num_timesteps=125496, episode_reward=-204.08 +/- 36.97
Episode length: 252.40 +/- 70.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 125496   |
---------------------------------
Eval num_timesteps=127488, episode_reward=-214.39 +/- 14.91
Episode length: 210.40 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 127488   |
---------------------------------
Eval num_timesteps=129480, episode_reward=-221.94 +/- 14.98
Episode length: 216.20 +/- 32.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 129480   |
---------------------------------
Eval num_timesteps=131472, episode_reward=-186.75 +/- 44.55
Episode length: 239.00 +/- 70.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 131472   |
---------------------------------
Eval num_timesteps=133464, episode_reward=-168.54 +/- 33.74
Episode length: 280.00 +/- 44.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 133464   |
---------------------------------
Eval num_timesteps=135456, episode_reward=-211.06 +/- 27.73
Episode length: 225.00 +/- 42.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 135456   |
---------------------------------
Eval num_timesteps=137448, episode_reward=-190.22 +/- 63.96
Episode length: 217.00 +/- 61.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 137448   |
---------------------------------
Eval num_timesteps=139440, episode_reward=-184.30 +/- 68.43
Episode length: 264.00 +/- 89.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 139440   |
---------------------------------
Eval num_timesteps=141432, episode_reward=-208.91 +/- 29.86
Episode length: 216.20 +/- 33.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 141432   |
---------------------------------
Eval num_timesteps=143424, episode_reward=-165.03 +/- 52.00
Episode length: 265.80 +/- 51.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 143424   |
---------------------------------
Eval num_timesteps=145416, episode_reward=-171.95 +/- 42.56
Episode length: 251.00 +/- 67.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 145416   |
---------------------------------
Eval num_timesteps=147408, episode_reward=-218.88 +/- 40.35
Episode length: 203.00 +/- 30.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 147408   |
---------------------------------
Eval num_timesteps=149400, episode_reward=-178.42 +/- 37.20
Episode length: 286.00 +/- 76.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | -178         |
| time/                   |              |
|    total_timesteps      | 149400       |
| train/                  |              |
|    approx_kl            | 0.0033820758 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.615        |
|    learning_rate        | 0.001        |
|    loss                 | 666          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.05         |
|    value_loss           | 1.62e+03     |
------------------------------------------
Eval num_timesteps=151392, episode_reward=-220.20 +/- 18.62
Episode length: 201.60 +/- 32.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 151392   |
---------------------------------
Eval num_timesteps=153384, episode_reward=-178.21 +/- 42.66
Episode length: 260.60 +/- 30.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 153384   |
---------------------------------
Eval num_timesteps=155376, episode_reward=-179.54 +/- 30.17
Episode length: 233.20 +/- 79.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 155376   |
---------------------------------
Eval num_timesteps=157368, episode_reward=-212.46 +/- 28.73
Episode length: 254.00 +/- 109.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 157368   |
---------------------------------
Eval num_timesteps=159360, episode_reward=-177.16 +/- 25.59
Episode length: 299.40 +/- 29.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 159360   |
---------------------------------
Eval num_timesteps=161352, episode_reward=-179.94 +/- 40.82
Episode length: 251.80 +/- 47.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 161352   |
---------------------------------
Eval num_timesteps=163344, episode_reward=-167.47 +/- 70.05
Episode length: 266.80 +/- 67.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 163344   |
---------------------------------
Eval num_timesteps=165336, episode_reward=-192.55 +/- 22.38
Episode length: 211.20 +/- 22.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 165336   |
---------------------------------
Eval num_timesteps=167328, episode_reward=-188.38 +/- 27.12
Episode length: 249.20 +/- 38.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 167328   |
---------------------------------
Eval num_timesteps=169320, episode_reward=-164.30 +/- 32.64
Episode length: 287.40 +/- 65.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 169320   |
---------------------------------
Eval num_timesteps=171312, episode_reward=-201.17 +/- 15.96
Episode length: 240.60 +/- 62.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 171312   |
---------------------------------
Eval num_timesteps=173304, episode_reward=-137.76 +/- 34.66
Episode length: 309.20 +/- 68.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 173304   |
---------------------------------
New best mean reward!
Eval num_timesteps=175296, episode_reward=-182.96 +/- 46.47
Episode length: 247.60 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 175296   |
---------------------------------
Eval num_timesteps=177288, episode_reward=-177.33 +/- 44.30
Episode length: 259.60 +/- 89.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 177288   |
---------------------------------
Eval num_timesteps=179280, episode_reward=-186.82 +/- 83.25
Episode length: 240.60 +/- 97.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 179280   |
---------------------------------
Eval num_timesteps=181272, episode_reward=-214.70 +/- 19.00
Episode length: 213.80 +/- 36.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 181272   |
---------------------------------
Eval num_timesteps=183264, episode_reward=-180.57 +/- 54.31
Episode length: 225.40 +/- 60.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 183264   |
---------------------------------
Eval num_timesteps=185256, episode_reward=-218.37 +/- 21.15
Episode length: 209.00 +/- 44.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 185256   |
---------------------------------
Eval num_timesteps=187248, episode_reward=-140.54 +/- 84.10
Episode length: 263.20 +/- 79.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 187248   |
---------------------------------
Eval num_timesteps=189240, episode_reward=-177.33 +/- 47.80
Episode length: 291.80 +/- 71.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 189240   |
---------------------------------
Eval num_timesteps=191232, episode_reward=-172.21 +/- 31.76
Episode length: 245.60 +/- 44.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 191232   |
---------------------------------
Eval num_timesteps=193224, episode_reward=-166.17 +/- 37.10
Episode length: 257.80 +/- 63.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 193224   |
---------------------------------
Eval num_timesteps=195216, episode_reward=-191.39 +/- 36.73
Episode length: 259.60 +/- 81.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 195216   |
---------------------------------
Eval num_timesteps=197208, episode_reward=-148.96 +/- 36.72
Episode length: 330.00 +/- 87.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | -149         |
| time/                   |              |
|    total_timesteps      | 197208       |
| train/                  |              |
|    approx_kl            | 0.0041104667 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 499          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.05         |
|    value_loss           | 1.26e+03     |
------------------------------------------
Eval num_timesteps=199200, episode_reward=-126.32 +/- 44.96
Episode length: 319.20 +/- 82.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
New best mean reward!
Eval num_timesteps=201192, episode_reward=-135.33 +/- 76.39
Episode length: 287.80 +/- 77.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 201192   |
---------------------------------
Eval num_timesteps=203184, episode_reward=-80.30 +/- 120.60
Episode length: 411.40 +/- 156.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -80.3    |
| time/              |          |
|    total_timesteps | 203184   |
---------------------------------
New best mean reward!
Eval num_timesteps=205176, episode_reward=-79.16 +/- 161.33
Episode length: 357.80 +/- 143.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -79.2    |
| time/              |          |
|    total_timesteps | 205176   |
---------------------------------
New best mean reward!
Eval num_timesteps=207168, episode_reward=-80.34 +/- 135.39
Episode length: 367.60 +/- 205.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -80.3    |
| time/              |          |
|    total_timesteps | 207168   |
---------------------------------
Eval num_timesteps=209160, episode_reward=-116.44 +/- 43.03
Episode length: 316.20 +/- 42.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 209160   |
---------------------------------
Eval num_timesteps=211152, episode_reward=-151.32 +/- 45.91
Episode length: 262.80 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 211152   |
---------------------------------
Eval num_timesteps=213144, episode_reward=-141.69 +/- 28.77
Episode length: 375.20 +/- 35.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 213144   |
---------------------------------
Eval num_timesteps=215136, episode_reward=-126.31 +/- 69.16
Episode length: 336.60 +/- 50.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 215136   |
---------------------------------
Eval num_timesteps=217128, episode_reward=-144.64 +/- 58.49
Episode length: 316.60 +/- 78.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 217128   |
---------------------------------
Eval num_timesteps=219120, episode_reward=-105.34 +/- 73.30
Episode length: 405.40 +/- 95.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 219120   |
---------------------------------
Eval num_timesteps=221112, episode_reward=-130.62 +/- 103.23
Episode length: 295.80 +/- 117.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 221112   |
---------------------------------
Eval num_timesteps=223104, episode_reward=-126.92 +/- 79.13
Episode length: 306.40 +/- 67.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 223104   |
---------------------------------
Eval num_timesteps=225096, episode_reward=-93.49 +/- 56.39
Episode length: 336.80 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -93.5    |
| time/              |          |
|    total_timesteps | 225096   |
---------------------------------
Eval num_timesteps=227088, episode_reward=-167.22 +/- 19.22
Episode length: 318.20 +/- 51.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 227088   |
---------------------------------
Eval num_timesteps=229080, episode_reward=-125.51 +/- 51.55
Episode length: 434.60 +/- 161.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 229080   |
---------------------------------
Eval num_timesteps=231072, episode_reward=-140.09 +/- 49.75
Episode length: 335.40 +/- 68.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 231072   |
---------------------------------
Eval num_timesteps=233064, episode_reward=-141.93 +/- 90.88
Episode length: 341.60 +/- 118.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 233064   |
---------------------------------
Eval num_timesteps=235056, episode_reward=-112.79 +/- 25.26
Episode length: 349.40 +/- 90.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 235056   |
---------------------------------
Eval num_timesteps=237048, episode_reward=-149.44 +/- 38.42
Episode length: 331.60 +/- 116.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 332      |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 237048   |
---------------------------------
Eval num_timesteps=239040, episode_reward=-141.76 +/- 78.19
Episode length: 368.40 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 239040   |
---------------------------------
Eval num_timesteps=241032, episode_reward=-92.77 +/- 41.57
Episode length: 424.60 +/- 45.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | -92.8    |
| time/              |          |
|    total_timesteps | 241032   |
---------------------------------
Eval num_timesteps=243024, episode_reward=-124.44 +/- 66.37
Episode length: 349.40 +/- 94.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 243024   |
---------------------------------
Eval num_timesteps=245016, episode_reward=-169.04 +/- 29.91
Episode length: 317.40 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 245016   |
---------------------------------
Eval num_timesteps=247008, episode_reward=-68.88 +/- 75.42
Episode length: 454.80 +/- 119.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | -68.9        |
| time/                   |              |
|    total_timesteps      | 247008       |
| train/                  |              |
|    approx_kl            | 0.0045296527 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00269     |
|    std                  | 1.06         |
|    value_loss           | 865          |
------------------------------------------
New best mean reward!
Eval num_timesteps=249000, episode_reward=-47.22 +/- 89.22
Episode length: 431.60 +/- 93.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | -47.2    |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
New best mean reward!
Eval num_timesteps=250992, episode_reward=-125.88 +/- 76.39
Episode length: 378.20 +/- 161.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 250992   |
---------------------------------
Eval num_timesteps=252984, episode_reward=3.69 +/- 224.84
Episode length: 519.00 +/- 194.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 3.69     |
| time/              |          |
|    total_timesteps | 252984   |
---------------------------------
New best mean reward!
Eval num_timesteps=254976, episode_reward=-96.08 +/- 84.66
Episode length: 406.00 +/- 180.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | -96.1    |
| time/              |          |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=256968, episode_reward=-67.56 +/- 104.49
Episode length: 465.80 +/- 154.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | -67.6    |
| time/              |          |
|    total_timesteps | 256968   |
---------------------------------
Eval num_timesteps=258960, episode_reward=-29.04 +/- 71.24
Episode length: 466.60 +/- 70.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 258960   |
---------------------------------
Eval num_timesteps=260952, episode_reward=-85.54 +/- 67.13
Episode length: 432.80 +/- 128.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | -85.5    |
| time/              |          |
|    total_timesteps | 260952   |
---------------------------------
Eval num_timesteps=262944, episode_reward=-100.48 +/- 51.84
Episode length: 426.60 +/- 96.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 262944   |
---------------------------------
Eval num_timesteps=264936, episode_reward=-76.14 +/- 124.72
Episode length: 421.60 +/- 91.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -76.1    |
| time/              |          |
|    total_timesteps | 264936   |
---------------------------------
Eval num_timesteps=266928, episode_reward=-97.01 +/- 57.42
Episode length: 445.80 +/- 72.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | -97      |
| time/              |          |
|    total_timesteps | 266928   |
---------------------------------
Eval num_timesteps=268920, episode_reward=-80.56 +/- 101.82
Episode length: 453.80 +/- 104.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | -80.6    |
| time/              |          |
|    total_timesteps | 268920   |
---------------------------------
Eval num_timesteps=270912, episode_reward=-94.55 +/- 63.26
Episode length: 383.00 +/- 67.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | -94.6    |
| time/              |          |
|    total_timesteps | 270912   |
---------------------------------
Eval num_timesteps=272904, episode_reward=-65.41 +/- 88.27
Episode length: 443.20 +/- 148.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 443      |
|    mean_reward     | -65.4    |
| time/              |          |
|    total_timesteps | 272904   |
---------------------------------
Eval num_timesteps=274896, episode_reward=-92.86 +/- 72.38
Episode length: 410.80 +/- 134.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -92.9    |
| time/              |          |
|    total_timesteps | 274896   |
---------------------------------
Eval num_timesteps=276888, episode_reward=-67.32 +/- 44.53
Episode length: 489.20 +/- 92.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -67.3    |
| time/              |          |
|    total_timesteps | 276888   |
---------------------------------
Eval num_timesteps=278880, episode_reward=-112.94 +/- 43.36
Episode length: 357.20 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 357      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 278880   |
---------------------------------
Eval num_timesteps=280872, episode_reward=-78.58 +/- 112.71
Episode length: 441.00 +/- 187.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | -78.6    |
| time/              |          |
|    total_timesteps | 280872   |
---------------------------------
Eval num_timesteps=282864, episode_reward=-71.00 +/- 107.43
Episode length: 440.40 +/- 160.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | -71      |
| time/              |          |
|    total_timesteps | 282864   |
---------------------------------
Eval num_timesteps=284856, episode_reward=-63.20 +/- 41.10
Episode length: 494.20 +/- 54.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -63.2    |
| time/              |          |
|    total_timesteps | 284856   |
---------------------------------
Eval num_timesteps=286848, episode_reward=-59.72 +/- 48.93
Episode length: 471.80 +/- 53.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | -59.7    |
| time/              |          |
|    total_timesteps | 286848   |
---------------------------------
Eval num_timesteps=288840, episode_reward=-61.26 +/- 94.42
Episode length: 395.00 +/- 108.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | -61.3    |
| time/              |          |
|    total_timesteps | 288840   |
---------------------------------
Eval num_timesteps=290832, episode_reward=-143.56 +/- 48.22
Episode length: 306.20 +/- 44.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 290832   |
---------------------------------
Eval num_timesteps=292824, episode_reward=-84.54 +/- 55.41
Episode length: 400.40 +/- 66.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -84.5    |
| time/              |          |
|    total_timesteps | 292824   |
---------------------------------
Eval num_timesteps=294816, episode_reward=-3.27 +/- 145.18
Episode length: 480.20 +/- 115.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | -3.27    |
| time/              |          |
|    total_timesteps | 294816   |
---------------------------------
Eval num_timesteps=296808, episode_reward=13.30 +/- 138.98
Episode length: 552.80 +/- 122.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 296808       |
| train/                  |              |
|    approx_kl            | 0.0043285317 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 360          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.07         |
|    value_loss           | 641          |
------------------------------------------
New best mean reward!
Eval num_timesteps=298800, episode_reward=-57.26 +/- 73.46
Episode length: 516.60 +/- 150.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -57.3    |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
Eval num_timesteps=300792, episode_reward=-1.20 +/- 100.05
Episode length: 566.40 +/- 190.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 300792   |
---------------------------------
Eval num_timesteps=302784, episode_reward=7.23 +/- 124.15
Episode length: 644.00 +/- 139.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 7.23     |
| time/              |          |
|    total_timesteps | 302784   |
---------------------------------
Eval num_timesteps=304776, episode_reward=-60.19 +/- 109.63
Episode length: 542.60 +/- 89.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | -60.2    |
| time/              |          |
|    total_timesteps | 304776   |
---------------------------------
Eval num_timesteps=306768, episode_reward=-55.05 +/- 163.22
Episode length: 503.20 +/- 262.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 503      |
|    mean_reward     | -55      |
| time/              |          |
|    total_timesteps | 306768   |
---------------------------------
Eval num_timesteps=308760, episode_reward=-131.01 +/- 108.72
Episode length: 419.40 +/- 137.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 308760   |
---------------------------------
Eval num_timesteps=310752, episode_reward=-24.90 +/- 74.26
Episode length: 551.20 +/- 68.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | -24.9    |
| time/              |          |
|    total_timesteps | 310752   |
---------------------------------
Eval num_timesteps=312744, episode_reward=-22.93 +/- 121.80
Episode length: 479.40 +/- 118.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 479      |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 312744   |
---------------------------------
Eval num_timesteps=314736, episode_reward=-76.14 +/- 63.76
Episode length: 568.00 +/- 135.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | -76.1    |
| time/              |          |
|    total_timesteps | 314736   |
---------------------------------
Eval num_timesteps=316728, episode_reward=-106.53 +/- 35.85
Episode length: 419.40 +/- 138.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 316728   |
---------------------------------
Eval num_timesteps=318720, episode_reward=-77.68 +/- 68.43
Episode length: 428.80 +/- 155.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | -77.7    |
| time/              |          |
|    total_timesteps | 318720   |
---------------------------------
Eval num_timesteps=320712, episode_reward=125.24 +/- 241.93
Episode length: 687.60 +/- 186.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 320712   |
---------------------------------
New best mean reward!
Eval num_timesteps=322704, episode_reward=-135.47 +/- 38.93
Episode length: 457.00 +/- 161.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 322704   |
---------------------------------
Eval num_timesteps=324696, episode_reward=96.35 +/- 208.51
Episode length: 619.40 +/- 141.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 96.3     |
| time/              |          |
|    total_timesteps | 324696   |
---------------------------------
Eval num_timesteps=326688, episode_reward=60.51 +/- 178.13
Episode length: 534.40 +/- 181.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 60.5     |
| time/              |          |
|    total_timesteps | 326688   |
---------------------------------
Eval num_timesteps=328680, episode_reward=-75.73 +/- 118.02
Episode length: 422.20 +/- 124.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -75.7    |
| time/              |          |
|    total_timesteps | 328680   |
---------------------------------
Eval num_timesteps=330672, episode_reward=53.00 +/- 181.11
Episode length: 555.80 +/- 149.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 53       |
| time/              |          |
|    total_timesteps | 330672   |
---------------------------------
Eval num_timesteps=332664, episode_reward=9.30 +/- 216.11
Episode length: 596.40 +/- 191.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 9.3      |
| time/              |          |
|    total_timesteps | 332664   |
---------------------------------
Eval num_timesteps=334656, episode_reward=-13.77 +/- 90.60
Episode length: 580.00 +/- 172.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | -13.8    |
| time/              |          |
|    total_timesteps | 334656   |
---------------------------------
Eval num_timesteps=336648, episode_reward=-29.95 +/- 88.41
Episode length: 488.60 +/- 159.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -30      |
| time/              |          |
|    total_timesteps | 336648   |
---------------------------------
Eval num_timesteps=338640, episode_reward=-1.31 +/- 159.04
Episode length: 475.80 +/- 180.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -1.31    |
| time/              |          |
|    total_timesteps | 338640   |
---------------------------------
Eval num_timesteps=340632, episode_reward=-37.02 +/- 59.98
Episode length: 466.60 +/- 137.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 340632   |
---------------------------------
Eval num_timesteps=342624, episode_reward=67.04 +/- 191.76
Episode length: 639.20 +/- 300.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 67       |
| time/              |          |
|    total_timesteps | 342624   |
---------------------------------
Eval num_timesteps=344616, episode_reward=30.44 +/- 146.47
Episode length: 686.40 +/- 387.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 686         |
|    mean_reward          | 30.4        |
| time/                   |             |
|    total_timesteps      | 344616      |
| train/                  |             |
|    approx_kl            | 0.004280772 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.001       |
|    loss                 | 213         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00176    |
|    std                  | 1.08        |
|    value_loss           | 492         |
-----------------------------------------
Eval num_timesteps=346608, episode_reward=-47.58 +/- 70.98
Episode length: 626.20 +/- 143.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | -47.6    |
| time/              |          |
|    total_timesteps | 346608   |
---------------------------------
Eval num_timesteps=348600, episode_reward=28.65 +/- 124.59
Episode length: 596.40 +/- 157.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 348600   |
---------------------------------
Eval num_timesteps=350592, episode_reward=-15.79 +/- 176.48
Episode length: 616.60 +/- 154.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | -15.8    |
| time/              |          |
|    total_timesteps | 350592   |
---------------------------------
Eval num_timesteps=352584, episode_reward=69.83 +/- 87.74
Episode length: 774.80 +/- 294.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 69.8     |
| time/              |          |
|    total_timesteps | 352584   |
---------------------------------
Eval num_timesteps=354576, episode_reward=-68.65 +/- 86.75
Episode length: 542.60 +/- 95.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | -68.6    |
| time/              |          |
|    total_timesteps | 354576   |
---------------------------------
Eval num_timesteps=356568, episode_reward=27.60 +/- 129.22
Episode length: 561.00 +/- 185.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 27.6     |
| time/              |          |
|    total_timesteps | 356568   |
---------------------------------
Eval num_timesteps=358560, episode_reward=-20.80 +/- 157.24
Episode length: 481.80 +/- 103.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | -20.8    |
| time/              |          |
|    total_timesteps | 358560   |
---------------------------------
Eval num_timesteps=360552, episode_reward=-77.20 +/- 95.10
Episode length: 433.20 +/- 125.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | -77.2    |
| time/              |          |
|    total_timesteps | 360552   |
---------------------------------
Eval num_timesteps=362544, episode_reward=37.18 +/- 151.42
Episode length: 638.60 +/- 93.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 37.2     |
| time/              |          |
|    total_timesteps | 362544   |
---------------------------------
Eval num_timesteps=364536, episode_reward=-28.38 +/- 194.95
Episode length: 569.00 +/- 204.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 364536   |
---------------------------------
Eval num_timesteps=366528, episode_reward=53.83 +/- 121.10
Episode length: 578.20 +/- 219.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 53.8     |
| time/              |          |
|    total_timesteps | 366528   |
---------------------------------
Eval num_timesteps=368520, episode_reward=-53.86 +/- 105.69
Episode length: 489.60 +/- 133.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | -53.9    |
| time/              |          |
|    total_timesteps | 368520   |
---------------------------------
Eval num_timesteps=370512, episode_reward=-92.16 +/- 43.30
Episode length: 521.60 +/- 159.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | -92.2    |
| time/              |          |
|    total_timesteps | 370512   |
---------------------------------
Eval num_timesteps=372504, episode_reward=-91.93 +/- 49.75
Episode length: 535.20 +/- 152.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | -91.9    |
| time/              |          |
|    total_timesteps | 372504   |
---------------------------------
Eval num_timesteps=374496, episode_reward=-9.69 +/- 105.97
Episode length: 646.60 +/- 134.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | -9.69    |
| time/              |          |
|    total_timesteps | 374496   |
---------------------------------
Eval num_timesteps=376488, episode_reward=412.31 +/- 415.77
Episode length: 817.40 +/- 191.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 376488   |
---------------------------------
New best mean reward!
Eval num_timesteps=378480, episode_reward=55.38 +/- 116.79
Episode length: 539.80 +/- 121.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 55.4     |
| time/              |          |
|    total_timesteps | 378480   |
---------------------------------
Eval num_timesteps=380472, episode_reward=150.20 +/- 194.89
Episode length: 785.80 +/- 177.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 380472   |
---------------------------------
Eval num_timesteps=382464, episode_reward=5.29 +/- 202.73
Episode length: 542.40 +/- 232.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | 5.29     |
| time/              |          |
|    total_timesteps | 382464   |
---------------------------------
Eval num_timesteps=384456, episode_reward=139.13 +/- 239.91
Episode length: 850.20 +/- 258.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 384456   |
---------------------------------
Eval num_timesteps=386448, episode_reward=99.70 +/- 199.07
Episode length: 654.60 +/- 153.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 99.7     |
| time/              |          |
|    total_timesteps | 386448   |
---------------------------------
Eval num_timesteps=388440, episode_reward=-19.97 +/- 54.49
Episode length: 582.00 +/- 143.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 388440   |
---------------------------------
Eval num_timesteps=390432, episode_reward=-6.35 +/- 83.72
Episode length: 495.40 +/- 170.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -6.35    |
| time/              |          |
|    total_timesteps | 390432   |
---------------------------------
Eval num_timesteps=392424, episode_reward=78.91 +/- 149.58
Episode length: 748.00 +/- 302.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 78.9     |
| time/              |          |
|    total_timesteps | 392424   |
---------------------------------
Eval num_timesteps=394416, episode_reward=209.59 +/- 142.35
Episode length: 663.00 +/- 206.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 663          |
|    mean_reward          | 210          |
| time/                   |              |
|    total_timesteps      | 394416       |
| train/                  |              |
|    approx_kl            | 0.0051307133 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 184          |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.1          |
|    value_loss           | 397          |
------------------------------------------
Eval num_timesteps=396408, episode_reward=115.58 +/- 75.00
Episode length: 691.20 +/- 135.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 116      |
| time/              |          |
|    total_timesteps | 396408   |
---------------------------------
Eval num_timesteps=398400, episode_reward=330.66 +/- 331.37
Episode length: 750.00 +/- 188.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=400392, episode_reward=123.03 +/- 174.90
Episode length: 674.80 +/- 126.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 123      |
| time/              |          |
|    total_timesteps | 400392   |
---------------------------------
Eval num_timesteps=402384, episode_reward=205.01 +/- 511.25
Episode length: 693.80 +/- 391.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 402384   |
---------------------------------
Eval num_timesteps=404376, episode_reward=-4.80 +/- 126.87
Episode length: 606.40 +/- 136.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 404376   |
---------------------------------
Eval num_timesteps=406368, episode_reward=66.82 +/- 133.09
Episode length: 779.60 +/- 286.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 66.8     |
| time/              |          |
|    total_timesteps | 406368   |
---------------------------------
Eval num_timesteps=408360, episode_reward=44.37 +/- 100.73
Episode length: 590.00 +/- 79.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 44.4     |
| time/              |          |
|    total_timesteps | 408360   |
---------------------------------
Eval num_timesteps=410352, episode_reward=-33.96 +/- 106.09
Episode length: 584.00 +/- 235.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | -34      |
| time/              |          |
|    total_timesteps | 410352   |
---------------------------------
Eval num_timesteps=412344, episode_reward=62.47 +/- 163.82
Episode length: 725.00 +/- 179.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 412344   |
---------------------------------
Eval num_timesteps=414336, episode_reward=136.09 +/- 62.39
Episode length: 778.00 +/- 136.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 414336   |
---------------------------------
Eval num_timesteps=416328, episode_reward=161.14 +/- 128.14
Episode length: 784.40 +/- 290.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 416328   |
---------------------------------
Eval num_timesteps=418320, episode_reward=79.97 +/- 103.28
Episode length: 696.80 +/- 263.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 80       |
| time/              |          |
|    total_timesteps | 418320   |
---------------------------------
Eval num_timesteps=420312, episode_reward=45.37 +/- 118.86
Episode length: 629.60 +/- 72.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 45.4     |
| time/              |          |
|    total_timesteps | 420312   |
---------------------------------
Eval num_timesteps=422304, episode_reward=0.33 +/- 176.38
Episode length: 569.20 +/- 146.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 0.335    |
| time/              |          |
|    total_timesteps | 422304   |
---------------------------------
Eval num_timesteps=424296, episode_reward=-89.73 +/- 54.12
Episode length: 450.20 +/- 182.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -89.7    |
| time/              |          |
|    total_timesteps | 424296   |
---------------------------------
Eval num_timesteps=426288, episode_reward=14.50 +/- 124.03
Episode length: 508.60 +/- 98.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 426288   |
---------------------------------
Eval num_timesteps=428280, episode_reward=67.42 +/- 128.92
Episode length: 629.20 +/- 74.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 67.4     |
| time/              |          |
|    total_timesteps | 428280   |
---------------------------------
Eval num_timesteps=430272, episode_reward=41.44 +/- 130.23
Episode length: 676.60 +/- 243.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 41.4     |
| time/              |          |
|    total_timesteps | 430272   |
---------------------------------
Eval num_timesteps=432264, episode_reward=273.00 +/- 273.69
Episode length: 895.60 +/- 170.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 432264   |
---------------------------------
Eval num_timesteps=434256, episode_reward=-77.74 +/- 66.91
Episode length: 621.40 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -77.7    |
| time/              |          |
|    total_timesteps | 434256   |
---------------------------------
Eval num_timesteps=436248, episode_reward=215.65 +/- 261.44
Episode length: 742.20 +/- 265.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 436248   |
---------------------------------
Eval num_timesteps=438240, episode_reward=270.35 +/- 216.26
Episode length: 910.80 +/- 158.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 438240   |
---------------------------------
Eval num_timesteps=440232, episode_reward=106.92 +/- 227.93
Episode length: 662.40 +/- 115.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 440232   |
---------------------------------
Eval num_timesteps=442224, episode_reward=75.85 +/- 106.44
Episode length: 740.80 +/- 66.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 75.9     |
| time/              |          |
|    total_timesteps | 442224   |
---------------------------------
Eval num_timesteps=444216, episode_reward=324.60 +/- 245.59
Episode length: 823.60 +/- 218.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 824          |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 444216       |
| train/                  |              |
|    approx_kl            | 0.0043234522 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.11         |
|    value_loss           | 318          |
------------------------------------------
Eval num_timesteps=446208, episode_reward=169.10 +/- 252.68
Episode length: 625.40 +/- 138.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 446208   |
---------------------------------
Eval num_timesteps=448200, episode_reward=106.61 +/- 382.78
Episode length: 576.80 +/- 208.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 448200   |
---------------------------------
Eval num_timesteps=450192, episode_reward=99.68 +/- 93.23
Episode length: 748.60 +/- 243.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 99.7     |
| time/              |          |
|    total_timesteps | 450192   |
---------------------------------
Eval num_timesteps=452184, episode_reward=188.14 +/- 235.23
Episode length: 704.60 +/- 199.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 452184   |
---------------------------------
Eval num_timesteps=454176, episode_reward=111.39 +/- 70.70
Episode length: 695.00 +/- 46.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 454176   |
---------------------------------
Eval num_timesteps=456168, episode_reward=268.89 +/- 314.16
Episode length: 898.00 +/- 268.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 456168   |
---------------------------------
Eval num_timesteps=458160, episode_reward=42.12 +/- 114.36
Episode length: 568.20 +/- 126.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 42.1     |
| time/              |          |
|    total_timesteps | 458160   |
---------------------------------
Eval num_timesteps=460152, episode_reward=231.14 +/- 318.76
Episode length: 1052.00 +/- 384.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 460152   |
---------------------------------
Eval num_timesteps=462144, episode_reward=-22.19 +/- 111.33
Episode length: 576.00 +/- 148.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 462144   |
---------------------------------
Eval num_timesteps=464136, episode_reward=159.58 +/- 220.20
Episode length: 953.20 +/- 388.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 464136   |
---------------------------------
Eval num_timesteps=466128, episode_reward=152.28 +/- 242.43
Episode length: 656.40 +/- 202.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 466128   |
---------------------------------
Eval num_timesteps=468120, episode_reward=154.06 +/- 338.31
Episode length: 705.20 +/- 231.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 154      |
| time/              |          |
|    total_timesteps | 468120   |
---------------------------------
Eval num_timesteps=470112, episode_reward=32.14 +/- 142.34
Episode length: 746.20 +/- 175.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 32.1     |
| time/              |          |
|    total_timesteps | 470112   |
---------------------------------
Eval num_timesteps=472104, episode_reward=276.01 +/- 457.67
Episode length: 921.00 +/- 242.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 472104   |
---------------------------------
Eval num_timesteps=474096, episode_reward=71.09 +/- 129.74
Episode length: 707.60 +/- 96.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 71.1     |
| time/              |          |
|    total_timesteps | 474096   |
---------------------------------
Eval num_timesteps=476088, episode_reward=329.87 +/- 279.57
Episode length: 822.60 +/- 71.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 476088   |
---------------------------------
Eval num_timesteps=478080, episode_reward=378.87 +/- 565.23
Episode length: 875.60 +/- 230.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 478080   |
---------------------------------
Eval num_timesteps=480072, episode_reward=227.98 +/- 191.95
Episode length: 874.80 +/- 350.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 480072   |
---------------------------------
Eval num_timesteps=482064, episode_reward=317.22 +/- 420.85
Episode length: 921.00 +/- 59.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 482064   |
---------------------------------
Eval num_timesteps=484056, episode_reward=438.79 +/- 396.07
Episode length: 773.20 +/- 161.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 484056   |
---------------------------------
New best mean reward!
Eval num_timesteps=486048, episode_reward=341.98 +/- 497.81
Episode length: 873.40 +/- 292.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 486048   |
---------------------------------
Eval num_timesteps=488040, episode_reward=438.24 +/- 361.44
Episode length: 884.20 +/- 164.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 488040   |
---------------------------------
Eval num_timesteps=490032, episode_reward=276.46 +/- 185.98
Episode length: 802.20 +/- 111.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 490032   |
---------------------------------
Eval num_timesteps=492024, episode_reward=30.30 +/- 263.73
Episode length: 675.40 +/- 143.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 30.3         |
| time/                   |              |
|    total_timesteps      | 492024       |
| train/                  |              |
|    approx_kl            | 0.0038974397 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.12         |
|    value_loss           | 279          |
------------------------------------------
Eval num_timesteps=494016, episode_reward=98.11 +/- 198.96
Episode length: 557.60 +/- 141.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 98.1     |
| time/              |          |
|    total_timesteps | 494016   |
---------------------------------
Eval num_timesteps=496008, episode_reward=227.96 +/- 118.44
Episode length: 931.40 +/- 93.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 496008   |
---------------------------------
Eval num_timesteps=498000, episode_reward=128.13 +/- 251.68
Episode length: 845.40 +/- 194.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 128      |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=499992, episode_reward=572.31 +/- 437.90
Episode length: 947.60 +/- 230.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 499992   |
---------------------------------
New best mean reward!
Eval num_timesteps=501984, episode_reward=363.45 +/- 539.02
Episode length: 1002.00 +/- 191.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 501984   |
---------------------------------
Eval num_timesteps=503976, episode_reward=229.75 +/- 351.47
Episode length: 700.80 +/- 280.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 503976   |
---------------------------------
Eval num_timesteps=505968, episode_reward=517.85 +/- 1117.33
Episode length: 788.60 +/- 315.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 505968   |
---------------------------------
Eval num_timesteps=507960, episode_reward=142.52 +/- 168.17
Episode length: 704.40 +/- 83.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 507960   |
---------------------------------
Eval num_timesteps=509952, episode_reward=600.95 +/- 525.88
Episode length: 1001.60 +/- 245.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 509952   |
---------------------------------
New best mean reward!
Eval num_timesteps=511944, episode_reward=135.54 +/- 72.83
Episode length: 728.60 +/- 107.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 511944   |
---------------------------------
Eval num_timesteps=513936, episode_reward=91.86 +/- 139.87
Episode length: 968.40 +/- 347.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 91.9     |
| time/              |          |
|    total_timesteps | 513936   |
---------------------------------
Eval num_timesteps=515928, episode_reward=558.01 +/- 813.58
Episode length: 968.20 +/- 236.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 515928   |
---------------------------------
Eval num_timesteps=517920, episode_reward=9.70 +/- 126.45
Episode length: 641.80 +/- 184.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 9.7      |
| time/              |          |
|    total_timesteps | 517920   |
---------------------------------
Eval num_timesteps=519912, episode_reward=357.08 +/- 329.19
Episode length: 874.00 +/- 126.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 519912   |
---------------------------------
Eval num_timesteps=521904, episode_reward=386.18 +/- 261.53
Episode length: 1141.80 +/- 343.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 521904   |
---------------------------------
Eval num_timesteps=523896, episode_reward=155.03 +/- 220.80
Episode length: 790.60 +/- 375.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 523896   |
---------------------------------
Eval num_timesteps=525888, episode_reward=22.82 +/- 59.10
Episode length: 719.60 +/- 182.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 525888   |
---------------------------------
Eval num_timesteps=527880, episode_reward=31.32 +/- 182.09
Episode length: 777.80 +/- 249.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 31.3     |
| time/              |          |
|    total_timesteps | 527880   |
---------------------------------
Eval num_timesteps=529872, episode_reward=519.34 +/- 629.27
Episode length: 885.20 +/- 123.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 529872   |
---------------------------------
Eval num_timesteps=531864, episode_reward=306.63 +/- 255.50
Episode length: 973.20 +/- 281.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 531864   |
---------------------------------
Eval num_timesteps=533856, episode_reward=432.07 +/- 421.17
Episode length: 787.40 +/- 186.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 533856   |
---------------------------------
Eval num_timesteps=535848, episode_reward=56.91 +/- 73.57
Episode length: 791.80 +/- 349.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 56.9     |
| time/              |          |
|    total_timesteps | 535848   |
---------------------------------
Eval num_timesteps=537840, episode_reward=657.83 +/- 817.27
Episode length: 1068.20 +/- 326.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 537840   |
---------------------------------
New best mean reward!
Eval num_timesteps=539832, episode_reward=54.79 +/- 126.25
Episode length: 627.00 +/- 231.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 54.8     |
| time/              |          |
|    total_timesteps | 539832   |
---------------------------------
Eval num_timesteps=541824, episode_reward=521.15 +/- 435.96
Episode length: 1131.60 +/- 351.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 521         |
| time/                   |             |
|    total_timesteps      | 541824      |
| train/                  |             |
|    approx_kl            | 0.004145916 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.001       |
|    loss                 | 99.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.13        |
|    value_loss           | 271         |
-----------------------------------------
Eval num_timesteps=543816, episode_reward=417.52 +/- 368.65
Episode length: 812.40 +/- 112.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 543816   |
---------------------------------
Eval num_timesteps=545808, episode_reward=353.17 +/- 411.41
Episode length: 1086.20 +/- 210.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 545808   |
---------------------------------
Eval num_timesteps=547800, episode_reward=105.82 +/- 92.81
Episode length: 781.60 +/- 142.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 547800   |
---------------------------------
Eval num_timesteps=549792, episode_reward=260.98 +/- 230.81
Episode length: 810.00 +/- 134.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 549792   |
---------------------------------
Eval num_timesteps=551784, episode_reward=396.97 +/- 281.52
Episode length: 970.80 +/- 261.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 551784   |
---------------------------------
Eval num_timesteps=553776, episode_reward=122.09 +/- 172.09
Episode length: 827.20 +/- 109.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 553776   |
---------------------------------
Eval num_timesteps=555768, episode_reward=353.61 +/- 354.04
Episode length: 809.20 +/- 81.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 555768   |
---------------------------------
Eval num_timesteps=557760, episode_reward=476.15 +/- 875.67
Episode length: 738.40 +/- 155.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 557760   |
---------------------------------
Eval num_timesteps=559752, episode_reward=211.88 +/- 147.31
Episode length: 935.60 +/- 119.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 559752   |
---------------------------------
Eval num_timesteps=561744, episode_reward=143.46 +/- 194.51
Episode length: 823.60 +/- 124.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 561744   |
---------------------------------
Eval num_timesteps=563736, episode_reward=279.76 +/- 324.06
Episode length: 939.80 +/- 189.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 563736   |
---------------------------------
Eval num_timesteps=565728, episode_reward=-96.30 +/- 87.76
Episode length: 560.60 +/- 191.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | -96.3    |
| time/              |          |
|    total_timesteps | 565728   |
---------------------------------
Eval num_timesteps=567720, episode_reward=265.05 +/- 135.03
Episode length: 1003.80 +/- 158.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 567720   |
---------------------------------
Eval num_timesteps=569712, episode_reward=703.10 +/- 1049.36
Episode length: 876.00 +/- 366.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 569712   |
---------------------------------
New best mean reward!
Eval num_timesteps=571704, episode_reward=329.98 +/- 186.98
Episode length: 1004.60 +/- 114.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 571704   |
---------------------------------
Eval num_timesteps=573696, episode_reward=146.77 +/- 154.38
Episode length: 924.60 +/- 206.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 573696   |
---------------------------------
Eval num_timesteps=575688, episode_reward=185.76 +/- 416.66
Episode length: 952.60 +/- 202.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 575688   |
---------------------------------
Eval num_timesteps=577680, episode_reward=759.20 +/- 605.48
Episode length: 910.80 +/- 160.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 577680   |
---------------------------------
New best mean reward!
Eval num_timesteps=579672, episode_reward=11.90 +/- 138.86
Episode length: 724.40 +/- 108.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 579672   |
---------------------------------
Eval num_timesteps=581664, episode_reward=210.54 +/- 181.70
Episode length: 927.20 +/- 234.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 581664   |
---------------------------------
Eval num_timesteps=583656, episode_reward=71.17 +/- 205.64
Episode length: 674.80 +/- 218.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 71.2     |
| time/              |          |
|    total_timesteps | 583656   |
---------------------------------
Eval num_timesteps=585648, episode_reward=191.08 +/- 182.71
Episode length: 915.20 +/- 209.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 585648   |
---------------------------------
Eval num_timesteps=587640, episode_reward=225.00 +/- 174.16
Episode length: 900.40 +/- 184.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 587640   |
---------------------------------
Eval num_timesteps=589632, episode_reward=39.77 +/- 61.49
Episode length: 697.80 +/- 185.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 39.8     |
| time/              |          |
|    total_timesteps | 589632   |
---------------------------------
Eval num_timesteps=591624, episode_reward=853.70 +/- 857.70
Episode length: 984.80 +/- 314.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 985          |
|    mean_reward          | 854          |
| time/                   |              |
|    total_timesteps      | 591624       |
| train/                  |              |
|    approx_kl            | 0.0047262926 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 125          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.15         |
|    value_loss           | 333          |
------------------------------------------
New best mean reward!
Eval num_timesteps=593616, episode_reward=341.81 +/- 291.06
Episode length: 964.00 +/- 239.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 593616   |
---------------------------------
Eval num_timesteps=595608, episode_reward=184.38 +/- 175.09
Episode length: 938.60 +/- 365.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 595608   |
---------------------------------
Eval num_timesteps=597600, episode_reward=328.52 +/- 237.63
Episode length: 871.80 +/- 252.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 597600   |
---------------------------------
Eval num_timesteps=599592, episode_reward=456.44 +/- 471.03
Episode length: 949.20 +/- 277.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 599592   |
---------------------------------
Eval num_timesteps=601584, episode_reward=622.27 +/- 809.10
Episode length: 1032.60 +/- 131.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 601584   |
---------------------------------
Eval num_timesteps=603576, episode_reward=161.36 +/- 256.16
Episode length: 756.60 +/- 150.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 603576   |
---------------------------------
Eval num_timesteps=605568, episode_reward=382.87 +/- 199.13
Episode length: 945.60 +/- 161.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 605568   |
---------------------------------
Eval num_timesteps=607560, episode_reward=115.37 +/- 105.51
Episode length: 824.60 +/- 204.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 607560   |
---------------------------------
Eval num_timesteps=609552, episode_reward=431.87 +/- 699.24
Episode length: 783.80 +/- 300.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 609552   |
---------------------------------
Eval num_timesteps=611544, episode_reward=225.89 +/- 103.40
Episode length: 849.40 +/- 231.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 611544   |
---------------------------------
Eval num_timesteps=613536, episode_reward=445.14 +/- 621.16
Episode length: 882.00 +/- 254.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 613536   |
---------------------------------
Eval num_timesteps=615528, episode_reward=135.42 +/- 194.25
Episode length: 691.80 +/- 166.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 135      |
| time/              |          |
|    total_timesteps | 615528   |
---------------------------------
Eval num_timesteps=617520, episode_reward=210.02 +/- 446.34
Episode length: 728.40 +/- 172.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 617520   |
---------------------------------
Eval num_timesteps=619512, episode_reward=457.30 +/- 560.71
Episode length: 1046.00 +/- 238.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 619512   |
---------------------------------
Eval num_timesteps=621504, episode_reward=418.41 +/- 413.65
Episode length: 948.80 +/- 200.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 621504   |
---------------------------------
Eval num_timesteps=623496, episode_reward=267.36 +/- 297.56
Episode length: 978.20 +/- 298.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 623496   |
---------------------------------
Eval num_timesteps=625488, episode_reward=167.39 +/- 187.45
Episode length: 858.40 +/- 173.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 625488   |
---------------------------------
Eval num_timesteps=627480, episode_reward=60.66 +/- 178.11
Episode length: 637.20 +/- 96.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 60.7     |
| time/              |          |
|    total_timesteps | 627480   |
---------------------------------
Eval num_timesteps=629472, episode_reward=241.60 +/- 127.91
Episode length: 898.80 +/- 114.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 629472   |
---------------------------------
Eval num_timesteps=631464, episode_reward=260.40 +/- 331.32
Episode length: 840.00 +/- 202.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 631464   |
---------------------------------
Eval num_timesteps=633456, episode_reward=231.07 +/- 320.98
Episode length: 710.40 +/- 161.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 633456   |
---------------------------------
Eval num_timesteps=635448, episode_reward=223.28 +/- 182.96
Episode length: 786.40 +/- 187.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 635448   |
---------------------------------
Eval num_timesteps=637440, episode_reward=143.86 +/- 122.93
Episode length: 937.80 +/- 236.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 144      |
| time/              |          |
|    total_timesteps | 637440   |
---------------------------------
Eval num_timesteps=639432, episode_reward=283.69 +/- 430.85
Episode length: 796.20 +/- 173.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 796         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 639432      |
| train/                  |             |
|    approx_kl            | 0.004008304 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.28       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.001       |
|    loss                 | 82.6        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00119    |
|    std                  | 1.17        |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=641424, episode_reward=546.01 +/- 367.27
Episode length: 904.20 +/- 150.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 641424   |
---------------------------------
Eval num_timesteps=643416, episode_reward=697.55 +/- 1088.23
Episode length: 834.20 +/- 109.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 643416   |
---------------------------------
Eval num_timesteps=645408, episode_reward=231.96 +/- 282.32
Episode length: 768.80 +/- 104.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 645408   |
---------------------------------
Eval num_timesteps=647400, episode_reward=260.98 +/- 319.34
Episode length: 808.00 +/- 205.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 647400   |
---------------------------------
Eval num_timesteps=649392, episode_reward=309.06 +/- 291.29
Episode length: 829.60 +/- 135.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 649392   |
---------------------------------
Eval num_timesteps=651384, episode_reward=564.54 +/- 328.93
Episode length: 999.40 +/- 175.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 651384   |
---------------------------------
Eval num_timesteps=653376, episode_reward=30.50 +/- 60.14
Episode length: 616.60 +/- 103.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 30.5     |
| time/              |          |
|    total_timesteps | 653376   |
---------------------------------
Eval num_timesteps=655368, episode_reward=139.63 +/- 146.49
Episode length: 722.20 +/- 185.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 655368   |
---------------------------------
Eval num_timesteps=657360, episode_reward=742.60 +/- 1178.91
Episode length: 795.00 +/- 108.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 657360   |
---------------------------------
Eval num_timesteps=659352, episode_reward=260.52 +/- 405.58
Episode length: 862.80 +/- 252.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 659352   |
---------------------------------
Eval num_timesteps=661344, episode_reward=487.85 +/- 533.79
Episode length: 860.80 +/- 283.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 661344   |
---------------------------------
Eval num_timesteps=663336, episode_reward=329.44 +/- 422.17
Episode length: 819.80 +/- 230.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 663336   |
---------------------------------
Eval num_timesteps=665328, episode_reward=679.26 +/- 716.64
Episode length: 989.00 +/- 197.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 989      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 665328   |
---------------------------------
Eval num_timesteps=667320, episode_reward=904.37 +/- 1057.29
Episode length: 865.40 +/- 139.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 667320   |
---------------------------------
New best mean reward!
Eval num_timesteps=669312, episode_reward=744.92 +/- 720.32
Episode length: 789.80 +/- 172.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 669312   |
---------------------------------
Eval num_timesteps=671304, episode_reward=648.09 +/- 648.82
Episode length: 876.80 +/- 178.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 671304   |
---------------------------------
Eval num_timesteps=673296, episode_reward=812.46 +/- 458.06
Episode length: 913.40 +/- 194.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 673296   |
---------------------------------
Eval num_timesteps=675288, episode_reward=252.20 +/- 315.53
Episode length: 762.80 +/- 147.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 675288   |
---------------------------------
Eval num_timesteps=677280, episode_reward=528.23 +/- 372.32
Episode length: 1080.40 +/- 289.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 677280   |
---------------------------------
Eval num_timesteps=679272, episode_reward=694.34 +/- 572.24
Episode length: 910.20 +/- 247.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 679272   |
---------------------------------
Eval num_timesteps=681264, episode_reward=323.25 +/- 369.43
Episode length: 798.80 +/- 240.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 681264   |
---------------------------------
Eval num_timesteps=683256, episode_reward=518.00 +/- 598.85
Episode length: 830.80 +/- 253.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 683256   |
---------------------------------
Eval num_timesteps=685248, episode_reward=361.08 +/- 292.31
Episode length: 830.60 +/- 109.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 685248   |
---------------------------------
Eval num_timesteps=687240, episode_reward=155.99 +/- 106.48
Episode length: 688.00 +/- 99.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 687240   |
---------------------------------
Eval num_timesteps=689232, episode_reward=399.07 +/- 254.36
Episode length: 827.80 +/- 160.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 828         |
|    mean_reward          | 399         |
| time/                   |             |
|    total_timesteps      | 689232      |
| train/                  |             |
|    approx_kl            | 0.004853659 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.001       |
|    loss                 | 260         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00144    |
|    std                  | 1.18        |
|    value_loss           | 571         |
-----------------------------------------
Eval num_timesteps=691224, episode_reward=628.83 +/- 402.57
Episode length: 727.20 +/- 81.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 691224   |
---------------------------------
Eval num_timesteps=693216, episode_reward=245.90 +/- 240.88
Episode length: 668.80 +/- 111.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 693216   |
---------------------------------
Eval num_timesteps=695208, episode_reward=750.17 +/- 607.10
Episode length: 1000.40 +/- 377.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 695208   |
---------------------------------
Eval num_timesteps=697200, episode_reward=713.48 +/- 339.78
Episode length: 822.80 +/- 112.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 697200   |
---------------------------------
Eval num_timesteps=699192, episode_reward=1051.29 +/- 1728.30
Episode length: 801.40 +/- 280.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 699192   |
---------------------------------
New best mean reward!
Eval num_timesteps=701184, episode_reward=426.16 +/- 229.66
Episode length: 838.00 +/- 138.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 701184   |
---------------------------------
Eval num_timesteps=703176, episode_reward=322.51 +/- 192.83
Episode length: 755.20 +/- 49.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 703176   |
---------------------------------
Eval num_timesteps=705168, episode_reward=243.57 +/- 118.76
Episode length: 721.60 +/- 143.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 705168   |
---------------------------------
Eval num_timesteps=707160, episode_reward=601.27 +/- 457.46
Episode length: 956.40 +/- 234.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 956      |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 707160   |
---------------------------------
Eval num_timesteps=709152, episode_reward=314.08 +/- 146.49
Episode length: 725.80 +/- 136.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 709152   |
---------------------------------
Eval num_timesteps=711144, episode_reward=1094.76 +/- 1668.16
Episode length: 921.20 +/- 266.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 711144   |
---------------------------------
New best mean reward!
Eval num_timesteps=713136, episode_reward=455.17 +/- 206.96
Episode length: 874.20 +/- 184.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 713136   |
---------------------------------
Eval num_timesteps=715128, episode_reward=540.63 +/- 211.27
Episode length: 798.20 +/- 75.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 715128   |
---------------------------------
Eval num_timesteps=717120, episode_reward=439.15 +/- 290.06
Episode length: 877.80 +/- 204.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 717120   |
---------------------------------
Eval num_timesteps=719112, episode_reward=384.70 +/- 294.10
Episode length: 977.60 +/- 304.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 719112   |
---------------------------------
Eval num_timesteps=721104, episode_reward=729.19 +/- 672.14
Episode length: 858.00 +/- 185.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 721104   |
---------------------------------
Eval num_timesteps=723096, episode_reward=423.31 +/- 302.88
Episode length: 777.20 +/- 214.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 723096   |
---------------------------------
Eval num_timesteps=725088, episode_reward=348.97 +/- 295.51
Episode length: 744.40 +/- 151.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 725088   |
---------------------------------
Eval num_timesteps=727080, episode_reward=350.90 +/- 195.95
Episode length: 741.80 +/- 123.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 727080   |
---------------------------------
Eval num_timesteps=729072, episode_reward=150.11 +/- 124.87
Episode length: 793.00 +/- 223.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 729072   |
---------------------------------
Eval num_timesteps=731064, episode_reward=507.62 +/- 189.11
Episode length: 786.00 +/- 43.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 731064   |
---------------------------------
Eval num_timesteps=733056, episode_reward=408.39 +/- 399.81
Episode length: 808.60 +/- 164.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 733056   |
---------------------------------
Eval num_timesteps=735048, episode_reward=903.15 +/- 465.76
Episode length: 1032.00 +/- 288.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 735048   |
---------------------------------
Eval num_timesteps=737040, episode_reward=1835.06 +/- 852.76
Episode length: 1043.00 +/- 239.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 737040   |
---------------------------------
New best mean reward!
Eval num_timesteps=739032, episode_reward=807.64 +/- 414.85
Episode length: 763.20 +/- 96.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 763         |
|    mean_reward          | 808         |
| time/                   |             |
|    total_timesteps      | 739032      |
| train/                  |             |
|    approx_kl            | 0.004363532 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.001       |
|    loss                 | 244         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00123    |
|    std                  | 1.19        |
|    value_loss           | 744         |
-----------------------------------------
Eval num_timesteps=741024, episode_reward=466.13 +/- 223.80
Episode length: 725.40 +/- 151.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 741024   |
---------------------------------
Eval num_timesteps=743016, episode_reward=587.65 +/- 270.18
Episode length: 804.40 +/- 224.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 743016   |
---------------------------------
Eval num_timesteps=745008, episode_reward=379.46 +/- 347.67
Episode length: 636.00 +/- 142.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 745008   |
---------------------------------
Eval num_timesteps=747000, episode_reward=254.08 +/- 60.67
Episode length: 656.40 +/- 111.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=748992, episode_reward=168.63 +/- 80.47
Episode length: 614.60 +/- 42.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 748992   |
---------------------------------
Eval num_timesteps=750984, episode_reward=670.64 +/- 800.94
Episode length: 675.00 +/- 185.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 750984   |
---------------------------------
Eval num_timesteps=752976, episode_reward=537.93 +/- 253.58
Episode length: 799.40 +/- 67.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 752976   |
---------------------------------
Eval num_timesteps=754968, episode_reward=1151.96 +/- 846.17
Episode length: 914.20 +/- 215.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 754968   |
---------------------------------
Eval num_timesteps=756960, episode_reward=257.62 +/- 123.23
Episode length: 667.00 +/- 78.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 756960   |
---------------------------------
Eval num_timesteps=758952, episode_reward=631.09 +/- 560.14
Episode length: 763.00 +/- 105.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 758952   |
---------------------------------
Eval num_timesteps=760944, episode_reward=630.92 +/- 397.52
Episode length: 844.20 +/- 263.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 760944   |
---------------------------------
Eval num_timesteps=762936, episode_reward=265.47 +/- 191.95
Episode length: 640.40 +/- 51.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 762936   |
---------------------------------
Eval num_timesteps=764928, episode_reward=470.15 +/- 353.11
Episode length: 721.80 +/- 152.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 764928   |
---------------------------------
Eval num_timesteps=766920, episode_reward=403.57 +/- 186.79
Episode length: 849.60 +/- 147.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 766920   |
---------------------------------
Eval num_timesteps=768912, episode_reward=640.21 +/- 165.12
Episode length: 872.60 +/- 122.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 768912   |
---------------------------------
Eval num_timesteps=770904, episode_reward=567.12 +/- 646.86
Episode length: 703.00 +/- 165.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 770904   |
---------------------------------
Eval num_timesteps=772896, episode_reward=556.42 +/- 494.40
Episode length: 807.20 +/- 370.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 772896   |
---------------------------------
Eval num_timesteps=774888, episode_reward=426.25 +/- 331.55
Episode length: 672.40 +/- 109.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 774888   |
---------------------------------
Eval num_timesteps=776880, episode_reward=782.09 +/- 758.82
Episode length: 789.00 +/- 219.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 776880   |
---------------------------------
Eval num_timesteps=778872, episode_reward=285.11 +/- 151.10
Episode length: 699.40 +/- 86.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 778872   |
---------------------------------
Eval num_timesteps=780864, episode_reward=888.68 +/- 503.64
Episode length: 867.20 +/- 98.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 780864   |
---------------------------------
Eval num_timesteps=782856, episode_reward=812.68 +/- 480.99
Episode length: 921.80 +/- 194.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 782856   |
---------------------------------
Eval num_timesteps=784848, episode_reward=528.27 +/- 175.63
Episode length: 943.20 +/- 73.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 784848   |
---------------------------------
Eval num_timesteps=786840, episode_reward=467.64 +/- 104.36
Episode length: 702.80 +/- 79.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 703          |
|    mean_reward          | 468          |
| time/                   |              |
|    total_timesteps      | 786840       |
| train/                  |              |
|    approx_kl            | 0.0036011515 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.39        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.001        |
|    loss                 | 361          |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.2          |
|    value_loss           | 652          |
------------------------------------------
Eval num_timesteps=788832, episode_reward=735.37 +/- 268.78
Episode length: 876.40 +/- 132.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 788832   |
---------------------------------
Eval num_timesteps=790824, episode_reward=489.69 +/- 259.88
Episode length: 719.40 +/- 160.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 490      |
| time/              |          |
|    total_timesteps | 790824   |
---------------------------------
Eval num_timesteps=792816, episode_reward=370.74 +/- 163.80
Episode length: 663.00 +/- 118.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 792816   |
---------------------------------
Eval num_timesteps=794808, episode_reward=418.26 +/- 102.99
Episode length: 778.00 +/- 77.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 794808   |
---------------------------------
Eval num_timesteps=796800, episode_reward=1063.53 +/- 797.21
Episode length: 760.40 +/- 90.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 796800   |
---------------------------------
Eval num_timesteps=798792, episode_reward=780.59 +/- 764.23
Episode length: 718.20 +/- 185.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 798792   |
---------------------------------
Eval num_timesteps=800784, episode_reward=438.20 +/- 181.65
Episode length: 654.40 +/- 86.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 800784   |
---------------------------------
Eval num_timesteps=802776, episode_reward=404.07 +/- 377.71
Episode length: 759.80 +/- 155.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 802776   |
---------------------------------
Eval num_timesteps=804768, episode_reward=477.20 +/- 145.31
Episode length: 816.60 +/- 240.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 804768   |
---------------------------------
Eval num_timesteps=806760, episode_reward=375.11 +/- 329.14
Episode length: 689.80 +/- 80.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 806760   |
---------------------------------
Eval num_timesteps=808752, episode_reward=727.96 +/- 370.32
Episode length: 762.80 +/- 90.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 808752   |
---------------------------------
Eval num_timesteps=810744, episode_reward=575.49 +/- 166.87
Episode length: 815.00 +/- 137.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 810744   |
---------------------------------
Eval num_timesteps=812736, episode_reward=342.36 +/- 114.78
Episode length: 796.60 +/- 233.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 812736   |
---------------------------------
Eval num_timesteps=814728, episode_reward=241.52 +/- 74.42
Episode length: 634.20 +/- 116.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 814728   |
---------------------------------
Eval num_timesteps=816720, episode_reward=682.96 +/- 423.15
Episode length: 805.40 +/- 163.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 816720   |
---------------------------------
Eval num_timesteps=818712, episode_reward=815.08 +/- 586.94
Episode length: 718.60 +/- 97.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 818712   |
---------------------------------
Eval num_timesteps=820704, episode_reward=430.50 +/- 126.78
Episode length: 699.20 +/- 71.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 820704   |
---------------------------------
Eval num_timesteps=822696, episode_reward=703.22 +/- 775.44
Episode length: 837.40 +/- 265.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 822696   |
---------------------------------
Eval num_timesteps=824688, episode_reward=681.42 +/- 186.37
Episode length: 884.20 +/- 157.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 824688   |
---------------------------------
Eval num_timesteps=826680, episode_reward=786.97 +/- 442.32
Episode length: 709.00 +/- 121.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 826680   |
---------------------------------
Eval num_timesteps=828672, episode_reward=634.47 +/- 360.58
Episode length: 842.60 +/- 160.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 828672   |
---------------------------------
Eval num_timesteps=830664, episode_reward=1032.55 +/- 722.04
Episode length: 830.20 +/- 123.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 830664   |
---------------------------------
Eval num_timesteps=832656, episode_reward=420.50 +/- 315.41
Episode length: 742.40 +/- 88.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 832656   |
---------------------------------
Eval num_timesteps=834648, episode_reward=444.61 +/- 202.52
Episode length: 781.40 +/- 193.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 834648   |
---------------------------------
Eval num_timesteps=836640, episode_reward=130.90 +/- 92.78
Episode length: 584.20 +/- 123.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 584          |
|    mean_reward          | 131          |
| time/                   |              |
|    total_timesteps      | 836640       |
| train/                  |              |
|    approx_kl            | 0.0027600108 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.44        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 149          |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.21         |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=838632, episode_reward=845.38 +/- 919.05
Episode length: 729.80 +/- 176.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 838632   |
---------------------------------
Eval num_timesteps=840624, episode_reward=181.94 +/- 114.15
Episode length: 550.40 +/- 53.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 840624   |
---------------------------------
Eval num_timesteps=842616, episode_reward=220.36 +/- 137.41
Episode length: 574.00 +/- 65.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 842616   |
---------------------------------
Eval num_timesteps=844608, episode_reward=380.35 +/- 111.37
Episode length: 592.40 +/- 43.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 844608   |
---------------------------------
Eval num_timesteps=846600, episode_reward=657.79 +/- 458.37
Episode length: 724.80 +/- 163.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 846600   |
---------------------------------
Eval num_timesteps=848592, episode_reward=502.30 +/- 203.50
Episode length: 686.20 +/- 128.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 848592   |
---------------------------------
Eval num_timesteps=850584, episode_reward=636.59 +/- 274.80
Episode length: 728.40 +/- 104.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 850584   |
---------------------------------
Eval num_timesteps=852576, episode_reward=692.12 +/- 482.00
Episode length: 686.60 +/- 126.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 852576   |
---------------------------------
Eval num_timesteps=854568, episode_reward=599.68 +/- 265.45
Episode length: 648.60 +/- 120.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 854568   |
---------------------------------
Eval num_timesteps=856560, episode_reward=473.80 +/- 265.26
Episode length: 682.20 +/- 97.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 856560   |
---------------------------------
Eval num_timesteps=858552, episode_reward=611.48 +/- 382.54
Episode length: 717.00 +/- 118.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 858552   |
---------------------------------
Eval num_timesteps=860544, episode_reward=610.57 +/- 341.70
Episode length: 672.80 +/- 125.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 860544   |
---------------------------------
Eval num_timesteps=862536, episode_reward=530.24 +/- 174.73
Episode length: 737.60 +/- 151.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 862536   |
---------------------------------
Eval num_timesteps=864528, episode_reward=452.04 +/- 303.56
Episode length: 643.80 +/- 127.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 864528   |
---------------------------------
Eval num_timesteps=866520, episode_reward=363.38 +/- 161.77
Episode length: 692.60 +/- 95.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 866520   |
---------------------------------
Eval num_timesteps=868512, episode_reward=638.11 +/- 331.12
Episode length: 712.40 +/- 83.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 868512   |
---------------------------------
Eval num_timesteps=870504, episode_reward=569.30 +/- 307.29
Episode length: 745.60 +/- 173.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 870504   |
---------------------------------
Eval num_timesteps=872496, episode_reward=665.42 +/- 275.05
Episode length: 783.80 +/- 127.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 872496   |
---------------------------------
Eval num_timesteps=874488, episode_reward=517.87 +/- 276.58
Episode length: 702.00 +/- 65.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 874488   |
---------------------------------
Eval num_timesteps=876480, episode_reward=244.72 +/- 75.63
Episode length: 590.60 +/- 71.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 876480   |
---------------------------------
Eval num_timesteps=878472, episode_reward=1128.97 +/- 767.63
Episode length: 748.80 +/- 200.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 878472   |
---------------------------------
Eval num_timesteps=880464, episode_reward=438.93 +/- 233.81
Episode length: 630.00 +/- 63.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 880464   |
---------------------------------
Eval num_timesteps=882456, episode_reward=1113.30 +/- 674.37
Episode length: 880.00 +/- 243.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 882456   |
---------------------------------
Eval num_timesteps=884448, episode_reward=721.98 +/- 402.27
Episode length: 749.20 +/- 154.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 884448   |
---------------------------------
Eval num_timesteps=886440, episode_reward=600.85 +/- 299.52
Episode length: 640.40 +/- 66.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 640          |
|    mean_reward          | 601          |
| time/                   |              |
|    total_timesteps      | 886440       |
| train/                  |              |
|    approx_kl            | 0.0038542878 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 48.9         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.22         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=888432, episode_reward=620.42 +/- 259.90
Episode length: 709.80 +/- 86.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 888432   |
---------------------------------
Eval num_timesteps=890424, episode_reward=473.92 +/- 288.44
Episode length: 645.60 +/- 89.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 890424   |
---------------------------------
Eval num_timesteps=892416, episode_reward=422.03 +/- 208.21
Episode length: 615.00 +/- 89.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 892416   |
---------------------------------
Eval num_timesteps=894408, episode_reward=911.23 +/- 514.91
Episode length: 713.80 +/- 73.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 894408   |
---------------------------------
Eval num_timesteps=896400, episode_reward=785.73 +/- 550.43
Episode length: 696.80 +/- 86.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 896400   |
---------------------------------
Eval num_timesteps=898392, episode_reward=511.29 +/- 201.08
Episode length: 658.80 +/- 82.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 898392   |
---------------------------------
Eval num_timesteps=900384, episode_reward=589.92 +/- 350.38
Episode length: 708.20 +/- 163.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 900384   |
---------------------------------
Eval num_timesteps=902376, episode_reward=397.41 +/- 177.48
Episode length: 608.00 +/- 56.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 902376   |
---------------------------------
Eval num_timesteps=904368, episode_reward=217.86 +/- 196.39
Episode length: 553.80 +/- 163.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 904368   |
---------------------------------
Eval num_timesteps=906360, episode_reward=314.02 +/- 181.03
Episode length: 656.20 +/- 99.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 906360   |
---------------------------------
Eval num_timesteps=908352, episode_reward=623.19 +/- 411.18
Episode length: 729.40 +/- 90.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 908352   |
---------------------------------
Eval num_timesteps=910344, episode_reward=731.26 +/- 138.73
Episode length: 715.20 +/- 54.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 910344   |
---------------------------------
Eval num_timesteps=912336, episode_reward=470.40 +/- 134.84
Episode length: 689.20 +/- 97.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 912336   |
---------------------------------
Eval num_timesteps=914328, episode_reward=679.95 +/- 400.57
Episode length: 646.40 +/- 95.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 914328   |
---------------------------------
Eval num_timesteps=916320, episode_reward=572.50 +/- 188.20
Episode length: 699.40 +/- 138.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 916320   |
---------------------------------
Eval num_timesteps=918312, episode_reward=474.61 +/- 343.09
Episode length: 594.00 +/- 113.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 918312   |
---------------------------------
Eval num_timesteps=920304, episode_reward=451.72 +/- 158.60
Episode length: 744.20 +/- 270.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 920304   |
---------------------------------
Eval num_timesteps=922296, episode_reward=343.19 +/- 210.76
Episode length: 603.40 +/- 52.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 922296   |
---------------------------------
Eval num_timesteps=924288, episode_reward=442.58 +/- 101.76
Episode length: 656.00 +/- 109.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 924288   |
---------------------------------
Eval num_timesteps=926280, episode_reward=486.12 +/- 144.52
Episode length: 626.40 +/- 98.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 926280   |
---------------------------------
Eval num_timesteps=928272, episode_reward=263.04 +/- 179.77
Episode length: 533.00 +/- 56.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 928272   |
---------------------------------
Eval num_timesteps=930264, episode_reward=474.70 +/- 362.72
Episode length: 584.60 +/- 157.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 930264   |
---------------------------------
Eval num_timesteps=932256, episode_reward=540.71 +/- 93.54
Episode length: 679.60 +/- 70.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 932256   |
---------------------------------
Eval num_timesteps=934248, episode_reward=384.45 +/- 191.94
Episode length: 600.00 +/- 38.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 600          |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 934248       |
| train/                  |              |
|    approx_kl            | 0.0036617455 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 82.3         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.22         |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=936240, episode_reward=785.20 +/- 464.77
Episode length: 672.20 +/- 72.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 936240   |
---------------------------------
Eval num_timesteps=938232, episode_reward=414.91 +/- 164.00
Episode length: 615.40 +/- 58.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 938232   |
---------------------------------
Eval num_timesteps=940224, episode_reward=297.60 +/- 196.17
Episode length: 556.40 +/- 86.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 940224   |
---------------------------------
Eval num_timesteps=942216, episode_reward=620.40 +/- 131.12
Episode length: 691.80 +/- 78.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 942216   |
---------------------------------
Eval num_timesteps=944208, episode_reward=829.00 +/- 468.64
Episode length: 675.60 +/- 40.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 944208   |
---------------------------------
Eval num_timesteps=946200, episode_reward=776.04 +/- 422.81
Episode length: 636.60 +/- 68.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 946200   |
---------------------------------
Eval num_timesteps=948192, episode_reward=448.29 +/- 151.43
Episode length: 635.60 +/- 66.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 948192   |
---------------------------------
Eval num_timesteps=950184, episode_reward=688.25 +/- 423.93
Episode length: 671.00 +/- 69.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 950184   |
---------------------------------
Eval num_timesteps=952176, episode_reward=696.45 +/- 462.84
Episode length: 694.80 +/- 217.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 952176   |
---------------------------------
Eval num_timesteps=954168, episode_reward=1143.50 +/- 728.77
Episode length: 741.60 +/- 71.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 954168   |
---------------------------------
Eval num_timesteps=956160, episode_reward=470.84 +/- 234.94
Episode length: 622.80 +/- 23.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 956160   |
---------------------------------
Eval num_timesteps=958152, episode_reward=427.23 +/- 94.17
Episode length: 619.20 +/- 103.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 958152   |
---------------------------------
Eval num_timesteps=960144, episode_reward=401.60 +/- 235.41
Episode length: 606.80 +/- 86.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 960144   |
---------------------------------
Eval num_timesteps=962136, episode_reward=572.86 +/- 268.19
Episode length: 636.40 +/- 101.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 962136   |
---------------------------------
Eval num_timesteps=964128, episode_reward=335.38 +/- 175.36
Episode length: 568.00 +/- 75.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 964128   |
---------------------------------
Eval num_timesteps=966120, episode_reward=503.54 +/- 176.69
Episode length: 667.20 +/- 86.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 966120   |
---------------------------------
Eval num_timesteps=968112, episode_reward=486.77 +/- 159.90
Episode length: 649.60 +/- 36.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 968112   |
---------------------------------
Eval num_timesteps=970104, episode_reward=415.82 +/- 234.72
Episode length: 572.00 +/- 61.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 970104   |
---------------------------------
Eval num_timesteps=972096, episode_reward=610.68 +/- 123.68
Episode length: 696.40 +/- 54.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 972096   |
---------------------------------
Eval num_timesteps=974088, episode_reward=405.47 +/- 221.65
Episode length: 581.40 +/- 84.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 974088   |
---------------------------------
Eval num_timesteps=976080, episode_reward=444.84 +/- 75.76
Episode length: 600.80 +/- 36.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 976080   |
---------------------------------
Eval num_timesteps=978072, episode_reward=1083.95 +/- 785.84
Episode length: 683.40 +/- 127.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 978072   |
---------------------------------
Eval num_timesteps=980064, episode_reward=646.70 +/- 309.50
Episode length: 678.60 +/- 110.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 980064   |
---------------------------------
Eval num_timesteps=982056, episode_reward=684.18 +/- 249.95
Episode length: 678.00 +/- 85.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 982056   |
---------------------------------
Eval num_timesteps=984048, episode_reward=727.48 +/- 345.36
Episode length: 706.60 +/- 121.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 707          |
|    mean_reward          | 727          |
| time/                   |              |
|    total_timesteps      | 984048       |
| train/                  |              |
|    approx_kl            | 0.0031312143 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.5         |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 54.5         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00128     |
|    std                  | 1.23         |
|    value_loss           | 324          |
------------------------------------------
Eval num_timesteps=986040, episode_reward=661.06 +/- 338.83
Episode length: 646.80 +/- 88.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 986040   |
---------------------------------
Eval num_timesteps=988032, episode_reward=817.46 +/- 496.31
Episode length: 655.60 +/- 145.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 988032   |
---------------------------------
Eval num_timesteps=990024, episode_reward=1673.80 +/- 1068.69
Episode length: 865.00 +/- 192.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 990024   |
---------------------------------
Eval num_timesteps=992016, episode_reward=485.69 +/- 144.99
Episode length: 627.20 +/- 98.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 992016   |
---------------------------------
Eval num_timesteps=994008, episode_reward=548.14 +/- 222.48
Episode length: 644.60 +/- 63.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 994008   |
---------------------------------
Eval num_timesteps=996000, episode_reward=567.87 +/- 213.79
Episode length: 621.00 +/- 75.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=997992, episode_reward=917.10 +/- 289.64
Episode length: 658.20 +/- 34.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 997992   |
---------------------------------
Eval num_timesteps=999984, episode_reward=538.21 +/- 109.20
Episode length: 706.60 +/- 112.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 999984   |
---------------------------------
Eval num_timesteps=1001976, episode_reward=600.17 +/- 282.89
Episode length: 683.20 +/- 63.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 1001976  |
---------------------------------
Eval num_timesteps=1003968, episode_reward=484.41 +/- 118.01
Episode length: 694.40 +/- 145.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 1003968  |
---------------------------------
Eval num_timesteps=1005960, episode_reward=712.28 +/- 540.12
Episode length: 624.40 +/- 118.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 1005960  |
---------------------------------
Eval num_timesteps=1007952, episode_reward=433.62 +/- 278.52
Episode length: 677.60 +/- 95.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 1007952  |
---------------------------------
Eval num_timesteps=1009944, episode_reward=715.90 +/- 433.09
Episode length: 630.60 +/- 76.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 1009944  |
---------------------------------
Eval num_timesteps=1011936, episode_reward=1261.72 +/- 1328.52
Episode length: 693.40 +/- 196.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1011936  |
---------------------------------
Eval num_timesteps=1013928, episode_reward=829.69 +/- 290.64
Episode length: 649.60 +/- 86.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 1013928  |
---------------------------------
Eval num_timesteps=1015920, episode_reward=381.10 +/- 291.24
Episode length: 625.20 +/- 129.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 1015920  |
---------------------------------
Eval num_timesteps=1017912, episode_reward=1029.52 +/- 379.36
Episode length: 754.20 +/- 127.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1017912  |
---------------------------------
Eval num_timesteps=1019904, episode_reward=723.45 +/- 469.02
Episode length: 618.80 +/- 91.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 1019904  |
---------------------------------
Eval num_timesteps=1021896, episode_reward=552.20 +/- 193.06
Episode length: 626.80 +/- 50.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 1021896  |
---------------------------------
Eval num_timesteps=1023888, episode_reward=569.75 +/- 197.40
Episode length: 782.80 +/- 186.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 1023888  |
---------------------------------
Eval num_timesteps=1025880, episode_reward=655.28 +/- 238.12
Episode length: 662.60 +/- 70.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 1025880  |
---------------------------------
Eval num_timesteps=1027872, episode_reward=769.59 +/- 324.95
Episode length: 708.40 +/- 101.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 1027872  |
---------------------------------
Eval num_timesteps=1029864, episode_reward=599.58 +/- 302.34
Episode length: 686.00 +/- 195.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 1029864  |
---------------------------------
Eval num_timesteps=1031856, episode_reward=704.79 +/- 455.84
Episode length: 764.60 +/- 179.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 1031856  |
---------------------------------
Eval num_timesteps=1033848, episode_reward=478.19 +/- 247.89
Episode length: 608.40 +/- 107.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 608          |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 1033848      |
| train/                  |              |
|    approx_kl            | 0.0035634916 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.52        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 49.6         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.23         |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=1035840, episode_reward=228.88 +/- 69.61
Episode length: 641.60 +/- 67.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 1035840  |
---------------------------------
Eval num_timesteps=1037832, episode_reward=594.69 +/- 261.67
Episode length: 669.80 +/- 102.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 1037832  |
---------------------------------
Eval num_timesteps=1039824, episode_reward=682.99 +/- 530.70
Episode length: 636.00 +/- 150.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 1039824  |
---------------------------------
Eval num_timesteps=1041816, episode_reward=588.87 +/- 235.83
Episode length: 629.60 +/- 76.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 1041816  |
---------------------------------
Eval num_timesteps=1043808, episode_reward=524.12 +/- 212.84
Episode length: 690.40 +/- 125.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 524      |
| time/              |          |
|    total_timesteps | 1043808  |
---------------------------------
Eval num_timesteps=1045800, episode_reward=568.11 +/- 262.51
Episode length: 645.40 +/- 87.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 1045800  |
---------------------------------
Eval num_timesteps=1047792, episode_reward=566.82 +/- 299.57
Episode length: 684.60 +/- 111.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1047792  |
---------------------------------
Eval num_timesteps=1049784, episode_reward=885.70 +/- 439.86
Episode length: 641.00 +/- 73.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 1049784  |
---------------------------------
Eval num_timesteps=1051776, episode_reward=912.28 +/- 717.89
Episode length: 661.00 +/- 148.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 1051776  |
---------------------------------
Eval num_timesteps=1053768, episode_reward=660.72 +/- 146.15
Episode length: 664.20 +/- 63.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 1053768  |
---------------------------------
Eval num_timesteps=1055760, episode_reward=635.26 +/- 429.58
Episode length: 678.60 +/- 124.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 1055760  |
---------------------------------
Eval num_timesteps=1057752, episode_reward=736.23 +/- 387.84
Episode length: 692.60 +/- 122.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 1057752  |
---------------------------------
Eval num_timesteps=1059744, episode_reward=695.76 +/- 196.00
Episode length: 763.60 +/- 135.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 1059744  |
---------------------------------
Eval num_timesteps=1061736, episode_reward=861.21 +/- 530.11
Episode length: 671.60 +/- 92.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 1061736  |
---------------------------------
Eval num_timesteps=1063728, episode_reward=621.61 +/- 309.48
Episode length: 709.20 +/- 118.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 1063728  |
---------------------------------
Eval num_timesteps=1065720, episode_reward=637.80 +/- 457.22
Episode length: 633.00 +/- 86.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 1065720  |
---------------------------------
Eval num_timesteps=1067712, episode_reward=1260.68 +/- 810.72
Episode length: 739.00 +/- 161.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1067712  |
---------------------------------
Eval num_timesteps=1069704, episode_reward=1027.46 +/- 720.34
Episode length: 707.00 +/- 91.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1069704  |
---------------------------------
Eval num_timesteps=1071696, episode_reward=698.95 +/- 366.81
Episode length: 651.60 +/- 75.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 1071696  |
---------------------------------
Eval num_timesteps=1073688, episode_reward=654.03 +/- 239.33
Episode length: 616.40 +/- 45.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 1073688  |
---------------------------------
Eval num_timesteps=1075680, episode_reward=723.14 +/- 265.37
Episode length: 698.40 +/- 93.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 1075680  |
---------------------------------
Eval num_timesteps=1077672, episode_reward=603.96 +/- 273.23
Episode length: 774.20 +/- 223.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 1077672  |
---------------------------------
Eval num_timesteps=1079664, episode_reward=536.46 +/- 320.35
Episode length: 647.60 +/- 100.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 1079664  |
---------------------------------
Eval num_timesteps=1081656, episode_reward=900.92 +/- 251.67
Episode length: 768.40 +/- 148.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | 901         |
| time/                   |             |
|    total_timesteps      | 1081656     |
| train/                  |             |
|    approx_kl            | 0.004218197 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.56       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 257         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.000942   |
|    std                  | 1.25        |
|    value_loss           | 645         |
-----------------------------------------
Eval num_timesteps=1083648, episode_reward=517.16 +/- 152.76
Episode length: 625.60 +/- 81.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 1083648  |
---------------------------------
Eval num_timesteps=1085640, episode_reward=807.93 +/- 247.40
Episode length: 704.20 +/- 105.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 1085640  |
---------------------------------
Eval num_timesteps=1087632, episode_reward=787.35 +/- 532.30
Episode length: 609.40 +/- 120.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 1087632  |
---------------------------------
Eval num_timesteps=1089624, episode_reward=495.93 +/- 224.95
Episode length: 645.80 +/- 41.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 1089624  |
---------------------------------
Eval num_timesteps=1091616, episode_reward=610.93 +/- 332.21
Episode length: 582.80 +/- 88.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 1091616  |
---------------------------------
Eval num_timesteps=1093608, episode_reward=901.82 +/- 709.71
Episode length: 741.60 +/- 190.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 1093608  |
---------------------------------
Eval num_timesteps=1095600, episode_reward=840.23 +/- 586.20
Episode length: 700.20 +/- 56.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 1095600  |
---------------------------------
Eval num_timesteps=1097592, episode_reward=1377.38 +/- 831.23
Episode length: 776.00 +/- 184.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 1097592  |
---------------------------------
Eval num_timesteps=1099584, episode_reward=736.55 +/- 340.94
Episode length: 623.40 +/- 97.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 1099584  |
---------------------------------
Eval num_timesteps=1101576, episode_reward=526.12 +/- 371.81
Episode length: 676.80 +/- 93.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 1101576  |
---------------------------------
Eval num_timesteps=1103568, episode_reward=920.38 +/- 426.03
Episode length: 746.20 +/- 164.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 1103568  |
---------------------------------
Eval num_timesteps=1105560, episode_reward=684.03 +/- 284.74
Episode length: 651.40 +/- 62.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 1105560  |
---------------------------------
Eval num_timesteps=1107552, episode_reward=476.69 +/- 227.84
Episode length: 614.80 +/- 113.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 1107552  |
---------------------------------
Eval num_timesteps=1109544, episode_reward=809.11 +/- 435.70
Episode length: 749.20 +/- 134.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 1109544  |
---------------------------------
Eval num_timesteps=1111536, episode_reward=1025.49 +/- 625.95
Episode length: 632.00 +/- 92.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1111536  |
---------------------------------
Eval num_timesteps=1113528, episode_reward=363.26 +/- 189.08
Episode length: 591.60 +/- 80.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 1113528  |
---------------------------------
Eval num_timesteps=1115520, episode_reward=1537.15 +/- 1352.75
Episode length: 720.60 +/- 156.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1115520  |
---------------------------------
Eval num_timesteps=1117512, episode_reward=841.74 +/- 429.90
Episode length: 743.60 +/- 110.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 1117512  |
---------------------------------
Eval num_timesteps=1119504, episode_reward=832.95 +/- 338.64
Episode length: 757.60 +/- 142.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 1119504  |
---------------------------------
Eval num_timesteps=1121496, episode_reward=529.41 +/- 227.39
Episode length: 592.80 +/- 92.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 1121496  |
---------------------------------
Eval num_timesteps=1123488, episode_reward=785.55 +/- 552.50
Episode length: 706.80 +/- 187.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 1123488  |
---------------------------------
Eval num_timesteps=1125480, episode_reward=1136.54 +/- 702.24
Episode length: 831.20 +/- 140.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1125480  |
---------------------------------
Eval num_timesteps=1127472, episode_reward=678.72 +/- 393.13
Episode length: 615.60 +/- 58.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 1127472  |
---------------------------------
Eval num_timesteps=1129464, episode_reward=566.56 +/- 175.88
Episode length: 635.40 +/- 106.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1129464  |
---------------------------------
Eval num_timesteps=1131456, episode_reward=619.95 +/- 347.14
Episode length: 590.40 +/- 58.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 590         |
|    mean_reward          | 620         |
| time/                   |             |
|    total_timesteps      | 1131456     |
| train/                  |             |
|    approx_kl            | 0.003947948 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.62       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 178         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.27        |
|    value_loss           | 380         |
-----------------------------------------
Eval num_timesteps=1133448, episode_reward=567.17 +/- 379.15
Episode length: 634.80 +/- 162.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1133448  |
---------------------------------
Eval num_timesteps=1135440, episode_reward=915.94 +/- 529.34
Episode length: 603.80 +/- 68.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 1135440  |
---------------------------------
Eval num_timesteps=1137432, episode_reward=505.65 +/- 174.22
Episode length: 558.20 +/- 37.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 1137432  |
---------------------------------
Eval num_timesteps=1139424, episode_reward=691.35 +/- 273.89
Episode length: 693.80 +/- 76.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 1139424  |
---------------------------------
Eval num_timesteps=1141416, episode_reward=683.93 +/- 396.32
Episode length: 639.00 +/- 111.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 1141416  |
---------------------------------
Eval num_timesteps=1143408, episode_reward=680.65 +/- 388.65
Episode length: 594.20 +/- 65.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 1143408  |
---------------------------------
Eval num_timesteps=1145400, episode_reward=835.28 +/- 773.61
Episode length: 663.20 +/- 142.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 1145400  |
---------------------------------
Eval num_timesteps=1147392, episode_reward=986.92 +/- 663.92
Episode length: 703.00 +/- 142.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 1147392  |
---------------------------------
Eval num_timesteps=1149384, episode_reward=632.03 +/- 236.62
Episode length: 660.40 +/- 123.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 1149384  |
---------------------------------
Eval num_timesteps=1151376, episode_reward=774.62 +/- 514.60
Episode length: 722.80 +/- 216.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 1151376  |
---------------------------------
Eval num_timesteps=1153368, episode_reward=837.07 +/- 364.53
Episode length: 668.20 +/- 97.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 1153368  |
---------------------------------
Eval num_timesteps=1155360, episode_reward=832.81 +/- 502.65
Episode length: 612.80 +/- 82.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 1155360  |
---------------------------------
Eval num_timesteps=1157352, episode_reward=674.94 +/- 540.18
Episode length: 655.20 +/- 170.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1157352  |
---------------------------------
Eval num_timesteps=1159344, episode_reward=831.24 +/- 380.48
Episode length: 656.60 +/- 72.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 1159344  |
---------------------------------
Eval num_timesteps=1161336, episode_reward=648.63 +/- 193.43
Episode length: 646.60 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 1161336  |
---------------------------------
Eval num_timesteps=1163328, episode_reward=345.10 +/- 243.23
Episode length: 506.20 +/- 101.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 1163328  |
---------------------------------
Eval num_timesteps=1165320, episode_reward=619.11 +/- 173.28
Episode length: 569.40 +/- 55.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 1165320  |
---------------------------------
Eval num_timesteps=1167312, episode_reward=519.85 +/- 243.96
Episode length: 594.80 +/- 79.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 1167312  |
---------------------------------
Eval num_timesteps=1169304, episode_reward=385.35 +/- 263.76
Episode length: 565.00 +/- 112.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 1169304  |
---------------------------------
Eval num_timesteps=1171296, episode_reward=604.78 +/- 258.13
Episode length: 625.60 +/- 39.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 605      |
| time/              |          |
|    total_timesteps | 1171296  |
---------------------------------
Eval num_timesteps=1173288, episode_reward=565.83 +/- 124.26
Episode length: 585.40 +/- 36.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 1173288  |
---------------------------------
Eval num_timesteps=1175280, episode_reward=659.40 +/- 254.10
Episode length: 780.80 +/- 186.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 1175280  |
---------------------------------
Eval num_timesteps=1177272, episode_reward=505.29 +/- 222.15
Episode length: 579.60 +/- 94.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 505      |
| time/              |          |
|    total_timesteps | 1177272  |
---------------------------------
Eval num_timesteps=1179264, episode_reward=965.22 +/- 296.54
Episode length: 694.20 +/- 90.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 1179264  |
---------------------------------
Eval num_timesteps=1181256, episode_reward=675.60 +/- 257.76
Episode length: 573.20 +/- 55.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 573          |
|    mean_reward          | 676          |
| time/                   |              |
|    total_timesteps      | 1181256      |
| train/                  |              |
|    approx_kl            | 0.0043450096 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.68        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 295          |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000899    |
|    std                  | 1.29         |
|    value_loss           | 595          |
------------------------------------------
Eval num_timesteps=1183248, episode_reward=590.31 +/- 291.23
Episode length: 580.20 +/- 44.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1183248  |
---------------------------------
Eval num_timesteps=1185240, episode_reward=732.95 +/- 207.19
Episode length: 586.40 +/- 57.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 1185240  |
---------------------------------
Eval num_timesteps=1187232, episode_reward=468.68 +/- 265.97
Episode length: 539.80 +/- 74.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 1187232  |
---------------------------------
Eval num_timesteps=1189224, episode_reward=826.03 +/- 521.92
Episode length: 618.40 +/- 81.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 1189224  |
---------------------------------
Eval num_timesteps=1191216, episode_reward=698.50 +/- 263.51
Episode length: 601.20 +/- 56.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 1191216  |
---------------------------------
Eval num_timesteps=1193208, episode_reward=742.40 +/- 471.53
Episode length: 623.00 +/- 132.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 1193208  |
---------------------------------
Eval num_timesteps=1195200, episode_reward=1198.92 +/- 639.16
Episode length: 769.80 +/- 213.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1195200  |
---------------------------------
Eval num_timesteps=1197192, episode_reward=664.16 +/- 496.39
Episode length: 637.80 +/- 125.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 1197192  |
---------------------------------
Eval num_timesteps=1199184, episode_reward=1380.02 +/- 1355.52
Episode length: 769.40 +/- 204.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 1199184  |
---------------------------------
Eval num_timesteps=1201176, episode_reward=548.88 +/- 192.03
Episode length: 621.80 +/- 95.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 1201176  |
---------------------------------
Eval num_timesteps=1203168, episode_reward=513.19 +/- 353.76
Episode length: 558.40 +/- 85.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 1203168  |
---------------------------------
Eval num_timesteps=1205160, episode_reward=866.40 +/- 366.36
Episode length: 623.40 +/- 51.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 1205160  |
---------------------------------
Eval num_timesteps=1207152, episode_reward=801.69 +/- 509.96
Episode length: 628.40 +/- 138.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 1207152  |
---------------------------------
Eval num_timesteps=1209144, episode_reward=863.27 +/- 405.68
Episode length: 599.00 +/- 88.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 1209144  |
---------------------------------
Eval num_timesteps=1211136, episode_reward=968.44 +/- 521.16
Episode length: 653.00 +/- 120.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 1211136  |
---------------------------------
Eval num_timesteps=1213128, episode_reward=561.66 +/- 319.50
Episode length: 592.40 +/- 109.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 1213128  |
---------------------------------
Eval num_timesteps=1215120, episode_reward=493.93 +/- 272.77
Episode length: 575.60 +/- 121.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 1215120  |
---------------------------------
Eval num_timesteps=1217112, episode_reward=660.31 +/- 98.80
Episode length: 654.20 +/- 40.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 1217112  |
---------------------------------
Eval num_timesteps=1219104, episode_reward=877.24 +/- 906.76
Episode length: 654.40 +/- 229.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 1219104  |
---------------------------------
Eval num_timesteps=1221096, episode_reward=536.45 +/- 492.34
Episode length: 636.80 +/- 182.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 1221096  |
---------------------------------
Eval num_timesteps=1223088, episode_reward=640.49 +/- 547.81
Episode length: 579.00 +/- 115.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 1223088  |
---------------------------------
Eval num_timesteps=1225080, episode_reward=714.62 +/- 360.64
Episode length: 720.20 +/- 237.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 1225080  |
---------------------------------
Eval num_timesteps=1227072, episode_reward=559.37 +/- 368.72
Episode length: 630.20 +/- 215.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 1227072  |
---------------------------------
Eval num_timesteps=1229064, episode_reward=580.07 +/- 435.25
Episode length: 603.00 +/- 118.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 603          |
|    mean_reward          | 580          |
| time/                   |              |
|    total_timesteps      | 1229064      |
| train/                  |              |
|    approx_kl            | 0.0043974575 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 48.1         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.3          |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=1231056, episode_reward=607.95 +/- 584.32
Episode length: 600.00 +/- 209.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 1231056  |
---------------------------------
Eval num_timesteps=1233048, episode_reward=372.42 +/- 297.52
Episode length: 520.40 +/- 115.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 1233048  |
---------------------------------
Eval num_timesteps=1235040, episode_reward=477.13 +/- 330.89
Episode length: 526.40 +/- 97.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 1235040  |
---------------------------------
Eval num_timesteps=1237032, episode_reward=710.60 +/- 333.97
Episode length: 651.40 +/- 95.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 1237032  |
---------------------------------
Eval num_timesteps=1239024, episode_reward=500.20 +/- 335.91
Episode length: 541.20 +/- 66.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 1239024  |
---------------------------------
Eval num_timesteps=1241016, episode_reward=432.25 +/- 207.86
Episode length: 562.40 +/- 108.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 1241016  |
---------------------------------
Eval num_timesteps=1243008, episode_reward=802.24 +/- 593.74
Episode length: 609.20 +/- 174.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 1243008  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=600.46 +/- 178.26
Episode length: 617.60 +/- 73.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
Eval num_timesteps=1246992, episode_reward=404.19 +/- 191.16
Episode length: 539.60 +/- 95.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 1246992  |
---------------------------------
Eval num_timesteps=1248984, episode_reward=863.84 +/- 953.20
Episode length: 614.00 +/- 209.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 1248984  |
---------------------------------
Eval num_timesteps=1250976, episode_reward=835.57 +/- 618.58
Episode length: 704.20 +/- 256.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 1250976  |
---------------------------------
Eval num_timesteps=1252968, episode_reward=923.07 +/- 958.81
Episode length: 611.40 +/- 176.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 1252968  |
---------------------------------
Eval num_timesteps=1254960, episode_reward=675.14 +/- 312.68
Episode length: 711.00 +/- 230.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1254960  |
---------------------------------
Eval num_timesteps=1256952, episode_reward=496.31 +/- 238.73
Episode length: 539.20 +/- 60.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 1256952  |
---------------------------------
Eval num_timesteps=1258944, episode_reward=849.14 +/- 656.08
Episode length: 610.40 +/- 164.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 1258944  |
---------------------------------
Eval num_timesteps=1260936, episode_reward=367.05 +/- 66.42
Episode length: 563.00 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 1260936  |
---------------------------------
Eval num_timesteps=1262928, episode_reward=488.83 +/- 198.76
Episode length: 553.40 +/- 88.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 1262928  |
---------------------------------
Eval num_timesteps=1264920, episode_reward=473.26 +/- 247.88
Episode length: 584.40 +/- 89.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 1264920  |
---------------------------------
Eval num_timesteps=1266912, episode_reward=395.99 +/- 196.09
Episode length: 512.00 +/- 46.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 1266912  |
---------------------------------
Eval num_timesteps=1268904, episode_reward=551.01 +/- 226.59
Episode length: 561.60 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 1268904  |
---------------------------------
Eval num_timesteps=1270896, episode_reward=437.99 +/- 203.87
Episode length: 543.80 +/- 56.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 1270896  |
---------------------------------
Eval num_timesteps=1272888, episode_reward=1327.16 +/- 618.32
Episode length: 794.40 +/- 153.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1272888  |
---------------------------------
Eval num_timesteps=1274880, episode_reward=745.47 +/- 552.82
Episode length: 631.40 +/- 153.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 1274880  |
---------------------------------
Eval num_timesteps=1276872, episode_reward=842.83 +/- 657.12
Episode length: 614.60 +/- 104.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 1276872  |
---------------------------------
Eval num_timesteps=1278864, episode_reward=812.80 +/- 375.49
Episode length: 768.60 +/- 167.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 769          |
|    mean_reward          | 813          |
| time/                   |              |
|    total_timesteps      | 1278864      |
| train/                  |              |
|    approx_kl            | 0.0051642316 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 62.9         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.3          |
|    value_loss           | 233          |
------------------------------------------
Eval num_timesteps=1280856, episode_reward=699.85 +/- 265.92
Episode length: 670.60 +/- 92.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 1280856  |
---------------------------------
Eval num_timesteps=1282848, episode_reward=767.76 +/- 502.66
Episode length: 676.20 +/- 224.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 1282848  |
---------------------------------
Eval num_timesteps=1284840, episode_reward=549.33 +/- 197.47
Episode length: 600.80 +/- 44.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 1284840  |
---------------------------------
Eval num_timesteps=1286832, episode_reward=1126.09 +/- 1036.53
Episode length: 678.40 +/- 172.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1286832  |
---------------------------------
Eval num_timesteps=1288824, episode_reward=361.11 +/- 139.24
Episode length: 582.00 +/- 63.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 1288824  |
---------------------------------
Eval num_timesteps=1290816, episode_reward=1002.46 +/- 336.89
Episode length: 755.80 +/- 38.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 1290816  |
---------------------------------
Eval num_timesteps=1292808, episode_reward=662.32 +/- 526.40
Episode length: 649.60 +/- 156.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 1292808  |
---------------------------------
Eval num_timesteps=1294800, episode_reward=1441.31 +/- 1917.02
Episode length: 640.40 +/- 150.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 1294800  |
---------------------------------
Eval num_timesteps=1296792, episode_reward=361.79 +/- 251.59
Episode length: 587.60 +/- 129.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 1296792  |
---------------------------------
Eval num_timesteps=1298784, episode_reward=551.84 +/- 363.32
Episode length: 618.60 +/- 145.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 1298784  |
---------------------------------
Eval num_timesteps=1300776, episode_reward=685.05 +/- 603.27
Episode length: 580.20 +/- 126.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 1300776  |
---------------------------------
Eval num_timesteps=1302768, episode_reward=913.75 +/- 578.32
Episode length: 653.80 +/- 99.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 1302768  |
---------------------------------
Eval num_timesteps=1304760, episode_reward=979.23 +/- 936.98
Episode length: 672.40 +/- 193.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 1304760  |
---------------------------------
Eval num_timesteps=1306752, episode_reward=1364.01 +/- 1383.92
Episode length: 663.80 +/- 198.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1306752  |
---------------------------------
Eval num_timesteps=1308744, episode_reward=1498.41 +/- 1024.71
Episode length: 821.20 +/- 179.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1308744  |
---------------------------------
Eval num_timesteps=1310736, episode_reward=379.53 +/- 245.31
Episode length: 581.20 +/- 96.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 1310736  |
---------------------------------
Eval num_timesteps=1312728, episode_reward=729.83 +/- 803.40
Episode length: 590.60 +/- 153.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 1312728  |
---------------------------------
Eval num_timesteps=1314720, episode_reward=584.37 +/- 190.68
Episode length: 608.60 +/- 80.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 1314720  |
---------------------------------
Eval num_timesteps=1316712, episode_reward=519.46 +/- 288.15
Episode length: 585.40 +/- 121.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 1316712  |
---------------------------------
Eval num_timesteps=1318704, episode_reward=585.69 +/- 287.70
Episode length: 584.40 +/- 64.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 586      |
| time/              |          |
|    total_timesteps | 1318704  |
---------------------------------
Eval num_timesteps=1320696, episode_reward=675.00 +/- 234.94
Episode length: 705.00 +/- 97.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1320696  |
---------------------------------
Eval num_timesteps=1322688, episode_reward=457.37 +/- 178.97
Episode length: 606.80 +/- 100.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 1322688  |
---------------------------------
Eval num_timesteps=1324680, episode_reward=477.83 +/- 313.34
Episode length: 593.80 +/- 146.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 1324680  |
---------------------------------
Eval num_timesteps=1326672, episode_reward=788.48 +/- 236.53
Episode length: 656.00 +/- 100.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 1326672  |
---------------------------------
Eval num_timesteps=1328664, episode_reward=656.25 +/- 198.82
Episode length: 698.00 +/- 64.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 698          |
|    mean_reward          | 656          |
| time/                   |              |
|    total_timesteps      | 1328664      |
| train/                  |              |
|    approx_kl            | 0.0042954655 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.75        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.31         |
|    value_loss           | 522          |
------------------------------------------
Eval num_timesteps=1330656, episode_reward=839.90 +/- 637.63
Episode length: 681.00 +/- 164.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 1330656  |
---------------------------------
Eval num_timesteps=1332648, episode_reward=840.24 +/- 441.05
Episode length: 709.00 +/- 115.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 1332648  |
---------------------------------
Eval num_timesteps=1334640, episode_reward=1424.53 +/- 771.98
Episode length: 770.60 +/- 154.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 1334640  |
---------------------------------
Eval num_timesteps=1336632, episode_reward=1318.91 +/- 665.60
Episode length: 747.40 +/- 79.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1336632  |
---------------------------------
Eval num_timesteps=1338624, episode_reward=876.04 +/- 412.40
Episode length: 763.40 +/- 83.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 1338624  |
---------------------------------
Eval num_timesteps=1340616, episode_reward=590.08 +/- 212.57
Episode length: 665.20 +/- 94.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1340616  |
---------------------------------
Eval num_timesteps=1342608, episode_reward=1265.64 +/- 360.68
Episode length: 739.60 +/- 57.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1342608  |
---------------------------------
Eval num_timesteps=1344600, episode_reward=1302.97 +/- 849.46
Episode length: 761.60 +/- 116.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1344600  |
---------------------------------
Eval num_timesteps=1346592, episode_reward=889.43 +/- 719.46
Episode length: 770.60 +/- 139.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 1346592  |
---------------------------------
Eval num_timesteps=1348584, episode_reward=1021.64 +/- 370.71
Episode length: 730.00 +/- 71.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1348584  |
---------------------------------
Eval num_timesteps=1350576, episode_reward=539.41 +/- 198.64
Episode length: 625.60 +/- 45.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 1350576  |
---------------------------------
Eval num_timesteps=1352568, episode_reward=1062.02 +/- 1036.90
Episode length: 717.80 +/- 142.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1352568  |
---------------------------------
Eval num_timesteps=1354560, episode_reward=1059.11 +/- 628.13
Episode length: 757.80 +/- 162.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1354560  |
---------------------------------
Eval num_timesteps=1356552, episode_reward=1434.76 +/- 909.62
Episode length: 803.60 +/- 142.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1356552  |
---------------------------------
Eval num_timesteps=1358544, episode_reward=720.02 +/- 493.31
Episode length: 701.20 +/- 107.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 1358544  |
---------------------------------
Eval num_timesteps=1360536, episode_reward=672.15 +/- 323.33
Episode length: 679.20 +/- 45.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 1360536  |
---------------------------------
Eval num_timesteps=1362528, episode_reward=1677.39 +/- 1082.46
Episode length: 814.40 +/- 49.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1362528  |
---------------------------------
Eval num_timesteps=1364520, episode_reward=1528.18 +/- 884.81
Episode length: 866.60 +/- 164.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1364520  |
---------------------------------
Eval num_timesteps=1366512, episode_reward=1492.83 +/- 839.18
Episode length: 840.20 +/- 81.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1366512  |
---------------------------------
Eval num_timesteps=1368504, episode_reward=1403.45 +/- 620.91
Episode length: 797.20 +/- 104.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1368504  |
---------------------------------
Eval num_timesteps=1370496, episode_reward=1145.81 +/- 670.82
Episode length: 736.20 +/- 106.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 1370496  |
---------------------------------
Eval num_timesteps=1372488, episode_reward=1057.57 +/- 801.13
Episode length: 689.40 +/- 153.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1372488  |
---------------------------------
Eval num_timesteps=1374480, episode_reward=645.96 +/- 264.10
Episode length: 719.60 +/- 99.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 1374480  |
---------------------------------
Eval num_timesteps=1376472, episode_reward=1210.84 +/- 525.20
Episode length: 813.40 +/- 86.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | 1.21e+03     |
| time/                   |              |
|    total_timesteps      | 1376472      |
| train/                  |              |
|    approx_kl            | 0.0053712726 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.78        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.32         |
|    value_loss           | 509          |
------------------------------------------
Eval num_timesteps=1378464, episode_reward=1090.71 +/- 713.94
Episode length: 744.00 +/- 92.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1378464  |
---------------------------------
Eval num_timesteps=1380456, episode_reward=1968.88 +/- 827.00
Episode length: 859.40 +/- 91.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 1380456  |
---------------------------------
New best mean reward!
Eval num_timesteps=1382448, episode_reward=1530.61 +/- 650.06
Episode length: 797.20 +/- 134.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1382448  |
---------------------------------
Eval num_timesteps=1384440, episode_reward=1625.37 +/- 753.70
Episode length: 831.60 +/- 109.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1384440  |
---------------------------------
Eval num_timesteps=1386432, episode_reward=1911.47 +/- 618.26
Episode length: 937.20 +/- 74.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1386432  |
---------------------------------
Eval num_timesteps=1388424, episode_reward=1862.90 +/- 769.19
Episode length: 890.80 +/- 82.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1388424  |
---------------------------------
Eval num_timesteps=1390416, episode_reward=1887.62 +/- 976.90
Episode length: 841.80 +/- 124.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1390416  |
---------------------------------
Eval num_timesteps=1392408, episode_reward=1249.50 +/- 437.38
Episode length: 899.60 +/- 97.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1392408  |
---------------------------------
Eval num_timesteps=1394400, episode_reward=1163.82 +/- 806.89
Episode length: 818.60 +/- 220.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 1394400  |
---------------------------------
Eval num_timesteps=1396392, episode_reward=1590.16 +/- 719.83
Episode length: 844.00 +/- 87.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1396392  |
---------------------------------
Eval num_timesteps=1398384, episode_reward=1459.78 +/- 1268.78
Episode length: 762.00 +/- 143.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 1398384  |
---------------------------------
Eval num_timesteps=1400376, episode_reward=1727.60 +/- 774.39
Episode length: 925.40 +/- 167.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 1400376  |
---------------------------------
Eval num_timesteps=1402368, episode_reward=1004.02 +/- 841.85
Episode length: 741.60 +/- 140.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 1402368  |
---------------------------------
Eval num_timesteps=1404360, episode_reward=1928.45 +/- 1326.75
Episode length: 760.60 +/- 154.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1404360  |
---------------------------------
Eval num_timesteps=1406352, episode_reward=1693.10 +/- 1050.61
Episode length: 815.60 +/- 117.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1406352  |
---------------------------------
Eval num_timesteps=1408344, episode_reward=1368.66 +/- 939.09
Episode length: 764.20 +/- 128.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 1408344  |
---------------------------------
Eval num_timesteps=1410336, episode_reward=1084.72 +/- 484.89
Episode length: 778.00 +/- 117.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1410336  |
---------------------------------
Eval num_timesteps=1412328, episode_reward=2166.39 +/- 1330.86
Episode length: 817.40 +/- 102.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1412328  |
---------------------------------
New best mean reward!
Eval num_timesteps=1414320, episode_reward=2733.87 +/- 1328.97
Episode length: 840.80 +/- 80.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1414320  |
---------------------------------
New best mean reward!
Eval num_timesteps=1416312, episode_reward=1542.69 +/- 985.80
Episode length: 803.60 +/- 188.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1416312  |
---------------------------------
Eval num_timesteps=1418304, episode_reward=2471.27 +/- 1243.10
Episode length: 903.80 +/- 158.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1418304  |
---------------------------------
Eval num_timesteps=1420296, episode_reward=1413.11 +/- 596.31
Episode length: 799.40 +/- 38.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1420296  |
---------------------------------
Eval num_timesteps=1422288, episode_reward=1545.84 +/- 813.19
Episode length: 872.60 +/- 46.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1422288  |
---------------------------------
Eval num_timesteps=1424280, episode_reward=1400.94 +/- 708.71
Episode length: 877.20 +/- 63.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1424280  |
---------------------------------
Eval num_timesteps=1426272, episode_reward=2069.55 +/- 1126.46
Episode length: 823.60 +/- 118.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 824          |
|    mean_reward          | 2.07e+03     |
| time/                   |              |
|    total_timesteps      | 1426272      |
| train/                  |              |
|    approx_kl            | 0.0048676636 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.82        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.001        |
|    loss                 | 683          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.33         |
|    value_loss           | 2.16e+03     |
------------------------------------------
Eval num_timesteps=1428264, episode_reward=1577.19 +/- 920.34
Episode length: 813.80 +/- 161.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1428264  |
---------------------------------
Eval num_timesteps=1430256, episode_reward=1108.70 +/- 629.14
Episode length: 757.00 +/- 89.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 1430256  |
---------------------------------
Eval num_timesteps=1432248, episode_reward=2350.23 +/- 1297.54
Episode length: 801.20 +/- 40.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 1432248  |
---------------------------------
Eval num_timesteps=1434240, episode_reward=2468.52 +/- 1402.99
Episode length: 867.80 +/- 67.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1434240  |
---------------------------------
Eval num_timesteps=1436232, episode_reward=2079.48 +/- 437.14
Episode length: 929.20 +/- 47.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 929      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1436232  |
---------------------------------
Eval num_timesteps=1438224, episode_reward=1445.02 +/- 816.22
Episode length: 798.60 +/- 84.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1438224  |
---------------------------------
Eval num_timesteps=1440216, episode_reward=1279.85 +/- 917.28
Episode length: 792.60 +/- 92.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 1440216  |
---------------------------------
Eval num_timesteps=1442208, episode_reward=2719.66 +/- 1486.25
Episode length: 837.00 +/- 164.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1442208  |
---------------------------------
Eval num_timesteps=1444200, episode_reward=2289.63 +/- 661.27
Episode length: 923.80 +/- 51.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 1444200  |
---------------------------------
Eval num_timesteps=1446192, episode_reward=1268.17 +/- 622.92
Episode length: 844.00 +/- 109.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1446192  |
---------------------------------
Eval num_timesteps=1448184, episode_reward=1870.86 +/- 885.05
Episode length: 819.80 +/- 54.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1448184  |
---------------------------------
Eval num_timesteps=1450176, episode_reward=2579.63 +/- 1115.96
Episode length: 956.60 +/- 77.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1450176  |
---------------------------------
Eval num_timesteps=1452168, episode_reward=2155.69 +/- 955.26
Episode length: 905.40 +/- 118.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 1452168  |
---------------------------------
Eval num_timesteps=1454160, episode_reward=2491.17 +/- 1516.86
Episode length: 843.60 +/- 115.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1454160  |
---------------------------------
Eval num_timesteps=1456152, episode_reward=1401.33 +/- 452.63
Episode length: 767.00 +/- 94.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1456152  |
---------------------------------
Eval num_timesteps=1458144, episode_reward=710.32 +/- 470.36
Episode length: 720.20 +/- 110.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 1458144  |
---------------------------------
Eval num_timesteps=1460136, episode_reward=2001.34 +/- 480.92
Episode length: 864.00 +/- 69.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1460136  |
---------------------------------
Eval num_timesteps=1462128, episode_reward=2587.64 +/- 1900.90
Episode length: 909.00 +/- 112.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1462128  |
---------------------------------
Eval num_timesteps=1464120, episode_reward=1515.53 +/- 966.03
Episode length: 763.80 +/- 80.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1464120  |
---------------------------------
Eval num_timesteps=1466112, episode_reward=1752.22 +/- 1422.96
Episode length: 754.60 +/- 96.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1466112  |
---------------------------------
Eval num_timesteps=1468104, episode_reward=1710.95 +/- 345.50
Episode length: 849.80 +/- 89.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 1468104  |
---------------------------------
Eval num_timesteps=1470096, episode_reward=2430.38 +/- 1811.24
Episode length: 856.80 +/- 91.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1470096  |
---------------------------------
Eval num_timesteps=1472088, episode_reward=1211.45 +/- 540.14
Episode length: 851.20 +/- 143.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1472088  |
---------------------------------
Eval num_timesteps=1474080, episode_reward=2477.44 +/- 1035.99
Episode length: 885.40 +/- 99.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1474080  |
---------------------------------
Eval num_timesteps=1476072, episode_reward=1128.73 +/- 436.17
Episode length: 870.60 +/- 145.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 871          |
|    mean_reward          | 1.13e+03     |
| time/                   |              |
|    total_timesteps      | 1476072      |
| train/                  |              |
|    approx_kl            | 0.0039923075 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.34         |
|    value_loss           | 2.85e+03     |
------------------------------------------
Eval num_timesteps=1478064, episode_reward=1427.77 +/- 917.40
Episode length: 842.00 +/- 92.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1478064  |
---------------------------------
Eval num_timesteps=1480056, episode_reward=2030.27 +/- 586.63
Episode length: 851.20 +/- 45.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1480056  |
---------------------------------
Eval num_timesteps=1482048, episode_reward=1575.04 +/- 795.64
Episode length: 898.80 +/- 149.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1482048  |
---------------------------------
Eval num_timesteps=1484040, episode_reward=2424.48 +/- 1152.67
Episode length: 902.20 +/- 114.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1484040  |
---------------------------------
Eval num_timesteps=1486032, episode_reward=1646.31 +/- 787.77
Episode length: 840.20 +/- 129.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1486032  |
---------------------------------
Eval num_timesteps=1488024, episode_reward=1764.54 +/- 895.35
Episode length: 934.80 +/- 58.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 935      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 1488024  |
---------------------------------
Eval num_timesteps=1490016, episode_reward=2053.94 +/- 1249.03
Episode length: 820.20 +/- 208.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1490016  |
---------------------------------
Eval num_timesteps=1492008, episode_reward=2573.23 +/- 1001.16
Episode length: 866.60 +/- 95.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1492008  |
---------------------------------
Eval num_timesteps=1494000, episode_reward=2402.15 +/- 913.23
Episode length: 991.60 +/- 152.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1495992, episode_reward=1584.81 +/- 517.33
Episode length: 849.60 +/- 100.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1495992  |
---------------------------------
Eval num_timesteps=1497984, episode_reward=1929.52 +/- 833.36
Episode length: 881.80 +/- 95.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1497984  |
---------------------------------
Eval num_timesteps=1499976, episode_reward=2427.71 +/- 1203.36
Episode length: 919.40 +/- 75.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1499976  |
---------------------------------
Eval num_timesteps=1501968, episode_reward=2479.17 +/- 1116.19
Episode length: 914.80 +/- 101.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1501968  |
---------------------------------
Eval num_timesteps=1503960, episode_reward=1732.31 +/- 446.21
Episode length: 885.00 +/- 40.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 1503960  |
---------------------------------
Eval num_timesteps=1505952, episode_reward=1627.71 +/- 698.22
Episode length: 842.20 +/- 168.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1505952  |
---------------------------------
Eval num_timesteps=1507944, episode_reward=2625.01 +/- 935.84
Episode length: 953.80 +/- 52.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 1507944  |
---------------------------------
Eval num_timesteps=1509936, episode_reward=1473.93 +/- 602.01
Episode length: 781.60 +/- 54.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1509936  |
---------------------------------
Eval num_timesteps=1511928, episode_reward=2052.12 +/- 897.80
Episode length: 867.20 +/- 53.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1511928  |
---------------------------------
Eval num_timesteps=1513920, episode_reward=1484.45 +/- 676.63
Episode length: 804.40 +/- 180.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 1513920  |
---------------------------------
Eval num_timesteps=1515912, episode_reward=1991.64 +/- 1225.90
Episode length: 850.20 +/- 103.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1515912  |
---------------------------------
Eval num_timesteps=1517904, episode_reward=1367.38 +/- 1010.47
Episode length: 833.40 +/- 90.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 1517904  |
---------------------------------
Eval num_timesteps=1519896, episode_reward=2431.29 +/- 644.50
Episode length: 920.40 +/- 63.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1519896  |
---------------------------------
Eval num_timesteps=1521888, episode_reward=1282.38 +/- 410.33
Episode length: 872.60 +/- 64.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 1521888  |
---------------------------------
Eval num_timesteps=1523880, episode_reward=2712.14 +/- 695.49
Episode length: 989.80 +/- 96.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 990          |
|    mean_reward          | 2.71e+03     |
| time/                   |              |
|    total_timesteps      | 1523880      |
| train/                  |              |
|    approx_kl            | 0.0035986255 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | 0.844        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+03     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1.34         |
|    value_loss           | 3.58e+03     |
------------------------------------------
Eval num_timesteps=1525872, episode_reward=2226.59 +/- 866.43
Episode length: 964.20 +/- 50.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 1525872  |
---------------------------------
Eval num_timesteps=1527864, episode_reward=1313.97 +/- 1162.39
Episode length: 778.60 +/- 175.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1527864  |
---------------------------------
Eval num_timesteps=1529856, episode_reward=2473.70 +/- 1156.75
Episode length: 873.20 +/- 210.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1529856  |
---------------------------------
Eval num_timesteps=1531848, episode_reward=2120.07 +/- 2029.00
Episode length: 906.40 +/- 61.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1531848  |
---------------------------------
Eval num_timesteps=1533840, episode_reward=3045.96 +/- 1244.81
Episode length: 1037.20 +/- 85.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 1533840  |
---------------------------------
New best mean reward!
Eval num_timesteps=1535832, episode_reward=2216.57 +/- 1050.95
Episode length: 998.80 +/- 126.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1535832  |
---------------------------------
Eval num_timesteps=1537824, episode_reward=1717.91 +/- 370.40
Episode length: 964.60 +/- 108.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 965      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1537824  |
---------------------------------
Eval num_timesteps=1539816, episode_reward=1441.45 +/- 530.88
Episode length: 954.00 +/- 79.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 1539816  |
---------------------------------
Eval num_timesteps=1541808, episode_reward=2114.49 +/- 505.62
Episode length: 947.40 +/- 99.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 1541808  |
---------------------------------
Eval num_timesteps=1543800, episode_reward=2632.48 +/- 605.67
Episode length: 965.00 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 965      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 1543800  |
---------------------------------
Eval num_timesteps=1545792, episode_reward=1419.53 +/- 935.55
Episode length: 888.20 +/- 95.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 1545792  |
---------------------------------
Eval num_timesteps=1547784, episode_reward=2815.43 +/- 398.98
Episode length: 1030.80 +/- 63.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 1547784  |
---------------------------------
Eval num_timesteps=1549776, episode_reward=1943.24 +/- 736.27
Episode length: 968.60 +/- 53.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 1549776  |
---------------------------------
Eval num_timesteps=1551768, episode_reward=1729.50 +/- 931.44
Episode length: 933.40 +/- 104.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 1551768  |
---------------------------------
Eval num_timesteps=1553760, episode_reward=2034.26 +/- 1500.02
Episode length: 979.80 +/- 136.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1553760  |
---------------------------------
Eval num_timesteps=1555752, episode_reward=3091.46 +/- 1307.28
Episode length: 961.20 +/- 144.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 1555752  |
---------------------------------
New best mean reward!
Eval num_timesteps=1557744, episode_reward=2462.62 +/- 751.59
Episode length: 1018.00 +/- 102.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 1557744  |
---------------------------------
Eval num_timesteps=1559736, episode_reward=3465.58 +/- 1603.09
Episode length: 1042.20 +/- 199.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1559736  |
---------------------------------
New best mean reward!
Eval num_timesteps=1561728, episode_reward=3207.23 +/- 1411.14
Episode length: 1000.40 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 1561728  |
---------------------------------
Eval num_timesteps=1563720, episode_reward=1190.07 +/- 494.74
Episode length: 879.40 +/- 104.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1563720  |
---------------------------------
Eval num_timesteps=1565712, episode_reward=1646.03 +/- 932.30
Episode length: 898.60 +/- 76.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1565712  |
---------------------------------
Eval num_timesteps=1567704, episode_reward=2261.72 +/- 747.24
Episode length: 975.20 +/- 97.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 1567704  |
---------------------------------
Eval num_timesteps=1569696, episode_reward=1700.32 +/- 720.78
Episode length: 979.00 +/- 156.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 979      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1569696  |
---------------------------------
Eval num_timesteps=1571688, episode_reward=3221.52 +/- 1461.05
Episode length: 954.40 +/- 56.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1571688  |
---------------------------------
Eval num_timesteps=1573680, episode_reward=2642.30 +/- 1148.25
Episode length: 920.00 +/- 87.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 920          |
|    mean_reward          | 2.64e+03     |
| time/                   |              |
|    total_timesteps      | 1573680      |
| train/                  |              |
|    approx_kl            | 0.0030516332 |
|    clip_fraction        | 0.00722      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 994          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000623    |
|    std                  | 1.35         |
|    value_loss           | 2.06e+03     |
------------------------------------------
Eval num_timesteps=1575672, episode_reward=2138.93 +/- 394.00
Episode length: 885.60 +/- 51.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 1575672  |
---------------------------------
Eval num_timesteps=1577664, episode_reward=2015.44 +/- 1027.32
Episode length: 858.00 +/- 180.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1577664  |
---------------------------------
Eval num_timesteps=1579656, episode_reward=2227.25 +/- 1055.45
Episode length: 1042.80 +/- 68.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 1579656  |
---------------------------------
Eval num_timesteps=1581648, episode_reward=2278.36 +/- 960.33
Episode length: 876.40 +/- 101.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 1581648  |
---------------------------------
Eval num_timesteps=1583640, episode_reward=1800.19 +/- 719.19
Episode length: 840.60 +/- 64.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 1583640  |
---------------------------------
Eval num_timesteps=1585632, episode_reward=1718.59 +/- 642.31
Episode length: 904.40 +/- 109.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1585632  |
---------------------------------
Eval num_timesteps=1587624, episode_reward=1496.77 +/- 669.41
Episode length: 928.40 +/- 86.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1587624  |
---------------------------------
Eval num_timesteps=1589616, episode_reward=1217.87 +/- 537.26
Episode length: 826.00 +/- 121.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1589616  |
---------------------------------
Eval num_timesteps=1591608, episode_reward=1650.89 +/- 475.20
Episode length: 842.40 +/- 88.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1591608  |
---------------------------------
Eval num_timesteps=1593600, episode_reward=1356.10 +/- 687.05
Episode length: 942.80 +/- 241.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1593600  |
---------------------------------
Eval num_timesteps=1595592, episode_reward=2169.42 +/- 913.70
Episode length: 952.80 +/- 37.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1595592  |
---------------------------------
Eval num_timesteps=1597584, episode_reward=2221.77 +/- 1254.87
Episode length: 909.40 +/- 130.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1597584  |
---------------------------------
Eval num_timesteps=1599576, episode_reward=3451.34 +/- 1672.46
Episode length: 1006.40 +/- 133.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 1599576  |
---------------------------------
Eval num_timesteps=1601568, episode_reward=2488.42 +/- 1204.55
Episode length: 904.60 +/- 133.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1601568  |
---------------------------------
Eval num_timesteps=1603560, episode_reward=1822.38 +/- 676.40
Episode length: 857.80 +/- 79.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1603560  |
---------------------------------
Eval num_timesteps=1605552, episode_reward=1925.28 +/- 681.88
Episode length: 853.80 +/- 76.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1605552  |
---------------------------------
Eval num_timesteps=1607544, episode_reward=2099.80 +/- 1327.48
Episode length: 912.60 +/- 152.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1607544  |
---------------------------------
Eval num_timesteps=1609536, episode_reward=1533.86 +/- 359.33
Episode length: 860.40 +/- 88.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 860      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1609536  |
---------------------------------
Eval num_timesteps=1611528, episode_reward=1678.10 +/- 439.21
Episode length: 910.60 +/- 32.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1611528  |
---------------------------------
Eval num_timesteps=1613520, episode_reward=2065.73 +/- 868.90
Episode length: 869.60 +/- 80.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1613520  |
---------------------------------
Eval num_timesteps=1615512, episode_reward=2086.50 +/- 1145.24
Episode length: 847.80 +/- 111.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1615512  |
---------------------------------
Eval num_timesteps=1617504, episode_reward=2311.29 +/- 916.40
Episode length: 898.60 +/- 60.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1617504  |
---------------------------------
Eval num_timesteps=1619496, episode_reward=2170.48 +/- 1464.25
Episode length: 892.40 +/- 101.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1619496  |
---------------------------------
Eval num_timesteps=1621488, episode_reward=2059.10 +/- 416.21
Episode length: 1047.40 +/- 191.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1621488  |
---------------------------------
Eval num_timesteps=1623480, episode_reward=2007.30 +/- 970.15
Episode length: 864.00 +/- 126.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 864         |
|    mean_reward          | 2.01e+03    |
| time/                   |             |
|    total_timesteps      | 1623480     |
| train/                  |             |
|    approx_kl            | 0.002637659 |
|    clip_fraction        | 0.00609     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.001       |
|    loss                 | 1.78e+03    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00081    |
|    std                  | 1.36        |
|    value_loss           | 3.89e+03    |
-----------------------------------------
Eval num_timesteps=1625472, episode_reward=1565.74 +/- 888.38
Episode length: 844.20 +/- 125.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1625472  |
---------------------------------
Eval num_timesteps=1627464, episode_reward=1508.79 +/- 935.42
Episode length: 877.80 +/- 96.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 1627464  |
---------------------------------
Eval num_timesteps=1629456, episode_reward=3157.11 +/- 1632.48
Episode length: 939.60 +/- 120.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 1629456  |
---------------------------------
Eval num_timesteps=1631448, episode_reward=1509.56 +/- 705.26
Episode length: 904.60 +/- 53.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 1631448  |
---------------------------------
Eval num_timesteps=1633440, episode_reward=2239.58 +/- 805.58
Episode length: 906.40 +/- 77.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 1633440  |
---------------------------------
Eval num_timesteps=1635432, episode_reward=3202.56 +/- 1174.98
Episode length: 898.00 +/- 28.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1635432  |
---------------------------------
Eval num_timesteps=1637424, episode_reward=1830.63 +/- 1263.04
Episode length: 856.80 +/- 175.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1637424  |
---------------------------------
Eval num_timesteps=1639416, episode_reward=2660.67 +/- 646.41
Episode length: 935.20 +/- 63.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 935      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 1639416  |
---------------------------------
Eval num_timesteps=1641408, episode_reward=2858.24 +/- 748.18
Episode length: 915.80 +/- 87.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 1641408  |
---------------------------------
Eval num_timesteps=1643400, episode_reward=2630.60 +/- 1154.29
Episode length: 893.40 +/- 105.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 1643400  |
---------------------------------
Eval num_timesteps=1645392, episode_reward=1485.61 +/- 1047.92
Episode length: 857.40 +/- 127.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1645392  |
---------------------------------
Eval num_timesteps=1647384, episode_reward=2345.32 +/- 927.92
Episode length: 922.60 +/- 147.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 1647384  |
---------------------------------
Eval num_timesteps=1649376, episode_reward=1301.60 +/- 770.95
Episode length: 846.20 +/- 145.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1649376  |
---------------------------------
Eval num_timesteps=1651368, episode_reward=1876.64 +/- 738.76
Episode length: 903.00 +/- 78.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1651368  |
---------------------------------
Eval num_timesteps=1653360, episode_reward=1622.32 +/- 1014.08
Episode length: 948.40 +/- 73.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1653360  |
---------------------------------
Eval num_timesteps=1655352, episode_reward=1931.40 +/- 1425.44
Episode length: 869.00 +/- 120.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1655352  |
---------------------------------
Eval num_timesteps=1657344, episode_reward=2000.17 +/- 719.25
Episode length: 852.00 +/- 85.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1657344  |
---------------------------------
Eval num_timesteps=1659336, episode_reward=1396.47 +/- 645.72
Episode length: 870.40 +/- 44.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1659336  |
---------------------------------
Eval num_timesteps=1661328, episode_reward=2563.12 +/- 1463.52
Episode length: 889.20 +/- 124.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1661328  |
---------------------------------
Eval num_timesteps=1663320, episode_reward=2020.15 +/- 1235.57
Episode length: 836.80 +/- 89.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1663320  |
---------------------------------
Eval num_timesteps=1665312, episode_reward=1703.15 +/- 845.80
Episode length: 920.60 +/- 103.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1665312  |
---------------------------------
Eval num_timesteps=1667304, episode_reward=1867.01 +/- 642.10
Episode length: 855.60 +/- 9.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1667304  |
---------------------------------
Eval num_timesteps=1669296, episode_reward=2810.68 +/- 1063.73
Episode length: 908.60 +/- 95.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 1669296  |
---------------------------------
Eval num_timesteps=1671288, episode_reward=1240.45 +/- 747.27
Episode length: 821.40 +/- 69.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 821          |
|    mean_reward          | 1.24e+03     |
| time/                   |              |
|    total_timesteps      | 1671288      |
| train/                  |              |
|    approx_kl            | 0.0023098018 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+03     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.37         |
|    value_loss           | 2.76e+03     |
------------------------------------------
Eval num_timesteps=1673280, episode_reward=3052.07 +/- 1530.75
Episode length: 889.60 +/- 75.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 1673280  |
---------------------------------
Eval num_timesteps=1675272, episode_reward=990.28 +/- 278.28
Episode length: 746.00 +/- 46.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 1675272  |
---------------------------------
Eval num_timesteps=1677264, episode_reward=2013.97 +/- 1264.03
Episode length: 874.00 +/- 119.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1677264  |
---------------------------------
Eval num_timesteps=1679256, episode_reward=775.60 +/- 237.76
Episode length: 777.00 +/- 104.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 1679256  |
---------------------------------
Eval num_timesteps=1681248, episode_reward=1223.06 +/- 571.36
Episode length: 848.00 +/- 73.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1681248  |
---------------------------------
Eval num_timesteps=1683240, episode_reward=1618.65 +/- 996.35
Episode length: 864.20 +/- 68.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1683240  |
---------------------------------
Eval num_timesteps=1685232, episode_reward=1798.31 +/- 2134.93
Episode length: 806.40 +/- 117.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 1685232  |
---------------------------------
Eval num_timesteps=1687224, episode_reward=1218.33 +/- 1074.92
Episode length: 666.80 +/- 85.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1687224  |
---------------------------------
Eval num_timesteps=1689216, episode_reward=2189.28 +/- 1551.61
Episode length: 870.00 +/- 152.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 1689216  |
---------------------------------
Eval num_timesteps=1691208, episode_reward=1032.78 +/- 320.71
Episode length: 792.60 +/- 86.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1691208  |
---------------------------------
Eval num_timesteps=1693200, episode_reward=2917.88 +/- 1175.42
Episode length: 882.20 +/- 106.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 1693200  |
---------------------------------
Eval num_timesteps=1695192, episode_reward=1311.33 +/- 1319.55
Episode length: 824.60 +/- 85.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1695192  |
---------------------------------
Eval num_timesteps=1697184, episode_reward=1523.85 +/- 749.37
Episode length: 795.60 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1697184  |
---------------------------------
Eval num_timesteps=1699176, episode_reward=1099.81 +/- 332.31
Episode length: 857.40 +/- 50.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 1699176  |
---------------------------------
Eval num_timesteps=1701168, episode_reward=2584.95 +/- 1125.60
Episode length: 884.60 +/- 123.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1701168  |
---------------------------------
Eval num_timesteps=1703160, episode_reward=2300.76 +/- 1154.61
Episode length: 827.80 +/- 86.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 1703160  |
---------------------------------
Eval num_timesteps=1705152, episode_reward=2930.72 +/- 1756.94
Episode length: 945.80 +/- 106.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1705152  |
---------------------------------
Eval num_timesteps=1707144, episode_reward=1528.92 +/- 885.23
Episode length: 856.80 +/- 38.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1707144  |
---------------------------------
Eval num_timesteps=1709136, episode_reward=1225.31 +/- 863.89
Episode length: 753.40 +/- 88.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1709136  |
---------------------------------
Eval num_timesteps=1711128, episode_reward=2972.11 +/- 1496.02
Episode length: 883.20 +/- 51.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 1711128  |
---------------------------------
Eval num_timesteps=1713120, episode_reward=2769.89 +/- 1091.54
Episode length: 909.80 +/- 61.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 1713120  |
---------------------------------
Eval num_timesteps=1715112, episode_reward=2530.81 +/- 1010.67
Episode length: 884.80 +/- 83.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1715112  |
---------------------------------
Eval num_timesteps=1717104, episode_reward=785.80 +/- 358.67
Episode length: 769.60 +/- 81.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 1717104  |
---------------------------------
Eval num_timesteps=1719096, episode_reward=1269.50 +/- 952.72
Episode length: 760.60 +/- 99.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1719096  |
---------------------------------
Eval num_timesteps=1721088, episode_reward=1479.53 +/- 1247.79
Episode length: 765.40 +/- 103.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 765         |
|    mean_reward          | 1.48e+03    |
| time/                   |             |
|    total_timesteps      | 1721088     |
| train/                  |             |
|    approx_kl            | 0.003277031 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+03    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00125    |
|    std                  | 1.38        |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=1723080, episode_reward=800.00 +/- 473.54
Episode length: 766.00 +/- 130.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 1723080  |
---------------------------------
Eval num_timesteps=1725072, episode_reward=1949.34 +/- 1113.48
Episode length: 878.20 +/- 117.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1725072  |
---------------------------------
Eval num_timesteps=1727064, episode_reward=1154.66 +/- 496.84
Episode length: 770.00 +/- 85.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 1727064  |
---------------------------------
Eval num_timesteps=1729056, episode_reward=2580.72 +/- 1881.05
Episode length: 896.00 +/- 121.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1729056  |
---------------------------------
Eval num_timesteps=1731048, episode_reward=2824.00 +/- 1898.40
Episode length: 907.80 +/- 70.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 1731048  |
---------------------------------
Eval num_timesteps=1733040, episode_reward=2086.86 +/- 1712.57
Episode length: 815.20 +/- 147.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1733040  |
---------------------------------
Eval num_timesteps=1735032, episode_reward=1397.83 +/- 882.00
Episode length: 791.40 +/- 72.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1735032  |
---------------------------------
Eval num_timesteps=1737024, episode_reward=2440.25 +/- 1792.73
Episode length: 786.00 +/- 82.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1737024  |
---------------------------------
Eval num_timesteps=1739016, episode_reward=2049.29 +/- 1726.37
Episode length: 832.40 +/- 103.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1739016  |
---------------------------------
Eval num_timesteps=1741008, episode_reward=1550.45 +/- 1272.89
Episode length: 870.40 +/- 74.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1741008  |
---------------------------------
Eval num_timesteps=1743000, episode_reward=1432.62 +/- 889.18
Episode length: 930.80 +/- 205.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1743000  |
---------------------------------
Eval num_timesteps=1744992, episode_reward=1425.90 +/- 596.54
Episode length: 853.40 +/- 132.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1744992  |
---------------------------------
Eval num_timesteps=1746984, episode_reward=1808.94 +/- 374.75
Episode length: 910.40 +/- 73.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 1746984  |
---------------------------------
Eval num_timesteps=1748976, episode_reward=3170.41 +/- 1817.35
Episode length: 830.40 +/- 83.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 1748976  |
---------------------------------
Eval num_timesteps=1750968, episode_reward=2403.72 +/- 1309.48
Episode length: 840.00 +/- 111.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1750968  |
---------------------------------
Eval num_timesteps=1752960, episode_reward=3766.78 +/- 1590.85
Episode length: 908.40 +/- 44.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 3.77e+03 |
| time/              |          |
|    total_timesteps | 1752960  |
---------------------------------
New best mean reward!
Eval num_timesteps=1754952, episode_reward=2362.39 +/- 1385.83
Episode length: 823.40 +/- 170.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1754952  |
---------------------------------
Eval num_timesteps=1756944, episode_reward=1465.13 +/- 1372.44
Episode length: 781.00 +/- 114.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1756944  |
---------------------------------
Eval num_timesteps=1758936, episode_reward=2995.73 +/- 2103.87
Episode length: 875.20 +/- 142.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1758936  |
---------------------------------
Eval num_timesteps=1760928, episode_reward=757.84 +/- 350.33
Episode length: 753.20 +/- 76.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 1760928  |
---------------------------------
Eval num_timesteps=1762920, episode_reward=2732.38 +/- 1801.25
Episode length: 911.00 +/- 171.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1762920  |
---------------------------------
Eval num_timesteps=1764912, episode_reward=2201.15 +/- 1452.64
Episode length: 821.60 +/- 95.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 1764912  |
---------------------------------
Eval num_timesteps=1766904, episode_reward=1794.78 +/- 1619.06
Episode length: 919.40 +/- 65.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1766904  |
---------------------------------
Eval num_timesteps=1768896, episode_reward=2395.71 +/- 1298.52
Episode length: 904.00 +/- 58.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1768896  |
---------------------------------
Eval num_timesteps=1770888, episode_reward=2304.59 +/- 967.67
Episode length: 900.00 +/- 46.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 900          |
|    mean_reward          | 2.3e+03      |
| time/                   |              |
|    total_timesteps      | 1770888      |
| train/                  |              |
|    approx_kl            | 0.0019101324 |
|    clip_fraction        | 0.00336      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.98        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 931          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000394    |
|    std                  | 1.39         |
|    value_loss           | 2.19e+03     |
------------------------------------------
Eval num_timesteps=1772880, episode_reward=4125.91 +/- 1450.16
Episode length: 942.80 +/- 56.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | 4.13e+03 |
| time/              |          |
|    total_timesteps | 1772880  |
---------------------------------
New best mean reward!
Eval num_timesteps=1774872, episode_reward=2330.28 +/- 1513.67
Episode length: 897.80 +/- 100.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 1774872  |
---------------------------------
Eval num_timesteps=1776864, episode_reward=3164.35 +/- 1725.23
Episode length: 941.40 +/- 88.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 1776864  |
---------------------------------
Eval num_timesteps=1778856, episode_reward=3428.76 +/- 1350.16
Episode length: 941.20 +/- 40.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 3.43e+03 |
| time/              |          |
|    total_timesteps | 1778856  |
---------------------------------
Eval num_timesteps=1780848, episode_reward=1270.62 +/- 597.07
Episode length: 835.20 +/- 162.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1780848  |
---------------------------------
Eval num_timesteps=1782840, episode_reward=1816.69 +/- 1350.84
Episode length: 838.80 +/- 107.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1782840  |
---------------------------------
Eval num_timesteps=1784832, episode_reward=1914.52 +/- 595.60
Episode length: 891.60 +/- 112.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1784832  |
---------------------------------
Eval num_timesteps=1786824, episode_reward=2171.40 +/- 1517.40
Episode length: 900.20 +/- 132.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1786824  |
---------------------------------
Eval num_timesteps=1788816, episode_reward=2284.52 +/- 956.37
Episode length: 921.80 +/- 36.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 1788816  |
---------------------------------
Eval num_timesteps=1790808, episode_reward=2096.27 +/- 1756.00
Episode length: 856.40 +/- 89.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1790808  |
---------------------------------
Eval num_timesteps=1792800, episode_reward=2592.36 +/- 1068.00
Episode length: 952.20 +/- 33.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 952      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1792800  |
---------------------------------
Eval num_timesteps=1794792, episode_reward=1878.74 +/- 1196.20
Episode length: 870.00 +/- 89.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1794792  |
---------------------------------
Eval num_timesteps=1796784, episode_reward=2160.52 +/- 1108.53
Episode length: 903.80 +/- 31.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 1796784  |
---------------------------------
Eval num_timesteps=1798776, episode_reward=1817.97 +/- 1113.34
Episode length: 878.60 +/- 138.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1798776  |
---------------------------------
Eval num_timesteps=1800768, episode_reward=2585.90 +/- 2126.93
Episode length: 894.20 +/- 171.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1800768  |
---------------------------------
Eval num_timesteps=1802760, episode_reward=954.17 +/- 424.38
Episode length: 761.80 +/- 88.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 1802760  |
---------------------------------
Eval num_timesteps=1804752, episode_reward=2445.51 +/- 1912.03
Episode length: 773.00 +/- 69.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1804752  |
---------------------------------
Eval num_timesteps=1806744, episode_reward=2619.12 +/- 2203.72
Episode length: 855.60 +/- 193.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 1806744  |
---------------------------------
Eval num_timesteps=1808736, episode_reward=2059.26 +/- 1057.58
Episode length: 855.60 +/- 120.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1808736  |
---------------------------------
Eval num_timesteps=1810728, episode_reward=1541.03 +/- 828.62
Episode length: 901.60 +/- 139.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1810728  |
---------------------------------
Eval num_timesteps=1812720, episode_reward=1224.06 +/- 304.33
Episode length: 791.80 +/- 43.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1812720  |
---------------------------------
Eval num_timesteps=1814712, episode_reward=2891.90 +/- 842.26
Episode length: 921.40 +/- 35.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 1814712  |
---------------------------------
Eval num_timesteps=1816704, episode_reward=3739.70 +/- 749.78
Episode length: 874.60 +/- 77.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 3.74e+03 |
| time/              |          |
|    total_timesteps | 1816704  |
---------------------------------
Eval num_timesteps=1818696, episode_reward=2734.04 +/- 2134.79
Episode length: 946.80 +/- 101.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 947          |
|    mean_reward          | 2.73e+03     |
| time/                   |              |
|    total_timesteps      | 1818696      |
| train/                  |              |
|    approx_kl            | 0.0024357524 |
|    clip_fraction        | 0.00901      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.02        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+03     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0012      |
|    std                  | 1.4          |
|    value_loss           | 3.23e+03     |
------------------------------------------
Eval num_timesteps=1820688, episode_reward=1718.07 +/- 635.18
Episode length: 853.20 +/- 141.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1820688  |
---------------------------------
Eval num_timesteps=1822680, episode_reward=2927.72 +/- 2342.51
Episode length: 853.40 +/- 147.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1822680  |
---------------------------------
Eval num_timesteps=1824672, episode_reward=2724.82 +/- 2133.39
Episode length: 924.80 +/- 109.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1824672  |
---------------------------------
Eval num_timesteps=1826664, episode_reward=1295.82 +/- 1457.09
Episode length: 762.40 +/- 81.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1826664  |
---------------------------------
Eval num_timesteps=1828656, episode_reward=1678.75 +/- 1188.04
Episode length: 911.80 +/- 106.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1828656  |
---------------------------------
Eval num_timesteps=1830648, episode_reward=1995.47 +/- 1406.03
Episode length: 888.60 +/- 59.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1830648  |
---------------------------------
Eval num_timesteps=1832640, episode_reward=2711.21 +/- 1634.80
Episode length: 861.60 +/- 116.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1832640  |
---------------------------------
Eval num_timesteps=1834632, episode_reward=2368.50 +/- 1749.50
Episode length: 816.20 +/- 185.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 1834632  |
---------------------------------
Eval num_timesteps=1836624, episode_reward=1699.89 +/- 1518.03
Episode length: 780.80 +/- 135.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1836624  |
---------------------------------
Eval num_timesteps=1838616, episode_reward=2101.12 +/- 1008.08
Episode length: 851.80 +/- 34.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1838616  |
---------------------------------
Eval num_timesteps=1840608, episode_reward=1260.46 +/- 848.05
Episode length: 844.20 +/- 114.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1840608  |
---------------------------------
Eval num_timesteps=1842600, episode_reward=2546.13 +/- 1687.32
Episode length: 958.00 +/- 134.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 958      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1842600  |
---------------------------------
Eval num_timesteps=1844592, episode_reward=721.55 +/- 238.76
Episode length: 901.40 +/- 160.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 1844592  |
---------------------------------
Eval num_timesteps=1846584, episode_reward=3139.85 +/- 2015.60
Episode length: 867.80 +/- 135.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1846584  |
---------------------------------
Eval num_timesteps=1848576, episode_reward=2389.16 +/- 1322.57
Episode length: 823.40 +/- 97.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 1848576  |
---------------------------------
Eval num_timesteps=1850568, episode_reward=3197.32 +/- 1887.90
Episode length: 916.00 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1850568  |
---------------------------------
Eval num_timesteps=1852560, episode_reward=2440.26 +/- 1426.70
Episode length: 880.00 +/- 121.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1852560  |
---------------------------------
Eval num_timesteps=1854552, episode_reward=1213.78 +/- 521.62
Episode length: 744.20 +/- 125.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1854552  |
---------------------------------
Eval num_timesteps=1856544, episode_reward=2926.77 +/- 1882.81
Episode length: 856.00 +/- 94.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1856544  |
---------------------------------
Eval num_timesteps=1858536, episode_reward=1961.22 +/- 1998.17
Episode length: 855.80 +/- 120.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 1858536  |
---------------------------------
Eval num_timesteps=1860528, episode_reward=2267.03 +/- 1531.25
Episode length: 857.60 +/- 115.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 1860528  |
---------------------------------
Eval num_timesteps=1862520, episode_reward=1319.88 +/- 750.02
Episode length: 839.40 +/- 43.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1862520  |
---------------------------------
Eval num_timesteps=1864512, episode_reward=1360.68 +/- 664.64
Episode length: 850.40 +/- 57.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1864512  |
---------------------------------
Eval num_timesteps=1866504, episode_reward=2689.32 +/- 1606.33
Episode length: 836.60 +/- 55.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1866504  |
---------------------------------
Eval num_timesteps=1868496, episode_reward=1537.94 +/- 552.14
Episode length: 926.60 +/- 80.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 927          |
|    mean_reward          | 1.54e+03     |
| time/                   |              |
|    total_timesteps      | 1868496      |
| train/                  |              |
|    approx_kl            | 0.0028605654 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 746          |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000806    |
|    std                  | 1.41         |
|    value_loss           | 1.3e+03      |
------------------------------------------
Eval num_timesteps=1870488, episode_reward=2056.64 +/- 1395.29
Episode length: 825.20 +/- 121.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1870488  |
---------------------------------
Eval num_timesteps=1872480, episode_reward=1762.27 +/- 1911.54
Episode length: 949.00 +/- 105.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 1872480  |
---------------------------------
Eval num_timesteps=1874472, episode_reward=2794.98 +/- 1379.06
Episode length: 956.60 +/- 78.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 1874472  |
---------------------------------
Eval num_timesteps=1876464, episode_reward=3664.61 +/- 1301.13
Episode length: 912.20 +/- 34.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 3.66e+03 |
| time/              |          |
|    total_timesteps | 1876464  |
---------------------------------
Eval num_timesteps=1878456, episode_reward=1508.32 +/- 823.93
Episode length: 869.40 +/- 140.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 1878456  |
---------------------------------
Eval num_timesteps=1880448, episode_reward=1944.91 +/- 1033.06
Episode length: 830.80 +/- 72.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 1880448  |
---------------------------------
Eval num_timesteps=1882440, episode_reward=3174.46 +/- 1611.78
Episode length: 906.80 +/- 93.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 1882440  |
---------------------------------
Eval num_timesteps=1884432, episode_reward=2129.02 +/- 1851.77
Episode length: 900.60 +/- 230.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 1884432  |
---------------------------------
Eval num_timesteps=1886424, episode_reward=1596.17 +/- 1405.59
Episode length: 885.40 +/- 136.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1886424  |
---------------------------------
Eval num_timesteps=1888416, episode_reward=3203.57 +/- 1925.41
Episode length: 863.00 +/- 110.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1888416  |
---------------------------------
Eval num_timesteps=1890408, episode_reward=2273.30 +/- 1072.04
Episode length: 863.60 +/- 183.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 1890408  |
---------------------------------
Eval num_timesteps=1892400, episode_reward=3599.95 +/- 2029.65
Episode length: 998.80 +/- 126.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 3.6e+03  |
| time/              |          |
|    total_timesteps | 1892400  |
---------------------------------
Eval num_timesteps=1894392, episode_reward=2286.20 +/- 1753.49
Episode length: 914.60 +/- 161.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 1894392  |
---------------------------------
Eval num_timesteps=1896384, episode_reward=1879.00 +/- 863.98
Episode length: 873.40 +/- 102.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1896384  |
---------------------------------
Eval num_timesteps=1898376, episode_reward=2164.72 +/- 1500.93
Episode length: 862.20 +/- 87.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 1898376  |
---------------------------------
Eval num_timesteps=1900368, episode_reward=3274.98 +/- 1490.66
Episode length: 996.80 +/- 72.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 997      |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 1900368  |
---------------------------------
Eval num_timesteps=1902360, episode_reward=4017.20 +/- 2456.44
Episode length: 867.80 +/- 75.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 4.02e+03 |
| time/              |          |
|    total_timesteps | 1902360  |
---------------------------------
Eval num_timesteps=1904352, episode_reward=1653.94 +/- 1164.53
Episode length: 888.00 +/- 202.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1904352  |
---------------------------------
Eval num_timesteps=1906344, episode_reward=3272.67 +/- 1707.70
Episode length: 892.00 +/- 61.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 1906344  |
---------------------------------
Eval num_timesteps=1908336, episode_reward=2831.13 +/- 1607.70
Episode length: 987.80 +/- 88.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 1908336  |
---------------------------------
Eval num_timesteps=1910328, episode_reward=2165.34 +/- 1684.55
Episode length: 875.20 +/- 82.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1910328  |
---------------------------------
Eval num_timesteps=1912320, episode_reward=1519.53 +/- 647.25
Episode length: 835.60 +/- 77.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1912320  |
---------------------------------
Eval num_timesteps=1914312, episode_reward=2752.93 +/- 1987.32
Episode length: 949.40 +/- 128.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 1914312  |
---------------------------------
Eval num_timesteps=1916304, episode_reward=2725.14 +/- 1412.11
Episode length: 995.60 +/- 139.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 996      |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1916304  |
---------------------------------
Eval num_timesteps=1918296, episode_reward=2678.56 +/- 1432.98
Episode length: 844.80 +/- 165.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 845          |
|    mean_reward          | 2.68e+03     |
| time/                   |              |
|    total_timesteps      | 1918296      |
| train/                  |              |
|    approx_kl            | 0.0028707266 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.08        |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+03     |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.43         |
|    value_loss           | 2.94e+03     |
------------------------------------------
Eval num_timesteps=1920288, episode_reward=2833.16 +/- 1925.28
Episode length: 930.80 +/- 184.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 1920288  |
---------------------------------
Eval num_timesteps=1922280, episode_reward=2411.11 +/- 1367.20
Episode length: 891.00 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1922280  |
---------------------------------
Eval num_timesteps=1924272, episode_reward=2357.63 +/- 1435.82
Episode length: 985.40 +/- 145.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 985      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1924272  |
---------------------------------
Eval num_timesteps=1926264, episode_reward=2759.11 +/- 2227.45
Episode length: 843.40 +/- 138.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 1926264  |
---------------------------------
Eval num_timesteps=1928256, episode_reward=2771.60 +/- 1861.96
Episode length: 914.00 +/- 139.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 1928256  |
---------------------------------
Eval num_timesteps=1930248, episode_reward=3896.66 +/- 1951.94
Episode length: 945.80 +/- 56.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 3.9e+03  |
| time/              |          |
|    total_timesteps | 1930248  |
---------------------------------
Eval num_timesteps=1932240, episode_reward=2029.54 +/- 1307.78
Episode length: 843.00 +/- 148.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1932240  |
---------------------------------
Eval num_timesteps=1934232, episode_reward=2308.67 +/- 1344.17
Episode length: 851.00 +/- 122.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1934232  |
---------------------------------
Eval num_timesteps=1936224, episode_reward=1927.01 +/- 1615.65
Episode length: 826.80 +/- 61.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1936224  |
---------------------------------
Eval num_timesteps=1938216, episode_reward=2678.92 +/- 2149.92
Episode length: 761.00 +/- 143.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 1938216  |
---------------------------------
Eval num_timesteps=1940208, episode_reward=1803.53 +/- 716.99
Episode length: 873.80 +/- 142.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 1940208  |
---------------------------------
Eval num_timesteps=1942200, episode_reward=2319.89 +/- 1556.47
Episode length: 892.80 +/- 80.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 1942200  |
---------------------------------
Eval num_timesteps=1944192, episode_reward=1187.02 +/- 1032.08
Episode length: 810.80 +/- 132.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1944192  |
---------------------------------
Eval num_timesteps=1946184, episode_reward=3176.76 +/- 1577.46
Episode length: 868.00 +/- 92.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 1946184  |
---------------------------------
Eval num_timesteps=1948176, episode_reward=1856.03 +/- 894.78
Episode length: 767.80 +/- 79.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1948176  |
---------------------------------
Eval num_timesteps=1950168, episode_reward=2359.61 +/- 1425.50
Episode length: 806.20 +/- 117.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1950168  |
---------------------------------
Eval num_timesteps=1952160, episode_reward=2838.12 +/- 1992.11
Episode length: 806.60 +/- 172.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 1952160  |
---------------------------------
Eval num_timesteps=1954152, episode_reward=2089.44 +/- 1292.02
Episode length: 889.40 +/- 78.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1954152  |
---------------------------------
Eval num_timesteps=1956144, episode_reward=2163.49 +/- 1953.63
Episode length: 767.00 +/- 124.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 1956144  |
---------------------------------
Eval num_timesteps=1958136, episode_reward=1910.92 +/- 1751.07
Episode length: 751.20 +/- 165.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1958136  |
---------------------------------
Eval num_timesteps=1960128, episode_reward=3588.10 +/- 2265.99
Episode length: 946.60 +/- 161.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 3.59e+03 |
| time/              |          |
|    total_timesteps | 1960128  |
---------------------------------
Eval num_timesteps=1962120, episode_reward=1357.87 +/- 586.95
Episode length: 918.40 +/- 154.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1962120  |
---------------------------------
Eval num_timesteps=1964112, episode_reward=2467.66 +/- 1538.45
Episode length: 821.80 +/- 99.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1964112  |
---------------------------------
Eval num_timesteps=1966104, episode_reward=1897.12 +/- 1309.09
Episode length: 898.40 +/- 138.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 898          |
|    mean_reward          | 1.9e+03      |
| time/                   |              |
|    total_timesteps      | 1966104      |
| train/                  |              |
|    approx_kl            | 0.0024547174 |
|    clip_fraction        | 0.00866      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00084     |
|    std                  | 1.43         |
|    value_loss           | 2.05e+03     |
------------------------------------------
Eval num_timesteps=1968096, episode_reward=1503.49 +/- 1176.37
Episode length: 752.60 +/- 159.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1968096  |
---------------------------------
Eval num_timesteps=1970088, episode_reward=3136.10 +/- 2136.59
Episode length: 765.80 +/- 95.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1970088  |
---------------------------------
Eval num_timesteps=1972080, episode_reward=2788.84 +/- 1779.50
Episode length: 873.80 +/- 139.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 1972080  |
---------------------------------
Eval num_timesteps=1974072, episode_reward=1250.28 +/- 1063.98
Episode length: 840.80 +/- 214.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1974072  |
---------------------------------
Eval num_timesteps=1976064, episode_reward=672.91 +/- 491.18
Episode length: 718.40 +/- 221.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 1976064  |
---------------------------------
Eval num_timesteps=1978056, episode_reward=2299.52 +/- 2149.92
Episode length: 760.00 +/- 165.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 1978056  |
---------------------------------
Eval num_timesteps=1980048, episode_reward=1887.94 +/- 1579.15
Episode length: 749.80 +/- 167.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1980048  |
---------------------------------
Eval num_timesteps=1982040, episode_reward=2488.48 +/- 715.88
Episode length: 1006.80 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1982040  |
---------------------------------
Eval num_timesteps=1984032, episode_reward=2388.53 +/- 1412.41
Episode length: 914.00 +/- 181.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 1984032  |
---------------------------------
Eval num_timesteps=1986024, episode_reward=1904.52 +/- 2054.14
Episode length: 751.20 +/- 197.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1986024  |
---------------------------------
Eval num_timesteps=1988016, episode_reward=2916.22 +/- 2552.30
Episode length: 884.60 +/- 292.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 1988016  |
---------------------------------
Eval num_timesteps=1990008, episode_reward=2821.04 +/- 1470.92
Episode length: 917.60 +/- 114.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 1990008  |
---------------------------------
Eval num_timesteps=1992000, episode_reward=1867.84 +/- 1097.18
Episode length: 823.40 +/- 103.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
Eval num_timesteps=1993992, episode_reward=2618.13 +/- 1414.69
Episode length: 865.20 +/- 185.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 1993992  |
---------------------------------
Eval num_timesteps=1995984, episode_reward=3098.90 +/- 1737.49
Episode length: 988.00 +/- 186.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 1995984  |
---------------------------------
Eval num_timesteps=1997976, episode_reward=2476.67 +/- 1115.06
Episode length: 877.00 +/- 70.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1997976  |
---------------------------------
Eval num_timesteps=1999968, episode_reward=2098.74 +/- 1255.26
Episode length: 837.00 +/- 106.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1999968  |
---------------------------------
Eval num_timesteps=2001960, episode_reward=2348.32 +/- 1508.62
Episode length: 796.00 +/- 159.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2001960  |
---------------------------------
Eval num_timesteps=2003952, episode_reward=1873.26 +/- 837.33
Episode length: 927.60 +/- 156.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2003952  |
---------------------------------
Eval num_timesteps=2005944, episode_reward=3366.44 +/- 1586.73
Episode length: 915.00 +/- 101.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 3.37e+03 |
| time/              |          |
|    total_timesteps | 2005944  |
---------------------------------
Eval num_timesteps=2007936, episode_reward=2433.89 +/- 1438.28
Episode length: 808.60 +/- 195.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2007936  |
---------------------------------
Eval num_timesteps=2009928, episode_reward=2124.31 +/- 1185.93
Episode length: 858.20 +/- 260.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2009928  |
---------------------------------
Eval num_timesteps=2011920, episode_reward=2454.70 +/- 1197.14
Episode length: 897.60 +/- 59.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2011920  |
---------------------------------
Eval num_timesteps=2013912, episode_reward=1034.96 +/- 905.60
Episode length: 702.80 +/- 182.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2013912  |
---------------------------------
Eval num_timesteps=2015904, episode_reward=1954.76 +/- 1483.52
Episode length: 838.00 +/- 156.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 838          |
|    mean_reward          | 1.95e+03     |
| time/                   |              |
|    total_timesteps      | 2015904      |
| train/                  |              |
|    approx_kl            | 0.0021986233 |
|    clip_fraction        | 0.00447      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+03     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000588    |
|    std                  | 1.45         |
|    value_loss           | 3.13e+03     |
------------------------------------------
Eval num_timesteps=2017896, episode_reward=1259.72 +/- 603.78
Episode length: 831.60 +/- 193.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2017896  |
---------------------------------
Eval num_timesteps=2019888, episode_reward=2696.06 +/- 1492.24
Episode length: 966.80 +/- 56.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 967      |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2019888  |
---------------------------------
Eval num_timesteps=2021880, episode_reward=1698.72 +/- 1012.96
Episode length: 918.60 +/- 208.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2021880  |
---------------------------------
Eval num_timesteps=2023872, episode_reward=3129.85 +/- 1782.05
Episode length: 841.80 +/- 76.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2023872  |
---------------------------------
Eval num_timesteps=2025864, episode_reward=2404.45 +/- 2014.31
Episode length: 919.00 +/- 69.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 2025864  |
---------------------------------
Eval num_timesteps=2027856, episode_reward=1518.87 +/- 437.80
Episode length: 893.40 +/- 56.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 2027856  |
---------------------------------
Eval num_timesteps=2029848, episode_reward=1335.02 +/- 751.92
Episode length: 762.00 +/- 119.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2029848  |
---------------------------------
Eval num_timesteps=2031840, episode_reward=1840.39 +/- 462.19
Episode length: 949.60 +/- 145.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 2031840  |
---------------------------------
Eval num_timesteps=2033832, episode_reward=2224.79 +/- 1271.31
Episode length: 986.40 +/- 202.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2033832  |
---------------------------------
Eval num_timesteps=2035824, episode_reward=1478.33 +/- 962.29
Episode length: 871.40 +/- 151.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 871      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 2035824  |
---------------------------------
Eval num_timesteps=2037816, episode_reward=2429.03 +/- 1096.27
Episode length: 968.60 +/- 131.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2037816  |
---------------------------------
Eval num_timesteps=2039808, episode_reward=1715.00 +/- 606.05
Episode length: 897.80 +/- 78.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 2039808  |
---------------------------------
Eval num_timesteps=2041800, episode_reward=2076.73 +/- 1345.94
Episode length: 837.40 +/- 156.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2041800  |
---------------------------------
Eval num_timesteps=2043792, episode_reward=1643.97 +/- 1459.17
Episode length: 849.60 +/- 118.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2043792  |
---------------------------------
Eval num_timesteps=2045784, episode_reward=1888.38 +/- 1880.86
Episode length: 721.00 +/- 238.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2045784  |
---------------------------------
Eval num_timesteps=2047776, episode_reward=3012.86 +/- 1338.39
Episode length: 990.60 +/- 109.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 991      |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2047776  |
---------------------------------
Eval num_timesteps=2049768, episode_reward=505.89 +/- 304.14
Episode length: 637.40 +/- 117.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 2049768  |
---------------------------------
Eval num_timesteps=2051760, episode_reward=3070.48 +/- 1645.20
Episode length: 895.40 +/- 145.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 2051760  |
---------------------------------
Eval num_timesteps=2053752, episode_reward=2522.06 +/- 1720.10
Episode length: 1022.80 +/- 118.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 2053752  |
---------------------------------
Eval num_timesteps=2055744, episode_reward=2450.91 +/- 1339.43
Episode length: 906.40 +/- 136.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2055744  |
---------------------------------
Eval num_timesteps=2057736, episode_reward=2723.98 +/- 1990.22
Episode length: 946.20 +/- 168.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2057736  |
---------------------------------
Eval num_timesteps=2059728, episode_reward=2282.68 +/- 1740.81
Episode length: 852.20 +/- 190.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 2059728  |
---------------------------------
Eval num_timesteps=2061720, episode_reward=2547.55 +/- 1431.17
Episode length: 969.20 +/- 143.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2061720  |
---------------------------------
Eval num_timesteps=2063712, episode_reward=3247.10 +/- 1348.13
Episode length: 1022.80 +/- 147.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 2063712  |
---------------------------------
Eval num_timesteps=2065704, episode_reward=2607.98 +/- 1559.16
Episode length: 921.40 +/- 98.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 921          |
|    mean_reward          | 2.61e+03     |
| time/                   |              |
|    total_timesteps      | 2065704      |
| train/                  |              |
|    approx_kl            | 0.0032396421 |
|    clip_fraction        | 0.00816      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 790          |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000615    |
|    std                  | 1.47         |
|    value_loss           | 2.51e+03     |
------------------------------------------
Eval num_timesteps=2067696, episode_reward=1383.35 +/- 1153.07
Episode length: 749.60 +/- 213.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2067696  |
---------------------------------
Eval num_timesteps=2069688, episode_reward=2078.12 +/- 1196.61
Episode length: 920.80 +/- 184.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2069688  |
---------------------------------
Eval num_timesteps=2071680, episode_reward=1382.45 +/- 1254.72
Episode length: 727.00 +/- 145.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2071680  |
---------------------------------
Eval num_timesteps=2073672, episode_reward=2796.18 +/- 2280.60
Episode length: 777.00 +/- 155.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 2073672  |
---------------------------------
Eval num_timesteps=2075664, episode_reward=2063.65 +/- 1188.04
Episode length: 843.80 +/- 74.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2075664  |
---------------------------------
Eval num_timesteps=2077656, episode_reward=2165.19 +/- 1430.10
Episode length: 841.40 +/- 151.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 2077656  |
---------------------------------
Eval num_timesteps=2079648, episode_reward=2829.66 +/- 1662.63
Episode length: 872.00 +/- 63.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 2079648  |
---------------------------------
Eval num_timesteps=2081640, episode_reward=3792.17 +/- 1897.97
Episode length: 890.40 +/- 36.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 3.79e+03 |
| time/              |          |
|    total_timesteps | 2081640  |
---------------------------------
Eval num_timesteps=2083632, episode_reward=1444.28 +/- 1409.52
Episode length: 692.80 +/- 203.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2083632  |
---------------------------------
Eval num_timesteps=2085624, episode_reward=1553.69 +/- 1726.94
Episode length: 678.00 +/- 220.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2085624  |
---------------------------------
Eval num_timesteps=2087616, episode_reward=1858.35 +/- 1295.72
Episode length: 870.60 +/- 104.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 871      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 2087616  |
---------------------------------
Eval num_timesteps=2089608, episode_reward=3521.93 +/- 1605.01
Episode length: 920.40 +/- 80.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 3.52e+03 |
| time/              |          |
|    total_timesteps | 2089608  |
---------------------------------
Eval num_timesteps=2091600, episode_reward=1833.29 +/- 1503.38
Episode length: 779.60 +/- 88.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 2091600  |
---------------------------------
Eval num_timesteps=2093592, episode_reward=2339.37 +/- 1579.66
Episode length: 919.80 +/- 170.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2093592  |
---------------------------------
Eval num_timesteps=2095584, episode_reward=3389.80 +/- 2318.42
Episode length: 931.20 +/- 96.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 3.39e+03 |
| time/              |          |
|    total_timesteps | 2095584  |
---------------------------------
Eval num_timesteps=2097576, episode_reward=1566.69 +/- 1029.21
Episode length: 750.00 +/- 162.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2097576  |
---------------------------------
Eval num_timesteps=2099568, episode_reward=2447.82 +/- 1629.39
Episode length: 925.60 +/- 111.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2099568  |
---------------------------------
Eval num_timesteps=2101560, episode_reward=2042.14 +/- 960.57
Episode length: 887.40 +/- 108.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2101560  |
---------------------------------
Eval num_timesteps=2103552, episode_reward=3526.25 +/- 1246.19
Episode length: 863.20 +/- 75.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 3.53e+03 |
| time/              |          |
|    total_timesteps | 2103552  |
---------------------------------
Eval num_timesteps=2105544, episode_reward=1899.99 +/- 1657.70
Episode length: 794.60 +/- 222.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2105544  |
---------------------------------
Eval num_timesteps=2107536, episode_reward=975.04 +/- 357.03
Episode length: 740.00 +/- 113.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 2107536  |
---------------------------------
Eval num_timesteps=2109528, episode_reward=1160.13 +/- 719.18
Episode length: 819.60 +/- 226.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2109528  |
---------------------------------
Eval num_timesteps=2111520, episode_reward=1790.03 +/- 844.21
Episode length: 912.80 +/- 120.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2111520  |
---------------------------------
Eval num_timesteps=2113512, episode_reward=1467.64 +/- 679.71
Episode length: 865.00 +/- 147.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2113512  |
---------------------------------
Eval num_timesteps=2115504, episode_reward=1652.47 +/- 1517.87
Episode length: 896.80 +/- 126.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 897          |
|    mean_reward          | 1.65e+03     |
| time/                   |              |
|    total_timesteps      | 2115504      |
| train/                  |              |
|    approx_kl            | 0.0029492546 |
|    clip_fraction        | 0.0092       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00073     |
|    std                  | 1.49         |
|    value_loss           | 2.88e+03     |
------------------------------------------
Eval num_timesteps=2117496, episode_reward=3015.21 +/- 1815.86
Episode length: 917.60 +/- 139.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2117496  |
---------------------------------
Eval num_timesteps=2119488, episode_reward=1203.93 +/- 883.90
Episode length: 800.60 +/- 260.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2119488  |
---------------------------------
Eval num_timesteps=2121480, episode_reward=1908.31 +/- 936.14
Episode length: 960.80 +/- 219.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 2121480  |
---------------------------------
Eval num_timesteps=2123472, episode_reward=3005.83 +/- 2447.48
Episode length: 998.80 +/- 197.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2123472  |
---------------------------------
Eval num_timesteps=2125464, episode_reward=4109.53 +/- 2286.09
Episode length: 917.80 +/- 77.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 4.11e+03 |
| time/              |          |
|    total_timesteps | 2125464  |
---------------------------------
Eval num_timesteps=2127456, episode_reward=1135.13 +/- 277.43
Episode length: 867.20 +/- 141.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2127456  |
---------------------------------
Eval num_timesteps=2129448, episode_reward=2915.37 +/- 2588.65
Episode length: 934.60 +/- 203.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 935      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 2129448  |
---------------------------------
Eval num_timesteps=2131440, episode_reward=979.09 +/- 741.06
Episode length: 762.20 +/- 100.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 2131440  |
---------------------------------
Eval num_timesteps=2133432, episode_reward=2590.99 +/- 1348.36
Episode length: 995.40 +/- 213.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 995      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2133432  |
---------------------------------
Eval num_timesteps=2135424, episode_reward=540.47 +/- 343.83
Episode length: 652.80 +/- 101.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 2135424  |
---------------------------------
Eval num_timesteps=2137416, episode_reward=1827.95 +/- 1159.92
Episode length: 997.00 +/- 110.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 997      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 2137416  |
---------------------------------
Eval num_timesteps=2139408, episode_reward=1746.40 +/- 1421.37
Episode length: 805.60 +/- 145.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2139408  |
---------------------------------
Eval num_timesteps=2141400, episode_reward=1698.92 +/- 1537.89
Episode length: 822.80 +/- 106.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2141400  |
---------------------------------
Eval num_timesteps=2143392, episode_reward=1082.09 +/- 583.58
Episode length: 894.40 +/- 199.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2143392  |
---------------------------------
Eval num_timesteps=2145384, episode_reward=2222.83 +/- 1812.13
Episode length: 895.60 +/- 261.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2145384  |
---------------------------------
Eval num_timesteps=2147376, episode_reward=1174.34 +/- 586.05
Episode length: 861.60 +/- 166.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2147376  |
---------------------------------
Eval num_timesteps=2149368, episode_reward=2306.26 +/- 2445.43
Episode length: 876.20 +/- 141.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2149368  |
---------------------------------
Eval num_timesteps=2151360, episode_reward=2416.74 +/- 1359.08
Episode length: 801.60 +/- 166.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2151360  |
---------------------------------
Eval num_timesteps=2153352, episode_reward=422.29 +/- 242.67
Episode length: 753.20 +/- 231.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 2153352  |
---------------------------------
Eval num_timesteps=2155344, episode_reward=2069.07 +/- 1767.58
Episode length: 837.00 +/- 123.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2155344  |
---------------------------------
Eval num_timesteps=2157336, episode_reward=1109.64 +/- 535.97
Episode length: 833.00 +/- 158.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2157336  |
---------------------------------
Eval num_timesteps=2159328, episode_reward=1631.83 +/- 1181.82
Episode length: 913.00 +/- 194.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2159328  |
---------------------------------
Eval num_timesteps=2161320, episode_reward=1765.56 +/- 1763.03
Episode length: 854.60 +/- 112.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2161320  |
---------------------------------
Eval num_timesteps=2163312, episode_reward=2579.83 +/- 1308.60
Episode length: 889.80 +/- 146.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 890          |
|    mean_reward          | 2.58e+03     |
| time/                   |              |
|    total_timesteps      | 2163312      |
| train/                  |              |
|    approx_kl            | 0.0029312193 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.27        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+03     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000984    |
|    std                  | 1.49         |
|    value_loss           | 1.94e+03     |
------------------------------------------
Eval num_timesteps=2165304, episode_reward=719.31 +/- 412.56
Episode length: 744.00 +/- 216.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 2165304  |
---------------------------------
Eval num_timesteps=2167296, episode_reward=1735.42 +/- 977.88
Episode length: 805.40 +/- 77.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2167296  |
---------------------------------
Eval num_timesteps=2169288, episode_reward=1893.58 +/- 1795.04
Episode length: 931.60 +/- 82.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2169288  |
---------------------------------
Eval num_timesteps=2171280, episode_reward=1039.31 +/- 796.78
Episode length: 976.00 +/- 155.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2171280  |
---------------------------------
Eval num_timesteps=2173272, episode_reward=747.15 +/- 384.15
Episode length: 701.80 +/- 141.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 2173272  |
---------------------------------
Eval num_timesteps=2175264, episode_reward=2908.50 +/- 1915.64
Episode length: 858.80 +/- 240.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 2175264  |
---------------------------------
Eval num_timesteps=2177256, episode_reward=1768.94 +/- 1232.42
Episode length: 832.00 +/- 119.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2177256  |
---------------------------------
Eval num_timesteps=2179248, episode_reward=1808.58 +/- 1791.92
Episode length: 890.20 +/- 153.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2179248  |
---------------------------------
Eval num_timesteps=2181240, episode_reward=1334.70 +/- 1147.60
Episode length: 820.20 +/- 129.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2181240  |
---------------------------------
Eval num_timesteps=2183232, episode_reward=1539.25 +/- 473.03
Episode length: 778.80 +/- 137.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2183232  |
---------------------------------
Eval num_timesteps=2185224, episode_reward=3127.66 +/- 1916.97
Episode length: 864.40 +/- 178.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2185224  |
---------------------------------
Eval num_timesteps=2187216, episode_reward=1740.15 +/- 1678.87
Episode length: 779.20 +/- 148.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2187216  |
---------------------------------
Eval num_timesteps=2189208, episode_reward=1425.08 +/- 746.10
Episode length: 748.40 +/- 137.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 2189208  |
---------------------------------
Eval num_timesteps=2191200, episode_reward=1937.97 +/- 1695.18
Episode length: 847.60 +/- 160.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 2191200  |
---------------------------------
Eval num_timesteps=2193192, episode_reward=2141.43 +/- 1876.96
Episode length: 778.80 +/- 141.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 2193192  |
---------------------------------
Eval num_timesteps=2195184, episode_reward=2286.78 +/- 2377.60
Episode length: 808.20 +/- 119.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2195184  |
---------------------------------
Eval num_timesteps=2197176, episode_reward=1995.22 +/- 1553.50
Episode length: 828.80 +/- 172.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2197176  |
---------------------------------
Eval num_timesteps=2199168, episode_reward=2447.03 +/- 1291.22
Episode length: 875.20 +/- 164.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2199168  |
---------------------------------
Eval num_timesteps=2201160, episode_reward=755.07 +/- 1116.68
Episode length: 702.60 +/- 256.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 2201160  |
---------------------------------
Eval num_timesteps=2203152, episode_reward=1960.77 +/- 1977.82
Episode length: 643.40 +/- 161.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2203152  |
---------------------------------
Eval num_timesteps=2205144, episode_reward=692.80 +/- 261.54
Episode length: 798.80 +/- 175.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 2205144  |
---------------------------------
Eval num_timesteps=2207136, episode_reward=783.20 +/- 605.43
Episode length: 793.60 +/- 154.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 2207136  |
---------------------------------
Eval num_timesteps=2209128, episode_reward=1215.24 +/- 333.29
Episode length: 812.40 +/- 132.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2209128  |
---------------------------------
Eval num_timesteps=2211120, episode_reward=2048.14 +/- 1438.52
Episode length: 799.60 +/- 99.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2211120  |
---------------------------------
Eval num_timesteps=2213112, episode_reward=2688.85 +/- 2478.87
Episode length: 734.60 +/- 168.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 735          |
|    mean_reward          | 2.69e+03     |
| time/                   |              |
|    total_timesteps      | 2213112      |
| train/                  |              |
|    approx_kl            | 0.0031490016 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 990          |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000867    |
|    std                  | 1.51         |
|    value_loss           | 2.47e+03     |
------------------------------------------
Eval num_timesteps=2215104, episode_reward=1944.06 +/- 2761.90
Episode length: 717.00 +/- 95.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 2215104  |
---------------------------------
Eval num_timesteps=2217096, episode_reward=1658.96 +/- 1600.94
Episode length: 716.40 +/- 147.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 2217096  |
---------------------------------
Eval num_timesteps=2219088, episode_reward=1386.49 +/- 689.71
Episode length: 821.60 +/- 184.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2219088  |
---------------------------------
Eval num_timesteps=2221080, episode_reward=1693.54 +/- 442.35
Episode length: 842.00 +/- 60.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2221080  |
---------------------------------
Eval num_timesteps=2223072, episode_reward=2125.66 +/- 1659.03
Episode length: 857.20 +/- 168.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2223072  |
---------------------------------
Eval num_timesteps=2225064, episode_reward=1628.48 +/- 1308.64
Episode length: 800.40 +/- 242.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2225064  |
---------------------------------
Eval num_timesteps=2227056, episode_reward=1670.80 +/- 1458.11
Episode length: 757.80 +/- 228.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2227056  |
---------------------------------
Eval num_timesteps=2229048, episode_reward=856.78 +/- 259.96
Episode length: 736.80 +/- 99.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 2229048  |
---------------------------------
Eval num_timesteps=2231040, episode_reward=1230.52 +/- 1385.69
Episode length: 844.80 +/- 285.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2231040  |
---------------------------------
Eval num_timesteps=2233032, episode_reward=2388.93 +/- 1597.90
Episode length: 883.40 +/- 138.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 2233032  |
---------------------------------
Eval num_timesteps=2235024, episode_reward=631.10 +/- 231.84
Episode length: 738.40 +/- 154.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 2235024  |
---------------------------------
Eval num_timesteps=2237016, episode_reward=681.97 +/- 461.51
Episode length: 578.20 +/- 116.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 2237016  |
---------------------------------
Eval num_timesteps=2239008, episode_reward=1333.74 +/- 1030.61
Episode length: 790.60 +/- 217.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2239008  |
---------------------------------
Eval num_timesteps=2241000, episode_reward=1704.00 +/- 1751.74
Episode length: 857.40 +/- 162.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2241000  |
---------------------------------
Eval num_timesteps=2242992, episode_reward=2415.59 +/- 1968.79
Episode length: 755.00 +/- 157.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2242992  |
---------------------------------
Eval num_timesteps=2244984, episode_reward=1507.33 +/- 1770.46
Episode length: 607.40 +/- 142.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2244984  |
---------------------------------
Eval num_timesteps=2246976, episode_reward=1728.97 +/- 1614.27
Episode length: 732.40 +/- 237.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 2246976  |
---------------------------------
Eval num_timesteps=2248968, episode_reward=2598.52 +/- 2187.30
Episode length: 726.40 +/- 150.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2248968  |
---------------------------------
Eval num_timesteps=2250960, episode_reward=462.34 +/- 191.73
Episode length: 599.00 +/- 62.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 2250960  |
---------------------------------
Eval num_timesteps=2252952, episode_reward=1377.92 +/- 1053.89
Episode length: 829.00 +/- 220.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2252952  |
---------------------------------
Eval num_timesteps=2254944, episode_reward=2489.62 +/- 2395.89
Episode length: 842.40 +/- 178.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2254944  |
---------------------------------
Eval num_timesteps=2256936, episode_reward=2390.37 +/- 2160.79
Episode length: 798.40 +/- 253.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 2256936  |
---------------------------------
Eval num_timesteps=2258928, episode_reward=2378.18 +/- 1913.68
Episode length: 826.20 +/- 237.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2258928  |
---------------------------------
Eval num_timesteps=2260920, episode_reward=1514.40 +/- 2209.65
Episode length: 805.40 +/- 189.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2260920  |
---------------------------------
Eval num_timesteps=2262912, episode_reward=569.32 +/- 293.32
Episode length: 701.00 +/- 163.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 701          |
|    mean_reward          | 569          |
| time/                   |              |
|    total_timesteps      | 2262912      |
| train/                  |              |
|    approx_kl            | 0.0035089578 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.001        |
|    loss                 | 448          |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.51         |
|    value_loss           | 2.32e+03     |
------------------------------------------
Eval num_timesteps=2264904, episode_reward=2780.83 +/- 2174.54
Episode length: 720.20 +/- 165.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 2264904  |
---------------------------------
Eval num_timesteps=2266896, episode_reward=1621.65 +/- 1407.92
Episode length: 677.80 +/- 133.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2266896  |
---------------------------------
Eval num_timesteps=2268888, episode_reward=1026.25 +/- 453.79
Episode length: 695.40 +/- 68.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2268888  |
---------------------------------
Eval num_timesteps=2270880, episode_reward=1169.62 +/- 492.31
Episode length: 764.60 +/- 170.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2270880  |
---------------------------------
Eval num_timesteps=2272872, episode_reward=1650.26 +/- 1640.05
Episode length: 712.60 +/- 81.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2272872  |
---------------------------------
Eval num_timesteps=2274864, episode_reward=1693.71 +/- 1475.44
Episode length: 717.40 +/- 116.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2274864  |
---------------------------------
Eval num_timesteps=2276856, episode_reward=758.16 +/- 972.46
Episode length: 677.20 +/- 199.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 2276856  |
---------------------------------
Eval num_timesteps=2278848, episode_reward=2947.50 +/- 2398.70
Episode length: 826.80 +/- 59.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 2278848  |
---------------------------------
Eval num_timesteps=2280840, episode_reward=2208.86 +/- 2323.25
Episode length: 683.20 +/- 181.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2280840  |
---------------------------------
Eval num_timesteps=2282832, episode_reward=1171.86 +/- 593.84
Episode length: 724.80 +/- 111.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2282832  |
---------------------------------
Eval num_timesteps=2284824, episode_reward=2213.18 +/- 1680.67
Episode length: 745.40 +/- 91.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2284824  |
---------------------------------
Eval num_timesteps=2286816, episode_reward=1875.28 +/- 1339.37
Episode length: 769.20 +/- 131.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 2286816  |
---------------------------------
Eval num_timesteps=2288808, episode_reward=2570.88 +/- 1459.81
Episode length: 872.40 +/- 60.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2288808  |
---------------------------------
Eval num_timesteps=2290800, episode_reward=2263.83 +/- 2338.48
Episode length: 742.00 +/- 242.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2290800  |
---------------------------------
Eval num_timesteps=2292792, episode_reward=1788.13 +/- 1608.83
Episode length: 742.40 +/- 121.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2292792  |
---------------------------------
Eval num_timesteps=2294784, episode_reward=1992.63 +/- 1628.10
Episode length: 803.80 +/- 132.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2294784  |
---------------------------------
Eval num_timesteps=2296776, episode_reward=768.10 +/- 549.04
Episode length: 639.60 +/- 127.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 2296776  |
---------------------------------
Eval num_timesteps=2298768, episode_reward=1489.82 +/- 1721.70
Episode length: 667.00 +/- 160.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 2298768  |
---------------------------------
Eval num_timesteps=2300760, episode_reward=2762.23 +/- 1828.47
Episode length: 799.80 +/- 134.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 2300760  |
---------------------------------
Eval num_timesteps=2302752, episode_reward=1434.14 +/- 709.44
Episode length: 743.20 +/- 119.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 2302752  |
---------------------------------
Eval num_timesteps=2304744, episode_reward=1787.85 +/- 1334.68
Episode length: 688.20 +/- 116.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2304744  |
---------------------------------
Eval num_timesteps=2306736, episode_reward=1606.03 +/- 1185.56
Episode length: 685.20 +/- 43.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2306736  |
---------------------------------
Eval num_timesteps=2308728, episode_reward=1694.72 +/- 542.25
Episode length: 851.40 +/- 176.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2308728  |
---------------------------------
Eval num_timesteps=2310720, episode_reward=1684.53 +/- 1586.14
Episode length: 685.80 +/- 140.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 686          |
|    mean_reward          | 1.68e+03     |
| time/                   |              |
|    total_timesteps      | 2310720      |
| train/                  |              |
|    approx_kl            | 0.0036900584 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 738          |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.52         |
|    value_loss           | 2.77e+03     |
------------------------------------------
Eval num_timesteps=2312712, episode_reward=2948.42 +/- 1756.77
Episode length: 904.80 +/- 120.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 2312712  |
---------------------------------
Eval num_timesteps=2314704, episode_reward=2527.25 +/- 1893.51
Episode length: 761.20 +/- 94.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 2314704  |
---------------------------------
Eval num_timesteps=2316696, episode_reward=1501.35 +/- 1197.76
Episode length: 768.00 +/- 97.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 2316696  |
---------------------------------
Eval num_timesteps=2318688, episode_reward=1549.89 +/- 2011.50
Episode length: 733.20 +/- 171.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2318688  |
---------------------------------
Eval num_timesteps=2320680, episode_reward=2151.59 +/- 1855.55
Episode length: 901.20 +/- 196.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 2320680  |
---------------------------------
Eval num_timesteps=2322672, episode_reward=1967.69 +/- 1515.48
Episode length: 747.00 +/- 82.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 2322672  |
---------------------------------
Eval num_timesteps=2324664, episode_reward=1803.87 +/- 2003.99
Episode length: 952.80 +/- 78.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2324664  |
---------------------------------
Eval num_timesteps=2326656, episode_reward=3556.19 +/- 2386.18
Episode length: 854.60 +/- 81.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 3.56e+03 |
| time/              |          |
|    total_timesteps | 2326656  |
---------------------------------
Eval num_timesteps=2328648, episode_reward=1817.59 +/- 1140.13
Episode length: 691.40 +/- 78.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2328648  |
---------------------------------
Eval num_timesteps=2330640, episode_reward=1233.59 +/- 1507.17
Episode length: 658.20 +/- 162.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2330640  |
---------------------------------
Eval num_timesteps=2332632, episode_reward=1354.55 +/- 832.94
Episode length: 794.80 +/- 161.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2332632  |
---------------------------------
Eval num_timesteps=2334624, episode_reward=1987.75 +/- 1381.57
Episode length: 798.00 +/- 71.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2334624  |
---------------------------------
Eval num_timesteps=2336616, episode_reward=2418.27 +/- 1710.74
Episode length: 862.60 +/- 51.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2336616  |
---------------------------------
Eval num_timesteps=2338608, episode_reward=954.42 +/- 485.79
Episode length: 835.80 +/- 92.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 2338608  |
---------------------------------
Eval num_timesteps=2340600, episode_reward=2272.76 +/- 1782.09
Episode length: 808.60 +/- 148.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2340600  |
---------------------------------
Eval num_timesteps=2342592, episode_reward=960.02 +/- 681.93
Episode length: 737.40 +/- 210.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 2342592  |
---------------------------------
Eval num_timesteps=2344584, episode_reward=3649.76 +/- 2112.89
Episode length: 810.60 +/- 159.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 3.65e+03 |
| time/              |          |
|    total_timesteps | 2344584  |
---------------------------------
Eval num_timesteps=2346576, episode_reward=1689.00 +/- 1762.34
Episode length: 755.60 +/- 49.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2346576  |
---------------------------------
Eval num_timesteps=2348568, episode_reward=2180.06 +/- 2124.31
Episode length: 813.00 +/- 218.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 2348568  |
---------------------------------
Eval num_timesteps=2350560, episode_reward=3712.18 +/- 1683.31
Episode length: 910.60 +/- 76.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 3.71e+03 |
| time/              |          |
|    total_timesteps | 2350560  |
---------------------------------
Eval num_timesteps=2352552, episode_reward=801.20 +/- 418.02
Episode length: 808.60 +/- 142.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 2352552  |
---------------------------------
Eval num_timesteps=2354544, episode_reward=1037.59 +/- 1256.36
Episode length: 669.40 +/- 169.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2354544  |
---------------------------------
Eval num_timesteps=2356536, episode_reward=1013.54 +/- 672.08
Episode length: 906.40 +/- 158.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2356536  |
---------------------------------
Eval num_timesteps=2358528, episode_reward=2040.76 +/- 2551.40
Episode length: 730.00 +/- 196.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2358528  |
---------------------------------
Eval num_timesteps=2360520, episode_reward=1674.90 +/- 1535.80
Episode length: 859.80 +/- 205.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 860          |
|    mean_reward          | 1.67e+03     |
| time/                   |              |
|    total_timesteps      | 2360520      |
| train/                  |              |
|    approx_kl            | 0.0017693079 |
|    clip_fraction        | 0.00399      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.37        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+03     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000301    |
|    std                  | 1.53         |
|    value_loss           | 3.02e+03     |
------------------------------------------
Eval num_timesteps=2362512, episode_reward=2350.47 +/- 1783.95
Episode length: 820.80 +/- 148.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2362512  |
---------------------------------
Eval num_timesteps=2364504, episode_reward=1556.56 +/- 1503.86
Episode length: 774.80 +/- 145.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2364504  |
---------------------------------
Eval num_timesteps=2366496, episode_reward=457.49 +/- 400.84
Episode length: 722.20 +/- 306.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 2366496  |
---------------------------------
Eval num_timesteps=2368488, episode_reward=522.86 +/- 592.43
Episode length: 638.80 +/- 196.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 2368488  |
---------------------------------
Eval num_timesteps=2370480, episode_reward=1797.38 +/- 2055.97
Episode length: 899.20 +/- 58.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2370480  |
---------------------------------
Eval num_timesteps=2372472, episode_reward=1837.86 +/- 2232.19
Episode length: 746.00 +/- 136.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 2372472  |
---------------------------------
Eval num_timesteps=2374464, episode_reward=1317.38 +/- 936.08
Episode length: 814.00 +/- 331.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2374464  |
---------------------------------
Eval num_timesteps=2376456, episode_reward=1605.19 +/- 1731.96
Episode length: 782.20 +/- 201.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2376456  |
---------------------------------
Eval num_timesteps=2378448, episode_reward=1371.13 +/- 1468.69
Episode length: 774.20 +/- 269.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2378448  |
---------------------------------
Eval num_timesteps=2380440, episode_reward=2367.52 +/- 2783.90
Episode length: 746.00 +/- 132.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 2380440  |
---------------------------------
Eval num_timesteps=2382432, episode_reward=951.44 +/- 1096.13
Episode length: 689.80 +/- 283.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 2382432  |
---------------------------------
Eval num_timesteps=2384424, episode_reward=3771.38 +/- 2562.52
Episode length: 929.00 +/- 75.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 929      |
|    mean_reward     | 3.77e+03 |
| time/              |          |
|    total_timesteps | 2384424  |
---------------------------------
Eval num_timesteps=2386416, episode_reward=2115.57 +/- 1650.09
Episode length: 944.60 +/- 173.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 945      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2386416  |
---------------------------------
Eval num_timesteps=2388408, episode_reward=2803.27 +/- 2474.67
Episode length: 827.40 +/- 275.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 2388408  |
---------------------------------
Eval num_timesteps=2390400, episode_reward=1423.78 +/- 1678.19
Episode length: 698.20 +/- 186.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2390400  |
---------------------------------
Eval num_timesteps=2392392, episode_reward=1596.98 +/- 2563.86
Episode length: 691.40 +/- 219.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2392392  |
---------------------------------
Eval num_timesteps=2394384, episode_reward=1680.63 +/- 1654.40
Episode length: 816.20 +/- 88.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2394384  |
---------------------------------
Eval num_timesteps=2396376, episode_reward=1923.53 +/- 1906.42
Episode length: 896.20 +/- 306.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2396376  |
---------------------------------
Eval num_timesteps=2398368, episode_reward=1600.03 +/- 1001.46
Episode length: 741.40 +/- 200.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2398368  |
---------------------------------
Eval num_timesteps=2400360, episode_reward=1442.56 +/- 1624.93
Episode length: 909.40 +/- 75.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2400360  |
---------------------------------
Eval num_timesteps=2402352, episode_reward=1895.68 +/- 1826.99
Episode length: 759.80 +/- 177.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2402352  |
---------------------------------
Eval num_timesteps=2404344, episode_reward=2928.05 +/- 1690.02
Episode length: 974.80 +/- 174.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 2404344  |
---------------------------------
Eval num_timesteps=2406336, episode_reward=2463.14 +/- 2194.57
Episode length: 814.60 +/- 159.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 2406336  |
---------------------------------
Eval num_timesteps=2408328, episode_reward=2196.34 +/- 1922.59
Episode length: 707.00 +/- 272.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 2408328  |
---------------------------------
Eval num_timesteps=2410320, episode_reward=1738.45 +/- 2181.38
Episode length: 761.80 +/- 170.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 762          |
|    mean_reward          | 1.74e+03     |
| time/                   |              |
|    total_timesteps      | 2410320      |
| train/                  |              |
|    approx_kl            | 0.0037050035 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+03     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.55         |
|    value_loss           | 1.81e+03     |
------------------------------------------
Eval num_timesteps=2412312, episode_reward=3154.67 +/- 2948.38
Episode length: 723.20 +/- 102.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2412312  |
---------------------------------
Eval num_timesteps=2414304, episode_reward=1135.54 +/- 653.19
Episode length: 723.60 +/- 112.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2414304  |
---------------------------------
Eval num_timesteps=2416296, episode_reward=941.37 +/- 421.63
Episode length: 807.20 +/- 93.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 2416296  |
---------------------------------
Eval num_timesteps=2418288, episode_reward=3264.75 +/- 2274.09
Episode length: 812.00 +/- 65.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 2418288  |
---------------------------------
Eval num_timesteps=2420280, episode_reward=3471.44 +/- 2492.20
Episode length: 770.40 +/- 143.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2420280  |
---------------------------------
Eval num_timesteps=2422272, episode_reward=1690.37 +/- 761.76
Episode length: 793.60 +/- 133.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2422272  |
---------------------------------
Eval num_timesteps=2424264, episode_reward=1437.27 +/- 1003.66
Episode length: 841.80 +/- 275.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2424264  |
---------------------------------
Eval num_timesteps=2426256, episode_reward=1699.98 +/- 1882.22
Episode length: 704.80 +/- 140.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2426256  |
---------------------------------
Eval num_timesteps=2428248, episode_reward=3139.14 +/- 2107.39
Episode length: 910.20 +/- 170.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2428248  |
---------------------------------
Eval num_timesteps=2430240, episode_reward=2481.00 +/- 1976.50
Episode length: 963.60 +/- 99.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2430240  |
---------------------------------
Eval num_timesteps=2432232, episode_reward=2046.77 +/- 1976.33
Episode length: 673.00 +/- 177.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2432232  |
---------------------------------
Eval num_timesteps=2434224, episode_reward=3414.48 +/- 1898.14
Episode length: 789.00 +/- 62.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 3.41e+03 |
| time/              |          |
|    total_timesteps | 2434224  |
---------------------------------
Eval num_timesteps=2436216, episode_reward=2635.82 +/- 1645.52
Episode length: 742.00 +/- 53.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 2436216  |
---------------------------------
Eval num_timesteps=2438208, episode_reward=3320.87 +/- 2373.89
Episode length: 716.40 +/- 114.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 2438208  |
---------------------------------
Eval num_timesteps=2440200, episode_reward=1458.86 +/- 799.88
Episode length: 798.20 +/- 139.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2440200  |
---------------------------------
Eval num_timesteps=2442192, episode_reward=2641.89 +/- 1322.61
Episode length: 816.00 +/- 74.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 2442192  |
---------------------------------
Eval num_timesteps=2444184, episode_reward=1669.05 +/- 1619.24
Episode length: 925.20 +/- 150.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2444184  |
---------------------------------
Eval num_timesteps=2446176, episode_reward=1718.26 +/- 1794.33
Episode length: 724.00 +/- 148.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2446176  |
---------------------------------
Eval num_timesteps=2448168, episode_reward=2222.90 +/- 1864.08
Episode length: 780.20 +/- 98.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2448168  |
---------------------------------
Eval num_timesteps=2450160, episode_reward=1268.16 +/- 946.85
Episode length: 778.20 +/- 106.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2450160  |
---------------------------------
Eval num_timesteps=2452152, episode_reward=3141.45 +/- 2190.29
Episode length: 941.60 +/- 220.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2452152  |
---------------------------------
Eval num_timesteps=2454144, episode_reward=2582.19 +/- 1957.24
Episode length: 819.40 +/- 104.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2454144  |
---------------------------------
Eval num_timesteps=2456136, episode_reward=3060.25 +/- 1970.03
Episode length: 818.20 +/- 78.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 2456136  |
---------------------------------
Eval num_timesteps=2458128, episode_reward=1541.79 +/- 683.79
Episode length: 707.80 +/- 90.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 708          |
|    mean_reward          | 1.54e+03     |
| time/                   |              |
|    total_timesteps      | 2458128      |
| train/                  |              |
|    approx_kl            | 0.0027965382 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.44        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000664    |
|    std                  | 1.56         |
|    value_loss           | 2.53e+03     |
------------------------------------------
Eval num_timesteps=2460120, episode_reward=2788.00 +/- 994.96
Episode length: 781.00 +/- 48.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 2460120  |
---------------------------------
Eval num_timesteps=2462112, episode_reward=3593.75 +/- 2273.64
Episode length: 723.40 +/- 101.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 3.59e+03 |
| time/              |          |
|    total_timesteps | 2462112  |
---------------------------------
Eval num_timesteps=2464104, episode_reward=1527.93 +/- 1343.65
Episode length: 705.40 +/- 90.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2464104  |
---------------------------------
Eval num_timesteps=2466096, episode_reward=3951.67 +/- 1564.46
Episode length: 810.80 +/- 84.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 3.95e+03 |
| time/              |          |
|    total_timesteps | 2466096  |
---------------------------------
Eval num_timesteps=2468088, episode_reward=2988.75 +/- 1845.50
Episode length: 815.40 +/- 94.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2468088  |
---------------------------------
Eval num_timesteps=2470080, episode_reward=2525.18 +/- 1347.80
Episode length: 855.20 +/- 115.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 2470080  |
---------------------------------
Eval num_timesteps=2472072, episode_reward=1721.89 +/- 901.00
Episode length: 709.80 +/- 31.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2472072  |
---------------------------------
Eval num_timesteps=2474064, episode_reward=2048.55 +/- 1812.98
Episode length: 716.20 +/- 95.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2474064  |
---------------------------------
Eval num_timesteps=2476056, episode_reward=2256.26 +/- 2182.52
Episode length: 662.60 +/- 144.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2476056  |
---------------------------------
Eval num_timesteps=2478048, episode_reward=2031.13 +/- 1649.91
Episode length: 769.60 +/- 162.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 2478048  |
---------------------------------
Eval num_timesteps=2480040, episode_reward=1644.93 +/- 1524.43
Episode length: 755.00 +/- 110.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2480040  |
---------------------------------
Eval num_timesteps=2482032, episode_reward=2855.94 +/- 2022.77
Episode length: 709.00 +/- 76.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 2482032  |
---------------------------------
Eval num_timesteps=2484024, episode_reward=1963.42 +/- 1340.26
Episode length: 704.20 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2484024  |
---------------------------------
Eval num_timesteps=2486016, episode_reward=2494.48 +/- 2169.68
Episode length: 729.60 +/- 116.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2486016  |
---------------------------------
Eval num_timesteps=2488008, episode_reward=2250.54 +/- 1963.36
Episode length: 686.80 +/- 161.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 2488008  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=579.79 +/- 180.26
Episode length: 664.60 +/- 137.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 2490000  |
---------------------------------
Eval num_timesteps=2491992, episode_reward=1774.28 +/- 2417.88
Episode length: 640.40 +/- 187.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2491992  |
---------------------------------
Eval num_timesteps=2493984, episode_reward=1478.34 +/- 835.07
Episode length: 710.00 +/- 148.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 2493984  |
---------------------------------
Eval num_timesteps=2495976, episode_reward=2317.41 +/- 2364.71
Episode length: 841.60 +/- 130.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2495976  |
---------------------------------
Eval num_timesteps=2497968, episode_reward=1657.66 +/- 1059.16
Episode length: 735.80 +/- 88.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 2497968  |
---------------------------------
Eval num_timesteps=2499960, episode_reward=4564.30 +/- 1896.39
Episode length: 759.00 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 4.56e+03 |
| time/              |          |
|    total_timesteps | 2499960  |
---------------------------------
New best mean reward!
Eval num_timesteps=2501952, episode_reward=1197.60 +/- 712.66
Episode length: 706.80 +/- 97.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2501952  |
---------------------------------
Eval num_timesteps=2503944, episode_reward=2425.64 +/- 2079.46
Episode length: 722.60 +/- 114.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2503944  |
---------------------------------
Eval num_timesteps=2505936, episode_reward=3228.42 +/- 2218.71
Episode length: 761.00 +/- 55.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 2505936  |
---------------------------------
Eval num_timesteps=2507928, episode_reward=2658.10 +/- 1920.12
Episode length: 842.80 +/- 31.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 843        |
|    mean_reward          | 2.66e+03   |
| time/                   |            |
|    total_timesteps      | 2507928    |
| train/                  |            |
|    approx_kl            | 0.00141737 |
|    clip_fraction        | 0.00166    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.47      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.001      |
|    loss                 | 791        |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00038   |
|    std                  | 1.57       |
|    value_loss           | 3.14e+03   |
----------------------------------------
Eval num_timesteps=2509920, episode_reward=2443.14 +/- 2161.35
Episode length: 701.20 +/- 77.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2509920  |
---------------------------------
Eval num_timesteps=2511912, episode_reward=1694.60 +/- 1366.23
Episode length: 797.20 +/- 55.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2511912  |
---------------------------------
Eval num_timesteps=2513904, episode_reward=4169.13 +/- 1945.58
Episode length: 950.00 +/- 88.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 4.17e+03 |
| time/              |          |
|    total_timesteps | 2513904  |
---------------------------------
Eval num_timesteps=2515896, episode_reward=3624.12 +/- 1932.49
Episode length: 795.00 +/- 58.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 3.62e+03 |
| time/              |          |
|    total_timesteps | 2515896  |
---------------------------------
Eval num_timesteps=2517888, episode_reward=2004.66 +/- 1886.96
Episode length: 734.20 +/- 107.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2517888  |
---------------------------------
Eval num_timesteps=2519880, episode_reward=2053.50 +/- 1246.19
Episode length: 791.40 +/- 186.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2519880  |
---------------------------------
Eval num_timesteps=2521872, episode_reward=1945.16 +/- 1913.71
Episode length: 710.20 +/- 149.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2521872  |
---------------------------------
Eval num_timesteps=2523864, episode_reward=1734.01 +/- 1639.15
Episode length: 684.60 +/- 188.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 2523864  |
---------------------------------
Eval num_timesteps=2525856, episode_reward=1386.25 +/- 1743.55
Episode length: 762.40 +/- 153.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2525856  |
---------------------------------
Eval num_timesteps=2527848, episode_reward=980.39 +/- 793.94
Episode length: 756.40 +/- 129.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 2527848  |
---------------------------------
Eval num_timesteps=2529840, episode_reward=1182.91 +/- 639.16
Episode length: 807.80 +/- 80.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2529840  |
---------------------------------
Eval num_timesteps=2531832, episode_reward=1422.16 +/- 1843.67
Episode length: 644.60 +/- 182.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2531832  |
---------------------------------
Eval num_timesteps=2533824, episode_reward=3480.85 +/- 2420.76
Episode length: 817.60 +/- 55.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2533824  |
---------------------------------
Eval num_timesteps=2535816, episode_reward=2867.74 +/- 1693.26
Episode length: 758.60 +/- 76.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 2535816  |
---------------------------------
Eval num_timesteps=2537808, episode_reward=4618.76 +/- 2263.85
Episode length: 813.60 +/- 36.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 4.62e+03 |
| time/              |          |
|    total_timesteps | 2537808  |
---------------------------------
New best mean reward!
Eval num_timesteps=2539800, episode_reward=2502.77 +/- 2344.56
Episode length: 752.20 +/- 188.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2539800  |
---------------------------------
Eval num_timesteps=2541792, episode_reward=1505.47 +/- 710.14
Episode length: 685.20 +/- 125.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2541792  |
---------------------------------
Eval num_timesteps=2543784, episode_reward=2102.88 +/- 1816.54
Episode length: 869.00 +/- 132.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2543784  |
---------------------------------
Eval num_timesteps=2545776, episode_reward=1579.50 +/- 336.24
Episode length: 773.00 +/- 77.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 2545776  |
---------------------------------
Eval num_timesteps=2547768, episode_reward=3511.69 +/- 2323.39
Episode length: 786.80 +/- 85.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 3.51e+03 |
| time/              |          |
|    total_timesteps | 2547768  |
---------------------------------
Eval num_timesteps=2549760, episode_reward=2815.82 +/- 811.95
Episode length: 851.80 +/- 105.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2549760  |
---------------------------------
Eval num_timesteps=2551752, episode_reward=2740.14 +/- 1639.10
Episode length: 786.80 +/- 82.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 2551752  |
---------------------------------
Eval num_timesteps=2553744, episode_reward=2796.87 +/- 968.40
Episode length: 896.20 +/- 135.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 2553744  |
---------------------------------
Eval num_timesteps=2555736, episode_reward=2617.25 +/- 2485.64
Episode length: 685.00 +/- 199.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2555736  |
---------------------------------
Eval num_timesteps=2557728, episode_reward=2719.42 +/- 1437.39
Episode length: 720.60 +/- 46.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 721          |
|    mean_reward          | 2.72e+03     |
| time/                   |              |
|    total_timesteps      | 2557728      |
| train/                  |              |
|    approx_kl            | 0.0015133371 |
|    clip_fraction        | 0.00178      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 979          |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000504    |
|    std                  | 1.57         |
|    value_loss           | 2.86e+03     |
------------------------------------------
Eval num_timesteps=2559720, episode_reward=3074.49 +/- 1220.10
Episode length: 746.60 +/- 89.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 2559720  |
---------------------------------
Eval num_timesteps=2561712, episode_reward=2800.36 +/- 1613.71
Episode length: 803.40 +/- 70.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 2561712  |
---------------------------------
Eval num_timesteps=2563704, episode_reward=1924.85 +/- 1459.71
Episode length: 747.20 +/- 138.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2563704  |
---------------------------------
Eval num_timesteps=2565696, episode_reward=1473.79 +/- 630.10
Episode length: 718.00 +/- 117.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2565696  |
---------------------------------
Eval num_timesteps=2567688, episode_reward=1216.91 +/- 1197.52
Episode length: 706.20 +/- 176.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2567688  |
---------------------------------
Eval num_timesteps=2569680, episode_reward=2379.19 +/- 1139.38
Episode length: 852.80 +/- 65.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2569680  |
---------------------------------
Eval num_timesteps=2571672, episode_reward=2774.24 +/- 2022.36
Episode length: 789.80 +/- 100.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 2571672  |
---------------------------------
Eval num_timesteps=2573664, episode_reward=2142.09 +/- 1065.30
Episode length: 758.40 +/- 84.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 2573664  |
---------------------------------
Eval num_timesteps=2575656, episode_reward=2735.31 +/- 1721.75
Episode length: 860.80 +/- 103.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 2575656  |
---------------------------------
Eval num_timesteps=2577648, episode_reward=4117.91 +/- 1358.74
Episode length: 848.20 +/- 99.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 4.12e+03 |
| time/              |          |
|    total_timesteps | 2577648  |
---------------------------------
Eval num_timesteps=2579640, episode_reward=3239.19 +/- 1882.82
Episode length: 700.00 +/- 148.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 2579640  |
---------------------------------
Eval num_timesteps=2581632, episode_reward=2833.22 +/- 1608.50
Episode length: 786.60 +/- 177.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 2581632  |
---------------------------------
Eval num_timesteps=2583624, episode_reward=3643.59 +/- 1713.57
Episode length: 777.40 +/- 62.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 3.64e+03 |
| time/              |          |
|    total_timesteps | 2583624  |
---------------------------------
Eval num_timesteps=2585616, episode_reward=1666.22 +/- 1406.52
Episode length: 706.00 +/- 82.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2585616  |
---------------------------------
Eval num_timesteps=2587608, episode_reward=2154.21 +/- 1746.16
Episode length: 791.20 +/- 96.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 2587608  |
---------------------------------
Eval num_timesteps=2589600, episode_reward=1877.96 +/- 1915.68
Episode length: 748.80 +/- 64.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 2589600  |
---------------------------------
Eval num_timesteps=2591592, episode_reward=1571.03 +/- 810.40
Episode length: 767.40 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2591592  |
---------------------------------
Eval num_timesteps=2593584, episode_reward=2555.81 +/- 1520.15
Episode length: 777.80 +/- 53.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2593584  |
---------------------------------
Eval num_timesteps=2595576, episode_reward=1589.29 +/- 919.79
Episode length: 757.40 +/- 119.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2595576  |
---------------------------------
Eval num_timesteps=2597568, episode_reward=4406.50 +/- 1087.96
Episode length: 811.80 +/- 114.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 4.41e+03 |
| time/              |          |
|    total_timesteps | 2597568  |
---------------------------------
Eval num_timesteps=2599560, episode_reward=3139.20 +/- 2577.78
Episode length: 828.40 +/- 170.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2599560  |
---------------------------------
Eval num_timesteps=2601552, episode_reward=2621.28 +/- 2352.67
Episode length: 716.80 +/- 102.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2601552  |
---------------------------------
Eval num_timesteps=2603544, episode_reward=2462.21 +/- 1833.47
Episode length: 802.20 +/- 136.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 2603544  |
---------------------------------
Eval num_timesteps=2605536, episode_reward=1797.66 +/- 1442.68
Episode length: 708.20 +/- 155.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 708          |
|    mean_reward          | 1.8e+03      |
| time/                   |              |
|    total_timesteps      | 2605536      |
| train/                  |              |
|    approx_kl            | 0.0021462345 |
|    clip_fraction        | 0.00467      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 730          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000611    |
|    std                  | 1.58         |
|    value_loss           | 2.84e+03     |
------------------------------------------
Eval num_timesteps=2607528, episode_reward=3261.09 +/- 1273.99
Episode length: 754.40 +/- 79.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 2607528  |
---------------------------------
Eval num_timesteps=2609520, episode_reward=2580.05 +/- 1714.07
Episode length: 723.60 +/- 89.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2609520  |
---------------------------------
Eval num_timesteps=2611512, episode_reward=4283.84 +/- 1423.05
Episode length: 813.40 +/- 55.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 4.28e+03 |
| time/              |          |
|    total_timesteps | 2611512  |
---------------------------------
Eval num_timesteps=2613504, episode_reward=3876.59 +/- 1839.11
Episode length: 769.40 +/- 55.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 3.88e+03 |
| time/              |          |
|    total_timesteps | 2613504  |
---------------------------------
Eval num_timesteps=2615496, episode_reward=3053.29 +/- 1767.44
Episode length: 777.80 +/- 69.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 2615496  |
---------------------------------
Eval num_timesteps=2617488, episode_reward=3714.61 +/- 2806.49
Episode length: 738.80 +/- 64.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 3.71e+03 |
| time/              |          |
|    total_timesteps | 2617488  |
---------------------------------
Eval num_timesteps=2619480, episode_reward=3088.80 +/- 1540.84
Episode length: 905.40 +/- 195.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 2619480  |
---------------------------------
Eval num_timesteps=2621472, episode_reward=1946.06 +/- 1557.06
Episode length: 805.20 +/- 124.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2621472  |
---------------------------------
Eval num_timesteps=2623464, episode_reward=571.90 +/- 399.54
Episode length: 841.80 +/- 84.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 2623464  |
---------------------------------
Eval num_timesteps=2625456, episode_reward=1542.86 +/- 1401.05
Episode length: 829.20 +/- 116.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2625456  |
---------------------------------
Eval num_timesteps=2627448, episode_reward=1867.35 +/- 1093.93
Episode length: 763.40 +/- 74.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2627448  |
---------------------------------
Eval num_timesteps=2629440, episode_reward=1151.85 +/- 449.97
Episode length: 779.60 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2629440  |
---------------------------------
Eval num_timesteps=2631432, episode_reward=4158.78 +/- 1889.67
Episode length: 811.40 +/- 43.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 4.16e+03 |
| time/              |          |
|    total_timesteps | 2631432  |
---------------------------------
Eval num_timesteps=2633424, episode_reward=3929.61 +/- 2777.13
Episode length: 762.80 +/- 95.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 3.93e+03 |
| time/              |          |
|    total_timesteps | 2633424  |
---------------------------------
Eval num_timesteps=2635416, episode_reward=1053.32 +/- 892.65
Episode length: 735.00 +/- 77.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2635416  |
---------------------------------
Eval num_timesteps=2637408, episode_reward=3620.76 +/- 2152.84
Episode length: 770.00 +/- 83.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 3.62e+03 |
| time/              |          |
|    total_timesteps | 2637408  |
---------------------------------
Eval num_timesteps=2639400, episode_reward=3102.84 +/- 2049.87
Episode length: 786.80 +/- 56.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 2639400  |
---------------------------------
Eval num_timesteps=2641392, episode_reward=1505.42 +/- 1082.76
Episode length: 746.00 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2641392  |
---------------------------------
Eval num_timesteps=2643384, episode_reward=1743.41 +/- 1040.28
Episode length: 705.80 +/- 107.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2643384  |
---------------------------------
Eval num_timesteps=2645376, episode_reward=2850.81 +/- 2528.60
Episode length: 708.40 +/- 109.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 2645376  |
---------------------------------
Eval num_timesteps=2647368, episode_reward=1696.92 +/- 1006.48
Episode length: 776.00 +/- 78.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2647368  |
---------------------------------
Eval num_timesteps=2649360, episode_reward=3269.43 +/- 1145.25
Episode length: 829.60 +/- 110.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 2649360  |
---------------------------------
Eval num_timesteps=2651352, episode_reward=2346.80 +/- 1618.57
Episode length: 785.00 +/- 83.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2651352  |
---------------------------------
Eval num_timesteps=2653344, episode_reward=3503.36 +/- 1618.28
Episode length: 755.00 +/- 56.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 3.5e+03  |
| time/              |          |
|    total_timesteps | 2653344  |
---------------------------------
Eval num_timesteps=2655336, episode_reward=2209.93 +/- 1465.25
Episode length: 796.20 +/- 59.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 796          |
|    mean_reward          | 2.21e+03     |
| time/                   |              |
|    total_timesteps      | 2655336      |
| train/                  |              |
|    approx_kl            | 0.0023679794 |
|    clip_fraction        | 0.00378      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.52        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+03     |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.000549    |
|    std                  | 1.59         |
|    value_loss           | 3.56e+03     |
------------------------------------------
Eval num_timesteps=2657328, episode_reward=1169.93 +/- 625.80
Episode length: 694.00 +/- 113.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2657328  |
---------------------------------
Eval num_timesteps=2659320, episode_reward=2105.91 +/- 1247.20
Episode length: 721.00 +/- 72.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2659320  |
---------------------------------
Eval num_timesteps=2661312, episode_reward=3948.39 +/- 2224.12
Episode length: 804.00 +/- 105.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 3.95e+03 |
| time/              |          |
|    total_timesteps | 2661312  |
---------------------------------
Eval num_timesteps=2663304, episode_reward=551.41 +/- 152.55
Episode length: 754.60 +/- 93.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 2663304  |
---------------------------------
Eval num_timesteps=2665296, episode_reward=1964.06 +/- 2252.03
Episode length: 676.80 +/- 58.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2665296  |
---------------------------------
Eval num_timesteps=2667288, episode_reward=2358.61 +/- 1574.67
Episode length: 715.00 +/- 30.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 2667288  |
---------------------------------
Eval num_timesteps=2669280, episode_reward=1750.67 +/- 1752.65
Episode length: 734.80 +/- 46.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2669280  |
---------------------------------
Eval num_timesteps=2671272, episode_reward=2442.89 +/- 2407.92
Episode length: 755.80 +/- 54.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2671272  |
---------------------------------
Eval num_timesteps=2673264, episode_reward=1758.63 +/- 1422.21
Episode length: 701.40 +/- 68.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 2673264  |
---------------------------------
Eval num_timesteps=2675256, episode_reward=4266.52 +/- 1930.47
Episode length: 810.00 +/- 41.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 2675256  |
---------------------------------
Eval num_timesteps=2677248, episode_reward=1054.27 +/- 439.60
Episode length: 778.20 +/- 105.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2677248  |
---------------------------------
Eval num_timesteps=2679240, episode_reward=769.86 +/- 434.08
Episode length: 682.20 +/- 47.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 2679240  |
---------------------------------
Eval num_timesteps=2681232, episode_reward=3480.66 +/- 2098.39
Episode length: 682.80 +/- 76.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2681232  |
---------------------------------
Eval num_timesteps=2683224, episode_reward=1356.41 +/- 1305.46
Episode length: 694.60 +/- 68.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2683224  |
---------------------------------
Eval num_timesteps=2685216, episode_reward=1017.51 +/- 501.55
Episode length: 718.80 +/- 51.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2685216  |
---------------------------------
Eval num_timesteps=2687208, episode_reward=1853.29 +/- 1712.76
Episode length: 718.00 +/- 43.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2687208  |
---------------------------------
Eval num_timesteps=2689200, episode_reward=2395.96 +/- 1968.99
Episode length: 737.80 +/- 65.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 2689200  |
---------------------------------
Eval num_timesteps=2691192, episode_reward=2042.32 +/- 1896.68
Episode length: 808.60 +/- 148.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2691192  |
---------------------------------
Eval num_timesteps=2693184, episode_reward=1894.16 +/- 1929.22
Episode length: 761.60 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2693184  |
---------------------------------
Eval num_timesteps=2695176, episode_reward=3920.09 +/- 2803.33
Episode length: 765.40 +/- 53.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 3.92e+03 |
| time/              |          |
|    total_timesteps | 2695176  |
---------------------------------
Eval num_timesteps=2697168, episode_reward=1095.48 +/- 678.68
Episode length: 723.80 +/- 109.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2697168  |
---------------------------------
Eval num_timesteps=2699160, episode_reward=1767.69 +/- 1602.09
Episode length: 777.00 +/- 60.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2699160  |
---------------------------------
Eval num_timesteps=2701152, episode_reward=1381.21 +/- 1663.59
Episode length: 690.80 +/- 114.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2701152  |
---------------------------------
Eval num_timesteps=2703144, episode_reward=3036.56 +/- 2275.91
Episode length: 754.80 +/- 79.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 2703144  |
---------------------------------
Eval num_timesteps=2705136, episode_reward=2496.99 +/- 1721.10
Episode length: 786.60 +/- 64.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 787          |
|    mean_reward          | 2.5e+03      |
| time/                   |              |
|    total_timesteps      | 2705136      |
| train/                  |              |
|    approx_kl            | 0.0020729462 |
|    clip_fraction        | 0.00509      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.55        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000731    |
|    std                  | 1.6          |
|    value_loss           | 2.5e+03      |
------------------------------------------
Eval num_timesteps=2707128, episode_reward=3141.84 +/- 2278.99
Episode length: 829.40 +/- 59.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2707128  |
---------------------------------
Eval num_timesteps=2709120, episode_reward=2373.70 +/- 2030.60
Episode length: 683.40 +/- 34.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 2709120  |
---------------------------------
Eval num_timesteps=2711112, episode_reward=1565.76 +/- 1153.33
Episode length: 797.60 +/- 82.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2711112  |
---------------------------------
Eval num_timesteps=2713104, episode_reward=2337.74 +/- 1865.69
Episode length: 720.80 +/- 94.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2713104  |
---------------------------------
Eval num_timesteps=2715096, episode_reward=2661.37 +/- 2269.03
Episode length: 721.00 +/- 143.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 2715096  |
---------------------------------
Eval num_timesteps=2717088, episode_reward=866.59 +/- 276.45
Episode length: 696.40 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 2717088  |
---------------------------------
Eval num_timesteps=2719080, episode_reward=2785.99 +/- 1994.60
Episode length: 753.80 +/- 94.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 2719080  |
---------------------------------
Eval num_timesteps=2721072, episode_reward=2855.48 +/- 1505.08
Episode length: 715.80 +/- 68.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 2721072  |
---------------------------------
Eval num_timesteps=2723064, episode_reward=1054.87 +/- 424.30
Episode length: 724.40 +/- 83.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2723064  |
---------------------------------
Eval num_timesteps=2725056, episode_reward=3143.50 +/- 2007.09
Episode length: 750.00 +/- 78.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2725056  |
---------------------------------
Eval num_timesteps=2727048, episode_reward=2029.50 +/- 2016.55
Episode length: 761.80 +/- 27.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 2727048  |
---------------------------------
Eval num_timesteps=2729040, episode_reward=2544.69 +/- 2775.58
Episode length: 749.40 +/- 96.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2729040  |
---------------------------------
Eval num_timesteps=2731032, episode_reward=1774.51 +/- 1440.53
Episode length: 883.20 +/- 145.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2731032  |
---------------------------------
Eval num_timesteps=2733024, episode_reward=2299.88 +/- 1444.47
Episode length: 768.40 +/- 69.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 2733024  |
---------------------------------
Eval num_timesteps=2735016, episode_reward=1719.72 +/- 2004.25
Episode length: 717.80 +/- 168.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2735016  |
---------------------------------
Eval num_timesteps=2737008, episode_reward=3383.82 +/- 2251.60
Episode length: 743.80 +/- 56.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 3.38e+03 |
| time/              |          |
|    total_timesteps | 2737008  |
---------------------------------
Eval num_timesteps=2739000, episode_reward=2236.96 +/- 1729.58
Episode length: 771.60 +/- 68.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2739000  |
---------------------------------
Eval num_timesteps=2740992, episode_reward=3286.52 +/- 1993.23
Episode length: 758.60 +/- 80.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 2740992  |
---------------------------------
Eval num_timesteps=2742984, episode_reward=3620.90 +/- 2135.49
Episode length: 779.80 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 3.62e+03 |
| time/              |          |
|    total_timesteps | 2742984  |
---------------------------------
Eval num_timesteps=2744976, episode_reward=2677.54 +/- 1788.47
Episode length: 770.40 +/- 59.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 2744976  |
---------------------------------
Eval num_timesteps=2746968, episode_reward=2555.96 +/- 1787.68
Episode length: 838.80 +/- 120.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2746968  |
---------------------------------
Eval num_timesteps=2748960, episode_reward=1499.87 +/- 1717.30
Episode length: 665.60 +/- 62.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 2748960  |
---------------------------------
Eval num_timesteps=2750952, episode_reward=1158.15 +/- 472.54
Episode length: 797.60 +/- 23.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2750952  |
---------------------------------
Eval num_timesteps=2752944, episode_reward=2032.26 +/- 2160.48
Episode length: 785.80 +/- 54.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 786          |
|    mean_reward          | 2.03e+03     |
| time/                   |              |
|    total_timesteps      | 2752944      |
| train/                  |              |
|    approx_kl            | 0.0025775274 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+03     |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.000804    |
|    std                  | 1.62         |
|    value_loss           | 2.63e+03     |
------------------------------------------
Eval num_timesteps=2754936, episode_reward=1927.43 +/- 1958.05
Episode length: 786.80 +/- 51.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 2754936  |
---------------------------------
Eval num_timesteps=2756928, episode_reward=551.05 +/- 190.17
Episode length: 781.00 +/- 58.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 2756928  |
---------------------------------
Eval num_timesteps=2758920, episode_reward=1031.68 +/- 351.79
Episode length: 713.00 +/- 130.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2758920  |
---------------------------------
Eval num_timesteps=2760912, episode_reward=1416.47 +/- 1717.80
Episode length: 694.80 +/- 66.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2760912  |
---------------------------------
Eval num_timesteps=2762904, episode_reward=1930.62 +/- 1480.35
Episode length: 770.00 +/- 136.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 2762904  |
---------------------------------
Eval num_timesteps=2764896, episode_reward=933.42 +/- 448.65
Episode length: 748.20 +/- 38.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 2764896  |
---------------------------------
Eval num_timesteps=2766888, episode_reward=2888.44 +/- 2469.18
Episode length: 827.20 +/- 141.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 2766888  |
---------------------------------
Eval num_timesteps=2768880, episode_reward=2607.34 +/- 2113.37
Episode length: 719.20 +/- 46.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 2768880  |
---------------------------------
Eval num_timesteps=2770872, episode_reward=2646.13 +/- 3010.45
Episode length: 756.80 +/- 77.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 2770872  |
---------------------------------
Eval num_timesteps=2772864, episode_reward=918.10 +/- 227.96
Episode length: 715.20 +/- 76.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 2772864  |
---------------------------------
Eval num_timesteps=2774856, episode_reward=1679.55 +/- 2054.18
Episode length: 840.20 +/- 110.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2774856  |
---------------------------------
Eval num_timesteps=2776848, episode_reward=1143.41 +/- 797.13
Episode length: 792.80 +/- 25.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2776848  |
---------------------------------
Eval num_timesteps=2778840, episode_reward=1967.93 +/- 1500.42
Episode length: 778.80 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 2778840  |
---------------------------------
Eval num_timesteps=2780832, episode_reward=1637.78 +/- 1340.11
Episode length: 853.60 +/- 71.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2780832  |
---------------------------------
Eval num_timesteps=2782824, episode_reward=1225.55 +/- 1464.16
Episode length: 714.80 +/- 62.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2782824  |
---------------------------------
Eval num_timesteps=2784816, episode_reward=2608.26 +/- 2383.64
Episode length: 763.00 +/- 29.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 2784816  |
---------------------------------
Eval num_timesteps=2786808, episode_reward=3350.34 +/- 2816.18
Episode length: 768.80 +/- 95.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 3.35e+03 |
| time/              |          |
|    total_timesteps | 2786808  |
---------------------------------
Eval num_timesteps=2788800, episode_reward=1217.40 +/- 673.02
Episode length: 769.40 +/- 87.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2788800  |
---------------------------------
Eval num_timesteps=2790792, episode_reward=1277.23 +/- 1108.24
Episode length: 742.20 +/- 67.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2790792  |
---------------------------------
Eval num_timesteps=2792784, episode_reward=2824.56 +/- 2477.82
Episode length: 785.80 +/- 62.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2792784  |
---------------------------------
Eval num_timesteps=2794776, episode_reward=2839.20 +/- 2445.80
Episode length: 788.40 +/- 55.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 2794776  |
---------------------------------
Eval num_timesteps=2796768, episode_reward=1357.37 +/- 1257.63
Episode length: 758.20 +/- 65.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2796768  |
---------------------------------
Eval num_timesteps=2798760, episode_reward=1390.27 +/- 781.86
Episode length: 781.00 +/- 83.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2798760  |
---------------------------------
Eval num_timesteps=2800752, episode_reward=738.40 +/- 578.73
Episode length: 758.00 +/- 65.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 2800752  |
---------------------------------
Eval num_timesteps=2802744, episode_reward=592.66 +/- 466.86
Episode length: 728.40 +/- 67.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 728          |
|    mean_reward          | 593          |
| time/                   |              |
|    total_timesteps      | 2802744      |
| train/                  |              |
|    approx_kl            | 0.0014955619 |
|    clip_fraction        | 0.00222      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.62        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 603          |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.000458    |
|    std                  | 1.63         |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=2804736, episode_reward=732.25 +/- 439.26
Episode length: 778.20 +/- 36.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 2804736  |
---------------------------------
Eval num_timesteps=2806728, episode_reward=771.61 +/- 587.77
Episode length: 745.20 +/- 106.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 2806728  |
---------------------------------
Eval num_timesteps=2808720, episode_reward=1647.96 +/- 2104.25
Episode length: 798.80 +/- 34.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2808720  |
---------------------------------
Eval num_timesteps=2810712, episode_reward=482.73 +/- 166.27
Episode length: 672.80 +/- 92.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 2810712  |
---------------------------------
Eval num_timesteps=2812704, episode_reward=977.73 +/- 545.76
Episode length: 766.00 +/- 34.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 2812704  |
---------------------------------
Eval num_timesteps=2814696, episode_reward=1375.04 +/- 1506.91
Episode length: 740.00 +/- 55.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2814696  |
---------------------------------
Eval num_timesteps=2816688, episode_reward=478.82 +/- 196.92
Episode length: 744.60 +/- 71.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 2816688  |
---------------------------------
Eval num_timesteps=2818680, episode_reward=2468.11 +/- 2029.37
Episode length: 692.20 +/- 76.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2818680  |
---------------------------------
Eval num_timesteps=2820672, episode_reward=950.84 +/- 485.96
Episode length: 759.20 +/- 86.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 2820672  |
---------------------------------
Eval num_timesteps=2822664, episode_reward=555.95 +/- 383.32
Episode length: 693.60 +/- 102.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 2822664  |
---------------------------------
Eval num_timesteps=2824656, episode_reward=808.62 +/- 426.68
Episode length: 738.20 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 2824656  |
---------------------------------
Eval num_timesteps=2826648, episode_reward=583.73 +/- 232.76
Episode length: 741.60 +/- 85.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 2826648  |
---------------------------------
Eval num_timesteps=2828640, episode_reward=1474.07 +/- 1780.03
Episode length: 696.20 +/- 70.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2828640  |
---------------------------------
Eval num_timesteps=2830632, episode_reward=1591.05 +/- 1944.63
Episode length: 718.20 +/- 86.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2830632  |
---------------------------------
Eval num_timesteps=2832624, episode_reward=2785.04 +/- 2829.99
Episode length: 727.80 +/- 85.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 2832624  |
---------------------------------
Eval num_timesteps=2834616, episode_reward=1671.29 +/- 1781.67
Episode length: 767.20 +/- 75.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2834616  |
---------------------------------
Eval num_timesteps=2836608, episode_reward=2080.61 +/- 1857.64
Episode length: 710.20 +/- 93.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2836608  |
---------------------------------
Eval num_timesteps=2838600, episode_reward=569.80 +/- 488.33
Episode length: 672.80 +/- 71.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 2838600  |
---------------------------------
Eval num_timesteps=2840592, episode_reward=3614.12 +/- 2745.68
Episode length: 706.00 +/- 114.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 3.61e+03 |
| time/              |          |
|    total_timesteps | 2840592  |
---------------------------------
Eval num_timesteps=2842584, episode_reward=1283.52 +/- 1983.09
Episode length: 725.80 +/- 37.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2842584  |
---------------------------------
Eval num_timesteps=2844576, episode_reward=511.94 +/- 281.23
Episode length: 724.20 +/- 41.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 2844576  |
---------------------------------
Eval num_timesteps=2846568, episode_reward=752.39 +/- 248.36
Episode length: 763.20 +/- 45.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 2846568  |
---------------------------------
Eval num_timesteps=2848560, episode_reward=880.15 +/- 638.59
Episode length: 767.60 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 2848560  |
---------------------------------
Eval num_timesteps=2850552, episode_reward=2507.10 +/- 2220.30
Episode length: 742.20 +/- 68.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 2850552  |
---------------------------------
Eval num_timesteps=2852544, episode_reward=2967.74 +/- 2875.56
Episode length: 746.00 +/- 84.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 746          |
|    mean_reward          | 2.97e+03     |
| time/                   |              |
|    total_timesteps      | 2852544      |
| train/                  |              |
|    approx_kl            | 0.0031521984 |
|    clip_fraction        | 0.00919      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.000685    |
|    std                  | 1.64         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=2854536, episode_reward=708.43 +/- 576.08
Episode length: 740.60 +/- 73.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 2854536  |
---------------------------------
Eval num_timesteps=2856528, episode_reward=3123.10 +/- 2116.53
Episode length: 751.80 +/- 156.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 2856528  |
---------------------------------
Eval num_timesteps=2858520, episode_reward=1491.76 +/- 1125.87
Episode length: 734.20 +/- 113.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 2858520  |
---------------------------------
Eval num_timesteps=2860512, episode_reward=2358.10 +/- 2225.92
Episode length: 724.20 +/- 125.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 2860512  |
---------------------------------
Eval num_timesteps=2862504, episode_reward=3319.84 +/- 2050.25
Episode length: 745.60 +/- 98.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 2862504  |
---------------------------------
Eval num_timesteps=2864496, episode_reward=2388.20 +/- 2874.42
Episode length: 752.20 +/- 81.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 2864496  |
---------------------------------
Eval num_timesteps=2866488, episode_reward=2001.06 +/- 1545.68
Episode length: 702.20 +/- 44.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2866488  |
---------------------------------
Eval num_timesteps=2868480, episode_reward=2621.99 +/- 1965.29
Episode length: 714.00 +/- 45.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2868480  |
---------------------------------
Eval num_timesteps=2870472, episode_reward=2917.53 +/- 2022.97
Episode length: 694.20 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 2870472  |
---------------------------------
Eval num_timesteps=2872464, episode_reward=1455.93 +/- 1648.31
Episode length: 746.20 +/- 69.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2872464  |
---------------------------------
Eval num_timesteps=2874456, episode_reward=2112.42 +/- 2129.26
Episode length: 749.00 +/- 62.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2874456  |
---------------------------------
Eval num_timesteps=2876448, episode_reward=1948.52 +/- 1636.13
Episode length: 678.40 +/- 45.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2876448  |
---------------------------------
Eval num_timesteps=2878440, episode_reward=939.76 +/- 385.14
Episode length: 772.80 +/- 54.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 2878440  |
---------------------------------
Eval num_timesteps=2880432, episode_reward=2699.48 +/- 1848.50
Episode length: 737.00 +/- 104.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2880432  |
---------------------------------
Eval num_timesteps=2882424, episode_reward=3893.98 +/- 2439.76
Episode length: 774.00 +/- 73.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 3.89e+03 |
| time/              |          |
|    total_timesteps | 2882424  |
---------------------------------
Eval num_timesteps=2884416, episode_reward=2196.35 +/- 2008.72
Episode length: 767.00 +/- 92.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 2884416  |
---------------------------------
Eval num_timesteps=2886408, episode_reward=1715.77 +/- 2144.33
Episode length: 807.20 +/- 23.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2886408  |
---------------------------------
Eval num_timesteps=2888400, episode_reward=972.99 +/- 428.07
Episode length: 760.80 +/- 40.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 2888400  |
---------------------------------
Eval num_timesteps=2890392, episode_reward=2287.44 +/- 2335.52
Episode length: 728.40 +/- 73.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2890392  |
---------------------------------
Eval num_timesteps=2892384, episode_reward=2498.74 +/- 1211.85
Episode length: 751.20 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2892384  |
---------------------------------
Eval num_timesteps=2894376, episode_reward=1525.92 +/- 540.64
Episode length: 712.00 +/- 89.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2894376  |
---------------------------------
Eval num_timesteps=2896368, episode_reward=2323.85 +/- 1668.32
Episode length: 770.20 +/- 58.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2896368  |
---------------------------------
Eval num_timesteps=2898360, episode_reward=2809.21 +/- 2596.13
Episode length: 756.40 +/- 74.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2898360  |
---------------------------------
Eval num_timesteps=2900352, episode_reward=3873.75 +/- 2052.69
Episode length: 789.00 +/- 81.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 789          |
|    mean_reward          | 3.87e+03     |
| time/                   |              |
|    total_timesteps      | 2900352      |
| train/                  |              |
|    approx_kl            | 0.0022035928 |
|    clip_fraction        | 0.00248      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+03     |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000527    |
|    std                  | 1.65         |
|    value_loss           | 3.05e+03     |
------------------------------------------
Eval num_timesteps=2902344, episode_reward=2347.98 +/- 1109.62
Episode length: 679.20 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2902344  |
---------------------------------
Eval num_timesteps=2904336, episode_reward=2068.26 +/- 1627.95
Episode length: 669.20 +/- 129.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2904336  |
---------------------------------
Eval num_timesteps=2906328, episode_reward=3227.03 +/- 1973.55
Episode length: 732.80 +/- 55.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 2906328  |
---------------------------------
Eval num_timesteps=2908320, episode_reward=2944.60 +/- 2351.43
Episode length: 729.40 +/- 86.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 2908320  |
---------------------------------
Eval num_timesteps=2910312, episode_reward=3020.41 +/- 2071.16
Episode length: 690.40 +/- 102.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2910312  |
---------------------------------
Eval num_timesteps=2912304, episode_reward=1393.01 +/- 753.84
Episode length: 732.40 +/- 94.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2912304  |
---------------------------------
Eval num_timesteps=2914296, episode_reward=2073.92 +/- 1517.96
Episode length: 714.20 +/- 55.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2914296  |
---------------------------------
Eval num_timesteps=2916288, episode_reward=1121.85 +/- 480.70
Episode length: 782.80 +/- 58.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2916288  |
---------------------------------
Eval num_timesteps=2918280, episode_reward=1672.47 +/- 2158.99
Episode length: 726.20 +/- 121.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2918280  |
---------------------------------
Eval num_timesteps=2920272, episode_reward=1695.51 +/- 1231.11
Episode length: 749.20 +/- 66.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2920272  |
---------------------------------
Eval num_timesteps=2922264, episode_reward=3225.30 +/- 2671.08
Episode length: 748.40 +/- 87.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 2922264  |
---------------------------------
Eval num_timesteps=2924256, episode_reward=2816.84 +/- 1342.96
Episode length: 725.80 +/- 48.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2924256  |
---------------------------------
Eval num_timesteps=2926248, episode_reward=2004.93 +/- 1426.01
Episode length: 764.60 +/- 56.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2926248  |
---------------------------------
Eval num_timesteps=2928240, episode_reward=1741.01 +/- 1555.35
Episode length: 690.80 +/- 61.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2928240  |
---------------------------------
Eval num_timesteps=2930232, episode_reward=3192.31 +/- 1721.74
Episode length: 775.60 +/- 59.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 2930232  |
---------------------------------
Eval num_timesteps=2932224, episode_reward=1761.85 +/- 1097.45
Episode length: 708.20 +/- 69.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 2932224  |
---------------------------------
Eval num_timesteps=2934216, episode_reward=2424.10 +/- 2095.79
Episode length: 686.80 +/- 64.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2934216  |
---------------------------------
Eval num_timesteps=2936208, episode_reward=1632.09 +/- 1695.02
Episode length: 691.80 +/- 88.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2936208  |
---------------------------------
Eval num_timesteps=2938200, episode_reward=2870.49 +/- 1705.35
Episode length: 773.80 +/- 73.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 2938200  |
---------------------------------
Eval num_timesteps=2940192, episode_reward=3653.98 +/- 2042.70
Episode length: 760.40 +/- 93.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 3.65e+03 |
| time/              |          |
|    total_timesteps | 2940192  |
---------------------------------
Eval num_timesteps=2942184, episode_reward=1474.76 +/- 1073.22
Episode length: 647.00 +/- 127.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2942184  |
---------------------------------
Eval num_timesteps=2944176, episode_reward=1793.55 +/- 1416.71
Episode length: 724.40 +/- 81.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2944176  |
---------------------------------
Eval num_timesteps=2946168, episode_reward=2757.12 +/- 2882.32
Episode length: 705.60 +/- 151.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 2946168  |
---------------------------------
Eval num_timesteps=2948160, episode_reward=4017.88 +/- 2145.17
Episode length: 801.60 +/- 82.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 4.02e+03 |
| time/              |          |
|    total_timesteps | 2948160  |
---------------------------------
Eval num_timesteps=2950152, episode_reward=2419.01 +/- 1475.45
Episode length: 744.80 +/- 35.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 745           |
|    mean_reward          | 2.42e+03      |
| time/                   |               |
|    total_timesteps      | 2950152       |
| train/                  |               |
|    approx_kl            | 0.00089254574 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.69         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+03      |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000379     |
|    std                  | 1.66          |
|    value_loss           | 2.43e+03      |
-------------------------------------------
Eval num_timesteps=2952144, episode_reward=3834.48 +/- 1634.15
Episode length: 788.00 +/- 31.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 3.83e+03 |
| time/              |          |
|    total_timesteps | 2952144  |
---------------------------------
Eval num_timesteps=2954136, episode_reward=2086.99 +/- 1263.30
Episode length: 703.00 +/- 50.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 2954136  |
---------------------------------
Eval num_timesteps=2956128, episode_reward=1391.30 +/- 1136.84
Episode length: 715.00 +/- 98.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2956128  |
---------------------------------
Eval num_timesteps=2958120, episode_reward=2807.44 +/- 2289.33
Episode length: 675.60 +/- 129.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2958120  |
---------------------------------
Eval num_timesteps=2960112, episode_reward=3042.54 +/- 2703.61
Episode length: 800.00 +/- 42.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 2960112  |
---------------------------------
Eval num_timesteps=2962104, episode_reward=884.80 +/- 340.18
Episode length: 676.40 +/- 96.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 2962104  |
---------------------------------
Eval num_timesteps=2964096, episode_reward=795.03 +/- 637.69
Episode length: 669.80 +/- 37.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 2964096  |
---------------------------------
Eval num_timesteps=2966088, episode_reward=1617.60 +/- 1478.82
Episode length: 723.80 +/- 67.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2966088  |
---------------------------------
Eval num_timesteps=2968080, episode_reward=2449.12 +/- 2153.19
Episode length: 679.00 +/- 141.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2968080  |
---------------------------------
Eval num_timesteps=2970072, episode_reward=1407.77 +/- 1293.05
Episode length: 690.40 +/- 42.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2970072  |
---------------------------------
Eval num_timesteps=2972064, episode_reward=2357.07 +/- 2345.85
Episode length: 695.40 +/- 114.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 2972064  |
---------------------------------
Eval num_timesteps=2974056, episode_reward=2794.88 +/- 1727.36
Episode length: 656.60 +/- 83.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 2974056  |
---------------------------------
Eval num_timesteps=2976048, episode_reward=2888.04 +/- 2028.05
Episode length: 779.00 +/- 46.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 2976048  |
---------------------------------
Eval num_timesteps=2978040, episode_reward=1221.13 +/- 854.40
Episode length: 757.00 +/- 22.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2978040  |
---------------------------------
Eval num_timesteps=2980032, episode_reward=3018.84 +/- 1744.99
Episode length: 724.20 +/- 48.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2980032  |
---------------------------------
Eval num_timesteps=2982024, episode_reward=2777.03 +/- 1132.20
Episode length: 722.40 +/- 36.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 2982024  |
---------------------------------
Eval num_timesteps=2984016, episode_reward=2537.81 +/- 1908.61
Episode length: 720.40 +/- 60.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2984016  |
---------------------------------
Eval num_timesteps=2986008, episode_reward=2595.35 +/- 1545.62
Episode length: 788.60 +/- 132.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2986008  |
---------------------------------
Eval num_timesteps=2988000, episode_reward=1502.65 +/- 1436.04
Episode length: 711.80 +/- 77.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2989992, episode_reward=2320.59 +/- 1471.41
Episode length: 683.60 +/- 124.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2989992  |
---------------------------------
Eval num_timesteps=2991984, episode_reward=2263.94 +/- 1354.29
Episode length: 719.00 +/- 68.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2991984  |
---------------------------------
Eval num_timesteps=2993976, episode_reward=1090.06 +/- 900.47
Episode length: 657.60 +/- 49.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2993976  |
---------------------------------
Eval num_timesteps=2995968, episode_reward=1881.89 +/- 1187.12
Episode length: 653.00 +/- 58.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 2995968  |
---------------------------------
Eval num_timesteps=2997960, episode_reward=3052.55 +/- 2442.77
Episode length: 788.60 +/- 106.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 2997960  |
---------------------------------
Eval num_timesteps=2999952, episode_reward=2293.92 +/- 2568.17
Episode length: 705.20 +/- 125.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 705          |
|    mean_reward          | 2.29e+03     |
| time/                   |              |
|    total_timesteps      | 2999952      |
| train/                  |              |
|    approx_kl            | 0.0014259918 |
|    clip_fraction        | 0.000637     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.000508    |
|    std                  | 1.67         |
|    value_loss           | 2.45e+03     |
------------------------------------------
Eval num_timesteps=3001944, episode_reward=1727.12 +/- 1072.49
Episode length: 719.80 +/- 164.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3001944  |
---------------------------------
Eval num_timesteps=3003936, episode_reward=1279.74 +/- 856.79
Episode length: 697.00 +/- 124.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3003936  |
---------------------------------
Eval num_timesteps=3005928, episode_reward=3292.47 +/- 2441.47
Episode length: 743.20 +/- 128.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 3005928  |
---------------------------------
Eval num_timesteps=3007920, episode_reward=3226.84 +/- 1954.31
Episode length: 728.80 +/- 100.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 3007920  |
---------------------------------
Eval num_timesteps=3009912, episode_reward=1410.84 +/- 1841.46
Episode length: 612.60 +/- 80.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3009912  |
---------------------------------
Eval num_timesteps=3011904, episode_reward=896.66 +/- 424.55
Episode length: 692.80 +/- 112.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 3011904  |
---------------------------------
Eval num_timesteps=3013896, episode_reward=1042.35 +/- 524.60
Episode length: 747.60 +/- 102.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3013896  |
---------------------------------
Eval num_timesteps=3015888, episode_reward=3424.06 +/- 2253.33
Episode length: 779.20 +/- 88.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 3.42e+03 |
| time/              |          |
|    total_timesteps | 3015888  |
---------------------------------
Eval num_timesteps=3017880, episode_reward=1451.71 +/- 900.29
Episode length: 711.00 +/- 73.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3017880  |
---------------------------------
Eval num_timesteps=3019872, episode_reward=3425.49 +/- 2113.91
Episode length: 758.80 +/- 39.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 3.43e+03 |
| time/              |          |
|    total_timesteps | 3019872  |
---------------------------------
Eval num_timesteps=3021864, episode_reward=735.90 +/- 310.84
Episode length: 646.00 +/- 56.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 3021864  |
---------------------------------
Eval num_timesteps=3023856, episode_reward=1802.69 +/- 1766.27
Episode length: 706.20 +/- 88.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3023856  |
---------------------------------
Eval num_timesteps=3025848, episode_reward=3819.12 +/- 2127.26
Episode length: 788.40 +/- 43.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 3.82e+03 |
| time/              |          |
|    total_timesteps | 3025848  |
---------------------------------
Eval num_timesteps=3027840, episode_reward=2982.77 +/- 1635.67
Episode length: 769.80 +/- 30.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 3027840  |
---------------------------------
Eval num_timesteps=3029832, episode_reward=3022.95 +/- 2820.89
Episode length: 752.00 +/- 163.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 3029832  |
---------------------------------
Eval num_timesteps=3031824, episode_reward=3447.04 +/- 2354.49
Episode length: 716.20 +/- 91.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 3031824  |
---------------------------------
Eval num_timesteps=3033816, episode_reward=2090.48 +/- 1493.44
Episode length: 757.60 +/- 81.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3033816  |
---------------------------------
Eval num_timesteps=3035808, episode_reward=2343.26 +/- 1702.90
Episode length: 678.60 +/- 111.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3035808  |
---------------------------------
Eval num_timesteps=3037800, episode_reward=1924.98 +/- 1453.12
Episode length: 783.80 +/- 92.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3037800  |
---------------------------------
Eval num_timesteps=3039792, episode_reward=1060.89 +/- 833.12
Episode length: 617.60 +/- 126.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3039792  |
---------------------------------
Eval num_timesteps=3041784, episode_reward=3056.25 +/- 2482.14
Episode length: 746.60 +/- 76.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 3041784  |
---------------------------------
Eval num_timesteps=3043776, episode_reward=949.18 +/- 635.64
Episode length: 665.60 +/- 99.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 3043776  |
---------------------------------
Eval num_timesteps=3045768, episode_reward=3019.13 +/- 2277.86
Episode length: 745.00 +/- 145.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 3045768  |
---------------------------------
Eval num_timesteps=3047760, episode_reward=3570.67 +/- 2401.61
Episode length: 726.80 +/- 97.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 727          |
|    mean_reward          | 3.57e+03     |
| time/                   |              |
|    total_timesteps      | 3047760      |
| train/                  |              |
|    approx_kl            | 0.0013916543 |
|    clip_fraction        | 0.00119      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.75        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000448    |
|    std                  | 1.69         |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=3049752, episode_reward=4861.66 +/- 2307.42
Episode length: 785.40 +/- 76.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 4.86e+03 |
| time/              |          |
|    total_timesteps | 3049752  |
---------------------------------
New best mean reward!
Eval num_timesteps=3051744, episode_reward=579.45 +/- 374.69
Episode length: 542.20 +/- 111.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 3051744  |
---------------------------------
Eval num_timesteps=3053736, episode_reward=3123.36 +/- 1977.48
Episode length: 751.00 +/- 61.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 3053736  |
---------------------------------
Eval num_timesteps=3055728, episode_reward=2323.03 +/- 2792.99
Episode length: 749.20 +/- 83.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3055728  |
---------------------------------
Eval num_timesteps=3057720, episode_reward=2782.19 +/- 2176.22
Episode length: 733.00 +/- 92.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 3057720  |
---------------------------------
Eval num_timesteps=3059712, episode_reward=1528.23 +/- 1306.02
Episode length: 738.80 +/- 120.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3059712  |
---------------------------------
Eval num_timesteps=3061704, episode_reward=1900.13 +/- 2350.63
Episode length: 656.20 +/- 152.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3061704  |
---------------------------------
Eval num_timesteps=3063696, episode_reward=1470.41 +/- 1135.76
Episode length: 668.20 +/- 97.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3063696  |
---------------------------------
Eval num_timesteps=3065688, episode_reward=2088.76 +/- 1987.20
Episode length: 737.60 +/- 88.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3065688  |
---------------------------------
Eval num_timesteps=3067680, episode_reward=2262.78 +/- 1889.59
Episode length: 745.00 +/- 75.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3067680  |
---------------------------------
Eval num_timesteps=3069672, episode_reward=1822.60 +/- 2241.24
Episode length: 684.00 +/- 87.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3069672  |
---------------------------------
Eval num_timesteps=3071664, episode_reward=2087.83 +/- 1948.15
Episode length: 729.00 +/- 38.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3071664  |
---------------------------------
Eval num_timesteps=3073656, episode_reward=3074.00 +/- 1360.27
Episode length: 834.60 +/- 116.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 3073656  |
---------------------------------
Eval num_timesteps=3075648, episode_reward=2130.25 +/- 2078.12
Episode length: 770.00 +/- 118.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3075648  |
---------------------------------
Eval num_timesteps=3077640, episode_reward=2043.49 +/- 2063.07
Episode length: 680.80 +/- 77.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3077640  |
---------------------------------
Eval num_timesteps=3079632, episode_reward=2295.14 +/- 2406.99
Episode length: 725.60 +/- 80.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3079632  |
---------------------------------
Eval num_timesteps=3081624, episode_reward=643.12 +/- 346.75
Episode length: 668.60 +/- 85.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 3081624  |
---------------------------------
Eval num_timesteps=3083616, episode_reward=1875.30 +/- 2084.48
Episode length: 643.80 +/- 164.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3083616  |
---------------------------------
Eval num_timesteps=3085608, episode_reward=3285.85 +/- 2100.18
Episode length: 782.40 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 3085608  |
---------------------------------
Eval num_timesteps=3087600, episode_reward=2559.14 +/- 1674.85
Episode length: 734.40 +/- 82.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3087600  |
---------------------------------
Eval num_timesteps=3089592, episode_reward=1735.82 +/- 1453.87
Episode length: 624.60 +/- 115.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3089592  |
---------------------------------
Eval num_timesteps=3091584, episode_reward=1546.54 +/- 1916.70
Episode length: 573.00 +/- 154.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3091584  |
---------------------------------
Eval num_timesteps=3093576, episode_reward=654.94 +/- 609.65
Episode length: 580.60 +/- 168.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 3093576  |
---------------------------------
Eval num_timesteps=3095568, episode_reward=2597.74 +/- 2413.78
Episode length: 704.40 +/- 70.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 3095568  |
---------------------------------
Eval num_timesteps=3097560, episode_reward=2509.89 +/- 2380.90
Episode length: 648.20 +/- 124.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 648         |
|    mean_reward          | 2.51e+03    |
| time/                   |             |
|    total_timesteps      | 3097560     |
| train/                  |             |
|    approx_kl            | 0.002277745 |
|    clip_fraction        | 0.00492     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 1.33e+03    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.000701   |
|    std                  | 1.7         |
|    value_loss           | 2.89e+03    |
-----------------------------------------
Eval num_timesteps=3099552, episode_reward=2654.57 +/- 2507.38
Episode length: 745.00 +/- 174.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3099552  |
---------------------------------
Eval num_timesteps=3101544, episode_reward=1974.31 +/- 2232.90
Episode length: 660.60 +/- 146.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3101544  |
---------------------------------
Eval num_timesteps=3103536, episode_reward=1510.26 +/- 2105.35
Episode length: 602.40 +/- 127.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3103536  |
---------------------------------
Eval num_timesteps=3105528, episode_reward=1853.08 +/- 2048.94
Episode length: 574.60 +/- 159.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3105528  |
---------------------------------
Eval num_timesteps=3107520, episode_reward=1738.05 +/- 1936.60
Episode length: 668.00 +/- 45.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3107520  |
---------------------------------
Eval num_timesteps=3109512, episode_reward=2355.64 +/- 1992.22
Episode length: 739.40 +/- 79.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 3109512  |
---------------------------------
Eval num_timesteps=3111504, episode_reward=2211.70 +/- 2056.04
Episode length: 763.00 +/- 89.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 3111504  |
---------------------------------
Eval num_timesteps=3113496, episode_reward=1375.99 +/- 1384.08
Episode length: 693.20 +/- 130.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3113496  |
---------------------------------
Eval num_timesteps=3115488, episode_reward=3084.84 +/- 2131.21
Episode length: 710.20 +/- 50.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 3115488  |
---------------------------------
Eval num_timesteps=3117480, episode_reward=2035.71 +/- 2458.35
Episode length: 624.40 +/- 184.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3117480  |
---------------------------------
Eval num_timesteps=3119472, episode_reward=3433.77 +/- 2303.25
Episode length: 833.20 +/- 90.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 3.43e+03 |
| time/              |          |
|    total_timesteps | 3119472  |
---------------------------------
Eval num_timesteps=3121464, episode_reward=2612.44 +/- 2283.92
Episode length: 667.40 +/- 127.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 3121464  |
---------------------------------
Eval num_timesteps=3123456, episode_reward=3615.43 +/- 2276.15
Episode length: 734.80 +/- 76.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 3.62e+03 |
| time/              |          |
|    total_timesteps | 3123456  |
---------------------------------
Eval num_timesteps=3125448, episode_reward=1773.80 +/- 1828.08
Episode length: 668.20 +/- 177.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3125448  |
---------------------------------
Eval num_timesteps=3127440, episode_reward=1163.89 +/- 560.14
Episode length: 782.80 +/- 50.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3127440  |
---------------------------------
Eval num_timesteps=3129432, episode_reward=3712.01 +/- 2782.51
Episode length: 783.80 +/- 176.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 3.71e+03 |
| time/              |          |
|    total_timesteps | 3129432  |
---------------------------------
Eval num_timesteps=3131424, episode_reward=1815.48 +/- 1713.58
Episode length: 690.40 +/- 102.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3131424  |
---------------------------------
Eval num_timesteps=3133416, episode_reward=1905.01 +/- 2270.18
Episode length: 698.40 +/- 120.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3133416  |
---------------------------------
Eval num_timesteps=3135408, episode_reward=2718.98 +/- 2256.17
Episode length: 722.80 +/- 176.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3135408  |
---------------------------------
Eval num_timesteps=3137400, episode_reward=3056.75 +/- 2207.06
Episode length: 773.20 +/- 51.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 3137400  |
---------------------------------
Eval num_timesteps=3139392, episode_reward=3277.73 +/- 2644.23
Episode length: 672.60 +/- 161.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 3.28e+03 |
| time/              |          |
|    total_timesteps | 3139392  |
---------------------------------
Eval num_timesteps=3141384, episode_reward=5113.30 +/- 2958.95
Episode length: 769.40 +/- 105.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 5.11e+03 |
| time/              |          |
|    total_timesteps | 3141384  |
---------------------------------
New best mean reward!
Eval num_timesteps=3143376, episode_reward=2044.97 +/- 2024.55
Episode length: 686.60 +/- 137.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3143376  |
---------------------------------
Eval num_timesteps=3145368, episode_reward=3461.07 +/- 2544.60
Episode length: 732.40 +/- 137.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3145368  |
---------------------------------
Eval num_timesteps=3147360, episode_reward=1387.33 +/- 1662.24
Episode length: 647.60 +/- 92.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 648          |
|    mean_reward          | 1.39e+03     |
| time/                   |              |
|    total_timesteps      | 3147360      |
| train/                  |              |
|    approx_kl            | 0.0025040207 |
|    clip_fraction        | 0.0052       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.83        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+03     |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.000572    |
|    std                  | 1.72         |
|    value_loss           | 2.4e+03      |
------------------------------------------
Eval num_timesteps=3149352, episode_reward=257.11 +/- 240.65
Episode length: 630.80 +/- 136.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 3149352  |
---------------------------------
Eval num_timesteps=3151344, episode_reward=3243.85 +/- 2789.13
Episode length: 749.40 +/- 31.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 3151344  |
---------------------------------
Eval num_timesteps=3153336, episode_reward=2683.79 +/- 1586.24
Episode length: 763.20 +/- 60.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3153336  |
---------------------------------
Eval num_timesteps=3155328, episode_reward=2641.45 +/- 2606.12
Episode length: 695.60 +/- 162.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 3155328  |
---------------------------------
Eval num_timesteps=3157320, episode_reward=878.55 +/- 1445.96
Episode length: 547.60 +/- 180.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 3157320  |
---------------------------------
Eval num_timesteps=3159312, episode_reward=2894.69 +/- 2589.63
Episode length: 678.00 +/- 135.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 3159312  |
---------------------------------
Eval num_timesteps=3161304, episode_reward=3853.26 +/- 2205.55
Episode length: 718.40 +/- 83.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 3.85e+03 |
| time/              |          |
|    total_timesteps | 3161304  |
---------------------------------
Eval num_timesteps=3163296, episode_reward=967.02 +/- 649.66
Episode length: 787.80 +/- 101.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 3163296  |
---------------------------------
Eval num_timesteps=3165288, episode_reward=1566.53 +/- 2071.74
Episode length: 550.20 +/- 138.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3165288  |
---------------------------------
Eval num_timesteps=3167280, episode_reward=1239.52 +/- 719.65
Episode length: 654.80 +/- 109.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3167280  |
---------------------------------
Eval num_timesteps=3169272, episode_reward=3230.74 +/- 2593.89
Episode length: 720.20 +/- 142.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 3169272  |
---------------------------------
Eval num_timesteps=3171264, episode_reward=2007.10 +/- 2169.36
Episode length: 684.20 +/- 128.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3171264  |
---------------------------------
Eval num_timesteps=3173256, episode_reward=2714.02 +/- 2982.87
Episode length: 605.20 +/- 170.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3173256  |
---------------------------------
Eval num_timesteps=3175248, episode_reward=3969.83 +/- 3115.42
Episode length: 651.80 +/- 159.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 3.97e+03 |
| time/              |          |
|    total_timesteps | 3175248  |
---------------------------------
Eval num_timesteps=3177240, episode_reward=3086.44 +/- 2557.04
Episode length: 667.80 +/- 58.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 3177240  |
---------------------------------
Eval num_timesteps=3179232, episode_reward=2735.46 +/- 2259.20
Episode length: 743.40 +/- 83.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3179232  |
---------------------------------
Eval num_timesteps=3181224, episode_reward=825.89 +/- 313.31
Episode length: 615.00 +/- 49.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 3181224  |
---------------------------------
Eval num_timesteps=3183216, episode_reward=3611.36 +/- 2534.04
Episode length: 667.60 +/- 125.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 3.61e+03 |
| time/              |          |
|    total_timesteps | 3183216  |
---------------------------------
Eval num_timesteps=3185208, episode_reward=3003.64 +/- 2524.45
Episode length: 733.40 +/- 84.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3185208  |
---------------------------------
Eval num_timesteps=3187200, episode_reward=1441.10 +/- 1263.85
Episode length: 633.00 +/- 119.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3187200  |
---------------------------------
Eval num_timesteps=3189192, episode_reward=3505.86 +/- 2030.59
Episode length: 796.40 +/- 58.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 3.51e+03 |
| time/              |          |
|    total_timesteps | 3189192  |
---------------------------------
Eval num_timesteps=3191184, episode_reward=1392.30 +/- 1619.55
Episode length: 585.00 +/- 128.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3191184  |
---------------------------------
Eval num_timesteps=3193176, episode_reward=3406.54 +/- 2761.69
Episode length: 787.20 +/- 94.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 3.41e+03 |
| time/              |          |
|    total_timesteps | 3193176  |
---------------------------------
Eval num_timesteps=3195168, episode_reward=3453.70 +/- 3334.42
Episode length: 681.20 +/- 91.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 681          |
|    mean_reward          | 3.45e+03     |
| time/                   |              |
|    total_timesteps      | 3195168      |
| train/                  |              |
|    approx_kl            | 0.0017731301 |
|    clip_fraction        | 0.00233      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.85        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 859          |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00064     |
|    std                  | 1.73         |
|    value_loss           | 3.15e+03     |
------------------------------------------
Eval num_timesteps=3197160, episode_reward=1365.22 +/- 2364.93
Episode length: 557.20 +/- 194.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3197160  |
---------------------------------
Eval num_timesteps=3199152, episode_reward=2768.27 +/- 2650.38
Episode length: 694.40 +/- 154.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3199152  |
---------------------------------
Eval num_timesteps=3201144, episode_reward=4097.44 +/- 2853.27
Episode length: 813.00 +/- 53.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 4.1e+03  |
| time/              |          |
|    total_timesteps | 3201144  |
---------------------------------
Eval num_timesteps=3203136, episode_reward=1923.26 +/- 1704.64
Episode length: 778.80 +/- 218.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3203136  |
---------------------------------
Eval num_timesteps=3205128, episode_reward=4342.02 +/- 1926.25
Episode length: 746.80 +/- 134.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 4.34e+03 |
| time/              |          |
|    total_timesteps | 3205128  |
---------------------------------
Eval num_timesteps=3207120, episode_reward=3268.88 +/- 2640.83
Episode length: 727.40 +/- 67.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 3207120  |
---------------------------------
Eval num_timesteps=3209112, episode_reward=2395.18 +/- 2227.28
Episode length: 695.20 +/- 184.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3209112  |
---------------------------------
Eval num_timesteps=3211104, episode_reward=910.19 +/- 1545.06
Episode length: 505.20 +/- 140.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 3211104  |
---------------------------------
Eval num_timesteps=3213096, episode_reward=1881.81 +/- 1878.23
Episode length: 778.20 +/- 103.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3213096  |
---------------------------------
Eval num_timesteps=3215088, episode_reward=1141.22 +/- 589.99
Episode length: 649.60 +/- 140.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3215088  |
---------------------------------
Eval num_timesteps=3217080, episode_reward=1756.78 +/- 1799.91
Episode length: 726.60 +/- 129.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3217080  |
---------------------------------
Eval num_timesteps=3219072, episode_reward=2069.35 +/- 1748.84
Episode length: 727.80 +/- 101.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3219072  |
---------------------------------
Eval num_timesteps=3221064, episode_reward=1830.66 +/- 2965.12
Episode length: 599.20 +/- 154.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3221064  |
---------------------------------
Eval num_timesteps=3223056, episode_reward=1790.87 +/- 2120.92
Episode length: 664.40 +/- 187.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3223056  |
---------------------------------
Eval num_timesteps=3225048, episode_reward=4234.61 +/- 2527.75
Episode length: 721.40 +/- 70.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 4.23e+03 |
| time/              |          |
|    total_timesteps | 3225048  |
---------------------------------
Eval num_timesteps=3227040, episode_reward=744.44 +/- 336.50
Episode length: 650.20 +/- 121.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 3227040  |
---------------------------------
Eval num_timesteps=3229032, episode_reward=2594.63 +/- 2345.88
Episode length: 710.60 +/- 158.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 3229032  |
---------------------------------
Eval num_timesteps=3231024, episode_reward=2064.49 +/- 1880.11
Episode length: 700.00 +/- 96.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 3231024  |
---------------------------------
Eval num_timesteps=3233016, episode_reward=2169.73 +/- 1423.88
Episode length: 708.80 +/- 50.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3233016  |
---------------------------------
Eval num_timesteps=3235008, episode_reward=878.72 +/- 482.34
Episode length: 672.00 +/- 97.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 3235008  |
---------------------------------
Eval num_timesteps=3237000, episode_reward=1314.08 +/- 2176.68
Episode length: 523.00 +/- 159.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3237000  |
---------------------------------
Eval num_timesteps=3238992, episode_reward=1959.55 +/- 1818.33
Episode length: 671.00 +/- 90.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3238992  |
---------------------------------
Eval num_timesteps=3240984, episode_reward=1665.43 +/- 2133.39
Episode length: 731.60 +/- 50.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3240984  |
---------------------------------
Eval num_timesteps=3242976, episode_reward=2180.23 +/- 2141.17
Episode length: 804.00 +/- 195.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3242976  |
---------------------------------
Eval num_timesteps=3244968, episode_reward=3390.23 +/- 1847.22
Episode length: 750.60 +/- 68.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 751          |
|    mean_reward          | 3.39e+03     |
| time/                   |              |
|    total_timesteps      | 3244968      |
| train/                  |              |
|    approx_kl            | 0.0019956266 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 782          |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000676    |
|    std                  | 1.75         |
|    value_loss           | 1.95e+03     |
------------------------------------------
Eval num_timesteps=3246960, episode_reward=2559.62 +/- 2276.34
Episode length: 690.20 +/- 86.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3246960  |
---------------------------------
Eval num_timesteps=3248952, episode_reward=2563.78 +/- 2782.25
Episode length: 626.40 +/- 170.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3248952  |
---------------------------------
Eval num_timesteps=3250944, episode_reward=1931.74 +/- 2180.16
Episode length: 822.40 +/- 36.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3250944  |
---------------------------------
Eval num_timesteps=3252936, episode_reward=3772.50 +/- 2401.34
Episode length: 743.80 +/- 83.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 3.77e+03 |
| time/              |          |
|    total_timesteps | 3252936  |
---------------------------------
Eval num_timesteps=3254928, episode_reward=2645.59 +/- 2717.72
Episode length: 684.40 +/- 120.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3254928  |
---------------------------------
Eval num_timesteps=3256920, episode_reward=3301.22 +/- 2419.83
Episode length: 704.20 +/- 42.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 3256920  |
---------------------------------
Eval num_timesteps=3258912, episode_reward=3918.38 +/- 3029.62
Episode length: 736.20 +/- 62.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 3.92e+03 |
| time/              |          |
|    total_timesteps | 3258912  |
---------------------------------
Eval num_timesteps=3260904, episode_reward=2935.85 +/- 1987.77
Episode length: 734.60 +/- 95.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 3260904  |
---------------------------------
Eval num_timesteps=3262896, episode_reward=2494.72 +/- 2501.92
Episode length: 754.40 +/- 118.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3262896  |
---------------------------------
Eval num_timesteps=3264888, episode_reward=2316.37 +/- 2074.51
Episode length: 719.80 +/- 72.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3264888  |
---------------------------------
Eval num_timesteps=3266880, episode_reward=1080.02 +/- 420.60
Episode length: 636.00 +/- 80.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3266880  |
---------------------------------
Eval num_timesteps=3268872, episode_reward=3658.08 +/- 2026.80
Episode length: 771.40 +/- 116.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 3.66e+03 |
| time/              |          |
|    total_timesteps | 3268872  |
---------------------------------
Eval num_timesteps=3270864, episode_reward=2048.59 +/- 1742.49
Episode length: 714.80 +/- 73.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3270864  |
---------------------------------
Eval num_timesteps=3272856, episode_reward=1206.67 +/- 2161.12
Episode length: 572.40 +/- 154.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3272856  |
---------------------------------
Eval num_timesteps=3274848, episode_reward=1212.28 +/- 411.60
Episode length: 734.20 +/- 78.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3274848  |
---------------------------------
Eval num_timesteps=3276840, episode_reward=3885.23 +/- 3681.21
Episode length: 721.20 +/- 122.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 3.89e+03 |
| time/              |          |
|    total_timesteps | 3276840  |
---------------------------------
Eval num_timesteps=3278832, episode_reward=2300.08 +/- 1903.98
Episode length: 663.80 +/- 66.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3278832  |
---------------------------------
Eval num_timesteps=3280824, episode_reward=1358.14 +/- 2015.70
Episode length: 628.00 +/- 128.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3280824  |
---------------------------------
Eval num_timesteps=3282816, episode_reward=2287.90 +/- 1816.27
Episode length: 812.60 +/- 47.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 3282816  |
---------------------------------
Eval num_timesteps=3284808, episode_reward=2509.65 +/- 1565.16
Episode length: 723.20 +/- 80.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3284808  |
---------------------------------
Eval num_timesteps=3286800, episode_reward=675.85 +/- 423.57
Episode length: 663.80 +/- 102.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 3286800  |
---------------------------------
Eval num_timesteps=3288792, episode_reward=1051.84 +/- 541.88
Episode length: 776.60 +/- 119.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3288792  |
---------------------------------
Eval num_timesteps=3290784, episode_reward=3510.03 +/- 2270.44
Episode length: 668.20 +/- 119.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 3.51e+03 |
| time/              |          |
|    total_timesteps | 3290784  |
---------------------------------
Eval num_timesteps=3292776, episode_reward=2332.55 +/- 2183.94
Episode length: 655.20 +/- 64.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3292776  |
---------------------------------
Eval num_timesteps=3294768, episode_reward=1723.53 +/- 1858.97
Episode length: 754.20 +/- 159.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 754          |
|    mean_reward          | 1.72e+03     |
| time/                   |              |
|    total_timesteps      | 3294768      |
| train/                  |              |
|    approx_kl            | 0.0019438161 |
|    clip_fraction        | 0.00272      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.93        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 1.75e+03     |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.000515    |
|    std                  | 1.77         |
|    value_loss           | 2.76e+03     |
------------------------------------------
Eval num_timesteps=3296760, episode_reward=796.54 +/- 739.68
Episode length: 694.40 +/- 128.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 3296760  |
---------------------------------
Eval num_timesteps=3298752, episode_reward=721.67 +/- 613.80
Episode length: 669.20 +/- 150.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 3298752  |
---------------------------------
Eval num_timesteps=3300744, episode_reward=2954.25 +/- 2551.42
Episode length: 782.60 +/- 40.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 3300744  |
---------------------------------
Eval num_timesteps=3302736, episode_reward=1777.98 +/- 2328.44
Episode length: 800.80 +/- 28.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3302736  |
---------------------------------
Eval num_timesteps=3304728, episode_reward=2453.39 +/- 2547.33
Episode length: 734.80 +/- 110.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 3304728  |
---------------------------------
Eval num_timesteps=3306720, episode_reward=737.70 +/- 385.46
Episode length: 663.40 +/- 104.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 3306720  |
---------------------------------
Eval num_timesteps=3308712, episode_reward=3131.99 +/- 2682.17
Episode length: 745.40 +/- 86.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3308712  |
---------------------------------
Eval num_timesteps=3310704, episode_reward=3326.60 +/- 2933.89
Episode length: 796.00 +/- 55.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 3310704  |
---------------------------------
Eval num_timesteps=3312696, episode_reward=3843.57 +/- 2420.24
Episode length: 745.00 +/- 88.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 3.84e+03 |
| time/              |          |
|    total_timesteps | 3312696  |
---------------------------------
Eval num_timesteps=3314688, episode_reward=1564.76 +/- 1622.25
Episode length: 738.80 +/- 102.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3314688  |
---------------------------------
Eval num_timesteps=3316680, episode_reward=2372.05 +/- 2167.25
Episode length: 714.20 +/- 67.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 3316680  |
---------------------------------
Eval num_timesteps=3318672, episode_reward=501.16 +/- 478.26
Episode length: 672.40 +/- 55.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 3318672  |
---------------------------------
Eval num_timesteps=3320664, episode_reward=1540.39 +/- 2070.23
Episode length: 734.40 +/- 167.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3320664  |
---------------------------------
Eval num_timesteps=3322656, episode_reward=2405.09 +/- 2493.99
Episode length: 593.80 +/- 142.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 3322656  |
---------------------------------
Eval num_timesteps=3324648, episode_reward=2435.46 +/- 2959.92
Episode length: 737.40 +/- 100.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 3324648  |
---------------------------------
Eval num_timesteps=3326640, episode_reward=1512.00 +/- 1693.73
Episode length: 638.60 +/- 126.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3326640  |
---------------------------------
Eval num_timesteps=3328632, episode_reward=801.04 +/- 677.86
Episode length: 703.20 +/- 148.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 3328632  |
---------------------------------
Eval num_timesteps=3330624, episode_reward=2456.43 +/- 3195.22
Episode length: 653.80 +/- 123.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 3330624  |
---------------------------------
Eval num_timesteps=3332616, episode_reward=2837.53 +/- 2586.33
Episode length: 813.40 +/- 32.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 3332616  |
---------------------------------
Eval num_timesteps=3334608, episode_reward=2490.04 +/- 2580.56
Episode length: 659.60 +/- 169.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3334608  |
---------------------------------
Eval num_timesteps=3336600, episode_reward=670.58 +/- 631.25
Episode length: 782.80 +/- 63.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 3336600  |
---------------------------------
Eval num_timesteps=3338592, episode_reward=918.83 +/- 944.08
Episode length: 705.40 +/- 102.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 3338592  |
---------------------------------
Eval num_timesteps=3340584, episode_reward=3936.25 +/- 3362.79
Episode length: 723.00 +/- 55.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 3.94e+03 |
| time/              |          |
|    total_timesteps | 3340584  |
---------------------------------
Eval num_timesteps=3342576, episode_reward=574.69 +/- 573.44
Episode length: 622.60 +/- 125.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 623          |
|    mean_reward          | 575          |
| time/                   |              |
|    total_timesteps      | 3342576      |
| train/                  |              |
|    approx_kl            | 0.0017721872 |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+03     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.000414    |
|    std                  | 1.8          |
|    value_loss           | 2.31e+03     |
------------------------------------------
Eval num_timesteps=3344568, episode_reward=2181.08 +/- 1691.55
Episode length: 661.40 +/- 48.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3344568  |
---------------------------------
Eval num_timesteps=3346560, episode_reward=1991.56 +/- 2016.34
Episode length: 628.00 +/- 83.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3346560  |
---------------------------------
Eval num_timesteps=3348552, episode_reward=1728.53 +/- 1800.26
Episode length: 752.40 +/- 86.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3348552  |
---------------------------------
Eval num_timesteps=3350544, episode_reward=2230.94 +/- 2056.57
Episode length: 743.60 +/- 129.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3350544  |
---------------------------------
Eval num_timesteps=3352536, episode_reward=1790.18 +/- 2224.96
Episode length: 753.20 +/- 127.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3352536  |
---------------------------------
Eval num_timesteps=3354528, episode_reward=2014.85 +/- 2094.44
Episode length: 795.20 +/- 117.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3354528  |
---------------------------------
Eval num_timesteps=3356520, episode_reward=1286.18 +/- 2294.49
Episode length: 712.80 +/- 173.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3356520  |
---------------------------------
Eval num_timesteps=3358512, episode_reward=1637.32 +/- 1819.16
Episode length: 603.80 +/- 143.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3358512  |
---------------------------------
Eval num_timesteps=3360504, episode_reward=3338.92 +/- 2666.95
Episode length: 743.20 +/- 103.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 3360504  |
---------------------------------
Eval num_timesteps=3362496, episode_reward=2663.92 +/- 2884.78
Episode length: 822.20 +/- 229.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3362496  |
---------------------------------
Eval num_timesteps=3364488, episode_reward=2707.20 +/- 2779.36
Episode length: 704.00 +/- 130.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3364488  |
---------------------------------
Eval num_timesteps=3366480, episode_reward=1035.11 +/- 772.88
Episode length: 712.80 +/- 113.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3366480  |
---------------------------------
Eval num_timesteps=3368472, episode_reward=848.83 +/- 630.07
Episode length: 704.80 +/- 117.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 3368472  |
---------------------------------
Eval num_timesteps=3370464, episode_reward=1290.02 +/- 2161.74
Episode length: 694.80 +/- 130.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3370464  |
---------------------------------
Eval num_timesteps=3372456, episode_reward=455.30 +/- 302.46
Episode length: 575.80 +/- 156.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 3372456  |
---------------------------------
Eval num_timesteps=3374448, episode_reward=1898.20 +/- 1679.00
Episode length: 686.40 +/- 53.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3374448  |
---------------------------------
Eval num_timesteps=3376440, episode_reward=1389.48 +/- 2027.12
Episode length: 762.60 +/- 78.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3376440  |
---------------------------------
Eval num_timesteps=3378432, episode_reward=2636.89 +/- 2703.14
Episode length: 751.40 +/- 113.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 3378432  |
---------------------------------
Eval num_timesteps=3380424, episode_reward=1025.76 +/- 519.30
Episode length: 781.00 +/- 118.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3380424  |
---------------------------------
Eval num_timesteps=3382416, episode_reward=5360.68 +/- 2883.56
Episode length: 719.40 +/- 43.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 5.36e+03 |
| time/              |          |
|    total_timesteps | 3382416  |
---------------------------------
New best mean reward!
Eval num_timesteps=3384408, episode_reward=1473.47 +/- 2093.86
Episode length: 711.20 +/- 64.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3384408  |
---------------------------------
Eval num_timesteps=3386400, episode_reward=2628.69 +/- 2252.82
Episode length: 807.60 +/- 118.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 3386400  |
---------------------------------
Eval num_timesteps=3388392, episode_reward=1882.67 +/- 2079.42
Episode length: 625.00 +/- 139.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3388392  |
---------------------------------
Eval num_timesteps=3390384, episode_reward=1615.26 +/- 2225.23
Episode length: 715.20 +/- 70.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3390384  |
---------------------------------
Eval num_timesteps=3392376, episode_reward=631.71 +/- 677.51
Episode length: 674.80 +/- 137.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 632          |
| time/                   |              |
|    total_timesteps      | 3392376      |
| train/                  |              |
|    approx_kl            | 0.0017388472 |
|    clip_fraction        | 0.0053       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.05        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000759    |
|    std                  | 1.82         |
|    value_loss           | 2.82e+03     |
------------------------------------------
Eval num_timesteps=3394368, episode_reward=1684.42 +/- 2294.62
Episode length: 789.60 +/- 64.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3394368  |
---------------------------------
Eval num_timesteps=3396360, episode_reward=1866.92 +/- 1950.01
Episode length: 637.20 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3396360  |
---------------------------------
Eval num_timesteps=3398352, episode_reward=2663.22 +/- 3708.05
Episode length: 819.20 +/- 45.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3398352  |
---------------------------------
Eval num_timesteps=3400344, episode_reward=1882.46 +/- 2502.24
Episode length: 688.60 +/- 51.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3400344  |
---------------------------------
Eval num_timesteps=3402336, episode_reward=2524.17 +/- 2696.72
Episode length: 709.60 +/- 78.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3402336  |
---------------------------------
Eval num_timesteps=3404328, episode_reward=2433.66 +/- 3146.73
Episode length: 684.60 +/- 79.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3404328  |
---------------------------------
Eval num_timesteps=3406320, episode_reward=422.56 +/- 296.40
Episode length: 721.20 +/- 146.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 3406320  |
---------------------------------
Eval num_timesteps=3408312, episode_reward=2462.12 +/- 2247.73
Episode length: 688.20 +/- 156.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 3408312  |
---------------------------------
Eval num_timesteps=3410304, episode_reward=1672.31 +/- 2249.20
Episode length: 732.80 +/- 116.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3410304  |
---------------------------------
Eval num_timesteps=3412296, episode_reward=802.31 +/- 561.38
Episode length: 719.40 +/- 81.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 3412296  |
---------------------------------
Eval num_timesteps=3414288, episode_reward=2772.08 +/- 3069.92
Episode length: 660.00 +/- 135.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3414288  |
---------------------------------
Eval num_timesteps=3416280, episode_reward=2683.47 +/- 2656.83
Episode length: 713.20 +/- 92.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3416280  |
---------------------------------
Eval num_timesteps=3418272, episode_reward=2014.35 +/- 1894.65
Episode length: 716.60 +/- 77.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3418272  |
---------------------------------
Eval num_timesteps=3420264, episode_reward=715.50 +/- 439.31
Episode length: 696.80 +/- 56.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 3420264  |
---------------------------------
Eval num_timesteps=3422256, episode_reward=1226.94 +/- 777.20
Episode length: 753.60 +/- 60.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3422256  |
---------------------------------
Eval num_timesteps=3424248, episode_reward=1751.97 +/- 2099.87
Episode length: 694.40 +/- 134.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3424248  |
---------------------------------
Eval num_timesteps=3426240, episode_reward=1122.06 +/- 503.48
Episode length: 734.80 +/- 105.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3426240  |
---------------------------------
Eval num_timesteps=3428232, episode_reward=732.64 +/- 327.04
Episode length: 656.80 +/- 71.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 3428232  |
---------------------------------
Eval num_timesteps=3430224, episode_reward=2606.45 +/- 2089.18
Episode length: 699.80 +/- 125.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 3430224  |
---------------------------------
Eval num_timesteps=3432216, episode_reward=2679.46 +/- 2130.04
Episode length: 644.60 +/- 89.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3432216  |
---------------------------------
Eval num_timesteps=3434208, episode_reward=2038.71 +/- 2996.29
Episode length: 712.00 +/- 55.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3434208  |
---------------------------------
Eval num_timesteps=3436200, episode_reward=1733.78 +/- 1388.33
Episode length: 788.20 +/- 125.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3436200  |
---------------------------------
Eval num_timesteps=3438192, episode_reward=1174.26 +/- 456.06
Episode length: 740.80 +/- 66.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3438192  |
---------------------------------
Eval num_timesteps=3440184, episode_reward=920.69 +/- 697.04
Episode length: 682.60 +/- 118.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 3440184  |
---------------------------------
Eval num_timesteps=3442176, episode_reward=2324.66 +/- 3144.98
Episode length: 656.40 +/- 130.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 656         |
|    mean_reward          | 2.32e+03    |
| time/                   |             |
|    total_timesteps      | 3442176     |
| train/                  |             |
|    approx_kl            | 0.001646357 |
|    clip_fraction        | 0.00365     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 627         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.000751   |
|    std                  | 1.84        |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=3444168, episode_reward=1623.78 +/- 2075.32
Episode length: 703.60 +/- 122.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3444168  |
---------------------------------
Eval num_timesteps=3446160, episode_reward=3600.73 +/- 2047.09
Episode length: 709.80 +/- 93.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 3.6e+03  |
| time/              |          |
|    total_timesteps | 3446160  |
---------------------------------
Eval num_timesteps=3448152, episode_reward=1014.01 +/- 613.63
Episode length: 768.40 +/- 78.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3448152  |
---------------------------------
Eval num_timesteps=3450144, episode_reward=2145.64 +/- 2148.77
Episode length: 715.60 +/- 61.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3450144  |
---------------------------------
Eval num_timesteps=3452136, episode_reward=631.79 +/- 492.72
Episode length: 617.00 +/- 97.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 3452136  |
---------------------------------
Eval num_timesteps=3454128, episode_reward=2291.74 +/- 1744.57
Episode length: 642.80 +/- 62.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 3454128  |
---------------------------------
Eval num_timesteps=3456120, episode_reward=2018.55 +/- 2061.90
Episode length: 699.80 +/- 72.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3456120  |
---------------------------------
Eval num_timesteps=3458112, episode_reward=4525.11 +/- 3288.77
Episode length: 716.40 +/- 113.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 4.53e+03 |
| time/              |          |
|    total_timesteps | 3458112  |
---------------------------------
Eval num_timesteps=3460104, episode_reward=2428.46 +/- 2145.89
Episode length: 667.60 +/- 34.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3460104  |
---------------------------------
Eval num_timesteps=3462096, episode_reward=485.56 +/- 409.16
Episode length: 609.60 +/- 114.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 3462096  |
---------------------------------
Eval num_timesteps=3464088, episode_reward=1710.10 +/- 1169.87
Episode length: 769.00 +/- 140.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 3464088  |
---------------------------------
Eval num_timesteps=3466080, episode_reward=1813.20 +/- 2134.36
Episode length: 714.80 +/- 143.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3466080  |
---------------------------------
Eval num_timesteps=3468072, episode_reward=3121.49 +/- 2980.97
Episode length: 758.20 +/- 52.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 3468072  |
---------------------------------
Eval num_timesteps=3470064, episode_reward=1979.47 +/- 2969.01
Episode length: 688.80 +/- 98.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3470064  |
---------------------------------
Eval num_timesteps=3472056, episode_reward=803.25 +/- 400.80
Episode length: 791.40 +/- 109.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 3472056  |
---------------------------------
Eval num_timesteps=3474048, episode_reward=3182.67 +/- 2622.70
Episode length: 817.40 +/- 42.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 3474048  |
---------------------------------
Eval num_timesteps=3476040, episode_reward=1785.50 +/- 2084.87
Episode length: 683.40 +/- 106.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3476040  |
---------------------------------
Eval num_timesteps=3478032, episode_reward=2800.37 +/- 2155.84
Episode length: 696.80 +/- 109.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 3478032  |
---------------------------------
Eval num_timesteps=3480024, episode_reward=1248.98 +/- 1804.04
Episode length: 648.00 +/- 219.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3480024  |
---------------------------------
Eval num_timesteps=3482016, episode_reward=2574.40 +/- 2200.97
Episode length: 684.80 +/- 73.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 3482016  |
---------------------------------
Eval num_timesteps=3484008, episode_reward=2443.22 +/- 1963.75
Episode length: 748.00 +/- 92.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 3484008  |
---------------------------------
Eval num_timesteps=3486000, episode_reward=996.34 +/- 412.49
Episode length: 662.60 +/- 96.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 3486000  |
---------------------------------
Eval num_timesteps=3487992, episode_reward=2145.95 +/- 2320.43
Episode length: 667.00 +/- 70.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3487992  |
---------------------------------
Eval num_timesteps=3489984, episode_reward=1922.99 +/- 2075.89
Episode length: 790.80 +/- 88.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 791          |
|    mean_reward          | 1.92e+03     |
| time/                   |              |
|    total_timesteps      | 3489984      |
| train/                  |              |
|    approx_kl            | 0.0036018577 |
|    clip_fraction        | 0.00675      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 760          |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.000661    |
|    std                  | 1.85         |
|    value_loss           | 2.2e+03      |
------------------------------------------
Eval num_timesteps=3491976, episode_reward=658.32 +/- 248.31
Episode length: 613.20 +/- 46.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 3491976  |
---------------------------------
Eval num_timesteps=3493968, episode_reward=1928.58 +/- 2117.02
Episode length: 742.40 +/- 97.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3493968  |
---------------------------------
Eval num_timesteps=3495960, episode_reward=3020.25 +/- 2775.03
Episode length: 817.00 +/- 123.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 3495960  |
---------------------------------
Eval num_timesteps=3497952, episode_reward=1433.72 +/- 1526.79
Episode length: 644.60 +/- 65.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3497952  |
---------------------------------
Eval num_timesteps=3499944, episode_reward=2098.71 +/- 2566.25
Episode length: 666.60 +/- 123.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3499944  |
---------------------------------
Eval num_timesteps=3501936, episode_reward=663.99 +/- 132.94
Episode length: 595.00 +/- 82.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 3501936  |
---------------------------------
Eval num_timesteps=3503928, episode_reward=681.78 +/- 535.61
Episode length: 591.40 +/- 112.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 3503928  |
---------------------------------
Eval num_timesteps=3505920, episode_reward=690.53 +/- 349.02
Episode length: 656.80 +/- 72.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 3505920  |
---------------------------------
Eval num_timesteps=3507912, episode_reward=2017.47 +/- 1957.88
Episode length: 745.60 +/- 77.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3507912  |
---------------------------------
Eval num_timesteps=3509904, episode_reward=1831.70 +/- 2444.83
Episode length: 673.60 +/- 143.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3509904  |
---------------------------------
Eval num_timesteps=3511896, episode_reward=4273.20 +/- 1584.22
Episode length: 736.20 +/- 93.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 3511896  |
---------------------------------
Eval num_timesteps=3513888, episode_reward=837.72 +/- 684.97
Episode length: 638.40 +/- 180.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 3513888  |
---------------------------------
Eval num_timesteps=3515880, episode_reward=862.12 +/- 718.76
Episode length: 608.40 +/- 94.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 3515880  |
---------------------------------
Eval num_timesteps=3517872, episode_reward=3357.44 +/- 2292.07
Episode length: 773.20 +/- 64.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 3.36e+03 |
| time/              |          |
|    total_timesteps | 3517872  |
---------------------------------
Eval num_timesteps=3519864, episode_reward=982.49 +/- 434.81
Episode length: 749.60 +/- 134.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 3519864  |
---------------------------------
Eval num_timesteps=3521856, episode_reward=2797.13 +/- 2355.88
Episode length: 724.20 +/- 84.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 3521856  |
---------------------------------
Eval num_timesteps=3523848, episode_reward=2501.71 +/- 2630.67
Episode length: 740.00 +/- 87.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3523848  |
---------------------------------
Eval num_timesteps=3525840, episode_reward=1928.98 +/- 3145.25
Episode length: 640.60 +/- 101.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3525840  |
---------------------------------
Eval num_timesteps=3527832, episode_reward=2753.83 +/- 3719.53
Episode length: 748.20 +/- 69.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3527832  |
---------------------------------
Eval num_timesteps=3529824, episode_reward=1872.13 +/- 2345.34
Episode length: 692.00 +/- 121.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3529824  |
---------------------------------
Eval num_timesteps=3531816, episode_reward=2660.03 +/- 1838.67
Episode length: 713.80 +/- 58.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3531816  |
---------------------------------
Eval num_timesteps=3533808, episode_reward=2157.16 +/- 1970.66
Episode length: 713.80 +/- 38.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3533808  |
---------------------------------
Eval num_timesteps=3535800, episode_reward=1455.54 +/- 1159.40
Episode length: 654.60 +/- 96.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3535800  |
---------------------------------
Eval num_timesteps=3537792, episode_reward=1912.93 +/- 2057.35
Episode length: 706.40 +/- 99.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3537792  |
---------------------------------
Eval num_timesteps=3539784, episode_reward=1048.69 +/- 496.56
Episode length: 682.20 +/- 107.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 682          |
|    mean_reward          | 1.05e+03     |
| time/                   |              |
|    total_timesteps      | 3539784      |
| train/                  |              |
|    approx_kl            | 0.0021052046 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 472          |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000841    |
|    std                  | 1.86         |
|    value_loss           | 1.6e+03      |
------------------------------------------
Eval num_timesteps=3541776, episode_reward=1986.22 +/- 1694.05
Episode length: 793.00 +/- 144.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3541776  |
---------------------------------
Eval num_timesteps=3543768, episode_reward=1110.60 +/- 563.28
Episode length: 577.20 +/- 66.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3543768  |
---------------------------------
Eval num_timesteps=3545760, episode_reward=2361.01 +/- 2212.31
Episode length: 744.80 +/- 83.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 3545760  |
---------------------------------
Eval num_timesteps=3547752, episode_reward=967.03 +/- 643.58
Episode length: 646.40 +/- 40.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 3547752  |
---------------------------------
Eval num_timesteps=3549744, episode_reward=843.08 +/- 509.02
Episode length: 653.80 +/- 125.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 3549744  |
---------------------------------
Eval num_timesteps=3551736, episode_reward=1507.61 +/- 1386.61
Episode length: 645.20 +/- 68.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3551736  |
---------------------------------
Eval num_timesteps=3553728, episode_reward=5401.73 +/- 2682.21
Episode length: 779.00 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 5.4e+03  |
| time/              |          |
|    total_timesteps | 3553728  |
---------------------------------
New best mean reward!
Eval num_timesteps=3555720, episode_reward=1781.81 +/- 2072.62
Episode length: 664.00 +/- 149.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3555720  |
---------------------------------
Eval num_timesteps=3557712, episode_reward=5105.69 +/- 3174.13
Episode length: 759.80 +/- 35.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 5.11e+03 |
| time/              |          |
|    total_timesteps | 3557712  |
---------------------------------
Eval num_timesteps=3559704, episode_reward=1466.19 +/- 1681.47
Episode length: 675.00 +/- 69.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3559704  |
---------------------------------
Eval num_timesteps=3561696, episode_reward=1654.50 +/- 2158.06
Episode length: 631.00 +/- 130.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3561696  |
---------------------------------
Eval num_timesteps=3563688, episode_reward=1525.04 +/- 1695.37
Episode length: 700.40 +/- 102.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3563688  |
---------------------------------
Eval num_timesteps=3565680, episode_reward=1545.27 +/- 1558.99
Episode length: 648.40 +/- 55.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3565680  |
---------------------------------
Eval num_timesteps=3567672, episode_reward=2505.63 +/- 2345.87
Episode length: 723.00 +/- 68.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3567672  |
---------------------------------
Eval num_timesteps=3569664, episode_reward=4329.14 +/- 2739.04
Episode length: 774.00 +/- 32.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 4.33e+03 |
| time/              |          |
|    total_timesteps | 3569664  |
---------------------------------
Eval num_timesteps=3571656, episode_reward=1825.80 +/- 1201.80
Episode length: 669.60 +/- 32.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3571656  |
---------------------------------
Eval num_timesteps=3573648, episode_reward=1916.70 +/- 1912.68
Episode length: 675.40 +/- 102.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3573648  |
---------------------------------
Eval num_timesteps=3575640, episode_reward=2658.47 +/- 2017.50
Episode length: 675.40 +/- 82.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3575640  |
---------------------------------
Eval num_timesteps=3577632, episode_reward=1931.94 +/- 2184.57
Episode length: 599.20 +/- 151.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3577632  |
---------------------------------
Eval num_timesteps=3579624, episode_reward=3077.68 +/- 2652.06
Episode length: 722.40 +/- 139.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 3579624  |
---------------------------------
Eval num_timesteps=3581616, episode_reward=1058.53 +/- 512.64
Episode length: 674.60 +/- 85.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3581616  |
---------------------------------
Eval num_timesteps=3583608, episode_reward=1051.73 +/- 594.81
Episode length: 630.00 +/- 90.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3583608  |
---------------------------------
Eval num_timesteps=3585600, episode_reward=3157.45 +/- 2045.06
Episode length: 769.60 +/- 107.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 3585600  |
---------------------------------
Eval num_timesteps=3587592, episode_reward=2044.80 +/- 2465.41
Episode length: 656.20 +/- 133.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3587592  |
---------------------------------
Eval num_timesteps=3589584, episode_reward=1119.59 +/- 543.15
Episode length: 709.00 +/- 51.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 709          |
|    mean_reward          | 1.12e+03     |
| time/                   |              |
|    total_timesteps      | 3589584      |
| train/                  |              |
|    approx_kl            | 0.0024492533 |
|    clip_fraction        | 0.00357      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 372          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.000694    |
|    std                  | 1.87         |
|    value_loss           | 1.74e+03     |
------------------------------------------
Eval num_timesteps=3591576, episode_reward=1701.06 +/- 2348.89
Episode length: 662.40 +/- 133.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3591576  |
---------------------------------
Eval num_timesteps=3593568, episode_reward=3125.01 +/- 2320.06
Episode length: 704.00 +/- 81.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3593568  |
---------------------------------
Eval num_timesteps=3595560, episode_reward=2742.66 +/- 2458.61
Episode length: 691.20 +/- 135.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3595560  |
---------------------------------
Eval num_timesteps=3597552, episode_reward=2498.48 +/- 2013.20
Episode length: 736.00 +/- 125.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3597552  |
---------------------------------
Eval num_timesteps=3599544, episode_reward=563.68 +/- 507.95
Episode length: 551.40 +/- 197.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 3599544  |
---------------------------------
Eval num_timesteps=3601536, episode_reward=2757.14 +/- 2831.56
Episode length: 660.60 +/- 107.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3601536  |
---------------------------------
Eval num_timesteps=3603528, episode_reward=1826.44 +/- 2180.22
Episode length: 675.00 +/- 150.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3603528  |
---------------------------------
Eval num_timesteps=3605520, episode_reward=1322.33 +/- 1351.91
Episode length: 648.20 +/- 96.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3605520  |
---------------------------------
Eval num_timesteps=3607512, episode_reward=2912.31 +/- 3120.66
Episode length: 852.80 +/- 145.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 3607512  |
---------------------------------
Eval num_timesteps=3609504, episode_reward=2199.22 +/- 2653.91
Episode length: 709.20 +/- 90.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3609504  |
---------------------------------
Eval num_timesteps=3611496, episode_reward=1874.49 +/- 2242.21
Episode length: 652.40 +/- 111.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3611496  |
---------------------------------
Eval num_timesteps=3613488, episode_reward=3735.77 +/- 3710.21
Episode length: 780.00 +/- 182.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 3.74e+03 |
| time/              |          |
|    total_timesteps | 3613488  |
---------------------------------
Eval num_timesteps=3615480, episode_reward=1698.54 +/- 2260.44
Episode length: 616.80 +/- 129.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3615480  |
---------------------------------
Eval num_timesteps=3617472, episode_reward=1781.02 +/- 2109.33
Episode length: 670.00 +/- 161.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3617472  |
---------------------------------
Eval num_timesteps=3619464, episode_reward=2133.85 +/- 1661.86
Episode length: 713.60 +/- 49.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3619464  |
---------------------------------
Eval num_timesteps=3621456, episode_reward=958.02 +/- 571.29
Episode length: 651.60 +/- 115.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 3621456  |
---------------------------------
Eval num_timesteps=3623448, episode_reward=3214.80 +/- 2526.84
Episode length: 795.40 +/- 180.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 3623448  |
---------------------------------
Eval num_timesteps=3625440, episode_reward=1980.10 +/- 2175.72
Episode length: 622.20 +/- 148.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3625440  |
---------------------------------
Eval num_timesteps=3627432, episode_reward=1900.72 +/- 2203.89
Episode length: 686.80 +/- 190.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3627432  |
---------------------------------
Eval num_timesteps=3629424, episode_reward=2882.33 +/- 2702.09
Episode length: 667.40 +/- 125.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 3629424  |
---------------------------------
Eval num_timesteps=3631416, episode_reward=1959.05 +/- 3177.26
Episode length: 558.60 +/- 105.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3631416  |
---------------------------------
Eval num_timesteps=3633408, episode_reward=2900.15 +/- 3014.47
Episode length: 650.80 +/- 137.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 3633408  |
---------------------------------
Eval num_timesteps=3635400, episode_reward=435.04 +/- 393.30
Episode length: 529.80 +/- 144.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 3635400  |
---------------------------------
Eval num_timesteps=3637392, episode_reward=1660.41 +/- 2089.57
Episode length: 616.20 +/- 123.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 616         |
|    mean_reward          | 1.66e+03    |
| time/                   |             |
|    total_timesteps      | 3637392     |
| train/                  |             |
|    approx_kl            | 0.002452979 |
|    clip_fraction        | 0.00392     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 1.15e+03    |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.000479   |
|    std                  | 1.88        |
|    value_loss           | 2.52e+03    |
-----------------------------------------
Eval num_timesteps=3639384, episode_reward=2263.72 +/- 2586.67
Episode length: 657.00 +/- 214.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3639384  |
---------------------------------
Eval num_timesteps=3641376, episode_reward=1840.73 +/- 2321.56
Episode length: 669.60 +/- 92.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3641376  |
---------------------------------
Eval num_timesteps=3643368, episode_reward=2513.98 +/- 2293.23
Episode length: 715.00 +/- 115.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3643368  |
---------------------------------
Eval num_timesteps=3645360, episode_reward=1486.87 +/- 1467.76
Episode length: 632.60 +/- 191.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 3645360  |
---------------------------------
Eval num_timesteps=3647352, episode_reward=532.53 +/- 240.38
Episode length: 570.00 +/- 98.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 3647352  |
---------------------------------
Eval num_timesteps=3649344, episode_reward=2369.19 +/- 2107.38
Episode length: 670.20 +/- 150.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 3649344  |
---------------------------------
Eval num_timesteps=3651336, episode_reward=1955.95 +/- 2073.42
Episode length: 633.40 +/- 167.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3651336  |
---------------------------------
Eval num_timesteps=3653328, episode_reward=1338.96 +/- 1837.17
Episode length: 683.40 +/- 211.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3653328  |
---------------------------------
Eval num_timesteps=3655320, episode_reward=2186.38 +/- 3070.00
Episode length: 558.60 +/- 137.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3655320  |
---------------------------------
Eval num_timesteps=3657312, episode_reward=1550.74 +/- 2308.46
Episode length: 662.80 +/- 127.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3657312  |
---------------------------------
Eval num_timesteps=3659304, episode_reward=1721.73 +/- 1807.17
Episode length: 686.20 +/- 136.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3659304  |
---------------------------------
Eval num_timesteps=3661296, episode_reward=653.22 +/- 212.72
Episode length: 622.40 +/- 92.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 3661296  |
---------------------------------
Eval num_timesteps=3663288, episode_reward=618.68 +/- 608.26
Episode length: 592.40 +/- 144.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 3663288  |
---------------------------------
Eval num_timesteps=3665280, episode_reward=488.75 +/- 319.37
Episode length: 528.00 +/- 101.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 528      |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 3665280  |
---------------------------------
Eval num_timesteps=3667272, episode_reward=2504.04 +/- 2931.06
Episode length: 753.40 +/- 105.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3667272  |
---------------------------------
Eval num_timesteps=3669264, episode_reward=1389.97 +/- 1617.05
Episode length: 611.40 +/- 113.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3669264  |
---------------------------------
Eval num_timesteps=3671256, episode_reward=1293.69 +/- 2436.88
Episode length: 464.80 +/- 180.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 465      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3671256  |
---------------------------------
Eval num_timesteps=3673248, episode_reward=1142.76 +/- 885.28
Episode length: 679.20 +/- 125.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3673248  |
---------------------------------
Eval num_timesteps=3675240, episode_reward=593.07 +/- 430.65
Episode length: 572.60 +/- 108.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 3675240  |
---------------------------------
Eval num_timesteps=3677232, episode_reward=605.54 +/- 291.58
Episode length: 582.00 +/- 101.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 3677232  |
---------------------------------
Eval num_timesteps=3679224, episode_reward=2533.45 +/- 2765.38
Episode length: 639.80 +/- 171.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3679224  |
---------------------------------
Eval num_timesteps=3681216, episode_reward=2020.91 +/- 2221.47
Episode length: 764.60 +/- 155.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3681216  |
---------------------------------
Eval num_timesteps=3683208, episode_reward=246.16 +/- 352.00
Episode length: 520.00 +/- 195.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 3683208  |
---------------------------------
Eval num_timesteps=3685200, episode_reward=1374.32 +/- 1626.87
Episode length: 590.40 +/- 173.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3685200  |
---------------------------------
Eval num_timesteps=3687192, episode_reward=907.24 +/- 536.41
Episode length: 660.40 +/- 87.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 660          |
|    mean_reward          | 907          |
| time/                   |              |
|    total_timesteps      | 3687192      |
| train/                  |              |
|    approx_kl            | 0.0034582934 |
|    clip_fraction        | 0.00927      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 727          |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.000907    |
|    std                  | 1.89         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=3689184, episode_reward=1977.92 +/- 2309.50
Episode length: 646.40 +/- 139.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3689184  |
---------------------------------
Eval num_timesteps=3691176, episode_reward=1043.33 +/- 147.42
Episode length: 649.40 +/- 93.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3691176  |
---------------------------------
Eval num_timesteps=3693168, episode_reward=772.45 +/- 181.35
Episode length: 722.60 +/- 79.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 3693168  |
---------------------------------
Eval num_timesteps=3695160, episode_reward=885.01 +/- 1130.73
Episode length: 742.80 +/- 103.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 3695160  |
---------------------------------
Eval num_timesteps=3697152, episode_reward=3041.66 +/- 2932.44
Episode length: 714.80 +/- 106.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 3697152  |
---------------------------------
Eval num_timesteps=3699144, episode_reward=3131.46 +/- 2660.51
Episode length: 708.40 +/- 31.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3699144  |
---------------------------------
Eval num_timesteps=3701136, episode_reward=1867.69 +/- 2031.02
Episode length: 747.80 +/- 104.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3701136  |
---------------------------------
Eval num_timesteps=3703128, episode_reward=5639.45 +/- 487.30
Episode length: 832.60 +/- 110.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 5.64e+03 |
| time/              |          |
|    total_timesteps | 3703128  |
---------------------------------
New best mean reward!
Eval num_timesteps=3705120, episode_reward=2428.28 +/- 3478.74
Episode length: 763.80 +/- 107.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3705120  |
---------------------------------
Eval num_timesteps=3707112, episode_reward=698.01 +/- 515.67
Episode length: 586.00 +/- 141.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 3707112  |
---------------------------------
Eval num_timesteps=3709104, episode_reward=2141.04 +/- 1788.40
Episode length: 710.60 +/- 55.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3709104  |
---------------------------------
Eval num_timesteps=3711096, episode_reward=1548.57 +/- 1640.75
Episode length: 587.60 +/- 172.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3711096  |
---------------------------------
Eval num_timesteps=3713088, episode_reward=1620.51 +/- 1921.43
Episode length: 751.00 +/- 126.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3713088  |
---------------------------------
Eval num_timesteps=3715080, episode_reward=2206.92 +/- 2133.85
Episode length: 744.80 +/- 66.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 3715080  |
---------------------------------
Eval num_timesteps=3717072, episode_reward=2621.52 +/- 2494.69
Episode length: 690.40 +/- 93.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 3717072  |
---------------------------------
Eval num_timesteps=3719064, episode_reward=847.66 +/- 592.51
Episode length: 775.60 +/- 226.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 3719064  |
---------------------------------
Eval num_timesteps=3721056, episode_reward=2792.31 +/- 2065.97
Episode length: 755.20 +/- 26.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3721056  |
---------------------------------
Eval num_timesteps=3723048, episode_reward=839.88 +/- 508.77
Episode length: 664.60 +/- 196.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 3723048  |
---------------------------------
Eval num_timesteps=3725040, episode_reward=2522.70 +/- 2852.44
Episode length: 695.40 +/- 119.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3725040  |
---------------------------------
Eval num_timesteps=3727032, episode_reward=673.93 +/- 447.42
Episode length: 585.80 +/- 114.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 3727032  |
---------------------------------
Eval num_timesteps=3729024, episode_reward=2603.05 +/- 2505.12
Episode length: 717.60 +/- 114.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 3729024  |
---------------------------------
Eval num_timesteps=3731016, episode_reward=58.66 +/- 231.11
Episode length: 510.20 +/- 149.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 58.7     |
| time/              |          |
|    total_timesteps | 3731016  |
---------------------------------
Eval num_timesteps=3733008, episode_reward=706.35 +/- 312.37
Episode length: 743.20 +/- 92.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 3733008  |
---------------------------------
Eval num_timesteps=3735000, episode_reward=1807.86 +/- 1905.73
Episode length: 688.40 +/- 94.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3735000  |
---------------------------------
Eval num_timesteps=3736992, episode_reward=1356.75 +/- 1897.72
Episode length: 711.80 +/- 171.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 712         |
|    mean_reward          | 1.36e+03    |
| time/                   |             |
|    total_timesteps      | 3736992     |
| train/                  |             |
|    approx_kl            | 0.002606372 |
|    clip_fraction        | 0.0069      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 473         |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.000517   |
|    std                  | 1.91        |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=3738984, episode_reward=2578.92 +/- 2183.63
Episode length: 711.40 +/- 157.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 3738984  |
---------------------------------
Eval num_timesteps=3740976, episode_reward=1682.94 +/- 1922.08
Episode length: 722.20 +/- 72.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3740976  |
---------------------------------
Eval num_timesteps=3742968, episode_reward=708.03 +/- 434.66
Episode length: 696.00 +/- 121.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 3742968  |
---------------------------------
Eval num_timesteps=3744960, episode_reward=3418.38 +/- 2533.99
Episode length: 651.60 +/- 132.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 3.42e+03 |
| time/              |          |
|    total_timesteps | 3744960  |
---------------------------------
Eval num_timesteps=3746952, episode_reward=456.09 +/- 355.03
Episode length: 619.60 +/- 146.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 3746952  |
---------------------------------
Eval num_timesteps=3748944, episode_reward=819.40 +/- 567.87
Episode length: 618.40 +/- 167.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 3748944  |
---------------------------------
Eval num_timesteps=3750936, episode_reward=1119.41 +/- 469.10
Episode length: 702.80 +/- 95.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3750936  |
---------------------------------
Eval num_timesteps=3752928, episode_reward=1592.96 +/- 2180.34
Episode length: 654.60 +/- 142.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3752928  |
---------------------------------
Eval num_timesteps=3754920, episode_reward=1633.32 +/- 2032.58
Episode length: 706.80 +/- 101.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3754920  |
---------------------------------
Eval num_timesteps=3756912, episode_reward=511.15 +/- 368.29
Episode length: 640.80 +/- 184.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 3756912  |
---------------------------------
Eval num_timesteps=3758904, episode_reward=2370.90 +/- 2461.10
Episode length: 720.80 +/- 85.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 3758904  |
---------------------------------
Eval num_timesteps=3760896, episode_reward=1773.12 +/- 2371.93
Episode length: 638.00 +/- 92.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3760896  |
---------------------------------
Eval num_timesteps=3762888, episode_reward=2541.90 +/- 2096.15
Episode length: 776.40 +/- 88.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 3762888  |
---------------------------------
Eval num_timesteps=3764880, episode_reward=988.41 +/- 1014.58
Episode length: 626.40 +/- 162.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 3764880  |
---------------------------------
Eval num_timesteps=3766872, episode_reward=2476.09 +/- 1675.20
Episode length: 683.40 +/- 88.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3766872  |
---------------------------------
Eval num_timesteps=3768864, episode_reward=3798.92 +/- 3257.93
Episode length: 596.00 +/- 109.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 3.8e+03  |
| time/              |          |
|    total_timesteps | 3768864  |
---------------------------------
Eval num_timesteps=3770856, episode_reward=927.86 +/- 815.49
Episode length: 609.80 +/- 95.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 3770856  |
---------------------------------
Eval num_timesteps=3772848, episode_reward=913.29 +/- 503.29
Episode length: 653.20 +/- 99.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 3772848  |
---------------------------------
Eval num_timesteps=3774840, episode_reward=1584.80 +/- 1761.54
Episode length: 614.60 +/- 155.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3774840  |
---------------------------------
Eval num_timesteps=3776832, episode_reward=671.73 +/- 408.41
Episode length: 671.60 +/- 145.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 3776832  |
---------------------------------
Eval num_timesteps=3778824, episode_reward=1796.42 +/- 1970.83
Episode length: 587.00 +/- 67.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3778824  |
---------------------------------
Eval num_timesteps=3780816, episode_reward=550.20 +/- 614.75
Episode length: 534.00 +/- 193.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 3780816  |
---------------------------------
Eval num_timesteps=3782808, episode_reward=2593.02 +/- 2311.74
Episode length: 644.00 +/- 89.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 3782808  |
---------------------------------
Eval num_timesteps=3784800, episode_reward=2384.81 +/- 2398.46
Episode length: 731.80 +/- 41.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 732          |
|    mean_reward          | 2.38e+03     |
| time/                   |              |
|    total_timesteps      | 3784800      |
| train/                  |              |
|    approx_kl            | 0.0023396167 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 523          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.000517    |
|    std                  | 1.93         |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=3786792, episode_reward=389.25 +/- 417.07
Episode length: 642.80 +/- 70.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 3786792  |
---------------------------------
Eval num_timesteps=3788784, episode_reward=1405.69 +/- 2287.18
Episode length: 562.80 +/- 122.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3788784  |
---------------------------------
Eval num_timesteps=3790776, episode_reward=1763.61 +/- 2236.15
Episode length: 683.20 +/- 102.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3790776  |
---------------------------------
Eval num_timesteps=3792768, episode_reward=1896.06 +/- 2036.41
Episode length: 714.00 +/- 62.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3792768  |
---------------------------------
Eval num_timesteps=3794760, episode_reward=1312.70 +/- 983.88
Episode length: 736.60 +/- 27.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3794760  |
---------------------------------
Eval num_timesteps=3796752, episode_reward=2549.73 +/- 2793.39
Episode length: 680.20 +/- 88.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3796752  |
---------------------------------
Eval num_timesteps=3798744, episode_reward=978.29 +/- 626.38
Episode length: 687.40 +/- 145.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 3798744  |
---------------------------------
Eval num_timesteps=3800736, episode_reward=2290.92 +/- 2295.86
Episode length: 601.20 +/- 120.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 3800736  |
---------------------------------
Eval num_timesteps=3802728, episode_reward=1878.13 +/- 2368.02
Episode length: 681.60 +/- 64.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3802728  |
---------------------------------
Eval num_timesteps=3804720, episode_reward=611.89 +/- 533.96
Episode length: 630.60 +/- 179.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 3804720  |
---------------------------------
Eval num_timesteps=3806712, episode_reward=776.35 +/- 666.48
Episode length: 662.80 +/- 122.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 3806712  |
---------------------------------
Eval num_timesteps=3808704, episode_reward=3894.19 +/- 2841.78
Episode length: 654.40 +/- 156.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 3.89e+03 |
| time/              |          |
|    total_timesteps | 3808704  |
---------------------------------
Eval num_timesteps=3810696, episode_reward=2759.07 +/- 2142.36
Episode length: 727.60 +/- 37.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3810696  |
---------------------------------
Eval num_timesteps=3812688, episode_reward=799.29 +/- 448.87
Episode length: 629.40 +/- 111.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 3812688  |
---------------------------------
Eval num_timesteps=3814680, episode_reward=1645.30 +/- 1571.61
Episode length: 636.00 +/- 61.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3814680  |
---------------------------------
Eval num_timesteps=3816672, episode_reward=1738.27 +/- 1906.84
Episode length: 657.80 +/- 87.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3816672  |
---------------------------------
Eval num_timesteps=3818664, episode_reward=1436.70 +/- 2305.93
Episode length: 576.60 +/- 170.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3818664  |
---------------------------------
Eval num_timesteps=3820656, episode_reward=669.05 +/- 468.75
Episode length: 632.80 +/- 152.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 3820656  |
---------------------------------
Eval num_timesteps=3822648, episode_reward=3032.69 +/- 2363.02
Episode length: 786.20 +/- 34.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 3822648  |
---------------------------------
Eval num_timesteps=3824640, episode_reward=596.92 +/- 360.66
Episode length: 665.80 +/- 49.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 3824640  |
---------------------------------
Eval num_timesteps=3826632, episode_reward=1518.28 +/- 2126.27
Episode length: 680.40 +/- 162.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3826632  |
---------------------------------
Eval num_timesteps=3828624, episode_reward=2834.27 +/- 1928.85
Episode length: 624.60 +/- 61.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 3828624  |
---------------------------------
Eval num_timesteps=3830616, episode_reward=1554.47 +/- 1848.83
Episode length: 544.40 +/- 187.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3830616  |
---------------------------------
Eval num_timesteps=3832608, episode_reward=2526.32 +/- 2082.73
Episode length: 727.80 +/- 54.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3832608  |
---------------------------------
Eval num_timesteps=3834600, episode_reward=2887.62 +/- 3188.18
Episode length: 667.60 +/- 92.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 668          |
|    mean_reward          | 2.89e+03     |
| time/                   |              |
|    total_timesteps      | 3834600      |
| train/                  |              |
|    approx_kl            | 0.0029420357 |
|    clip_fraction        | 0.00901      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.33        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000939    |
|    std                  | 1.95         |
|    value_loss           | 2.28e+03     |
------------------------------------------
Eval num_timesteps=3836592, episode_reward=548.71 +/- 554.85
Episode length: 628.20 +/- 131.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 3836592  |
---------------------------------
Eval num_timesteps=3838584, episode_reward=1360.31 +/- 1440.84
Episode length: 657.00 +/- 91.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3838584  |
---------------------------------
Eval num_timesteps=3840576, episode_reward=2338.52 +/- 2512.89
Episode length: 712.40 +/- 59.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3840576  |
---------------------------------
Eval num_timesteps=3842568, episode_reward=1322.65 +/- 2067.80
Episode length: 641.80 +/- 148.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3842568  |
---------------------------------
Eval num_timesteps=3844560, episode_reward=1798.04 +/- 1028.70
Episode length: 670.00 +/- 67.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3844560  |
---------------------------------
Eval num_timesteps=3846552, episode_reward=1431.21 +/- 1203.01
Episode length: 667.00 +/- 76.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3846552  |
---------------------------------
Eval num_timesteps=3848544, episode_reward=1973.40 +/- 2159.41
Episode length: 575.40 +/- 93.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3848544  |
---------------------------------
Eval num_timesteps=3850536, episode_reward=1465.78 +/- 1480.56
Episode length: 701.00 +/- 101.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3850536  |
---------------------------------
Eval num_timesteps=3852528, episode_reward=2552.47 +/- 3408.68
Episode length: 662.60 +/- 67.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3852528  |
---------------------------------
Eval num_timesteps=3854520, episode_reward=4222.60 +/- 3611.80
Episode length: 626.60 +/- 88.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 4.22e+03 |
| time/              |          |
|    total_timesteps | 3854520  |
---------------------------------
Eval num_timesteps=3856512, episode_reward=2904.58 +/- 2464.57
Episode length: 693.80 +/- 64.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 3856512  |
---------------------------------
Eval num_timesteps=3858504, episode_reward=2297.84 +/- 2113.81
Episode length: 619.80 +/- 78.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3858504  |
---------------------------------
Eval num_timesteps=3860496, episode_reward=3089.40 +/- 2462.96
Episode length: 651.80 +/- 68.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 3860496  |
---------------------------------
Eval num_timesteps=3862488, episode_reward=1256.02 +/- 817.34
Episode length: 697.20 +/- 105.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3862488  |
---------------------------------
Eval num_timesteps=3864480, episode_reward=1027.38 +/- 553.13
Episode length: 697.40 +/- 49.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3864480  |
---------------------------------
Eval num_timesteps=3866472, episode_reward=2915.21 +/- 2424.76
Episode length: 715.00 +/- 60.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 3866472  |
---------------------------------
Eval num_timesteps=3868464, episode_reward=1445.43 +/- 1751.57
Episode length: 675.60 +/- 61.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3868464  |
---------------------------------
Eval num_timesteps=3870456, episode_reward=1828.78 +/- 1657.31
Episode length: 750.60 +/- 68.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3870456  |
---------------------------------
Eval num_timesteps=3872448, episode_reward=1735.98 +/- 1953.90
Episode length: 696.40 +/- 64.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3872448  |
---------------------------------
Eval num_timesteps=3874440, episode_reward=1823.79 +/- 2449.65
Episode length: 714.20 +/- 63.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3874440  |
---------------------------------
Eval num_timesteps=3876432, episode_reward=1629.61 +/- 1900.43
Episode length: 729.00 +/- 115.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3876432  |
---------------------------------
Eval num_timesteps=3878424, episode_reward=3693.36 +/- 2786.45
Episode length: 753.00 +/- 43.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 3.69e+03 |
| time/              |          |
|    total_timesteps | 3878424  |
---------------------------------
Eval num_timesteps=3880416, episode_reward=403.20 +/- 324.33
Episode length: 569.20 +/- 100.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 3880416  |
---------------------------------
Eval num_timesteps=3882408, episode_reward=1395.55 +/- 1917.07
Episode length: 543.60 +/- 150.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3882408  |
---------------------------------
Eval num_timesteps=3884400, episode_reward=1967.02 +/- 1543.49
Episode length: 654.40 +/- 67.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 654          |
|    mean_reward          | 1.97e+03     |
| time/                   |              |
|    total_timesteps      | 3884400      |
| train/                  |              |
|    approx_kl            | 0.0020409394 |
|    clip_fraction        | 0.00408      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.37        |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+03     |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.000625    |
|    std                  | 1.97         |
|    value_loss           | 2.72e+03     |
------------------------------------------
Eval num_timesteps=3886392, episode_reward=2578.30 +/- 2332.81
Episode length: 692.40 +/- 138.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 3886392  |
---------------------------------
Eval num_timesteps=3888384, episode_reward=2507.40 +/- 2280.37
Episode length: 737.20 +/- 111.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3888384  |
---------------------------------
Eval num_timesteps=3890376, episode_reward=1210.27 +/- 428.78
Episode length: 748.60 +/- 101.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3890376  |
---------------------------------
Eval num_timesteps=3892368, episode_reward=203.49 +/- 153.14
Episode length: 480.40 +/- 90.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 3892368  |
---------------------------------
Eval num_timesteps=3894360, episode_reward=423.26 +/- 291.01
Episode length: 575.00 +/- 172.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 3894360  |
---------------------------------
Eval num_timesteps=3896352, episode_reward=1808.13 +/- 2636.21
Episode length: 646.60 +/- 117.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3896352  |
---------------------------------
Eval num_timesteps=3898344, episode_reward=5230.00 +/- 1890.09
Episode length: 754.80 +/- 105.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 5.23e+03 |
| time/              |          |
|    total_timesteps | 3898344  |
---------------------------------
Eval num_timesteps=3900336, episode_reward=1847.17 +/- 1942.96
Episode length: 684.60 +/- 142.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3900336  |
---------------------------------
Eval num_timesteps=3902328, episode_reward=1526.78 +/- 1483.31
Episode length: 665.40 +/- 104.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3902328  |
---------------------------------
Eval num_timesteps=3904320, episode_reward=768.70 +/- 533.09
Episode length: 603.40 +/- 145.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 3904320  |
---------------------------------
Eval num_timesteps=3906312, episode_reward=2699.96 +/- 2507.85
Episode length: 640.80 +/- 151.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3906312  |
---------------------------------
Eval num_timesteps=3908304, episode_reward=674.97 +/- 423.51
Episode length: 622.20 +/- 108.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 3908304  |
---------------------------------
Eval num_timesteps=3910296, episode_reward=3041.21 +/- 2027.49
Episode length: 770.80 +/- 61.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 3910296  |
---------------------------------
Eval num_timesteps=3912288, episode_reward=1947.30 +/- 2366.11
Episode length: 719.40 +/- 113.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3912288  |
---------------------------------
Eval num_timesteps=3914280, episode_reward=1384.91 +/- 1889.02
Episode length: 633.60 +/- 77.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3914280  |
---------------------------------
Eval num_timesteps=3916272, episode_reward=1531.77 +/- 1604.64
Episode length: 686.40 +/- 99.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3916272  |
---------------------------------
Eval num_timesteps=3918264, episode_reward=1987.20 +/- 2265.60
Episode length: 671.00 +/- 76.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3918264  |
---------------------------------
Eval num_timesteps=3920256, episode_reward=2278.22 +/- 2788.54
Episode length: 695.20 +/- 99.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 3920256  |
---------------------------------
Eval num_timesteps=3922248, episode_reward=2856.81 +/- 2514.02
Episode length: 696.80 +/- 69.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 3922248  |
---------------------------------
Eval num_timesteps=3924240, episode_reward=2134.28 +/- 1777.90
Episode length: 690.20 +/- 150.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3924240  |
---------------------------------
Eval num_timesteps=3926232, episode_reward=2794.95 +/- 2274.75
Episode length: 713.00 +/- 214.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3926232  |
---------------------------------
Eval num_timesteps=3928224, episode_reward=1022.99 +/- 522.97
Episode length: 615.40 +/- 29.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3928224  |
---------------------------------
Eval num_timesteps=3930216, episode_reward=1469.94 +/- 1007.01
Episode length: 746.20 +/- 64.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3930216  |
---------------------------------
Eval num_timesteps=3932208, episode_reward=1501.63 +/- 2226.57
Episode length: 555.00 +/- 171.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 555          |
|    mean_reward          | 1.5e+03      |
| time/                   |              |
|    total_timesteps      | 3932208      |
| train/                  |              |
|    approx_kl            | 0.0010802817 |
|    clip_fraction        | 0.000875     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.41        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 513          |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00032     |
|    std                  | 1.99         |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=3934200, episode_reward=1330.43 +/- 1660.15
Episode length: 608.00 +/- 88.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3934200  |
---------------------------------
Eval num_timesteps=3936192, episode_reward=1053.46 +/- 582.98
Episode length: 600.00 +/- 142.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3936192  |
---------------------------------
Eval num_timesteps=3938184, episode_reward=1429.16 +/- 1183.90
Episode length: 656.80 +/- 72.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3938184  |
---------------------------------
Eval num_timesteps=3940176, episode_reward=637.76 +/- 320.47
Episode length: 613.00 +/- 69.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 3940176  |
---------------------------------
Eval num_timesteps=3942168, episode_reward=863.16 +/- 425.70
Episode length: 658.60 +/- 34.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 3942168  |
---------------------------------
Eval num_timesteps=3944160, episode_reward=702.60 +/- 533.45
Episode length: 564.80 +/- 146.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 3944160  |
---------------------------------
Eval num_timesteps=3946152, episode_reward=2122.52 +/- 2051.02
Episode length: 699.00 +/- 167.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3946152  |
---------------------------------
Eval num_timesteps=3948144, episode_reward=4387.76 +/- 2482.38
Episode length: 668.40 +/- 121.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 4.39e+03 |
| time/              |          |
|    total_timesteps | 3948144  |
---------------------------------
Eval num_timesteps=3950136, episode_reward=1806.36 +/- 1633.93
Episode length: 648.40 +/- 46.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3950136  |
---------------------------------
Eval num_timesteps=3952128, episode_reward=2676.13 +/- 2312.44
Episode length: 706.80 +/- 59.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3952128  |
---------------------------------
Eval num_timesteps=3954120, episode_reward=1398.12 +/- 1352.91
Episode length: 518.20 +/- 204.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3954120  |
---------------------------------
Eval num_timesteps=3956112, episode_reward=1527.19 +/- 1657.25
Episode length: 642.00 +/- 102.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3956112  |
---------------------------------
Eval num_timesteps=3958104, episode_reward=2572.44 +/- 2182.88
Episode length: 704.00 +/- 86.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 3958104  |
---------------------------------
Eval num_timesteps=3960096, episode_reward=2710.79 +/- 2723.50
Episode length: 635.00 +/- 53.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3960096  |
---------------------------------
Eval num_timesteps=3962088, episode_reward=2840.13 +/- 2378.03
Episode length: 683.20 +/- 29.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 3962088  |
---------------------------------
Eval num_timesteps=3964080, episode_reward=500.41 +/- 275.17
Episode length: 605.20 +/- 36.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 3964080  |
---------------------------------
Eval num_timesteps=3966072, episode_reward=6066.58 +/- 858.46
Episode length: 759.00 +/- 55.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 6.07e+03 |
| time/              |          |
|    total_timesteps | 3966072  |
---------------------------------
New best mean reward!
Eval num_timesteps=3968064, episode_reward=2401.29 +/- 1786.44
Episode length: 742.80 +/- 105.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3968064  |
---------------------------------
Eval num_timesteps=3970056, episode_reward=790.47 +/- 442.20
Episode length: 653.20 +/- 124.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 3970056  |
---------------------------------
Eval num_timesteps=3972048, episode_reward=3703.79 +/- 2111.90
Episode length: 736.20 +/- 77.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 3.7e+03  |
| time/              |          |
|    total_timesteps | 3972048  |
---------------------------------
Eval num_timesteps=3974040, episode_reward=1661.85 +/- 1414.11
Episode length: 653.60 +/- 52.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3974040  |
---------------------------------
Eval num_timesteps=3976032, episode_reward=2651.03 +/- 2626.28
Episode length: 753.20 +/- 63.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3976032  |
---------------------------------
Eval num_timesteps=3978024, episode_reward=1533.51 +/- 1229.49
Episode length: 705.40 +/- 62.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3978024  |
---------------------------------
Eval num_timesteps=3980016, episode_reward=2683.54 +/- 1994.89
Episode length: 732.40 +/- 72.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3980016  |
---------------------------------
Eval num_timesteps=3982008, episode_reward=1861.87 +/- 2374.12
Episode length: 699.00 +/- 86.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 699          |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 3982008      |
| train/                  |              |
|    approx_kl            | 0.0021461823 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 452          |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000631    |
|    std                  | 2            |
|    value_loss           | 1.22e+03     |
------------------------------------------
Eval num_timesteps=3984000, episode_reward=2328.19 +/- 2428.82
Episode length: 705.80 +/- 135.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3984000  |
---------------------------------
Eval num_timesteps=3985992, episode_reward=2741.31 +/- 2358.16
Episode length: 678.20 +/- 142.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3985992  |
---------------------------------
Eval num_timesteps=3987984, episode_reward=1196.67 +/- 895.77
Episode length: 637.60 +/- 71.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3987984  |
---------------------------------
Eval num_timesteps=3989976, episode_reward=4271.85 +/- 2451.84
Episode length: 726.00 +/- 65.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 3989976  |
---------------------------------
Eval num_timesteps=3991968, episode_reward=703.00 +/- 707.11
Episode length: 713.20 +/- 170.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 3991968  |
---------------------------------
Eval num_timesteps=3993960, episode_reward=684.48 +/- 687.04
Episode length: 693.80 +/- 193.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 3993960  |
---------------------------------
Eval num_timesteps=3995952, episode_reward=4081.14 +/- 3413.13
Episode length: 680.80 +/- 194.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 4.08e+03 |
| time/              |          |
|    total_timesteps | 3995952  |
---------------------------------
Eval num_timesteps=3997944, episode_reward=2146.45 +/- 3373.42
Episode length: 645.60 +/- 137.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3997944  |
---------------------------------
Eval num_timesteps=3999936, episode_reward=1749.81 +/- 1086.05
Episode length: 699.60 +/- 64.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3999936  |
---------------------------------
Eval num_timesteps=4001928, episode_reward=1093.34 +/- 627.51
Episode length: 668.80 +/- 90.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4001928  |
---------------------------------
Eval num_timesteps=4003920, episode_reward=1665.24 +/- 1644.01
Episode length: 644.80 +/- 67.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4003920  |
---------------------------------
Eval num_timesteps=4005912, episode_reward=855.06 +/- 1171.22
Episode length: 619.40 +/- 127.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 4005912  |
---------------------------------
Eval num_timesteps=4007904, episode_reward=3201.76 +/- 2852.03
Episode length: 755.60 +/- 58.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4007904  |
---------------------------------
Eval num_timesteps=4009896, episode_reward=2585.81 +/- 2310.41
Episode length: 673.60 +/- 140.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 4009896  |
---------------------------------
Eval num_timesteps=4011888, episode_reward=407.97 +/- 446.95
Episode length: 545.20 +/- 179.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 4011888  |
---------------------------------
Eval num_timesteps=4013880, episode_reward=1164.43 +/- 835.30
Episode length: 665.60 +/- 134.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4013880  |
---------------------------------
Eval num_timesteps=4015872, episode_reward=1844.67 +/- 2996.99
Episode length: 631.60 +/- 167.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 4015872  |
---------------------------------
Eval num_timesteps=4017864, episode_reward=3099.60 +/- 3010.39
Episode length: 771.00 +/- 137.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4017864  |
---------------------------------
Eval num_timesteps=4019856, episode_reward=1824.98 +/- 2026.54
Episode length: 791.40 +/- 53.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4019856  |
---------------------------------
Eval num_timesteps=4021848, episode_reward=2431.56 +/- 2375.53
Episode length: 624.40 +/- 176.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 4021848  |
---------------------------------
Eval num_timesteps=4023840, episode_reward=1457.13 +/- 2169.74
Episode length: 644.00 +/- 140.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4023840  |
---------------------------------
Eval num_timesteps=4025832, episode_reward=1166.37 +/- 664.42
Episode length: 701.80 +/- 137.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4025832  |
---------------------------------
Eval num_timesteps=4027824, episode_reward=1113.96 +/- 379.97
Episode length: 822.60 +/- 96.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4027824  |
---------------------------------
Eval num_timesteps=4029816, episode_reward=856.00 +/- 393.94
Episode length: 728.00 +/- 74.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 4029816  |
---------------------------------
Eval num_timesteps=4031808, episode_reward=1180.93 +/- 798.58
Episode length: 613.80 +/- 129.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 614         |
|    mean_reward          | 1.18e+03    |
| time/                   |             |
|    total_timesteps      | 4031808     |
| train/                  |             |
|    approx_kl            | 0.001382635 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.48       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 845         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.000439   |
|    std                  | 2.03        |
|    value_loss           | 2.28e+03    |
-----------------------------------------
Eval num_timesteps=4033800, episode_reward=623.89 +/- 483.36
Episode length: 612.60 +/- 114.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 4033800  |
---------------------------------
Eval num_timesteps=4035792, episode_reward=3799.38 +/- 2185.81
Episode length: 733.40 +/- 98.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 3.8e+03  |
| time/              |          |
|    total_timesteps | 4035792  |
---------------------------------
Eval num_timesteps=4037784, episode_reward=575.29 +/- 465.29
Episode length: 737.00 +/- 211.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 4037784  |
---------------------------------
Eval num_timesteps=4039776, episode_reward=3307.19 +/- 2323.07
Episode length: 698.60 +/- 115.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4039776  |
---------------------------------
Eval num_timesteps=4041768, episode_reward=1508.97 +/- 1856.32
Episode length: 765.00 +/- 62.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4041768  |
---------------------------------
Eval num_timesteps=4043760, episode_reward=1624.89 +/- 1718.35
Episode length: 622.00 +/- 121.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4043760  |
---------------------------------
Eval num_timesteps=4045752, episode_reward=1967.87 +/- 2786.44
Episode length: 674.80 +/- 152.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 4045752  |
---------------------------------
Eval num_timesteps=4047744, episode_reward=1724.70 +/- 1997.86
Episode length: 652.20 +/- 168.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4047744  |
---------------------------------
Eval num_timesteps=4049736, episode_reward=1822.67 +/- 2084.60
Episode length: 715.60 +/- 70.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4049736  |
---------------------------------
Eval num_timesteps=4051728, episode_reward=3945.30 +/- 2117.65
Episode length: 704.40 +/- 126.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 3.95e+03 |
| time/              |          |
|    total_timesteps | 4051728  |
---------------------------------
Eval num_timesteps=4053720, episode_reward=2022.83 +/- 1788.31
Episode length: 678.40 +/- 169.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 4053720  |
---------------------------------
Eval num_timesteps=4055712, episode_reward=2753.45 +/- 3756.78
Episode length: 690.00 +/- 138.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4055712  |
---------------------------------
Eval num_timesteps=4057704, episode_reward=1895.49 +/- 1821.32
Episode length: 667.20 +/- 125.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 4057704  |
---------------------------------
Eval num_timesteps=4059696, episode_reward=2588.97 +/- 2119.28
Episode length: 663.60 +/- 104.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 4059696  |
---------------------------------
Eval num_timesteps=4061688, episode_reward=912.31 +/- 549.47
Episode length: 712.60 +/- 79.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 4061688  |
---------------------------------
Eval num_timesteps=4063680, episode_reward=3078.46 +/- 2322.50
Episode length: 696.60 +/- 85.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4063680  |
---------------------------------
Eval num_timesteps=4065672, episode_reward=3485.74 +/- 2168.74
Episode length: 760.40 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 3.49e+03 |
| time/              |          |
|    total_timesteps | 4065672  |
---------------------------------
Eval num_timesteps=4067664, episode_reward=1392.81 +/- 2040.63
Episode length: 670.00 +/- 68.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4067664  |
---------------------------------
Eval num_timesteps=4069656, episode_reward=828.91 +/- 619.02
Episode length: 670.80 +/- 109.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 4069656  |
---------------------------------
Eval num_timesteps=4071648, episode_reward=1613.77 +/- 1889.53
Episode length: 638.00 +/- 258.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4071648  |
---------------------------------
Eval num_timesteps=4073640, episode_reward=452.92 +/- 459.07
Episode length: 598.40 +/- 160.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 4073640  |
---------------------------------
Eval num_timesteps=4075632, episode_reward=487.12 +/- 618.87
Episode length: 687.00 +/- 188.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 4075632  |
---------------------------------
Eval num_timesteps=4077624, episode_reward=1865.74 +/- 2375.31
Episode length: 753.40 +/- 139.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 4077624  |
---------------------------------
Eval num_timesteps=4079616, episode_reward=1282.06 +/- 1582.84
Episode length: 585.00 +/- 192.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4079616  |
---------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 675, in <module>
    if args.wandb:
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 408, in run_full
    # callback_on_new_best=callback_on_best,
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 223, in collect_rollouts
    rollout_buffer.add(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\buffers.py", line 469, in add
    self.observations[self.pos] = np.array(obs)
FloatingPointError: underflow encountered in cast
Eval num_timesteps=4081608, episode_reward=1906.93 +/- 2301.95
Episode length: 735.40 +/- 117.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 735          |
|    mean_reward          | 1.91e+03     |
| time/                   |              |
|    total_timesteps      | 4081608      |
| train/                  |              |
|    approx_kl            | 0.0021976843 |
|    clip_fraction        | 0.00304      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 972          |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000527    |
|    std                  | 2.05         |
|    value_loss           | 1.95e+03     |
------------------------------------------