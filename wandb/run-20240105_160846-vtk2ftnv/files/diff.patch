diff --git a/Sol/Model/PBDroneEnv.py b/Sol/Model/PBDroneEnv.py
index a249810..f3578d0 100644
--- a/Sol/Model/PBDroneEnv.py
+++ b/Sol/Model/PBDroneEnv.py
@@ -307,25 +307,28 @@ class PBDroneEnv(
 
         try:
             # reward -= distance_to_target ** 2
-            # Reward based on distance to target
-
-            # reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
-            # reward += np.exp(-distance_to_target * 5) * 50
-            # Additional reward for progressing towards the target
-            # reward += (self._prev_distance_to_target - distance_to_target) * 30
-            # self.reward += max(3.0 * self.waypoints.progress_to_target(), 0.0)
-
-            # Add a negative reward for spinning too fast
-            # reward += -np.linalg.norm(self.ang_v) / 3
-
-            # # Penalize large actions to avoid erratic behavior
-            # reward -= 0.01 * np.linalg.norm(self._last_action)
 
             if self._current_target_index > 0:
                 # Additional reward for progressing towards the target
                 reward += self.calculate_progress_reward(self._current_position, self._last_position,
                                                          self._target_points[self._current_target_index - 1],
-                                                         self._target_points[self._current_target_index]) * 1000
+                                                         self._target_points[self._current_target_index]) * 2000
+            else:
+
+                # Reward based on distance to target
+
+                reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
+                reward += np.exp(-distance_to_target * 5) * 50
+                # Additional reward for progressing towards the target
+                reward += (self._prev_distance_to_target - distance_to_target) * 30
+
+                # self.reward += max(3.0 * self.waypoints.progress_to_target(), 0.0)
+
+                # Add a negative reward for spinning too fast
+                reward += -np.linalg.norm(self.ang_v) / 3
+
+                # Penalize large actions to avoid erratic behavior
+                reward -= 0.01 * np.linalg.norm(self._last_action)
 
         except ZeroDivisionError:
             # Give a high reward if the drone is at the target (avoiding division by zero)
@@ -342,7 +345,7 @@ class PBDroneEnv(
                 self._is_done = True
             else:
                 # Reward for reaching a target
-                reward += 500 * (self._discount ** (self._steps / 10))
+                reward += 1000 * (self._discount ** (self._steps / 10))
 
                 if self.GUI:
                     self.remove_target()
diff --git a/Sol/Model/SBActorCritic.py b/Sol/Model/SBActorCritic.py
index 5ec7ae6..2d7c789 100644
--- a/Sol/Model/SBActorCritic.py
+++ b/Sol/Model/SBActorCritic.py
@@ -1,5 +1,6 @@
 from typing import Callable, Dict, List, Optional, Tuple, Type, Union
 
+import gym
 import torch
 from gymnasium import spaces
 import torch as th
@@ -18,16 +19,18 @@ from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
 
 class CustomFeatureExtractor(BaseFeaturesExtractor):
     """
-    Custom network for policy and value function.
+    Custom feature extractor for the actor-critic policy.
     It receives as input the features extracted by the features extractor.
-
-    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)
-    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network
-    :param last_layer_dim_vf: (int) number of units for the last layer of the value network
     """
 
-    def __init__(self, observation_space, net_arch, activation_fn=nn.ReLU):
-        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=net_arch[-1]['shared'])
+    def __init__(self,
+                 observation_space,
+                 net_arch,
+                 activation_fn: Type[nn.Module] = nn.Tanh,
+                 device: Union[th.device, str] = 'auto',
+                 ):
+        super(CustomFeatureExtractor, self).__init__(observation_space,
+                                                     features_dim=net_arch)
 
         # Assuming observation space is a Box space
         self.flatten = nn.Flatten()
@@ -36,50 +39,131 @@ class CustomFeatureExtractor(BaseFeaturesExtractor):
         self.shared_layers = nn.Sequential(
             nn.Linear(int(observation_space.shape[0]), net_arch[0]),
             activation_fn(),
-            nn.Linear(net_arch[0], net_arch[1]),
-            activation_fn()
         )
 
+        for i in range(1, len(net_arch) - 1):
+            if isinstance(net_arch[i], int):
+                self.shared_layers.add_module(
+                    "shared_fc{}".format(i),
+                    nn.Linear(net_arch[i], net_arch[i + 1]),
+                        )
+                self.shared_layers.add_module(
+                    "shared_act{}".format(i),
+                    activation_fn(),
+                    )
+
     def forward(self, x):
         x = self.flatten(x)
         x = self.shared_layers(x)
         return x
 
 
+class CustomNetwork(nn.Module):
+    """
+    Custom network for policy and value function.
+    It receives as input the features extracted by the feature extractor.
+
+    :param feature_dim: dimension of the features extracted with the features_extractor (e.g., features from a CNN)
+    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network
+    :param last_layer_dim_vf: (int) number of units for the last layer of the value network
+    """
+
+    def __init__(
+            self,
+            feature_dim: int,
+            last_layer_dim_pi: int = 64,
+            last_layer_dim_vf: int = 64,
+    ):
+        super(CustomNetwork, self).__init__()
+
+        # IMPORTANT:
+        # Save output dimensions, used to create the distributions
+        self.latent_dim_pi = last_layer_dim_pi
+        self.latent_dim_vf = last_layer_dim_vf
+
+        # Policy network
+        self.policy_net = nn.Sequential(
+            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()
+        )
+        # Value network
+        self.value_net = nn.Sequential(
+            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()
+        )
+
+    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:
+        """
+        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.
+            If all layers are shared, then ``latent_policy == latent_value``
+        """
+        return self.policy_net(features), self.value_net(features)
+
+
 class CustomActorCriticPolicy(ActorCriticPolicy):
-    def __init__(self, observation_space,
-                 action_space,
-                 net_arch,
+    def __init__(self, observation_space: gym.spaces.Space,
+                 action_space: gym.spaces.Space,
                  lr_schedule: Callable[[float], float],
-                 activation_fn=nn.ReLU,
+                 net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,
+                 activation_fn: Type[nn.Module] = nn.Tanh,
                  *args,
                  **kwargs):
 
-        super(CustomActorCriticPolicy, self).__init__(observation_space, action_space,
-                                                      lr_schedule,
-                                                      activation_fn=activation_fn,
-                                                      *args, **kwargs)
+        super(CustomActorCriticPolicy, self).__init__(
+            observation_space,
+            action_space,
+            lr_schedule,
+            net_arch,
+            activation_fn,
+            *args, **kwargs)
+
+        if net_arch is None:
+            net_arch = dict(pi=[64, 64], vf=[64, 64])
 
         # Disable orthogonal initialization
         kwargs["ortho_init"] = False
 
-
         # Create the custom feature extractor
         self.features_extractor = CustomFeatureExtractor(observation_space, net_arch, activation_fn)
 
-        # Actor head
-        self.actor = nn.Sequential(
-            nn.Linear(net_arch[1], net_arch[2]['pi'][0]),
-            activation_fn(),
-            nn.Linear(net_arch[2]['pi'][0], net_arch[2]['pi'][1])
-        )
+        latent_dim_pi = self.mlp_extractor.latent_dim_pi
+        latent_dim_vf = self.mlp_extractor.latent_dim_vf
+        pi_layers_dims = net_arch.get("pi", [])  # Layer sizes of the policy network
+        vf_layers_dims = net_arch.get("vf", [])  # Layer sizes of the value network
+        last_shared_layer_dim = net_arch["shared"][-1]
 
+        # Actor head
+        self.action_net = nn.Sequential(
+            nn.Linear(last_shared_layer_dim, pi_layers_dims[0]),
+            activation_fn(),)
+        for i in range(1, len(pi_layers_dims) - 2):
+            self.action_net.add_module(
+                "pi_fc{}".format(i),
+                nn.Linear(pi_layers_dims[i], pi_layers_dims[i + 1]),
+                    )
+            self.action_net.add_module(
+                "pi_act{}".format(i),
+                activation_fn(),
+                )
+        self.action_net.add_module(
+            "pi_fc_out",
+            nn.Linear(pi_layers_dims[-1], self.action_space.shape[0]))  # Last layer matches continuous action space dimensions
+        print(self.action_space.shape[0])
         # Critic head
-        self.critic = nn.Sequential(
-            nn.Linear(net_arch[1], net_arch[2]['vf'][0]),
+        self.value_net = nn.Sequential(
+            nn.Linear(last_shared_layer_dim, vf_layers_dims[0]),
             activation_fn(),
-            nn.Linear(net_arch[2]['vf'][0], net_arch[2]['vf'][1])
         )
+        for i in range(1, len(vf_layers_dims) - 2):
+            self.value_net.add_module(
+                "vf_fc{}".format(i),
+                nn.Linear(vf_layers_dims[i], vf_layers_dims[i + 1]),
+                    )
+            self.value_net.add_module(
+                "vf_act{}".format(i),
+                activation_fn(),
+                )
+        self.value_net.add_module(
+            "vf_fc_out",
+            nn.Linear(vf_layers_dims[-1], 1))
 
         self._initialize()
 
@@ -96,4 +180,6 @@ class CustomActorCriticPolicy(ActorCriticPolicy):
         return action, value
 
     def _build_mlp_extractor(self) -> None:
-        self.mlp_extractor = CustomActorCriticPolicy(self.features_dim)
\ No newline at end of file
+        self.mlp_extractor = CustomFeatureExtractor(
+            self.observation_space, self.net_arch, self.activation_fn,
+            self.device)
diff --git a/Sol/Model/SoftActorCritic.py b/Sol/Model/SoftActorCritic.py
index 68938ba..061833a 100644
--- a/Sol/Model/SoftActorCritic.py
+++ b/Sol/Model/SoftActorCritic.py
@@ -8,6 +8,7 @@ from tf_agents.environments import suite_pybullet
 from Sol.Model.PBDroneEnv import PBDroneEnv
 from Sol.PyBullet.enums import Physics
 
+
 class SoftAcorCritic():
 
     def __init__(self):
@@ -16,9 +17,7 @@ class SoftAcorCritic():
     def set_up_training(self, env):
         tempdir = tempfile.gettempdir()
 
-        # Use "num_iterations = 1e6" for better results (2 hrs)
-        # 1e5 is just so this doesn't take too long (1 hr)
-        num_iterations = 100000  # @param {type:"integer"}
+        num_iterations = 1e6  # @param {type:"integer"}
 
         initial_collect_steps = 10000  # @param {type:"integer"}
         collect_steps_per_iteration = 1  # @param {type:"integer"}
@@ -56,15 +55,13 @@ class SoftAcorCritic():
 
 
 if __name__ == '__main__':
-    drone_environment = PBDroneEnv(race_track=None,
-                                   target_points=[np.array([0.0, 0.0, 1.0]),
+    drone_environment = PBDroneEnv(target_points=[np.array([0.0, 0.0, 1.0]),
                                                   np.array([8.59735, -3.3286, -6.07256]),
                                                   np.array([1.5974, -5.0786, -4.32256]),
                                                   np.array([3.2474, 3.32137, -2.5725]),
                                                   np.array([1.3474, 1.6714, -2.07256]), ],
                                    threshold=1,
                                    discount=1,
-                                   drone=None,
                                    physics=Physics.PYB,
                                    gui=True,
                                    initial_xyzs=np.array([[0, 0, 0]])
diff --git a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc
index 75c6ef1..4247b22 100644
Binary files a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc and b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc differ
diff --git a/Sol/Model/pybullet_drone_simulator.py b/Sol/Model/pybullet_drone_simulator.py
index bbe4ae1..22b68c8 100644
--- a/Sol/Model/pybullet_drone_simulator.py
+++ b/Sol/Model/pybullet_drone_simulator.py
@@ -7,6 +7,12 @@ import argparse
 from distutils.util import strtobool
 # import sync, str2bool
 
+import sys
+
+sys.path.append("../")
+sys.path.append("./")
+
+
 from typing import Callable
 
 import gym.wrappers
@@ -42,6 +48,7 @@ import Sol.Model.Waypoints as Waypoints
 
 from Sol.Utilities.Plotter import plot_learning_curve, plot_metrics, plot_3d_targets
 import Sol.Utilities.Callbacks as Callbacks
+from Sol.Model.SBActorCritic import CustomActorCriticPolicy
 
 # from tf_agents.environments import py_environment
 
@@ -107,7 +114,7 @@ class PBDroneSimulator:
                 aviary_dim=aviary_dim,
             )
             env.reset(seed=seed + rank)
-            env = Monitor(env) # record stats such as returns
+            env = Monitor(env)  # record stats such as returns
             return env
 
         if multi:
@@ -142,7 +149,7 @@ class PBDroneSimulator:
         # action = np.array([-.9, -.9, -.9, -.9], dtype=np.float32)
         # action = np.array([.9, .9, .9, .9], dtype=np.float32)
         action = np.array([-1, -1, -1, -1], dtype=np.float32)
-        action *= -1/10
+        action *= -1 / 10
         # action = np.array([0, 0, 0, 0], dtype=np.float32)
 
         plot_3d_targets(self.targets)
@@ -187,9 +194,9 @@ class PBDroneSimulator:
     def test_saved(self):
         drone_environment = self.make_env(gui=True, aviary_dim=np.array([-2, -2, 0, 2, 2, 2]))
 
-        model = SAC.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.29.2023_11.55.59/best_model.zip")
-        # model = PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\sa/best_model.zip",
-        #                  env=drone_environment)
+        # model = SAC.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\/best_model.zip")
+        model = PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-01.03.2024_21.46.55/best_model.zip",
+                         env=drone_environment)
         # model = PPO.load(os.curdir + "\model_chkpts\success_model.zip")
         # model = SAC.load(os.curdir + "\model_chkpts\success_model.zip")
 
@@ -210,7 +217,7 @@ class PBDroneSimulator:
 
         for i in range(self.max_steps):
             action, _states = model.predict(obs,
-                                            deterministic=True
+                                            deterministic=False
                                             )
             obs, reward, terminated, truncated, info = drone_environment.step(action)
             print(i)
@@ -235,6 +242,43 @@ class PBDroneSimulator:
 
             time.sleep(1. / 240.)
 
+    def test_learning(self):
+
+        train_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i,
+                                                 aviary_dim=np.array([-2, -2, 0, 2, 2, 2])) for i in
+                                   range(1)])
+
+        # custom_policy = CustomActorCriticPolicy(train_env.observation_space, train_env.action_space,
+        #                                         net_arch=[512, 512, dict(vf=[256, 128],
+        #                                                                  pi=[256, 128])],
+        #                                         lr_schedule=linear_schedule(1e-3),
+        #                                         activation_fn=th.nn.Tanh)
+
+        onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
+
+        model = PPO("MlpPolicy",
+                    train_env,
+                    verbose=1,
+                    n_steps=2048,
+                    batch_size=49,
+                    device="auto",
+                    policy_kwargs=onpolicy_kwargs
+                    # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+                    )
+        print(model.policy)
+
+        model.learn(total_timesteps=int(5e2), )
+
+        print("#############################################")
+
+        # model = PPO(custom_policy,
+        #             train_env,
+        #             verbose=1,
+        #             n_steps=2048,
+        #             batch_size=49
+        #             # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+        #             )
+
     def run_full(self, args):
         start = time.perf_counter()
 
@@ -248,95 +292,106 @@ class PBDroneSimulator:
         train_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i,
                                                  aviary_dim=np.array([-2, -2, 0, 2, 2, 2])) for i in
                                    range(self.num_cpu)])
-        # train_env = VecCheckNan(train_env)
-        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
-                                 clip_obs=1)
 
         # eval_env = make_env(multi=False, gui=False, rank=0)
-        #
+
         eval_env = SubprocVecEnv([self.make_env(multi=True, save_model=True, save_path=filename, gui=True,
                                                 aviary_dim=np.array([-2, -2, 0, 2, 2, 2])), ])
         # eval_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i) for i in range(self.num_cpu)])
-        # eval_env = VecCheckNan(eval_env)
-        eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
-                                clip_obs=1)
 
-        # onpolicy_kwargs = dict(activation_fn=th.nn.ReLU,
-        #                        net_arch=dict(vf=[512, 512, 256, 128],
-        #                                      pi=[512, 512, 256, 128]))
-        # onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
+        if args.vec_check_nan:
+            train_env = VecCheckNan(train_env)
+            eval_env = VecCheckNan(eval_env)
 
-        model = PPO("MlpPolicy",
-                    train_env,
-                    verbose=1,
-                    n_steps=2048,
-                    batch_size=49152 // self.num_cpu,
-                    ent_coef=0.01,
-                    # use_sde=True,
-                    # sde_sample_freq=4,
-                    clip_range=0.2,
-                    learning_rate=1e-3,
-                    tensorboard_log="./logs/ppo_tensorboard/",
-                    device="auto",
-                    policy_kwargs=  # onpolicy_kwargs
-                    dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
-                    )
+        if args.vec_normalize:
+            train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
+                                     clip_obs=1)
+            eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
+                                    clip_obs=1)
 
-        # tensorboard --logdir ./logs/ppo_tensorboard/
+        # On-policy algorithms #################################
 
-        #### Off-policy algorithms #################################
-        #     offpolicy_kwargs = dict(activation_fn=torch.nn.ReLU,
-        #                             net_arch=[512, 512, 256, 128]
+        onpolicy_kwargs = dict(activation_fn=th.nn.Tanh,
+                               share_features_extractor=True,
+                               net_arch=dict(vf=[512, 512, 256, 128],
+                                             pi=[512, 512, 256, 128]))
+        # onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
+
+        custom_policy = dict(net_arch=[dict(share=[512, 512], vf=[256, 128], pi=[256, 128])],
+                             activation_fn=th.nn.Tanh)
+
+        # Off-policy algorithms #################################
+        offpolicy_kwargs = dict(activation_fn=th.nn.ReLU,
+                                net_arch=[512, 512, 256, 128])
 
         #     offpolicy_kwargs = dict(activation_fn=torch.nn.ReLU,
         #                             dict(net_arch=dict(qf=[256, 128, 64, 32], pi=[256, 128, 64, 32]))
 
-        # model = SAC(
-        #     "MultiInputPolicy",
-        #     train_env,
-        #     replay_buffer_class=HerReplayBuffer,
-        #     replay_buffer_kwargs=dict(
-        #         n_sampled_goal=len(self.targets),
-        #         goal_selection_strategy="future",
-        #     ),
-        #     verbose=0,
-        #     tensorboard_log="./logs/SAC_tensorboard/",
-        #     train_freq=1,
-        #     gradient_steps=2,
-        #     buffer_size=int(3e6),
-        #     learning_rate=1e-3,
-        #     # gamma=0.95,
-        #     batch_size=2048,
-        #     policy_kwargs=dict(net_arch=[256, 256, 256]),
-        #     device="auto",
-        # )
-        # train_env = make_vec_env(make_env(multi=False), n_envs=12)
-
-        # model = DDPG("MlpPolicy",
-        #              train_env,
-        #              verbose=1,
-        #              batch_size=1024,
-        #              learning_rate=1e-3,
-        #              train_freq=(10, "step"),
-        #              tensorboard_log="./logs/ddpg_tensorboard/",
-        #              device="auto",
-        #              policy_kwargs=dict(net_arch=[64, 64])
-        #              )
+        if args.agent == 'PPO':
+            model = PPO(ActorCriticPolicy,
+                        train_env,
+                        verbose=1,
+                        n_steps=2048,
+                        batch_size=49152 // self.num_cpu,
+                        ent_coef=0.01,
+                        # use_sde=True,
+                        # sde_sample_freq=4,
+                        clip_range=0.2,
+                        learning_rate=1e-3,
+                        tensorboard_log="./logs/ppo_tensorboard/",
+                        device="auto",
+                        policy_kwargs=onpolicy_kwargs
+                        # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+                        )
+
+        # tensorboard --logdir ./logs/ppo_tensorboard/
+
+        elif args.agent == 'SAC':
+
+            model = SAC(
+                "MlpPolicy",
+                train_env,
+                # replay_buffer_class=HerReplayBuffer,
+                # replay_buffer_kwargs=dict(
+                #     n_sampled_goal=len(self.targets),
+                #     goal_selection_strategy="future",
+                # ),
+                verbose=0,
+                tensorboard_log="./logs/SAC_tensorboard/",
+                train_freq=1,
+                gradient_steps=2,
+                buffer_size=int(4e6),
+                learning_rate=1e-3,
+                # gamma=0.95,
+                batch_size=49152 // self.num_cpu,
+                policy_kwargs=offpolicy_kwargs,  # dict(net_arch=[256, 256, 256]),
+                device="auto",
+            )
+            # train_env = make_vec_env(make_env(multi=False), n_envs=12)
+
+            # model = DDPG("MlpPolicy",
+            #              train_env,
+            #              verbose=1,
+            #              batch_size=1024,
+            #              learning_rate=1e-3,
+            #              train_freq=(10, "step"),
+            #              tensorboard_log="./logs/ddpg_tensorboard/",
+            #              device="auto",
+            #              policy_kwargs=dict(net_arch=[64, 64])
+            #              )
+
         model.set_random_seed(1)
 
         # vec_env = make_vec_env([make_env(gui=False, rank=i) for i in range(num_cpu)], n_envs=4, seed=0)
         # model = SAC("MlpPolicy", vec_env, train_freq=1, gradient_steps=2, verbose=1)
 
-        # train_env = stable_baselines3.common.monitor.Monitor(train_env)
-        # eval_env = stable_baselines3.common.monitor.Monitor(eval_env)
+        callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=100_000, verbose=1)
 
-        callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=100_000,
-                                                         verbose=1)
         stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)
 
         found_tar_callback = Callbacks.FoundTargetsCallback(log_dir=filename + '/')
 
-        wandb_callback = WandbCallback(gradient_save_freq=100, model_save_path=filename + '/' + "wand", verbose=2,)
+        wandb_callback = WandbCallback(gradient_save_freq=100, model_save_path=filename + '/' + "wand", verbose=2, )
 
         eval_callback = EvalCallback(eval_env,
                                      # callback_on_new_best=callback_on_best,
@@ -348,13 +403,14 @@ class PBDroneSimulator:
                                      render=False
                                      )
 
-        model.learn(total_timesteps=int(5e6),
+        model.learn(total_timesteps=int(args.max_steps),
                     callback=[eval_callback,
                               found_tar_callback,
                               wandb_callback
                               # AimCallback(repo='.Aim/', experiment_name='sb3_test')
                               ],
                     log_interval=1000,
+                    tb_log_name=args.agent + " " + datetime.now().strftime("%m.%d.%Y_%H.%M.%S"),
                     )
 
         model.save(os.curdir + filename + '/success_model.zip')
@@ -362,6 +418,8 @@ class PBDroneSimulator:
         stats_path = os.path.join(filename, "vec_normalize.pkl")
         eval_env.save(stats_path)
 
+        wandb.finish()
+
         # Test the model #########################################
 
         test_env = self.make_env(multi=False)
@@ -441,35 +499,39 @@ def parse_args():
     parser.add_argument('--env-kwargs', type=str, default='{}')
     parser.add_argument('--log-dir', type=str, default='logs')
     parser.add_argument('--seed', '-s', type=int, default=1)
-    parser.add_argument('--cuda', action='store_true', default=False)
-    parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI (default: True)',
-                        metavar='')
+    parser.add_argument('--cuda', action='store_true', default=True)
+    parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI for the eval env'
+                                                           '(default: True)')
+
+    parser.add_argument("--save-run", action="store_true", default=True,)
     parser.add_argument('--save-buffer', action='store_true', default=False)
     parser.add_argument('--save-model', action='store_true', default=True)
-    parser.add_argument('--save-obs', action='store_true', default=False)
-    parser.add_argument('--save-video', action='store_true', default=False)
     parser.add_argument('--save-dir', type=str, default='')
     parser.add_argument('--checkpoint-freq', type=int, default=100)
-    parser.add_argument('--checkpoint-at-end', action='store_true', default=False)
+
     parser.add_argument('--restore-agent', action='store_true', default=False)
 
+    # Wrapper specific arguments
+    parser.add_argument('--vec_check_nan', default=False, type=lambda x: bool(strtobool(x)))
+    parser.add_argument('--vec_normalize', default=False, type=lambda x: bool(strtobool(x)))
+    parser.add_argument('--ve_check_env', default=False, type=lambda x: bool(strtobool(x)))
 
     parser.add_argument('--num-cpus', type=int, default=1)
-    parser.add_argument('--num-workers', type=int, default=0)
-    parser.add_argument('--num_envs', type=int, default=1)
+
     parser.add_argument('--max_steps', type=int, default=5e6)
+
+    # RL Algorithm specific arguments
     parser.add_argument('--agent', type=str, default='PPO')
     parser.add_argument('--agent-config', type=str, default='default')
-    parser.add_argument('--policy', type=str, default='default')
-    parser.add_argument('--policy-config', type=str, default='default')
     parser.add_argument('--discount', type=int, default=0.999)
     parser.add_argument('--threshold', type=int, default=0.3)
     parser.add_argument('--batch-size', type=int, default=2048)
     parser.add_argument('--num-steps', type=int, default=2048)
-    parser.add_argument('--ent_coef', type=int, default=0)
     parser.add_argument('--learning-rate', type=int, default=1e-3)
-    parser.add_argument('--clip_range', type=int, default=0.2)
 
+    # PPO specific
+    parser.add_argument('--clip_range', type=int, default=0.2)
+    parser.add_argument('--ent_coef', type=int, default=0)
 
     parser.add_argument('--eval-criterion', type=str, default='default')
     parser.add_argument('--eval-criterion-config', type=str, default='default')
@@ -480,27 +542,16 @@ def parse_args():
     parser.add_argument('--criterion', type=str, default='default')
     parser.add_argument('--criterion-config', type=str, default='default')
 
-    parser.add_argument('--wandb', type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,)
-
-    parser.add_argument("--wandb-project-name", type=str, default="ppo-implementation-details",
-                        help="the wandb's project name")
+    # Wandb specific arguments
+    parser.add_argument('--wandb', type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True, )
     parser.add_argument("--wandb-entity", type=str, default=None,
                         help="the entity (team) of wandb's project")
+
     parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
                         help="weather to capture videos of the agent performances (check out `videos` folder)")
 
-    # #### Define and parse (optional) arguments for the script ##
-    # parser = argparse.ArgumentParser(description='Single agent reinforcement learning example script using HoverAviary')
-    # parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI (default: True)',
-    #                     metavar='')
-    # parser.add_argument('--record_video', default=DEFAULT_RECORD_VIDEO, type=str2bool,
-    #                     help='Whether to record a video (default: False)', metavar='')
-    # parser.add_argument('--output_folder', default=DEFAULT_OUTPUT_FOLDER, type=str,
-    #                     help='Folder where to save logs (default: "results")', metavar='')
-    # parser.add_argument('--colab', default=DEFAULT_COLAB, type=bool,
-    #                     help='Whether example is being run by a notebook (default: "False")', metavar='')
-    #
-    # run(**vars(ARGS))
+    parser.add_argument('--output_folder', default=DEFAULT_OUTPUT_FOLDER, type=str,
+                        help='Folder where to save logs (default: "results")', metavar='')
 
     args = parser.parse_args()
 
@@ -514,9 +565,7 @@ def init_wandb(args):
     config = {
         "env_name": args.env,
         "agent": args.agent,
-        "policy_type": args.policy,
         "total_timesteps": args.num_steps,
-        "policy_config": args.policy_config,
         "env_config": args.env_config,
         "seed": args.seed,
         "agent_config": args.agent_config,
@@ -531,9 +580,11 @@ def init_wandb(args):
         project="rl",
         config=args,
         name=run_name,
+        tensorboard=True,
         sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
         monitor_gym=True,  # auto-upload the videos of agents playing the game
         save_code=True,  # optional
+
     )
     writer = SummaryWriter(f"runs/{run_name}")
     writer.add_text(
@@ -542,41 +593,6 @@ def init_wandb(args):
     )
 
 
-if __name__ == "__main__":
-
-    args = parse_args()
-    print(args)
-    #
-
-    # ## seeding
-    # seed = args.seed
-    # random.seed(seed)
-    # np.random.seed(seed)
-    # th.manual_seed(seed)
-    # th.backends.cudnn.deterministic = False
-    #
-    # device = th.device("cuda" if th.cuda.is_available() and args.cuda else "cpu")
-
-    # targets = Waypoints.up_circle()
-    targets = Waypoints.rnd()
-
-    sim = PBDroneSimulator(targets, target_factor=0)
-
-    init_wandb(args)
-
-    sim.run_full(args)
-    #
-    # sim.run_test()
-
-    # sim.test_saved()
-    #
-
-    # video_recorder.record_video(
-    #     model=PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.04.2023_22.26.05/best_model.zip",
-    #                    video_folder="C:\Files\Egyetem\Szakdolgozat\RL\Sol/results/videos",
-    #                    ))
-
-
 def linear_schedule(initial_value: float) -> Callable[[float], float]:
     """
     Linear learning rate schedule.
@@ -605,26 +621,6 @@ def linear_schedule(initial_value: float) -> Callable[[float], float]:
 # loaded_model.load_replay_buffer("sac_replay_buffer")
 
 
-# import imageio
-# import numpy as np
-#
-# from stable_baselines3 import A2C
-#
-# model = A2C("MlpPolicy", "LunarLander-v2").learn(100_000)
-#
-# images = []
-# obs = model.env.reset()
-# img = model.env.render(mode="rgb_array")
-# for i in range(350):
-#     images.append(img)
-#     action, _ = model.predict(obs)
-#     obs, _, _ ,_ = model.env.step(action)
-#     img = model.env.render(mode="rgb_array")
-#
-# imageio.mimsave("lander_a2c.gif", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)
-
-
-
 def manual_pb_env():
     # Connect to the PyBullet physics server
     # physicsClient = p.connect(p.GUI)
@@ -647,3 +643,38 @@ def manual_pb_env():
     # print('time_step_spec.step_type:', tf_env.time_step_spec().step_type)
     # print('time_step_spec.discount:', tf_env.time_step_spec().discount)
     # print('time_step_spec.reward:', tf_env.time_step_spec().reward)
+
+
+if __name__ == "__main__":
+    args = parse_args()
+    print(args)
+    #
+
+    # ## seeding
+    # seed = args.seed
+    # random.seed(seed)
+    # np.random.seed(seed)
+    # th.manual_seed(seed)
+    # th.backends.cudnn.deterministic = False
+    #
+    # device = th.device("cuda" if th.cuda.is_available() and args.cuda else "cpu")
+
+    # targets = Waypoints.up_circle()
+    targets = Waypoints.rnd()
+
+    sim = PBDroneSimulator(targets, target_factor=0)
+
+    init_wandb(args)
+
+    # sim.test_learning()
+    sim.run_full(args)
+    #
+    # sim.run_test()
+
+    # sim.test_saved()
+    #
+
+    # video_recorder.record_video(
+    #     model=PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.04.2023_22.26.05/best_model.zip",
+    #                    video_folder="C:\Files\Egyetem\Szakdolgozat\RL\Sol/results/videos",
+    #                    ))
diff --git a/Sol/Utilities/Callbacks.py b/Sol/Utilities/Callbacks.py
index 80d63b8..6caffd4 100644
--- a/Sol/Utilities/Callbacks.py
+++ b/Sol/Utilities/Callbacks.py
@@ -1,10 +1,12 @@
 import matplotlib.pyplot as plt
 import numpy as np
 from stable_baselines3.common.callbacks import BaseCallback
-from stable_baselines3.common.logger import HParam
+from stable_baselines3.common.logger import HParam, TensorBoardOutputFormat
 from stable_baselines3.common.monitor import load_results
 from stable_baselines3.common.results_plotter import ts2xy
 import wandb
+from torch.utils.tensorboard import SummaryWriter
+
 
 class FoundTargetsCallback(BaseCallback):
     """
@@ -38,61 +40,47 @@ class HParamCallback(BaseCallback):
     """
     Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.
     """
-
     def _on_training_start(self) -> None:
-        hparam_dict = {
-            "algorithm": self.model.__class__.__name__,
-            "learning rate": self.model.learning_rate,
-            "gamma": self.model.gamma,
-            "batch_size": self.model.batch_size,
-            "ent_coef": self.model.ent_coef,
-            "clip_range": self.model.clip_range,
-            "n_epochs": self.model.n_epochs,
-            "n_steps": self.model.n_steps,
-            "vf_coef": self.model.vf_coef,
-            "max_grad_norm": self.model.max_grad_norm,
-            "gae_lambda": self.model.gae_lambda,
-            "policy_kwargs": self.model.policy_kwargs,
-            "policy": self.model.policy,
-            "n_envs": self.model.n_envs,
-
-        }
-        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag
-        # Tensorbaord will find & display metrics from the `SCALARS` tab
-        metric_dict = {
-            "rollout/ep_len_mean": 0,
-            "train/value_loss": 0.0,
-            "train/entropy_loss": 0.0,
-            "train/policy_loss": 0.0,
-            "train/approx_kl": 0.0,
-            "train/clip_fraction": 0.0,
-            "train/clip_range": 0.0,
-            "train/n_updates_total": 0,
-            "train/learning_rate": 0.0,
-            "train/found_targets": 0.0,
-            "train/ep_rew_mean": 0.0,
-            "train/ep_rew_std": 0.0,
-            "train/ep_len_mean": 0.0,
-            "train/ep_len_std": 0.0,
-            "train/success_rate": 0.0,
-            "train/success_rate_std": 0.0,
-            "train/success_rate_mean": 0.0,
-            "train/episodes": 0.0,
-            "train/time_elapsed": 0.0,
-            "train/total_timesteps": 0.0,
-            "train/total_updates": 0.0,
-            "train/explained_variance": 0.0,
-            "train/n_updates": 0.0,
-            "train/serial_timesteps": 0.0,
-            "train/serial_episodes": 0.0,
-            "train/ep_rew_max": 0.0,
-            "train/ep_rew_min": 0.0,
-        }
-        self.logger.record(
-            "hparams",
-            HParam(hparam_dict, metric_dict),
-            exclude=("stdout", "log", "json", "csv"),
-        )
+        self.hparams = self.model.get_parameters()
+
+        # Create a TensorBoard writer
+        log_dir = self.model.tensorboard_log
+        self.writer = SummaryWriter(log_dir)
+
+        # Log hyperparameters
+        for key, value in self.hparams.items():
+            self.writer.add_text("hyperparameters", f"{key}: {value}")
 
     def _on_step(self) -> bool:
-        return True
\ No newline at end of file
+        return True
+
+    def _on_training_end(self) -> None:
+        """
+        This method is called when the training ends.
+        It closes the TensorBoard writer.
+        """
+        if self.writer is not None:
+            self.writer.close()
+
+
+class SummaryWriterCallback(BaseCallback):
+
+    def _on_training_start(self):
+        self._log_freq = 1000  # log every 1000 calls
+
+        output_formats = self.logger.output_formats
+        # Save reference to tensorboard formatter object
+        # note: the failure case (not formatter found) is not handled here, should be done with try/except.
+        self.tb_formatter = next(formatter for formatter in output_formats if isinstance(formatter, TensorBoardOutputFormat))
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self._log_freq == 0:
+            # You can have access to info from the env using self.locals.
+            # for instance, when using one env (index 0 of locals["infos"]):
+            # lap_count = self.locals["infos"][0]["lap_count"]
+            # self.tb_formatter.writer.add_scalar("train/lap_count", lap_count, self.num_timesteps)
+
+            self.tb_formatter.writer.add_text("direct_access", "this is a value", self.num_timesteps)
+            self.tb_formatter.writer.flush()
+
+        return True
diff --git a/Sol/Utilities/HParamCallback.py b/Sol/Utilities/HParamCallback.py
deleted file mode 100644
index 2c22178..0000000
--- a/Sol/Utilities/HParamCallback.py
+++ /dev/null
@@ -1,30 +0,0 @@
-from stable_baselines3 import A2C
-from stable_baselines3.common.callbacks import BaseCallback
-from stable_baselines3.common.logger import HParam
-
-
-class HParamCallback(BaseCallback):
-    """
-    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.
-    """
-
-    def _on_training_start(self) -> None:
-        hparam_dict = {
-            "algorithm": self.model.__class__.__name__,
-            "learning rate": self.model.learning_rate,
-            "gamma": self.model.gamma,
-        }
-        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag
-        # Tensorbaord will find & display metrics from the `SCALARS` tab
-        metric_dict = {
-            "rollout/ep_len_mean": 0,
-            "train/value_loss": 0.0,
-        }
-        self.logger.record(
-            "hparams",
-            HParam(hparam_dict, metric_dict),
-            exclude=("stdout", "log", "json", "csv"),
-        )
-
-    def _on_step(self) -> bool:
-        return True
diff --git a/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc b/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc
index 0f8fc75..0efe45d 100644
Binary files a/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc and b/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc differ
diff --git a/Sol/setup.py b/Sol/setup.py
new file mode 100644
index 0000000..7d8ac59
--- /dev/null
+++ b/Sol/setup.py
@@ -0,0 +1,12 @@
+from setuptools import setup
+
+setup(
+    name='Sol',
+    version='',
+    packages=[],
+    url='',
+    license='',
+    author='rgb',
+    author_email='',
+    description=''
+)
diff --git a/setup.py b/setup.py
new file mode 100644
index 0000000..1767166
--- /dev/null
+++ b/setup.py
@@ -0,0 +1,19 @@
+from setuptools import setup
+
+setup(
+    name='RL',
+    version='',
+    packages=['Sol', 'AirSim-main', 'AirSim-main.PythonClient', 'AirSim-main.PythonClient.airsim',
+              'gym-pybullet-drones-main', 'gym-pybullet-drones-main.gym_pybullet_drones',
+              'gym-pybullet-drones-main.gym_pybullet_drones.envs',
+              'gym-pybullet-drones-main.gym_pybullet_drones.envs.multi_agent_rl',
+              'gym-pybullet-drones-main.gym_pybullet_drones.envs.single_agent_rl',
+              'gym-pybullet-drones-main.gym_pybullet_drones.utils',
+              'gym-pybullet-drones-main.gym_pybullet_drones.control',
+              'gym-pybullet-drones-main.gym_pybullet_drones.examples'],
+    url='',
+    license='',
+    author='rgb',
+    author_email='',
+    description=''
+)
