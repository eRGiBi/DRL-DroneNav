AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO_108
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-254.60 +/- 27.16
Episode length: 221.20 +/- 72.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-281.54 +/- 7.27
Episode length: 209.00 +/- 26.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-239.29 +/- 46.08
Episode length: 254.00 +/- 63.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
New best mean reward!
Eval num_timesteps=7968, episode_reward=-274.52 +/- 13.01
Episode length: 169.60 +/- 31.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-266.28 +/- 14.30
Episode length: 189.40 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-282.45 +/- 14.62
Episode length: 232.00 +/- 25.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-274.22 +/- 22.86
Episode length: 215.20 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Eval num_timesteps=15936, episode_reward=-275.19 +/- 19.53
Episode length: 218.40 +/- 61.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 15936    |
---------------------------------
Eval num_timesteps=17928, episode_reward=-273.77 +/- 19.90
Episode length: 219.80 +/- 56.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 17928    |
---------------------------------
Eval num_timesteps=19920, episode_reward=-270.22 +/- 11.74
Episode length: 191.00 +/- 40.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 19920    |
---------------------------------
Eval num_timesteps=21912, episode_reward=-273.66 +/- 21.97
Episode length: 242.20 +/- 54.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 21912    |
---------------------------------
Eval num_timesteps=23904, episode_reward=-281.72 +/- 14.01
Episode length: 207.40 +/- 40.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 23904    |
---------------------------------
Eval num_timesteps=25896, episode_reward=-277.77 +/- 13.48
Episode length: 244.80 +/- 40.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 25896    |
---------------------------------
Eval num_timesteps=27888, episode_reward=-281.88 +/- 12.83
Episode length: 223.00 +/- 72.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 27888    |
---------------------------------
Eval num_timesteps=29880, episode_reward=-283.87 +/- 9.40
Episode length: 199.40 +/- 32.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 29880    |
---------------------------------
Eval num_timesteps=31872, episode_reward=-276.10 +/- 11.43
Episode length: 227.40 +/- 40.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 31872    |
---------------------------------
Eval num_timesteps=33864, episode_reward=-279.33 +/- 15.29
Episode length: 199.00 +/- 55.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 33864    |
---------------------------------
Eval num_timesteps=35856, episode_reward=-274.81 +/- 25.56
Episode length: 239.60 +/- 63.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 35856    |
---------------------------------
Eval num_timesteps=37848, episode_reward=-255.78 +/- 22.05
Episode length: 227.00 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 37848    |
---------------------------------
Eval num_timesteps=39840, episode_reward=-279.99 +/- 16.72
Episode length: 194.00 +/- 30.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 39840    |
---------------------------------
Eval num_timesteps=41832, episode_reward=-286.47 +/- 12.44
Episode length: 223.00 +/- 30.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 41832    |
---------------------------------
Eval num_timesteps=43824, episode_reward=-270.55 +/- 20.60
Episode length: 225.00 +/- 71.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 43824    |
---------------------------------
Eval num_timesteps=45816, episode_reward=-280.57 +/- 13.70
Episode length: 190.40 +/- 52.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 45816    |
---------------------------------
Eval num_timesteps=47808, episode_reward=-260.28 +/- 41.21
Episode length: 205.00 +/- 52.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 47808    |
---------------------------------
Eval num_timesteps=49800, episode_reward=-266.48 +/- 17.27
Episode length: 236.60 +/- 52.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | -266         |
| time/                   |              |
|    total_timesteps      | 49800        |
| train/                  |              |
|    approx_kl            | 0.0038986977 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | -0.213       |
|    learning_rate        | 0.001        |
|    loss                 | 1.4          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0016      |
|    std                  | 1.01         |
|    value_loss           | 3.37         |
------------------------------------------
Eval num_timesteps=51792, episode_reward=-280.93 +/- 6.96
Episode length: 200.80 +/- 22.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 51792    |
---------------------------------
Eval num_timesteps=53784, episode_reward=-269.13 +/- 23.39
Episode length: 216.00 +/- 13.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 53784    |
---------------------------------
Eval num_timesteps=55776, episode_reward=-269.42 +/- 18.23
Episode length: 247.80 +/- 32.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 55776    |
---------------------------------
Eval num_timesteps=57768, episode_reward=-267.69 +/- 21.26
Episode length: 214.00 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 57768    |
---------------------------------
Eval num_timesteps=59760, episode_reward=-275.03 +/- 22.34
Episode length: 203.60 +/- 24.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 59760    |
---------------------------------
Eval num_timesteps=61752, episode_reward=-277.19 +/- 12.39
Episode length: 210.20 +/- 61.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 61752    |
---------------------------------
Eval num_timesteps=63744, episode_reward=-274.03 +/- 8.64
Episode length: 260.20 +/- 53.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 63744    |
---------------------------------
Eval num_timesteps=65736, episode_reward=-272.56 +/- 18.75
Episode length: 245.60 +/- 40.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 65736    |
---------------------------------
Eval num_timesteps=67728, episode_reward=-285.94 +/- 7.31
Episode length: 209.20 +/- 49.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 67728    |
---------------------------------
Eval num_timesteps=69720, episode_reward=-288.01 +/- 4.96
Episode length: 229.80 +/- 51.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 69720    |
---------------------------------
Eval num_timesteps=71712, episode_reward=-277.67 +/- 12.06
Episode length: 291.60 +/- 78.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 71712    |
---------------------------------
Eval num_timesteps=73704, episode_reward=-275.77 +/- 10.54
Episode length: 231.80 +/- 63.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 73704    |
---------------------------------
Eval num_timesteps=75696, episode_reward=-281.29 +/- 11.89
Episode length: 212.60 +/- 35.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 75696    |
---------------------------------
Eval num_timesteps=77688, episode_reward=-272.23 +/- 17.16
Episode length: 265.40 +/- 53.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 77688    |
---------------------------------
Eval num_timesteps=79680, episode_reward=-273.60 +/- 23.41
Episode length: 190.60 +/- 28.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 79680    |
---------------------------------
Eval num_timesteps=81672, episode_reward=-279.97 +/- 13.97
Episode length: 220.20 +/- 52.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 81672    |
---------------------------------
Eval num_timesteps=83664, episode_reward=-291.63 +/- 5.85
Episode length: 208.80 +/- 48.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 83664    |
---------------------------------
Eval num_timesteps=85656, episode_reward=-266.78 +/- 19.99
Episode length: 220.40 +/- 50.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 85656    |
---------------------------------
Eval num_timesteps=87648, episode_reward=-281.89 +/- 10.39
Episode length: 236.40 +/- 62.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 87648    |
---------------------------------
Eval num_timesteps=89640, episode_reward=-252.14 +/- 36.99
Episode length: 239.40 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 89640    |
---------------------------------
Eval num_timesteps=91632, episode_reward=-281.35 +/- 15.23
Episode length: 181.60 +/- 27.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 91632    |
---------------------------------
Eval num_timesteps=93624, episode_reward=-269.31 +/- 26.44
Episode length: 221.60 +/- 54.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 93624    |
---------------------------------
Eval num_timesteps=95616, episode_reward=-262.18 +/- 11.70
Episode length: 222.80 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 95616    |
---------------------------------
Eval num_timesteps=97608, episode_reward=-273.35 +/- 12.14
Episode length: 221.20 +/- 51.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 97608    |
---------------------------------
Eval num_timesteps=99600, episode_reward=-258.29 +/- 20.07
Episode length: 251.20 +/- 37.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 99600        |
| train/                  |              |
|    approx_kl            | 0.0029825661 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.001        |
|    loss                 | 0.74         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.02         |
|    value_loss           | 1.95         |
------------------------------------------
Eval num_timesteps=101592, episode_reward=-275.82 +/- 19.24
Episode length: 212.40 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 101592   |
---------------------------------
Eval num_timesteps=103584, episode_reward=-286.29 +/- 7.96
Episode length: 224.60 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 103584   |
---------------------------------
Eval num_timesteps=105576, episode_reward=-255.68 +/- 46.72
Episode length: 229.60 +/- 80.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 105576   |
---------------------------------
Eval num_timesteps=107568, episode_reward=-275.27 +/- 9.11
Episode length: 259.80 +/- 59.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 107568   |
---------------------------------
Eval num_timesteps=109560, episode_reward=-248.58 +/- 53.64
Episode length: 280.80 +/- 54.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 109560   |
---------------------------------
Eval num_timesteps=111552, episode_reward=-263.22 +/- 35.68
Episode length: 223.20 +/- 76.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 111552   |
---------------------------------
Eval num_timesteps=113544, episode_reward=-263.29 +/- 6.75
Episode length: 198.00 +/- 30.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 113544   |
---------------------------------
Eval num_timesteps=115536, episode_reward=-265.74 +/- 33.05
Episode length: 212.80 +/- 39.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 115536   |
---------------------------------
Eval num_timesteps=117528, episode_reward=-280.44 +/- 12.39
Episode length: 220.00 +/- 48.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 117528   |
---------------------------------
Eval num_timesteps=119520, episode_reward=-279.61 +/- 9.84
Episode length: 219.40 +/- 41.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 119520   |
---------------------------------
Eval num_timesteps=121512, episode_reward=-245.17 +/- 45.03
Episode length: 244.80 +/- 36.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 121512   |
---------------------------------
Eval num_timesteps=123504, episode_reward=-283.12 +/- 7.73
Episode length: 231.60 +/- 66.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 123504   |
---------------------------------
Eval num_timesteps=125496, episode_reward=-287.14 +/- 7.91
Episode length: 206.60 +/- 56.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 125496   |
---------------------------------
Eval num_timesteps=127488, episode_reward=-238.09 +/- 71.09
Episode length: 254.80 +/- 53.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 127488   |
---------------------------------
New best mean reward!
Eval num_timesteps=129480, episode_reward=-276.33 +/- 7.70
Episode length: 173.60 +/- 12.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 129480   |
---------------------------------
Eval num_timesteps=131472, episode_reward=-278.92 +/- 6.38
Episode length: 239.20 +/- 38.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 131472   |
---------------------------------
Eval num_timesteps=133464, episode_reward=-279.98 +/- 18.81
Episode length: 250.40 +/- 71.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 133464   |
---------------------------------
Eval num_timesteps=135456, episode_reward=-269.42 +/- 16.68
Episode length: 267.60 +/- 76.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 135456   |
---------------------------------
Eval num_timesteps=137448, episode_reward=-282.18 +/- 6.55
Episode length: 217.00 +/- 37.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 137448   |
---------------------------------
Eval num_timesteps=139440, episode_reward=-278.73 +/- 12.33
Episode length: 229.60 +/- 43.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 139440   |
---------------------------------
Eval num_timesteps=141432, episode_reward=-273.32 +/- 24.57
Episode length: 237.40 +/- 49.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 141432   |
---------------------------------
Eval num_timesteps=143424, episode_reward=-280.66 +/- 12.29
Episode length: 218.40 +/- 62.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 143424   |
---------------------------------
Eval num_timesteps=145416, episode_reward=-280.66 +/- 16.98
Episode length: 206.40 +/- 39.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 145416   |
---------------------------------
Eval num_timesteps=147408, episode_reward=-275.18 +/- 5.01
Episode length: 174.40 +/- 11.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 147408   |
---------------------------------
Eval num_timesteps=149400, episode_reward=-280.58 +/- 14.38
Episode length: 221.80 +/- 50.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 149400      |
| train/                  |             |
|    approx_kl            | 0.005136192 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.77       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 0.326       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00185    |
|    std                  | 1.03        |
|    value_loss           | 0.874       |
-----------------------------------------
Eval num_timesteps=151392, episode_reward=-209.69 +/- 94.31
Episode length: 366.80 +/- 164.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 151392   |
---------------------------------
New best mean reward!
Eval num_timesteps=153384, episode_reward=-284.01 +/- 9.61
Episode length: 248.60 +/- 60.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 153384   |
---------------------------------
Eval num_timesteps=155376, episode_reward=-269.00 +/- 21.03
Episode length: 194.00 +/- 30.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 155376   |
---------------------------------
Eval num_timesteps=157368, episode_reward=-272.68 +/- 15.95
Episode length: 240.80 +/- 43.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 157368   |
---------------------------------
Eval num_timesteps=159360, episode_reward=-284.53 +/- 11.57
Episode length: 251.00 +/- 26.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 159360   |
---------------------------------
Eval num_timesteps=161352, episode_reward=-252.59 +/- 40.52
Episode length: 231.20 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 161352   |
---------------------------------
Eval num_timesteps=163344, episode_reward=-261.82 +/- 23.56
Episode length: 256.40 +/- 89.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 163344   |
---------------------------------
Eval num_timesteps=165336, episode_reward=-246.77 +/- 82.79
Episode length: 277.20 +/- 81.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 165336   |
---------------------------------
Eval num_timesteps=167328, episode_reward=-274.09 +/- 26.22
Episode length: 247.80 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 167328   |
---------------------------------
Eval num_timesteps=169320, episode_reward=-251.35 +/- 38.95
Episode length: 273.60 +/- 61.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 169320   |
---------------------------------
Eval num_timesteps=171312, episode_reward=-261.73 +/- 35.41
Episode length: 225.80 +/- 40.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 171312   |
---------------------------------
Eval num_timesteps=173304, episode_reward=-273.38 +/- 19.78
Episode length: 220.80 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 173304   |
---------------------------------
Eval num_timesteps=175296, episode_reward=-266.31 +/- 21.55
Episode length: 222.00 +/- 52.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 175296   |
---------------------------------
Eval num_timesteps=177288, episode_reward=-273.87 +/- 10.06
Episode length: 218.20 +/- 29.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 177288   |
---------------------------------
Eval num_timesteps=179280, episode_reward=-276.04 +/- 16.66
Episode length: 246.80 +/- 41.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 179280   |
---------------------------------
Eval num_timesteps=181272, episode_reward=-273.36 +/- 14.05
Episode length: 227.00 +/- 47.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 181272   |
---------------------------------
Eval num_timesteps=183264, episode_reward=-264.65 +/- 25.34
Episode length: 235.00 +/- 25.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 183264   |
---------------------------------
Eval num_timesteps=185256, episode_reward=-287.52 +/- 8.87
Episode length: 245.80 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 185256   |
---------------------------------
Eval num_timesteps=187248, episode_reward=-269.31 +/- 10.44
Episode length: 233.60 +/- 39.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 187248   |
---------------------------------
Eval num_timesteps=189240, episode_reward=-265.44 +/- 26.25
Episode length: 263.60 +/- 107.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 189240   |
---------------------------------
Eval num_timesteps=191232, episode_reward=-286.64 +/- 9.23
Episode length: 216.80 +/- 38.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 191232   |
---------------------------------
Eval num_timesteps=193224, episode_reward=-276.72 +/- 22.24
Episode length: 261.60 +/- 64.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 193224   |
---------------------------------
Eval num_timesteps=195216, episode_reward=-222.26 +/- 95.76
Episode length: 240.00 +/- 68.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 195216   |
---------------------------------
Eval num_timesteps=197208, episode_reward=-266.09 +/- 14.57
Episode length: 256.60 +/- 61.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | -266         |
| time/                   |              |
|    total_timesteps      | 197208       |
| train/                  |              |
|    approx_kl            | 0.0052695326 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 0.197        |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00251     |
|    std                  | 1.03         |
|    value_loss           | 0.572        |
------------------------------------------
Eval num_timesteps=199200, episode_reward=-288.17 +/- 2.34
Episode length: 224.60 +/- 34.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
Eval num_timesteps=201192, episode_reward=-267.17 +/- 11.75
Episode length: 280.20 +/- 161.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 201192   |
---------------------------------
Eval num_timesteps=203184, episode_reward=-250.06 +/- 39.39
Episode length: 217.40 +/- 40.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 203184   |
---------------------------------
Eval num_timesteps=205176, episode_reward=-221.08 +/- 84.70
Episode length: 329.40 +/- 66.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 205176   |
---------------------------------
Eval num_timesteps=207168, episode_reward=-223.19 +/- 81.94
Episode length: 242.60 +/- 48.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 207168   |
---------------------------------
Eval num_timesteps=209160, episode_reward=-218.22 +/- 100.19
Episode length: 288.60 +/- 42.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 209160   |
---------------------------------
Eval num_timesteps=211152, episode_reward=-227.90 +/- 83.77
Episode length: 251.60 +/- 47.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 211152   |
---------------------------------
Eval num_timesteps=213144, episode_reward=-262.85 +/- 22.63
Episode length: 248.60 +/- 109.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 213144   |
---------------------------------
Eval num_timesteps=215136, episode_reward=-272.54 +/- 19.52
Episode length: 317.60 +/- 123.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 215136   |
---------------------------------
Eval num_timesteps=217128, episode_reward=-239.31 +/- 41.98
Episode length: 245.40 +/- 91.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 217128   |
---------------------------------
Eval num_timesteps=219120, episode_reward=-262.43 +/- 15.63
Episode length: 253.60 +/- 71.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 219120   |
---------------------------------
Eval num_timesteps=221112, episode_reward=-276.23 +/- 3.17
Episode length: 235.80 +/- 60.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 221112   |
---------------------------------
Eval num_timesteps=223104, episode_reward=-258.90 +/- 25.75
Episode length: 214.00 +/- 29.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 223104   |
---------------------------------
Eval num_timesteps=225096, episode_reward=-264.91 +/- 41.33
Episode length: 258.60 +/- 77.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 225096   |
---------------------------------
Eval num_timesteps=227088, episode_reward=-261.16 +/- 39.83
Episode length: 254.60 +/- 72.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 227088   |
---------------------------------
Eval num_timesteps=229080, episode_reward=-282.62 +/- 10.26
Episode length: 208.60 +/- 43.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 229080   |
---------------------------------
Eval num_timesteps=231072, episode_reward=-280.58 +/- 3.77
Episode length: 213.80 +/- 41.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 231072   |
---------------------------------
Eval num_timesteps=233064, episode_reward=-232.25 +/- 65.05
Episode length: 263.00 +/- 80.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 233064   |
---------------------------------
Eval num_timesteps=235056, episode_reward=-265.34 +/- 13.78
Episode length: 274.60 +/- 89.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 235056   |
---------------------------------
Eval num_timesteps=237048, episode_reward=-257.32 +/- 17.69
Episode length: 238.20 +/- 10.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 237048   |
---------------------------------
Eval num_timesteps=239040, episode_reward=-222.79 +/- 58.38
Episode length: 291.40 +/- 64.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 239040   |
---------------------------------
Eval num_timesteps=241032, episode_reward=-272.95 +/- 18.68
Episode length: 275.00 +/- 75.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 241032   |
---------------------------------
Eval num_timesteps=243024, episode_reward=-285.09 +/- 6.33
Episode length: 248.60 +/- 95.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 243024   |
---------------------------------
Eval num_timesteps=245016, episode_reward=-222.96 +/- 91.96
Episode length: 257.60 +/- 70.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 245016   |
---------------------------------
Eval num_timesteps=247008, episode_reward=-249.30 +/- 31.64
Episode length: 243.40 +/- 43.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | -249       |
| time/                   |            |
|    total_timesteps      | 247008     |
| train/                  |            |
|    approx_kl            | 0.00483381 |
|    clip_fraction        | 0.0643     |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.82      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.001      |
|    loss                 | 0.17       |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00247   |
|    std                  | 1.04       |
|    value_loss           | 0.493      |
----------------------------------------
Eval num_timesteps=249000, episode_reward=-251.39 +/- 26.89
Episode length: 280.40 +/- 51.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=250992, episode_reward=-236.15 +/- 40.48
Episode length: 334.80 +/- 104.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 250992   |
---------------------------------
Eval num_timesteps=252984, episode_reward=-256.17 +/- 42.86
Episode length: 304.40 +/- 90.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 252984   |
---------------------------------
Eval num_timesteps=254976, episode_reward=-272.03 +/- 15.45
Episode length: 272.20 +/- 61.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=256968, episode_reward=-262.44 +/- 22.02
Episode length: 281.80 +/- 80.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 256968   |
---------------------------------
Eval num_timesteps=258960, episode_reward=-251.19 +/- 39.07
Episode length: 289.40 +/- 56.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 258960   |
---------------------------------
Eval num_timesteps=260952, episode_reward=-274.32 +/- 8.51
Episode length: 238.60 +/- 64.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 260952   |
---------------------------------
Eval num_timesteps=262944, episode_reward=-253.34 +/- 20.06
Episode length: 223.60 +/- 86.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 262944   |
---------------------------------
Eval num_timesteps=264936, episode_reward=-258.74 +/- 15.77
Episode length: 234.80 +/- 97.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 264936   |
---------------------------------
Eval num_timesteps=266928, episode_reward=-274.50 +/- 19.39
Episode length: 228.20 +/- 26.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 266928   |
---------------------------------
Eval num_timesteps=268920, episode_reward=-263.10 +/- 25.51
Episode length: 238.00 +/- 51.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 268920   |
---------------------------------
Eval num_timesteps=270912, episode_reward=-270.19 +/- 25.71
Episode length: 296.80 +/- 70.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 270912   |
---------------------------------
Eval num_timesteps=272904, episode_reward=-263.86 +/- 11.00
Episode length: 204.80 +/- 47.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 272904   |
---------------------------------
Eval num_timesteps=274896, episode_reward=-278.64 +/- 6.10
Episode length: 210.40 +/- 48.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 274896   |
---------------------------------
Eval num_timesteps=276888, episode_reward=-215.59 +/- 114.08
Episode length: 361.40 +/- 128.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 361      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 276888   |
---------------------------------
Eval num_timesteps=278880, episode_reward=-268.47 +/- 30.39
Episode length: 288.80 +/- 106.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 278880   |
---------------------------------
Eval num_timesteps=280872, episode_reward=-245.78 +/- 28.52
Episode length: 211.80 +/- 57.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 280872   |
---------------------------------
Eval num_timesteps=282864, episode_reward=-247.91 +/- 11.62
Episode length: 237.20 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 282864   |
---------------------------------
Eval num_timesteps=284856, episode_reward=-258.00 +/- 15.70
Episode length: 306.20 +/- 79.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 284856   |
---------------------------------
Eval num_timesteps=286848, episode_reward=-255.86 +/- 16.97
Episode length: 223.40 +/- 38.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 286848   |
---------------------------------
Eval num_timesteps=288840, episode_reward=-259.70 +/- 29.07
Episode length: 260.20 +/- 72.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 288840   |
---------------------------------
Eval num_timesteps=290832, episode_reward=-258.41 +/- 14.69
Episode length: 221.20 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 290832   |
---------------------------------
Eval num_timesteps=292824, episode_reward=-267.18 +/- 11.65
Episode length: 232.80 +/- 45.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 292824   |
---------------------------------
Eval num_timesteps=294816, episode_reward=-260.95 +/- 12.92
Episode length: 265.80 +/- 118.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 294816   |
---------------------------------
Eval num_timesteps=296808, episode_reward=-251.77 +/- 23.39
Episode length: 261.60 +/- 95.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | -252        |
| time/                   |             |
|    total_timesteps      | 296808      |
| train/                  |             |
|    approx_kl            | 0.008010144 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.84       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 0.169       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0022     |
|    std                  | 1.04        |
|    value_loss           | 0.533       |
-----------------------------------------
Eval num_timesteps=298800, episode_reward=-234.89 +/- 56.83
Episode length: 376.20 +/- 62.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
Eval num_timesteps=300792, episode_reward=-278.63 +/- 12.97
Episode length: 221.40 +/- 89.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 300792   |
---------------------------------
Eval num_timesteps=302784, episode_reward=-264.21 +/- 12.14
Episode length: 183.60 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 302784   |
---------------------------------
Eval num_timesteps=304776, episode_reward=-235.26 +/- 48.82
Episode length: 336.40 +/- 59.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 336      |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 304776   |
---------------------------------
Eval num_timesteps=306768, episode_reward=-248.65 +/- 13.52
Episode length: 221.20 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 306768   |
---------------------------------
Eval num_timesteps=308760, episode_reward=-267.05 +/- 15.22
Episode length: 214.60 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 308760   |
---------------------------------
Eval num_timesteps=310752, episode_reward=-255.48 +/- 15.85
Episode length: 220.40 +/- 27.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 310752   |
---------------------------------
Eval num_timesteps=312744, episode_reward=-248.91 +/- 25.34
Episode length: 254.00 +/- 37.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 312744   |
---------------------------------
Eval num_timesteps=314736, episode_reward=-239.81 +/- 33.95
Episode length: 208.00 +/- 44.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 314736   |
---------------------------------
Eval num_timesteps=316728, episode_reward=-245.84 +/- 17.36
Episode length: 237.40 +/- 94.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 316728   |
---------------------------------
Eval num_timesteps=318720, episode_reward=-271.67 +/- 23.98
Episode length: 236.20 +/- 91.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 318720   |
---------------------------------
Eval num_timesteps=320712, episode_reward=-262.98 +/- 26.28
Episode length: 250.00 +/- 74.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 320712   |
---------------------------------
Eval num_timesteps=322704, episode_reward=-236.62 +/- 49.13
Episode length: 241.20 +/- 96.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 322704   |
---------------------------------
Eval num_timesteps=324696, episode_reward=-263.81 +/- 12.92
Episode length: 279.40 +/- 150.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 324696   |
---------------------------------
Eval num_timesteps=326688, episode_reward=-253.57 +/- 26.45
Episode length: 213.80 +/- 57.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 326688   |
---------------------------------
Eval num_timesteps=328680, episode_reward=-235.45 +/- 43.88
Episode length: 247.20 +/- 46.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 328680   |
---------------------------------
Eval num_timesteps=330672, episode_reward=-225.01 +/- 58.90
Episode length: 297.80 +/- 110.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 330672   |
---------------------------------
Eval num_timesteps=332664, episode_reward=-268.32 +/- 12.90
Episode length: 209.40 +/- 41.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 332664   |
---------------------------------
Eval num_timesteps=334656, episode_reward=-256.36 +/- 28.74
Episode length: 250.20 +/- 97.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 334656   |
---------------------------------
Eval num_timesteps=336648, episode_reward=-271.57 +/- 5.17
Episode length: 194.40 +/- 11.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 336648   |
---------------------------------
Eval num_timesteps=338640, episode_reward=-264.77 +/- 8.28
Episode length: 234.40 +/- 52.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 338640   |
---------------------------------
Eval num_timesteps=340632, episode_reward=-258.01 +/- 19.52
Episode length: 208.60 +/- 26.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 340632   |
---------------------------------
Eval num_timesteps=342624, episode_reward=-250.55 +/- 39.76
Episode length: 199.20 +/- 63.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 342624   |
---------------------------------
Eval num_timesteps=344616, episode_reward=-206.75 +/- 62.84
Episode length: 268.00 +/- 69.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | -207        |
| time/                   |             |
|    total_timesteps      | 344616      |
| train/                  |             |
|    approx_kl            | 0.008371988 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.85       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 0.164       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.05        |
|    value_loss           | 0.515       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=346608, episode_reward=-185.56 +/- 71.16
Episode length: 314.00 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 346608   |
---------------------------------
New best mean reward!
Eval num_timesteps=348600, episode_reward=-234.81 +/- 42.98
Episode length: 306.40 +/- 148.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 348600   |
---------------------------------
Eval num_timesteps=350592, episode_reward=-231.70 +/- 41.06
Episode length: 213.40 +/- 46.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 350592   |
---------------------------------
Eval num_timesteps=352584, episode_reward=-198.25 +/- 43.76
Episode length: 316.80 +/- 37.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 352584   |
---------------------------------
Eval num_timesteps=354576, episode_reward=-214.31 +/- 51.57
Episode length: 331.60 +/- 107.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 332      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 354576   |
---------------------------------
Eval num_timesteps=356568, episode_reward=-246.05 +/- 28.82
Episode length: 253.80 +/- 59.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 356568   |
---------------------------------
Eval num_timesteps=358560, episode_reward=-190.13 +/- 121.20
Episode length: 289.00 +/- 107.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 358560   |
---------------------------------
Eval num_timesteps=360552, episode_reward=-212.78 +/- 69.86
Episode length: 258.40 +/- 64.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 360552   |
---------------------------------
Eval num_timesteps=362544, episode_reward=-231.80 +/- 42.22
Episode length: 221.40 +/- 43.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 362544   |
---------------------------------
Eval num_timesteps=364536, episode_reward=-156.29 +/- 123.52
Episode length: 350.60 +/- 98.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 364536   |
---------------------------------
New best mean reward!
Eval num_timesteps=366528, episode_reward=-164.30 +/- 67.44
Episode length: 335.40 +/- 52.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 366528   |
---------------------------------
Eval num_timesteps=368520, episode_reward=-236.31 +/- 48.80
Episode length: 256.60 +/- 66.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 368520   |
---------------------------------
Eval num_timesteps=370512, episode_reward=-264.53 +/- 23.79
Episode length: 237.80 +/- 71.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 370512   |
---------------------------------
Eval num_timesteps=372504, episode_reward=-244.54 +/- 21.14
Episode length: 194.60 +/- 27.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 372504   |
---------------------------------
Eval num_timesteps=374496, episode_reward=-230.28 +/- 39.13
Episode length: 274.60 +/- 104.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 374496   |
---------------------------------
Eval num_timesteps=376488, episode_reward=-226.54 +/- 41.63
Episode length: 233.40 +/- 24.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 376488   |
---------------------------------
Eval num_timesteps=378480, episode_reward=-224.61 +/- 75.56
Episode length: 235.00 +/- 104.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 378480   |
---------------------------------
Eval num_timesteps=380472, episode_reward=-267.85 +/- 15.56
Episode length: 240.00 +/- 103.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 380472   |
---------------------------------
Eval num_timesteps=382464, episode_reward=-249.95 +/- 36.09
Episode length: 185.80 +/- 36.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 382464   |
---------------------------------
Eval num_timesteps=384456, episode_reward=-224.57 +/- 25.21
Episode length: 274.00 +/- 60.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 384456   |
---------------------------------
Eval num_timesteps=386448, episode_reward=-212.79 +/- 58.94
Episode length: 335.20 +/- 92.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 386448   |
---------------------------------
Eval num_timesteps=388440, episode_reward=-150.32 +/- 109.84
Episode length: 290.60 +/- 106.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 388440   |
---------------------------------
New best mean reward!
Eval num_timesteps=390432, episode_reward=-227.55 +/- 88.31
Episode length: 257.00 +/- 107.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 390432   |
---------------------------------
Eval num_timesteps=392424, episode_reward=-261.69 +/- 8.84
Episode length: 190.20 +/- 27.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 392424   |
---------------------------------
Eval num_timesteps=394416, episode_reward=-177.61 +/- 87.06
Episode length: 384.80 +/- 63.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 385         |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 394416      |
| train/                  |             |
|    approx_kl            | 0.009873828 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 0.196       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00328    |
|    std                  | 1.05        |
|    value_loss           | 0.553       |
-----------------------------------------
Eval num_timesteps=396408, episode_reward=-296.55 +/- 14.01
Episode length: 314.00 +/- 65.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 396408   |
---------------------------------
Eval num_timesteps=398400, episode_reward=-238.52 +/- 52.09
Episode length: 286.20 +/- 64.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=400392, episode_reward=-251.68 +/- 44.54
Episode length: 300.40 +/- 85.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 400392   |
---------------------------------
Eval num_timesteps=402384, episode_reward=-245.71 +/- 69.47
Episode length: 297.80 +/- 87.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 402384   |
---------------------------------
Eval num_timesteps=404376, episode_reward=-246.50 +/- 31.77
Episode length: 291.20 +/- 68.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 404376   |
---------------------------------
Eval num_timesteps=406368, episode_reward=-65.56 +/- 309.17
Episode length: 352.80 +/- 152.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 353      |
|    mean_reward     | -65.6    |
| time/              |          |
|    total_timesteps | 406368   |
---------------------------------
New best mean reward!
Eval num_timesteps=408360, episode_reward=-216.01 +/- 40.76
Episode length: 320.80 +/- 94.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 321      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 408360   |
---------------------------------
Eval num_timesteps=410352, episode_reward=-267.93 +/- 45.38
Episode length: 392.80 +/- 130.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 410352   |
---------------------------------
Eval num_timesteps=412344, episode_reward=-229.16 +/- 35.88
Episode length: 277.60 +/- 52.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 412344   |
---------------------------------
Eval num_timesteps=414336, episode_reward=-265.99 +/- 27.98
Episode length: 270.40 +/- 39.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 414336   |
---------------------------------
Eval num_timesteps=416328, episode_reward=-301.04 +/- 13.11
Episode length: 275.60 +/- 46.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 416328   |
---------------------------------
Eval num_timesteps=418320, episode_reward=-258.19 +/- 29.14
Episode length: 361.20 +/- 159.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 361      |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 418320   |
---------------------------------
Eval num_timesteps=420312, episode_reward=-139.79 +/- 86.77
Episode length: 348.40 +/- 69.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 348      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 420312   |
---------------------------------
Eval num_timesteps=422304, episode_reward=-212.49 +/- 71.70
Episode length: 350.00 +/- 99.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 422304   |
---------------------------------
Eval num_timesteps=424296, episode_reward=-97.83 +/- 156.86
Episode length: 342.40 +/- 107.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | -97.8    |
| time/              |          |
|    total_timesteps | 424296   |
---------------------------------
Eval num_timesteps=426288, episode_reward=-203.74 +/- 31.58
Episode length: 302.40 +/- 66.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 426288   |
---------------------------------
Eval num_timesteps=428280, episode_reward=-259.64 +/- 26.07
Episode length: 247.80 +/- 103.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 428280   |
---------------------------------
Eval num_timesteps=430272, episode_reward=-249.95 +/- 41.16
Episode length: 267.20 +/- 35.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 430272   |
---------------------------------
Eval num_timesteps=432264, episode_reward=-244.63 +/- 38.89
Episode length: 366.40 +/- 118.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 366      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 432264   |
---------------------------------
Eval num_timesteps=434256, episode_reward=-222.90 +/- 54.52
Episode length: 347.00 +/- 205.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 434256   |
---------------------------------
Eval num_timesteps=436248, episode_reward=-207.18 +/- 177.46
Episode length: 399.20 +/- 56.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 436248   |
---------------------------------
Eval num_timesteps=438240, episode_reward=-276.94 +/- 27.42
Episode length: 272.40 +/- 72.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 438240   |
---------------------------------
Eval num_timesteps=440232, episode_reward=-48.31 +/- 255.02
Episode length: 349.20 +/- 140.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -48.3    |
| time/              |          |
|    total_timesteps | 440232   |
---------------------------------
New best mean reward!
Eval num_timesteps=442224, episode_reward=-203.84 +/- 71.91
Episode length: 429.60 +/- 93.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 442224   |
---------------------------------
Eval num_timesteps=444216, episode_reward=-231.47 +/- 140.42
Episode length: 326.80 +/- 104.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 327         |
|    mean_reward          | -231        |
| time/                   |             |
|    total_timesteps      | 444216      |
| train/                  |             |
|    approx_kl            | 0.008703504 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 0.185       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00389    |
|    std                  | 1.06        |
|    value_loss           | 0.545       |
-----------------------------------------
Eval num_timesteps=446208, episode_reward=-305.81 +/- 30.00
Episode length: 412.00 +/- 35.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 446208   |
---------------------------------
Eval num_timesteps=448200, episode_reward=-143.22 +/- 230.57
Episode length: 350.20 +/- 103.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 448200   |
---------------------------------
Eval num_timesteps=450192, episode_reward=-230.18 +/- 103.75
Episode length: 405.00 +/- 70.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 450192   |
---------------------------------
Eval num_timesteps=452184, episode_reward=-161.41 +/- 209.48
Episode length: 431.80 +/- 70.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 452184   |
---------------------------------
Eval num_timesteps=454176, episode_reward=-241.44 +/- 95.95
Episode length: 426.20 +/- 116.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 426      |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 454176   |
---------------------------------
Eval num_timesteps=456168, episode_reward=-253.75 +/- 54.37
Episode length: 390.80 +/- 107.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 456168   |
---------------------------------
Eval num_timesteps=458160, episode_reward=-213.20 +/- 77.41
Episode length: 383.60 +/- 151.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 458160   |
---------------------------------
Eval num_timesteps=460152, episode_reward=-230.40 +/- 62.59
Episode length: 376.60 +/- 140.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 460152   |
---------------------------------
Eval num_timesteps=462144, episode_reward=-106.55 +/- 200.17
Episode length: 513.20 +/- 136.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 462144   |
---------------------------------
Eval num_timesteps=464136, episode_reward=-271.88 +/- 31.01
Episode length: 505.00 +/- 170.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 464136   |
---------------------------------
Eval num_timesteps=466128, episode_reward=-290.12 +/- 20.77
Episode length: 400.40 +/- 50.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 466128   |
---------------------------------
Eval num_timesteps=468120, episode_reward=-166.76 +/- 131.87
Episode length: 441.20 +/- 97.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 468120   |
---------------------------------
Eval num_timesteps=470112, episode_reward=-145.82 +/- 152.29
Episode length: 422.40 +/- 78.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 470112   |
---------------------------------
Eval num_timesteps=472104, episode_reward=-279.61 +/- 36.74
Episode length: 362.20 +/- 69.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 362      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 472104   |
---------------------------------
Eval num_timesteps=474096, episode_reward=-252.49 +/- 68.27
Episode length: 383.00 +/- 89.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 474096   |
---------------------------------
Eval num_timesteps=476088, episode_reward=-295.61 +/- 17.41
Episode length: 349.40 +/- 111.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 476088   |
---------------------------------
Eval num_timesteps=478080, episode_reward=-240.38 +/- 91.47
Episode length: 345.80 +/- 48.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 346      |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 478080   |
---------------------------------
Eval num_timesteps=480072, episode_reward=-285.33 +/- 32.29
Episode length: 421.00 +/- 188.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 480072   |
---------------------------------
Eval num_timesteps=482064, episode_reward=-274.33 +/- 28.44
Episode length: 484.80 +/- 87.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 482064   |
---------------------------------
Eval num_timesteps=484056, episode_reward=-266.36 +/- 49.54
Episode length: 375.00 +/- 56.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 484056   |
---------------------------------
Eval num_timesteps=486048, episode_reward=-178.19 +/- 135.18
Episode length: 428.00 +/- 123.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 486048   |
---------------------------------
Eval num_timesteps=488040, episode_reward=-267.05 +/- 72.16
Episode length: 442.40 +/- 81.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 442      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 488040   |
---------------------------------
Eval num_timesteps=490032, episode_reward=-260.31 +/- 79.65
Episode length: 429.60 +/- 88.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 490032   |
---------------------------------
Eval num_timesteps=492024, episode_reward=-245.29 +/- 90.89
Episode length: 353.40 +/- 94.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | -245        |
| time/                   |             |
|    total_timesteps      | 492024      |
| train/                  |             |
|    approx_kl            | 0.007533019 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 0.216       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00307    |
|    std                  | 1.06        |
|    value_loss           | 0.612       |
-----------------------------------------
Eval num_timesteps=494016, episode_reward=-305.06 +/- 12.98
Episode length: 355.40 +/- 111.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 494016   |
---------------------------------
Eval num_timesteps=496008, episode_reward=-306.09 +/- 16.90
Episode length: 312.80 +/- 122.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 496008   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-299.60 +/- 25.20
Episode length: 366.40 +/- 53.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 366      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=499992, episode_reward=-309.10 +/- 15.95
Episode length: 369.20 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 499992   |
---------------------------------
Eval num_timesteps=501984, episode_reward=-290.88 +/- 27.69
Episode length: 354.80 +/- 60.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 501984   |
---------------------------------
Eval num_timesteps=503976, episode_reward=-286.94 +/- 27.52
Episode length: 451.80 +/- 87.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 503976   |
---------------------------------
Eval num_timesteps=505968, episode_reward=-301.77 +/- 13.17
Episode length: 399.80 +/- 63.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 505968   |
---------------------------------
Eval num_timesteps=507960, episode_reward=-298.16 +/- 22.97
Episode length: 375.60 +/- 57.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 507960   |
---------------------------------
Eval num_timesteps=509952, episode_reward=-309.26 +/- 23.82
Episode length: 351.60 +/- 50.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=511944, episode_reward=-222.31 +/- 174.69
Episode length: 391.00 +/- 85.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 511944   |
---------------------------------
Eval num_timesteps=513936, episode_reward=-245.22 +/- 82.24
Episode length: 444.40 +/- 140.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 513936   |
---------------------------------
Eval num_timesteps=515928, episode_reward=-273.32 +/- 52.81
Episode length: 470.60 +/- 149.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 515928   |
---------------------------------
Eval num_timesteps=517920, episode_reward=-304.92 +/- 8.95
Episode length: 339.00 +/- 131.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 517920   |
---------------------------------
Eval num_timesteps=519912, episode_reward=-302.43 +/- 18.27
Episode length: 377.20 +/- 105.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 519912   |
---------------------------------
Eval num_timesteps=521904, episode_reward=-288.09 +/- 14.52
Episode length: 322.40 +/- 92.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 521904   |
---------------------------------
Eval num_timesteps=523896, episode_reward=-283.96 +/- 53.65
Episode length: 458.20 +/- 64.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 523896   |
---------------------------------
Eval num_timesteps=525888, episode_reward=-301.05 +/- 23.51
Episode length: 384.40 +/- 107.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 525888   |
---------------------------------
Eval num_timesteps=527880, episode_reward=-139.71 +/- 340.07
Episode length: 379.00 +/- 129.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 527880   |
---------------------------------
Eval num_timesteps=529872, episode_reward=-300.67 +/- 5.62
Episode length: 321.80 +/- 153.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 529872   |
---------------------------------
Eval num_timesteps=531864, episode_reward=-298.49 +/- 18.36
Episode length: 304.80 +/- 95.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 531864   |
---------------------------------
Eval num_timesteps=533856, episode_reward=-297.98 +/- 3.97
Episode length: 284.00 +/- 39.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 533856   |
---------------------------------
Eval num_timesteps=535848, episode_reward=-304.54 +/- 15.41
Episode length: 316.00 +/- 99.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 535848   |
---------------------------------
Eval num_timesteps=537840, episode_reward=-305.53 +/- 19.61
Episode length: 408.20 +/- 47.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 537840   |
---------------------------------
Eval num_timesteps=539832, episode_reward=-296.27 +/- 21.39
Episode length: 314.00 +/- 44.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 539832   |
---------------------------------
Eval num_timesteps=541824, episode_reward=-310.08 +/- 13.82
Episode length: 317.80 +/- 87.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 318         |
|    mean_reward          | -310        |
| time/                   |             |
|    total_timesteps      | 541824      |
| train/                  |             |
|    approx_kl            | 0.006473658 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 0.151       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.000142   |
|    std                  | 1.06        |
|    value_loss           | 0.52        |
-----------------------------------------
Eval num_timesteps=543816, episode_reward=-312.12 +/- 18.43
Episode length: 297.00 +/- 92.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 543816   |
---------------------------------
Eval num_timesteps=545808, episode_reward=-293.78 +/- 30.81
Episode length: 344.00 +/- 86.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 344      |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 545808   |
---------------------------------
Eval num_timesteps=547800, episode_reward=-295.18 +/- 10.37
Episode length: 278.60 +/- 89.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 547800   |
---------------------------------
Eval num_timesteps=549792, episode_reward=-304.71 +/- 17.80
Episode length: 341.80 +/- 85.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 549792   |
---------------------------------
Eval num_timesteps=551784, episode_reward=-303.16 +/- 34.03
Episode length: 396.20 +/- 72.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 551784   |
---------------------------------
Eval num_timesteps=553776, episode_reward=-290.54 +/- 25.24
Episode length: 333.20 +/- 83.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 553776   |
---------------------------------
Eval num_timesteps=555768, episode_reward=-305.90 +/- 10.48
Episode length: 338.00 +/- 38.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 555768   |
---------------------------------
Eval num_timesteps=557760, episode_reward=-303.10 +/- 12.00
Episode length: 302.60 +/- 106.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 557760   |
---------------------------------
Eval num_timesteps=559752, episode_reward=-301.66 +/- 26.32
Episode length: 354.80 +/- 90.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 559752   |
---------------------------------
Eval num_timesteps=561744, episode_reward=-300.04 +/- 11.04
Episode length: 304.40 +/- 89.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 561744   |
---------------------------------
Eval num_timesteps=563736, episode_reward=-290.79 +/- 14.16
Episode length: 240.00 +/- 42.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 563736   |
---------------------------------
Eval num_timesteps=565728, episode_reward=-299.82 +/- 7.83
Episode length: 276.40 +/- 85.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 565728   |
---------------------------------
Eval num_timesteps=567720, episode_reward=-304.27 +/- 16.24
Episode length: 329.80 +/- 115.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 567720   |
---------------------------------
Eval num_timesteps=569712, episode_reward=-293.41 +/- 9.73
Episode length: 333.60 +/- 177.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 569712   |
---------------------------------
Eval num_timesteps=571704, episode_reward=-297.69 +/- 6.13
Episode length: 307.20 +/- 73.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 571704   |
---------------------------------
Eval num_timesteps=573696, episode_reward=-302.16 +/- 12.42
Episode length: 306.40 +/- 112.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 573696   |
---------------------------------
Eval num_timesteps=575688, episode_reward=-302.17 +/- 9.88
Episode length: 342.60 +/- 55.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 575688   |
---------------------------------
Eval num_timesteps=577680, episode_reward=-305.34 +/- 17.34
Episode length: 325.00 +/- 108.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 325      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 577680   |
---------------------------------
Eval num_timesteps=579672, episode_reward=-288.13 +/- 6.98
Episode length: 260.80 +/- 53.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 579672   |
---------------------------------
Eval num_timesteps=581664, episode_reward=-293.10 +/- 48.79
Episode length: 434.20 +/- 103.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 434      |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 581664   |
---------------------------------
Eval num_timesteps=583656, episode_reward=-309.13 +/- 13.98
Episode length: 358.00 +/- 91.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 583656   |
---------------------------------
Eval num_timesteps=585648, episode_reward=-299.79 +/- 7.64
Episode length: 295.40 +/- 105.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 585648   |
---------------------------------
Eval num_timesteps=587640, episode_reward=-297.32 +/- 11.69
Episode length: 276.00 +/- 66.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 587640   |
---------------------------------
Eval num_timesteps=589632, episode_reward=-295.10 +/- 9.84
Episode length: 246.80 +/- 55.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 589632   |
---------------------------------
Eval num_timesteps=591624, episode_reward=-299.63 +/- 19.69
Episode length: 298.80 +/- 134.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 299         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 591624      |
| train/                  |             |
|    approx_kl            | 0.009130489 |
|    clip_fraction        | 0.0747      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 0.17        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.000185   |
|    std                  | 1.06        |
|    value_loss           | 0.517       |
-----------------------------------------
Eval num_timesteps=593616, episode_reward=-283.17 +/- 3.64
Episode length: 239.40 +/- 54.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 593616   |
---------------------------------
Eval num_timesteps=595608, episode_reward=-287.26 +/- 10.90
Episode length: 251.20 +/- 21.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 595608   |
---------------------------------
Eval num_timesteps=597600, episode_reward=-282.82 +/- 12.57
Episode length: 221.40 +/- 10.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 597600   |
---------------------------------
Eval num_timesteps=599592, episode_reward=-283.63 +/- 8.47
Episode length: 250.80 +/- 39.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 599592   |
---------------------------------
Eval num_timesteps=601584, episode_reward=-289.28 +/- 8.95
Episode length: 283.60 +/- 90.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 601584   |
---------------------------------
Eval num_timesteps=603576, episode_reward=-294.49 +/- 16.71
Episode length: 292.40 +/- 88.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 603576   |
---------------------------------
Eval num_timesteps=605568, episode_reward=-281.90 +/- 18.01
Episode length: 255.80 +/- 50.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 605568   |
---------------------------------
Eval num_timesteps=607560, episode_reward=-313.46 +/- 14.51
Episode length: 406.40 +/- 60.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 607560   |
---------------------------------
Eval num_timesteps=609552, episode_reward=-294.75 +/- 25.93
Episode length: 390.60 +/- 93.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 609552   |
---------------------------------
Eval num_timesteps=611544, episode_reward=-301.30 +/- 25.46
Episode length: 318.00 +/- 109.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 611544   |
---------------------------------
Eval num_timesteps=613536, episode_reward=-306.48 +/- 13.00
Episode length: 312.00 +/- 100.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 312      |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 613536   |
---------------------------------
Eval num_timesteps=615528, episode_reward=-287.35 +/- 10.69
Episode length: 243.80 +/- 58.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 615528   |
---------------------------------
Eval num_timesteps=617520, episode_reward=-289.51 +/- 6.64
Episode length: 263.20 +/- 50.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 617520   |
---------------------------------
Eval num_timesteps=619512, episode_reward=-296.42 +/- 3.43
Episode length: 298.40 +/- 121.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 619512   |
---------------------------------
Eval num_timesteps=621504, episode_reward=-287.48 +/- 8.42
Episode length: 213.20 +/- 33.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 621504   |
---------------------------------
Eval num_timesteps=623496, episode_reward=-284.49 +/- 10.12
Episode length: 266.20 +/- 67.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 623496   |
---------------------------------
Eval num_timesteps=625488, episode_reward=-291.72 +/- 9.54
Episode length: 212.60 +/- 57.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 625488   |
---------------------------------
Eval num_timesteps=627480, episode_reward=-267.74 +/- 27.90
Episode length: 266.40 +/- 68.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 627480   |
---------------------------------
Eval num_timesteps=629472, episode_reward=-287.57 +/- 9.92
Episode length: 248.60 +/- 56.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 629472   |
---------------------------------
Eval num_timesteps=631464, episode_reward=-289.04 +/- 7.63
Episode length: 293.00 +/- 98.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 631464   |
---------------------------------
Eval num_timesteps=633456, episode_reward=-301.03 +/- 15.14
Episode length: 296.80 +/- 67.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 633456   |
---------------------------------
Eval num_timesteps=635448, episode_reward=-286.15 +/- 5.10
Episode length: 232.60 +/- 33.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 635448   |
---------------------------------
Eval num_timesteps=637440, episode_reward=-283.75 +/- 6.79
Episode length: 259.40 +/- 42.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 637440   |
---------------------------------
Eval num_timesteps=639432, episode_reward=-290.46 +/- 18.15
Episode length: 364.00 +/- 99.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 364          |
|    mean_reward          | -290         |
| time/                   |              |
|    total_timesteps      | 639432       |
| train/                  |              |
|    approx_kl            | 0.0069418056 |
|    clip_fraction        | 0.0864       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 0.144        |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.06         |
|    value_loss           | 0.485        |
------------------------------------------
Eval num_timesteps=641424, episode_reward=-255.93 +/- 47.78
Episode length: 459.40 +/- 96.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 641424   |
---------------------------------
Eval num_timesteps=643416, episode_reward=-296.55 +/- 26.48
Episode length: 392.60 +/- 72.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 643416   |
---------------------------------
Eval num_timesteps=645408, episode_reward=-285.46 +/- 25.10
Episode length: 415.40 +/- 189.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 645408   |
---------------------------------
Eval num_timesteps=647400, episode_reward=-295.56 +/- 15.87
Episode length: 285.60 +/- 134.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 647400   |
---------------------------------
Eval num_timesteps=649392, episode_reward=-298.68 +/- 17.64
Episode length: 413.40 +/- 99.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 649392   |
---------------------------------
Eval num_timesteps=651384, episode_reward=-294.31 +/- 32.54
Episode length: 383.80 +/- 101.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 651384   |
---------------------------------
Eval num_timesteps=653376, episode_reward=-305.08 +/- 22.75
Episode length: 434.20 +/- 113.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 434      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 653376   |
---------------------------------
Eval num_timesteps=655368, episode_reward=-279.05 +/- 30.44
Episode length: 341.00 +/- 107.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 341      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 655368   |
---------------------------------
Eval num_timesteps=657360, episode_reward=-282.79 +/- 16.19
Episode length: 360.00 +/- 100.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 657360   |
---------------------------------
Eval num_timesteps=659352, episode_reward=-296.95 +/- 35.65
Episode length: 492.80 +/- 96.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 659352   |
---------------------------------
Eval num_timesteps=661344, episode_reward=-256.04 +/- 49.60
Episode length: 322.20 +/- 98.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 661344   |
---------------------------------
Eval num_timesteps=663336, episode_reward=-286.20 +/- 12.43
Episode length: 349.80 +/- 105.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 663336   |
---------------------------------
Eval num_timesteps=665328, episode_reward=-251.56 +/- 64.11
Episode length: 328.40 +/- 141.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 328      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 665328   |
---------------------------------
Eval num_timesteps=667320, episode_reward=-269.76 +/- 24.22
Episode length: 316.60 +/- 51.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 667320   |
---------------------------------
Eval num_timesteps=669312, episode_reward=-286.60 +/- 13.22
Episode length: 241.40 +/- 101.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 669312   |
---------------------------------
Eval num_timesteps=671304, episode_reward=-287.53 +/- 25.75
Episode length: 316.40 +/- 105.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 671304   |
---------------------------------
Eval num_timesteps=673296, episode_reward=-296.58 +/- 15.22
Episode length: 352.00 +/- 100.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 673296   |
---------------------------------
Eval num_timesteps=675288, episode_reward=-269.99 +/- 58.44
Episode length: 364.40 +/- 144.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 364      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 675288   |
---------------------------------
Eval num_timesteps=677280, episode_reward=-266.65 +/- 34.92
Episode length: 337.40 +/- 138.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 677280   |
---------------------------------
Eval num_timesteps=679272, episode_reward=-277.97 +/- 51.84
Episode length: 408.20 +/- 117.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 679272   |
---------------------------------
Eval num_timesteps=681264, episode_reward=-296.30 +/- 16.90
Episode length: 340.40 +/- 101.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 340      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 681264   |
---------------------------------
Eval num_timesteps=683256, episode_reward=-307.23 +/- 20.88
Episode length: 427.20 +/- 97.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 683256   |
---------------------------------
Eval num_timesteps=685248, episode_reward=-293.22 +/- 11.50
Episode length: 333.00 +/- 78.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 685248   |
---------------------------------
Eval num_timesteps=687240, episode_reward=-289.79 +/- 10.92
Episode length: 285.60 +/- 27.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 687240   |
---------------------------------
Eval num_timesteps=689232, episode_reward=-267.41 +/- 36.59
Episode length: 492.60 +/- 210.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | -267        |
| time/                   |             |
|    total_timesteps      | 689232      |
| train/                  |             |
|    approx_kl            | 0.007627567 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.92       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 0.232       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00201    |
|    std                  | 1.06        |
|    value_loss           | 0.62        |
-----------------------------------------
Eval num_timesteps=691224, episode_reward=-213.55 +/- 119.22
Episode length: 318.80 +/- 119.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 691224   |
---------------------------------
Eval num_timesteps=693216, episode_reward=-277.69 +/- 21.20
Episode length: 546.80 +/- 192.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 693216   |
---------------------------------
Eval num_timesteps=695208, episode_reward=-249.94 +/- 40.67
Episode length: 420.40 +/- 144.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 695208   |
---------------------------------
Eval num_timesteps=697200, episode_reward=-241.08 +/- 46.67
Episode length: 415.20 +/- 69.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 697200   |
---------------------------------
Eval num_timesteps=699192, episode_reward=-97.18 +/- 210.61
Episode length: 435.40 +/- 146.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -97.2    |
| time/              |          |
|    total_timesteps | 699192   |
---------------------------------
Eval num_timesteps=701184, episode_reward=-278.96 +/- 39.20
Episode length: 404.40 +/- 92.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 404      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 701184   |
---------------------------------
Eval num_timesteps=703176, episode_reward=-225.29 +/- 38.84
Episode length: 360.80 +/- 67.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 361      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 703176   |
---------------------------------
Eval num_timesteps=705168, episode_reward=-234.45 +/- 83.04
Episode length: 359.80 +/- 113.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 705168   |
---------------------------------
Eval num_timesteps=707160, episode_reward=-262.87 +/- 18.51
Episode length: 411.00 +/- 103.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 707160   |
---------------------------------
Eval num_timesteps=709152, episode_reward=-279.77 +/- 21.31
Episode length: 310.60 +/- 54.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 709152   |
---------------------------------
Eval num_timesteps=711144, episode_reward=-191.60 +/- 146.39
Episode length: 385.40 +/- 153.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 711144   |
---------------------------------
Eval num_timesteps=713136, episode_reward=-140.71 +/- 78.90
Episode length: 472.40 +/- 164.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 713136   |
---------------------------------
Eval num_timesteps=715128, episode_reward=-268.37 +/- 15.12
Episode length: 278.60 +/- 82.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 715128   |
---------------------------------
Eval num_timesteps=717120, episode_reward=-123.69 +/- 223.55
Episode length: 493.80 +/- 153.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 717120   |
---------------------------------
Eval num_timesteps=719112, episode_reward=-237.45 +/- 80.85
Episode length: 369.00 +/- 75.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 719112   |
---------------------------------
Eval num_timesteps=721104, episode_reward=-219.15 +/- 64.20
Episode length: 347.80 +/- 53.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 348      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 721104   |
---------------------------------
Eval num_timesteps=723096, episode_reward=-179.69 +/- 153.86
Episode length: 357.00 +/- 125.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 357      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 723096   |
---------------------------------
Eval num_timesteps=725088, episode_reward=-126.94 +/- 143.41
Episode length: 358.20 +/- 99.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 725088   |
---------------------------------
Eval num_timesteps=727080, episode_reward=-218.58 +/- 87.88
Episode length: 442.20 +/- 128.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 442      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 727080   |
---------------------------------
Eval num_timesteps=729072, episode_reward=-170.48 +/- 163.47
Episode length: 457.20 +/- 70.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 729072   |
---------------------------------
Eval num_timesteps=731064, episode_reward=-220.84 +/- 50.01
Episode length: 520.60 +/- 143.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 731064   |
---------------------------------
Eval num_timesteps=733056, episode_reward=-209.88 +/- 100.06
Episode length: 417.20 +/- 124.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 733056   |
---------------------------------
Eval num_timesteps=735048, episode_reward=-201.36 +/- 135.08
Episode length: 476.40 +/- 105.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 735048   |
---------------------------------
Eval num_timesteps=737040, episode_reward=-156.67 +/- 174.07
Episode length: 430.80 +/- 126.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 737040   |
---------------------------------
Eval num_timesteps=739032, episode_reward=-190.75 +/- 58.95
Episode length: 430.60 +/- 87.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 431         |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 739032      |
| train/                  |             |
|    approx_kl            | 0.006224362 |
|    clip_fraction        | 0.0701      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 0.245       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00182    |
|    std                  | 1.07        |
|    value_loss           | 0.683       |
-----------------------------------------
Eval num_timesteps=741024, episode_reward=14.85 +/- 59.58
Episode length: 402.00 +/- 23.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 741024   |
---------------------------------
New best mean reward!
Eval num_timesteps=743016, episode_reward=-188.54 +/- 73.23
Episode length: 404.40 +/- 68.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 404      |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 743016   |
---------------------------------
Eval num_timesteps=745008, episode_reward=-184.41 +/- 146.42
Episode length: 407.40 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 407      |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 745008   |
---------------------------------
Eval num_timesteps=747000, episode_reward=-157.90 +/- 64.53
Episode length: 333.00 +/- 102.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=748992, episode_reward=-126.38 +/- 125.69
Episode length: 379.60 +/- 45.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 748992   |
---------------------------------
Eval num_timesteps=750984, episode_reward=-100.47 +/- 212.98
Episode length: 354.20 +/- 93.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 354      |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 750984   |
---------------------------------
Eval num_timesteps=752976, episode_reward=-142.57 +/- 97.81
Episode length: 382.60 +/- 77.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 752976   |
---------------------------------
Eval num_timesteps=754968, episode_reward=10.94 +/- 249.60
Episode length: 436.40 +/- 112.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 436      |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 754968   |
---------------------------------
Eval num_timesteps=756960, episode_reward=-135.36 +/- 84.82
Episode length: 337.60 +/- 53.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 756960   |
---------------------------------
Eval num_timesteps=758952, episode_reward=-169.50 +/- 74.10
Episode length: 377.80 +/- 56.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 758952   |
---------------------------------
Eval num_timesteps=760944, episode_reward=110.12 +/- 234.73
Episode length: 464.60 +/- 53.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 465      |
|    mean_reward     | 110      |
| time/              |          |
|    total_timesteps | 760944   |
---------------------------------
New best mean reward!
Eval num_timesteps=762936, episode_reward=-160.48 +/- 90.26
Episode length: 369.80 +/- 48.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 370      |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 762936   |
---------------------------------
Eval num_timesteps=764928, episode_reward=-144.08 +/- 130.06
Episode length: 374.40 +/- 76.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 374      |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 764928   |
---------------------------------
Eval num_timesteps=766920, episode_reward=-57.36 +/- 132.68
Episode length: 404.80 +/- 108.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | -57.4    |
| time/              |          |
|    total_timesteps | 766920   |
---------------------------------
Eval num_timesteps=768912, episode_reward=-158.27 +/- 114.97
Episode length: 541.00 +/- 162.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 768912   |
---------------------------------
Eval num_timesteps=770904, episode_reward=-63.26 +/- 219.16
Episode length: 364.60 +/- 87.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 365      |
|    mean_reward     | -63.3    |
| time/              |          |
|    total_timesteps | 770904   |
---------------------------------
Eval num_timesteps=772896, episode_reward=-74.77 +/- 109.19
Episode length: 369.60 +/- 64.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 370      |
|    mean_reward     | -74.8    |
| time/              |          |
|    total_timesteps | 772896   |
---------------------------------
Eval num_timesteps=774888, episode_reward=-102.76 +/- 157.51
Episode length: 395.60 +/- 49.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 774888   |
---------------------------------
Eval num_timesteps=776880, episode_reward=-98.55 +/- 104.54
Episode length: 336.40 +/- 93.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 336      |
|    mean_reward     | -98.6    |
| time/              |          |
|    total_timesteps | 776880   |
---------------------------------
Eval num_timesteps=778872, episode_reward=-49.20 +/- 175.55
Episode length: 411.00 +/- 79.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -49.2    |
| time/              |          |
|    total_timesteps | 778872   |
---------------------------------
Eval num_timesteps=780864, episode_reward=-84.96 +/- 157.89
Episode length: 369.20 +/- 43.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -85      |
| time/              |          |
|    total_timesteps | 780864   |
---------------------------------
Eval num_timesteps=782856, episode_reward=-141.84 +/- 103.69
Episode length: 394.40 +/- 62.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 394      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 782856   |
---------------------------------
Eval num_timesteps=784848, episode_reward=-164.48 +/- 116.57
Episode length: 350.80 +/- 108.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 784848   |
---------------------------------
Eval num_timesteps=786840, episode_reward=-98.20 +/- 166.15
Episode length: 364.80 +/- 67.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 365        |
|    mean_reward          | -98.2      |
| time/                   |            |
|    total_timesteps      | 786840     |
| train/                  |            |
|    approx_kl            | 0.01107601 |
|    clip_fraction        | 0.0562     |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.95      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.001      |
|    loss                 | 0.34       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.00139   |
|    std                  | 1.07       |
|    value_loss           | 0.89       |
----------------------------------------
Eval num_timesteps=788832, episode_reward=-95.31 +/- 96.62
Episode length: 368.40 +/- 49.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -95.3    |
| time/              |          |
|    total_timesteps | 788832   |
---------------------------------
Eval num_timesteps=790824, episode_reward=-61.41 +/- 229.45
Episode length: 392.80 +/- 48.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | -61.4    |
| time/              |          |
|    total_timesteps | 790824   |
---------------------------------
Eval num_timesteps=792816, episode_reward=-181.69 +/- 73.38
Episode length: 413.60 +/- 67.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 792816   |
---------------------------------
Eval num_timesteps=794808, episode_reward=3.86 +/- 304.65
Episode length: 370.40 +/- 75.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 370      |
|    mean_reward     | 3.86     |
| time/              |          |
|    total_timesteps | 794808   |
---------------------------------
Eval num_timesteps=796800, episode_reward=-88.60 +/- 100.22
Episode length: 336.60 +/- 61.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -88.6    |
| time/              |          |
|    total_timesteps | 796800   |
---------------------------------
Eval num_timesteps=798792, episode_reward=-63.19 +/- 119.84
Episode length: 384.60 +/- 34.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | -63.2    |
| time/              |          |
|    total_timesteps | 798792   |
---------------------------------
Eval num_timesteps=800784, episode_reward=-133.18 +/- 113.85
Episode length: 367.40 +/- 94.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 800784   |
---------------------------------
Eval num_timesteps=802776, episode_reward=-184.21 +/- 77.83
Episode length: 322.60 +/- 71.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 802776   |
---------------------------------
Eval num_timesteps=804768, episode_reward=-48.05 +/- 119.36
Episode length: 351.40 +/- 69.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -48.1    |
| time/              |          |
|    total_timesteps | 804768   |
---------------------------------
Eval num_timesteps=806760, episode_reward=36.61 +/- 235.74
Episode length: 409.00 +/- 60.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 409      |
|    mean_reward     | 36.6     |
| time/              |          |
|    total_timesteps | 806760   |
---------------------------------
Eval num_timesteps=808752, episode_reward=-160.65 +/- 83.74
Episode length: 326.60 +/- 80.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 327      |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 808752   |
---------------------------------
Eval num_timesteps=810744, episode_reward=-143.43 +/- 60.90
Episode length: 363.60 +/- 63.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 364      |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 810744   |
---------------------------------
Eval num_timesteps=812736, episode_reward=-163.79 +/- 115.88
Episode length: 359.80 +/- 74.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 812736   |
---------------------------------
Eval num_timesteps=814728, episode_reward=-241.89 +/- 27.95
Episode length: 353.40 +/- 90.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 353      |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 814728   |
---------------------------------
Eval num_timesteps=816720, episode_reward=-108.92 +/- 196.69
Episode length: 299.20 +/- 77.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 816720   |
---------------------------------
Eval num_timesteps=818712, episode_reward=-76.17 +/- 144.15
Episode length: 369.20 +/- 33.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -76.2    |
| time/              |          |
|    total_timesteps | 818712   |
---------------------------------
Eval num_timesteps=820704, episode_reward=-147.75 +/- 74.66
Episode length: 304.40 +/- 56.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 820704   |
---------------------------------
Eval num_timesteps=822696, episode_reward=-101.36 +/- 63.49
Episode length: 366.80 +/- 110.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 822696   |
---------------------------------
Eval num_timesteps=824688, episode_reward=86.47 +/- 361.78
Episode length: 427.20 +/- 79.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 86.5     |
| time/              |          |
|    total_timesteps | 824688   |
---------------------------------
Eval num_timesteps=826680, episode_reward=58.54 +/- 222.43
Episode length: 392.40 +/- 57.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | 58.5     |
| time/              |          |
|    total_timesteps | 826680   |
---------------------------------
Eval num_timesteps=828672, episode_reward=-48.52 +/- 38.81
Episode length: 358.40 +/- 40.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -48.5    |
| time/              |          |
|    total_timesteps | 828672   |
---------------------------------
Eval num_timesteps=830664, episode_reward=-108.21 +/- 88.78
Episode length: 360.20 +/- 65.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 830664   |
---------------------------------
Eval num_timesteps=832656, episode_reward=29.14 +/- 277.38
Episode length: 422.00 +/- 25.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 832656   |
---------------------------------
Eval num_timesteps=834648, episode_reward=9.71 +/- 162.72
Episode length: 355.80 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 9.71     |
| time/              |          |
|    total_timesteps | 834648   |
---------------------------------
Eval num_timesteps=836640, episode_reward=26.80 +/- 258.16
Episode length: 436.40 +/- 61.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 436         |
|    mean_reward          | 26.8        |
| time/                   |             |
|    total_timesteps      | 836640      |
| train/                  |             |
|    approx_kl            | 0.006069483 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 0.37        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00105    |
|    std                  | 1.08        |
|    value_loss           | 0.932       |
-----------------------------------------
Eval num_timesteps=838632, episode_reward=14.77 +/- 192.88
Episode length: 376.20 +/- 68.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 838632   |
---------------------------------
Eval num_timesteps=840624, episode_reward=-121.72 +/- 196.66
Episode length: 389.60 +/- 53.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 840624   |
---------------------------------
Eval num_timesteps=842616, episode_reward=9.04 +/- 234.16
Episode length: 400.00 +/- 84.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 9.04     |
| time/              |          |
|    total_timesteps | 842616   |
---------------------------------
Eval num_timesteps=844608, episode_reward=-51.68 +/- 247.06
Episode length: 375.00 +/- 101.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | -51.7    |
| time/              |          |
|    total_timesteps | 844608   |
---------------------------------
Eval num_timesteps=846600, episode_reward=-2.76 +/- 245.31
Episode length: 400.00 +/- 121.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -2.76    |
| time/              |          |
|    total_timesteps | 846600   |
---------------------------------
Eval num_timesteps=848592, episode_reward=-155.32 +/- 49.66
Episode length: 367.40 +/- 76.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 848592   |
---------------------------------
Eval num_timesteps=850584, episode_reward=-9.83 +/- 250.24
Episode length: 395.20 +/- 39.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | -9.83    |
| time/              |          |
|    total_timesteps | 850584   |
---------------------------------
Eval num_timesteps=852576, episode_reward=83.72 +/- 221.33
Episode length: 394.80 +/- 44.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | 83.7     |
| time/              |          |
|    total_timesteps | 852576   |
---------------------------------
Eval num_timesteps=854568, episode_reward=-107.70 +/- 146.28
Episode length: 386.40 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 854568   |
---------------------------------
Eval num_timesteps=856560, episode_reward=-126.98 +/- 62.98
Episode length: 366.60 +/- 44.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 856560   |
---------------------------------
Eval num_timesteps=858552, episode_reward=18.71 +/- 344.15
Episode length: 428.40 +/- 111.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 858552   |
---------------------------------
Eval num_timesteps=860544, episode_reward=-110.49 +/- 155.38
Episode length: 431.20 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 860544   |
---------------------------------
Eval num_timesteps=862536, episode_reward=122.12 +/- 227.34
Episode length: 448.20 +/- 49.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 862536   |
---------------------------------
New best mean reward!
Eval num_timesteps=864528, episode_reward=-24.29 +/- 166.10
Episode length: 375.80 +/- 87.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | -24.3    |
| time/              |          |
|    total_timesteps | 864528   |
---------------------------------
Eval num_timesteps=866520, episode_reward=89.27 +/- 316.43
Episode length: 441.20 +/- 111.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | 89.3     |
| time/              |          |
|    total_timesteps | 866520   |
---------------------------------
Eval num_timesteps=868512, episode_reward=-36.65 +/- 160.54
Episode length: 438.00 +/- 80.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | -36.6    |
| time/              |          |
|    total_timesteps | 868512   |
---------------------------------
Eval num_timesteps=870504, episode_reward=-147.95 +/- 117.98
Episode length: 435.20 +/- 39.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 870504   |
---------------------------------
Eval num_timesteps=872496, episode_reward=-218.17 +/- 44.45
Episode length: 389.60 +/- 38.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 872496   |
---------------------------------
Eval num_timesteps=874488, episode_reward=-219.94 +/- 48.73
Episode length: 386.20 +/- 45.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 874488   |
---------------------------------
Eval num_timesteps=876480, episode_reward=137.85 +/- 377.06
Episode length: 464.00 +/- 91.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 876480   |
---------------------------------
New best mean reward!
Eval num_timesteps=878472, episode_reward=-83.96 +/- 136.05
Episode length: 437.60 +/- 21.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | -84      |
| time/              |          |
|    total_timesteps | 878472   |
---------------------------------
Eval num_timesteps=880464, episode_reward=-42.29 +/- 230.63
Episode length: 390.20 +/- 72.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | -42.3    |
| time/              |          |
|    total_timesteps | 880464   |
---------------------------------
Eval num_timesteps=882456, episode_reward=7.96 +/- 151.43
Episode length: 406.80 +/- 62.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 407      |
|    mean_reward     | 7.96     |
| time/              |          |
|    total_timesteps | 882456   |
---------------------------------
Eval num_timesteps=884448, episode_reward=-132.62 +/- 126.49
Episode length: 411.40 +/- 50.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 884448   |
---------------------------------
Eval num_timesteps=886440, episode_reward=130.88 +/- 305.94
Episode length: 455.60 +/- 90.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 886440      |
| train/                  |             |
|    approx_kl            | 0.009193076 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.001       |
|    loss                 | 0.373       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000908   |
|    std                  | 1.08        |
|    value_loss           | 0.947       |
-----------------------------------------
Eval num_timesteps=888432, episode_reward=-211.16 +/- 54.90
Episode length: 540.20 +/- 12.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 888432   |
---------------------------------
Eval num_timesteps=890424, episode_reward=44.97 +/- 346.75
Episode length: 526.20 +/- 66.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 45       |
| time/              |          |
|    total_timesteps | 890424   |
---------------------------------
Eval num_timesteps=892416, episode_reward=-218.58 +/- 88.46
Episode length: 585.20 +/- 26.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 892416   |
---------------------------------
Eval num_timesteps=894408, episode_reward=162.77 +/- 458.46
Episode length: 528.40 +/- 55.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 528      |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 894408   |
---------------------------------
New best mean reward!
Eval num_timesteps=896400, episode_reward=-216.19 +/- 111.10
Episode length: 459.00 +/- 31.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 896400   |
---------------------------------
Eval num_timesteps=898392, episode_reward=206.90 +/- 318.12
Episode length: 476.20 +/- 73.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 898392   |
---------------------------------
New best mean reward!
Eval num_timesteps=900384, episode_reward=15.93 +/- 347.62
Episode length: 535.20 +/- 103.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 900384   |
---------------------------------
Eval num_timesteps=902376, episode_reward=-176.35 +/- 185.25
Episode length: 479.80 +/- 51.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 902376   |
---------------------------------
Eval num_timesteps=904368, episode_reward=-116.68 +/- 134.38
Episode length: 495.40 +/- 82.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 904368   |
---------------------------------
Eval num_timesteps=906360, episode_reward=-254.49 +/- 74.83
Episode length: 455.80 +/- 39.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 906360   |
---------------------------------
Eval num_timesteps=908352, episode_reward=-118.64 +/- 222.77
Episode length: 535.80 +/- 100.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 908352   |
---------------------------------
Eval num_timesteps=910344, episode_reward=-152.21 +/- 108.09
Episode length: 481.80 +/- 67.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 910344   |
---------------------------------
Eval num_timesteps=912336, episode_reward=233.31 +/- 384.35
Episode length: 509.60 +/- 75.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 912336   |
---------------------------------
New best mean reward!
Eval num_timesteps=914328, episode_reward=-242.61 +/- 41.46
Episode length: 480.80 +/- 117.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 914328   |
---------------------------------
Eval num_timesteps=916320, episode_reward=-150.19 +/- 128.01
Episode length: 404.00 +/- 45.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 404      |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 916320   |
---------------------------------
Eval num_timesteps=918312, episode_reward=-110.78 +/- 177.25
Episode length: 440.80 +/- 30.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 918312   |
---------------------------------
Eval num_timesteps=920304, episode_reward=-53.67 +/- 271.23
Episode length: 516.00 +/- 78.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -53.7    |
| time/              |          |
|    total_timesteps | 920304   |
---------------------------------
Eval num_timesteps=922296, episode_reward=44.23 +/- 329.70
Episode length: 474.40 +/- 108.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | 44.2     |
| time/              |          |
|    total_timesteps | 922296   |
---------------------------------
Eval num_timesteps=924288, episode_reward=-36.42 +/- 276.01
Episode length: 555.20 +/- 83.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -36.4    |
| time/              |          |
|    total_timesteps | 924288   |
---------------------------------
Eval num_timesteps=926280, episode_reward=42.58 +/- 152.37
Episode length: 526.00 +/- 87.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 42.6     |
| time/              |          |
|    total_timesteps | 926280   |
---------------------------------
Eval num_timesteps=928272, episode_reward=-130.32 +/- 172.96
Episode length: 491.00 +/- 54.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 928272   |
---------------------------------
Eval num_timesteps=930264, episode_reward=-248.07 +/- 65.63
Episode length: 461.60 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 462      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 930264   |
---------------------------------
Eval num_timesteps=932256, episode_reward=32.70 +/- 344.57
Episode length: 476.20 +/- 60.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 32.7     |
| time/              |          |
|    total_timesteps | 932256   |
---------------------------------
Eval num_timesteps=934248, episode_reward=-194.00 +/- 184.73
Episode length: 555.00 +/- 90.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 555          |
|    mean_reward          | -194         |
| time/                   |              |
|    total_timesteps      | 934248       |
| train/                  |              |
|    approx_kl            | 0.0056882016 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6           |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 0.319        |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.09         |
|    value_loss           | 0.791        |
------------------------------------------
Eval num_timesteps=936240, episode_reward=14.74 +/- 487.00
Episode length: 529.00 +/- 58.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 936240   |
---------------------------------
Eval num_timesteps=938232, episode_reward=-172.45 +/- 168.11
Episode length: 548.20 +/- 124.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 938232   |
---------------------------------
Eval num_timesteps=940224, episode_reward=-229.82 +/- 60.61
Episode length: 535.80 +/- 146.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 940224   |
---------------------------------
Eval num_timesteps=942216, episode_reward=-43.94 +/- 319.18
Episode length: 576.80 +/- 116.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | -43.9    |
| time/              |          |
|    total_timesteps | 942216   |
---------------------------------
Eval num_timesteps=944208, episode_reward=-222.62 +/- 56.15
Episode length: 519.80 +/- 70.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 944208   |
---------------------------------
Eval num_timesteps=946200, episode_reward=-213.86 +/- 69.49
Episode length: 494.40 +/- 139.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 946200   |
---------------------------------
Eval num_timesteps=948192, episode_reward=-222.73 +/- 96.86
Episode length: 493.20 +/- 105.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 948192   |
---------------------------------
Eval num_timesteps=950184, episode_reward=-221.38 +/- 67.99
Episode length: 578.60 +/- 108.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 950184   |
---------------------------------
Eval num_timesteps=952176, episode_reward=-239.37 +/- 34.86
Episode length: 569.00 +/- 47.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 952176   |
---------------------------------
Eval num_timesteps=954168, episode_reward=47.25 +/- 423.30
Episode length: 640.60 +/- 195.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 47.2     |
| time/              |          |
|    total_timesteps | 954168   |
---------------------------------
Eval num_timesteps=956160, episode_reward=-202.28 +/- 78.14
Episode length: 567.20 +/- 81.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 956160   |
---------------------------------
Eval num_timesteps=958152, episode_reward=-86.45 +/- 351.66
Episode length: 566.80 +/- 130.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | -86.5    |
| time/              |          |
|    total_timesteps | 958152   |
---------------------------------
Eval num_timesteps=960144, episode_reward=-242.62 +/- 56.25
Episode length: 572.00 +/- 138.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 960144   |
---------------------------------
Eval num_timesteps=962136, episode_reward=3.23 +/- 507.41
Episode length: 572.20 +/- 50.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 3.23     |
| time/              |          |
|    total_timesteps | 962136   |
---------------------------------
Eval num_timesteps=964128, episode_reward=-155.60 +/- 163.20
Episode length: 556.60 +/- 114.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 964128   |
---------------------------------
Eval num_timesteps=966120, episode_reward=-255.49 +/- 60.88
Episode length: 526.80 +/- 102.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 527      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 966120   |
---------------------------------
Eval num_timesteps=968112, episode_reward=-145.69 +/- 150.24
Episode length: 540.60 +/- 91.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 968112   |
---------------------------------
Eval num_timesteps=970104, episode_reward=-40.45 +/- 270.46
Episode length: 550.80 +/- 117.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | -40.5    |
| time/              |          |
|    total_timesteps | 970104   |
---------------------------------
Eval num_timesteps=972096, episode_reward=209.57 +/- 369.55
Episode length: 557.60 +/- 48.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 972096   |
---------------------------------
Eval num_timesteps=974088, episode_reward=-168.92 +/- 116.07
Episode length: 580.60 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 974088   |
---------------------------------
Eval num_timesteps=976080, episode_reward=-32.31 +/- 365.13
Episode length: 525.60 +/- 38.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | -32.3    |
| time/              |          |
|    total_timesteps | 976080   |
---------------------------------
Eval num_timesteps=978072, episode_reward=-120.96 +/- 118.47
Episode length: 485.60 +/- 36.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 978072   |
---------------------------------
Eval num_timesteps=980064, episode_reward=83.61 +/- 641.84
Episode length: 611.80 +/- 129.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 83.6     |
| time/              |          |
|    total_timesteps | 980064   |
---------------------------------
Eval num_timesteps=982056, episode_reward=-140.27 +/- 175.45
Episode length: 555.40 +/- 134.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 982056   |
---------------------------------
Eval num_timesteps=984048, episode_reward=-116.53 +/- 347.53
Episode length: 665.20 +/- 96.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 665         |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 984048      |
| train/                  |             |
|    approx_kl            | 0.007410969 |
|    clip_fraction        | 0.0546      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.02       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.001       |
|    loss                 | 0.245       |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00182    |
|    std                  | 1.09        |
|    value_loss           | 0.647       |
-----------------------------------------
Eval num_timesteps=986040, episode_reward=-29.83 +/- 380.10
Episode length: 608.60 +/- 78.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | -29.8    |
| time/              |          |
|    total_timesteps | 986040   |
---------------------------------
Eval num_timesteps=988032, episode_reward=-177.04 +/- 164.66
Episode length: 559.60 +/- 133.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 988032   |
---------------------------------
Eval num_timesteps=990024, episode_reward=95.73 +/- 494.47
Episode length: 657.40 +/- 63.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 95.7     |
| time/              |          |
|    total_timesteps | 990024   |
---------------------------------
Eval num_timesteps=992016, episode_reward=-28.57 +/- 370.79
Episode length: 704.40 +/- 66.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | -28.6    |
| time/              |          |
|    total_timesteps | 992016   |
---------------------------------
Eval num_timesteps=994008, episode_reward=-244.44 +/- 51.39
Episode length: 560.80 +/- 107.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 994008   |
---------------------------------
Eval num_timesteps=996000, episode_reward=202.92 +/- 384.10
Episode length: 579.60 +/- 53.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=997992, episode_reward=-243.68 +/- 42.04
Episode length: 754.60 +/- 139.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 997992   |
---------------------------------
Eval num_timesteps=999984, episode_reward=-210.03 +/- 51.64
Episode length: 558.40 +/- 56.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 999984   |
---------------------------------
Eval num_timesteps=1001976, episode_reward=-245.23 +/- 55.28
Episode length: 574.40 +/- 65.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 1001976  |
---------------------------------
Eval num_timesteps=1003968, episode_reward=11.01 +/- 434.36
Episode length: 579.40 +/- 94.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 1003968  |
---------------------------------
Eval num_timesteps=1005960, episode_reward=-218.97 +/- 127.88
Episode length: 653.80 +/- 98.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 1005960  |
---------------------------------
Eval num_timesteps=1007952, episode_reward=-222.97 +/- 55.87
Episode length: 706.20 +/- 156.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 1007952  |
---------------------------------
Eval num_timesteps=1009944, episode_reward=-202.79 +/- 31.05
Episode length: 584.40 +/- 69.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1009944  |
---------------------------------
Eval num_timesteps=1011936, episode_reward=-222.18 +/- 141.28
Episode length: 585.20 +/- 80.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 1011936  |
---------------------------------
Eval num_timesteps=1013928, episode_reward=84.37 +/- 639.29
Episode length: 571.00 +/- 114.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 84.4     |
| time/              |          |
|    total_timesteps | 1013928  |
---------------------------------
Eval num_timesteps=1015920, episode_reward=-279.39 +/- 27.71
Episode length: 538.60 +/- 100.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 1015920  |
---------------------------------
Eval num_timesteps=1017912, episode_reward=-6.61 +/- 451.83
Episode length: 649.20 +/- 182.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | -6.61    |
| time/              |          |
|    total_timesteps | 1017912  |
---------------------------------
Eval num_timesteps=1019904, episode_reward=-208.84 +/- 156.72
Episode length: 618.40 +/- 229.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 1019904  |
---------------------------------
Eval num_timesteps=1021896, episode_reward=-112.73 +/- 268.51
Episode length: 562.00 +/- 60.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 1021896  |
---------------------------------
Eval num_timesteps=1023888, episode_reward=42.74 +/- 376.81
Episode length: 559.40 +/- 93.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 42.7     |
| time/              |          |
|    total_timesteps | 1023888  |
---------------------------------
Eval num_timesteps=1025880, episode_reward=-77.62 +/- 326.51
Episode length: 657.60 +/- 163.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | -77.6    |
| time/              |          |
|    total_timesteps | 1025880  |
---------------------------------
Eval num_timesteps=1027872, episode_reward=-62.00 +/- 427.11
Episode length: 617.40 +/- 47.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | -62      |
| time/              |          |
|    total_timesteps | 1027872  |
---------------------------------
Eval num_timesteps=1029864, episode_reward=-255.77 +/- 65.01
Episode length: 666.60 +/- 216.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 1029864  |
---------------------------------
Eval num_timesteps=1031856, episode_reward=-192.82 +/- 164.92
Episode length: 601.60 +/- 139.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 1031856  |
---------------------------------
Eval num_timesteps=1033848, episode_reward=-294.79 +/- 27.42
Episode length: 442.60 +/- 58.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | -295         |
| time/                   |              |
|    total_timesteps      | 1033848      |
| train/                  |              |
|    approx_kl            | 0.0041577294 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.04        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 0.216        |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000969    |
|    std                  | 1.1          |
|    value_loss           | 0.614        |
------------------------------------------
Eval num_timesteps=1035840, episode_reward=-167.73 +/- 165.43
Episode length: 665.20 +/- 114.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 1035840  |
---------------------------------
Eval num_timesteps=1037832, episode_reward=45.87 +/- 672.91
Episode length: 561.40 +/- 87.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 45.9     |
| time/              |          |
|    total_timesteps | 1037832  |
---------------------------------
Eval num_timesteps=1039824, episode_reward=-22.56 +/- 527.56
Episode length: 554.80 +/- 100.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -22.6    |
| time/              |          |
|    total_timesteps | 1039824  |
---------------------------------
Eval num_timesteps=1041816, episode_reward=-171.13 +/- 164.91
Episode length: 673.40 +/- 144.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 1041816  |
---------------------------------
Eval num_timesteps=1043808, episode_reward=-187.95 +/- 163.20
Episode length: 582.60 +/- 175.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 1043808  |
---------------------------------
Eval num_timesteps=1045800, episode_reward=-184.34 +/- 258.45
Episode length: 509.60 +/- 108.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 1045800  |
---------------------------------
Eval num_timesteps=1047792, episode_reward=-275.20 +/- 35.38
Episode length: 639.00 +/- 51.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1047792  |
---------------------------------
Eval num_timesteps=1049784, episode_reward=-252.73 +/- 79.67
Episode length: 662.00 +/- 227.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 1049784  |
---------------------------------
Eval num_timesteps=1051776, episode_reward=-65.29 +/- 464.39
Episode length: 588.40 +/- 162.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | -65.3    |
| time/              |          |
|    total_timesteps | 1051776  |
---------------------------------
Eval num_timesteps=1053768, episode_reward=-250.95 +/- 78.35
Episode length: 561.00 +/- 152.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 1053768  |
---------------------------------
Eval num_timesteps=1055760, episode_reward=-1.01 +/- 533.54
Episode length: 551.20 +/- 37.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | -1.01    |
| time/              |          |
|    total_timesteps | 1055760  |
---------------------------------
Eval num_timesteps=1057752, episode_reward=-290.98 +/- 35.71
Episode length: 563.40 +/- 135.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 1057752  |
---------------------------------
Eval num_timesteps=1059744, episode_reward=-293.69 +/- 20.03
Episode length: 537.20 +/- 98.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 1059744  |
---------------------------------
Eval num_timesteps=1061736, episode_reward=9.69 +/- 358.58
Episode length: 650.20 +/- 207.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 9.69     |
| time/              |          |
|    total_timesteps | 1061736  |
---------------------------------
Eval num_timesteps=1063728, episode_reward=-259.33 +/- 38.45
Episode length: 630.80 +/- 230.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 1063728  |
---------------------------------
Eval num_timesteps=1065720, episode_reward=-146.28 +/- 347.61
Episode length: 544.20 +/- 44.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 1065720  |
---------------------------------
Eval num_timesteps=1067712, episode_reward=82.78 +/- 454.33
Episode length: 665.20 +/- 87.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 82.8     |
| time/              |          |
|    total_timesteps | 1067712  |
---------------------------------
Eval num_timesteps=1069704, episode_reward=-225.28 +/- 97.95
Episode length: 544.00 +/- 98.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1069704  |
---------------------------------
Eval num_timesteps=1071696, episode_reward=-238.24 +/- 61.95
Episode length: 645.00 +/- 210.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1071696  |
---------------------------------
Eval num_timesteps=1073688, episode_reward=-73.22 +/- 348.88
Episode length: 660.60 +/- 117.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | -73.2    |
| time/              |          |
|    total_timesteps | 1073688  |
---------------------------------
Eval num_timesteps=1075680, episode_reward=-225.14 +/- 95.94
Episode length: 555.00 +/- 65.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1075680  |
---------------------------------
Eval num_timesteps=1077672, episode_reward=167.90 +/- 670.69
Episode length: 564.80 +/- 118.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 1077672  |
---------------------------------
Eval num_timesteps=1079664, episode_reward=-272.14 +/- 60.35
Episode length: 556.40 +/- 157.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 1079664  |
---------------------------------
Eval num_timesteps=1081656, episode_reward=-234.46 +/- 49.46
Episode length: 660.00 +/- 162.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 660          |
|    mean_reward          | -234         |
| time/                   |              |
|    total_timesteps      | 1081656      |
| train/                  |              |
|    approx_kl            | 0.0026157852 |
|    clip_fraction        | 0.0628       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 0.182        |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000992    |
|    std                  | 1.11         |
|    value_loss           | 0.518        |
------------------------------------------
Eval num_timesteps=1083648, episode_reward=-268.13 +/- 44.30
Episode length: 628.00 +/- 139.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 1083648  |
---------------------------------
Eval num_timesteps=1085640, episode_reward=208.38 +/- 841.87
Episode length: 683.40 +/- 151.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 1085640  |
---------------------------------
Eval num_timesteps=1087632, episode_reward=-261.19 +/- 58.09
Episode length: 552.00 +/- 93.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 1087632  |
---------------------------------
Eval num_timesteps=1089624, episode_reward=-295.44 +/- 30.04
Episode length: 495.80 +/- 76.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 1089624  |
---------------------------------
Eval num_timesteps=1091616, episode_reward=-251.68 +/- 81.24
Episode length: 582.20 +/- 82.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1091616  |
---------------------------------
Eval num_timesteps=1093608, episode_reward=-172.96 +/- 234.27
Episode length: 525.20 +/- 61.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1093608  |
---------------------------------
Eval num_timesteps=1095600, episode_reward=-159.65 +/- 303.31
Episode length: 494.60 +/- 85.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 1095600  |
---------------------------------
Eval num_timesteps=1097592, episode_reward=-262.59 +/- 69.32
Episode length: 698.20 +/- 226.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 1097592  |
---------------------------------
Eval num_timesteps=1099584, episode_reward=-304.70 +/- 18.13
Episode length: 664.80 +/- 126.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 1099584  |
---------------------------------
Eval num_timesteps=1101576, episode_reward=-135.53 +/- 267.35
Episode length: 605.00 +/- 91.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 1101576  |
---------------------------------
Eval num_timesteps=1103568, episode_reward=-251.65 +/- 62.18
Episode length: 599.20 +/- 105.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1103568  |
---------------------------------
Eval num_timesteps=1105560, episode_reward=-283.58 +/- 38.80
Episode length: 623.40 +/- 136.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 1105560  |
---------------------------------
Eval num_timesteps=1107552, episode_reward=-128.71 +/- 321.98
Episode length: 613.20 +/- 115.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 1107552  |
---------------------------------
Eval num_timesteps=1109544, episode_reward=-256.83 +/- 47.16
Episode length: 560.00 +/- 79.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 1109544  |
---------------------------------
Eval num_timesteps=1111536, episode_reward=-252.34 +/- 105.11
Episode length: 605.40 +/- 108.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1111536  |
---------------------------------
Eval num_timesteps=1113528, episode_reward=-228.90 +/- 57.82
Episode length: 621.60 +/- 113.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1113528  |
---------------------------------
Eval num_timesteps=1115520, episode_reward=-303.56 +/- 23.58
Episode length: 566.80 +/- 71.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1115520  |
---------------------------------
Eval num_timesteps=1117512, episode_reward=-77.55 +/- 343.87
Episode length: 683.60 +/- 123.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | -77.5    |
| time/              |          |
|    total_timesteps | 1117512  |
---------------------------------
Eval num_timesteps=1119504, episode_reward=-216.08 +/- 86.86
Episode length: 816.60 +/- 236.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 1119504  |
---------------------------------
Eval num_timesteps=1121496, episode_reward=-252.24 +/- 63.89
Episode length: 587.40 +/- 165.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1121496  |
---------------------------------
Eval num_timesteps=1123488, episode_reward=-280.64 +/- 20.91
Episode length: 727.60 +/- 142.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 1123488  |
---------------------------------
Eval num_timesteps=1125480, episode_reward=-242.10 +/- 129.40
Episode length: 591.80 +/- 78.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1125480  |
---------------------------------
Eval num_timesteps=1127472, episode_reward=-269.23 +/- 61.03
Episode length: 595.00 +/- 79.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 1127472  |
---------------------------------
Eval num_timesteps=1129464, episode_reward=-172.16 +/- 189.71
Episode length: 649.00 +/- 230.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 1129464  |
---------------------------------
Eval num_timesteps=1131456, episode_reward=-264.18 +/- 37.96
Episode length: 668.40 +/- 113.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 668          |
|    mean_reward          | -264         |
| time/                   |              |
|    total_timesteps      | 1131456      |
| train/                  |              |
|    approx_kl            | 0.0024553323 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 0.109        |
|    n_updates            | 230          |
|    policy_gradient_loss | 4.41e-05     |
|    std                  | 1.11         |
|    value_loss           | 0.359        |
------------------------------------------
Eval num_timesteps=1133448, episode_reward=-149.18 +/- 259.77
Episode length: 694.00 +/- 158.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 1133448  |
---------------------------------
Eval num_timesteps=1135440, episode_reward=-300.07 +/- 28.01
Episode length: 510.20 +/- 71.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1135440  |
---------------------------------
Eval num_timesteps=1137432, episode_reward=113.25 +/- 754.70
Episode length: 644.40 +/- 100.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 1137432  |
---------------------------------
Eval num_timesteps=1139424, episode_reward=-190.80 +/- 149.82
Episode length: 703.60 +/- 220.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 1139424  |
---------------------------------
Eval num_timesteps=1141416, episode_reward=20.06 +/- 558.73
Episode length: 603.00 +/- 123.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 1141416  |
---------------------------------
Eval num_timesteps=1143408, episode_reward=-300.87 +/- 26.35
Episode length: 521.40 +/- 90.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 1143408  |
---------------------------------
Eval num_timesteps=1145400, episode_reward=-295.40 +/- 27.07
Episode length: 538.20 +/- 83.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 1145400  |
---------------------------------
Eval num_timesteps=1147392, episode_reward=-49.39 +/- 377.33
Episode length: 614.00 +/- 103.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | -49.4    |
| time/              |          |
|    total_timesteps | 1147392  |
---------------------------------
Eval num_timesteps=1149384, episode_reward=-188.76 +/- 114.91
Episode length: 609.80 +/- 126.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 1149384  |
---------------------------------
Eval num_timesteps=1151376, episode_reward=-252.87 +/- 65.64
Episode length: 667.00 +/- 150.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 1151376  |
---------------------------------
Eval num_timesteps=1153368, episode_reward=-246.75 +/- 57.58
Episode length: 620.40 +/- 91.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 1153368  |
---------------------------------
Eval num_timesteps=1155360, episode_reward=-297.28 +/- 29.28
Episode length: 543.60 +/- 80.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 1155360  |
---------------------------------
Eval num_timesteps=1157352, episode_reward=133.62 +/- 788.40
Episode length: 664.60 +/- 166.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 1157352  |
---------------------------------
Eval num_timesteps=1159344, episode_reward=-160.01 +/- 214.13
Episode length: 581.40 +/- 102.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 1159344  |
---------------------------------
Eval num_timesteps=1161336, episode_reward=85.06 +/- 644.83
Episode length: 605.00 +/- 98.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 85.1     |
| time/              |          |
|    total_timesteps | 1161336  |
---------------------------------
Eval num_timesteps=1163328, episode_reward=-176.72 +/- 110.46
Episode length: 637.60 +/- 124.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 1163328  |
---------------------------------
Eval num_timesteps=1165320, episode_reward=-278.19 +/- 54.17
Episode length: 619.20 +/- 52.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 1165320  |
---------------------------------
Eval num_timesteps=1167312, episode_reward=-279.81 +/- 32.81
Episode length: 554.60 +/- 103.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 1167312  |
---------------------------------
Eval num_timesteps=1169304, episode_reward=-62.52 +/- 428.42
Episode length: 699.40 +/- 228.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | -62.5    |
| time/              |          |
|    total_timesteps | 1169304  |
---------------------------------
Eval num_timesteps=1171296, episode_reward=-219.82 +/- 119.45
Episode length: 652.80 +/- 122.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 1171296  |
---------------------------------
Eval num_timesteps=1173288, episode_reward=-214.92 +/- 116.94
Episode length: 651.20 +/- 112.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 1173288  |
---------------------------------
Eval num_timesteps=1175280, episode_reward=-275.42 +/- 32.97
Episode length: 580.80 +/- 88.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1175280  |
---------------------------------
Eval num_timesteps=1177272, episode_reward=-90.28 +/- 222.48
Episode length: 641.20 +/- 172.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | -90.3    |
| time/              |          |
|    total_timesteps | 1177272  |
---------------------------------
Eval num_timesteps=1179264, episode_reward=11.19 +/- 461.20
Episode length: 596.20 +/- 180.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 1179264  |
---------------------------------
Eval num_timesteps=1181256, episode_reward=-246.15 +/- 70.40
Episode length: 575.80 +/- 124.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 576          |
|    mean_reward          | -246         |
| time/                   |              |
|    total_timesteps      | 1181256      |
| train/                  |              |
|    approx_kl            | 0.0049113873 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 0.119        |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000531    |
|    std                  | 1.11         |
|    value_loss           | 0.379        |
------------------------------------------
Eval num_timesteps=1183248, episode_reward=-315.34 +/- 10.74
Episode length: 468.60 +/- 115.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1183248  |
---------------------------------
Eval num_timesteps=1185240, episode_reward=-274.75 +/- 35.85
Episode length: 555.20 +/- 84.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1185240  |
---------------------------------
Eval num_timesteps=1187232, episode_reward=-277.38 +/- 42.35
Episode length: 526.40 +/- 128.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 1187232  |
---------------------------------
Eval num_timesteps=1189224, episode_reward=-307.01 +/- 6.30
Episode length: 515.60 +/- 101.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 1189224  |
---------------------------------
Eval num_timesteps=1191216, episode_reward=-241.69 +/- 99.41
Episode length: 534.20 +/- 112.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1191216  |
---------------------------------
Eval num_timesteps=1193208, episode_reward=-303.64 +/- 13.64
Episode length: 457.40 +/- 50.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1193208  |
---------------------------------
Eval num_timesteps=1195200, episode_reward=-327.93 +/- 11.91
Episode length: 434.80 +/- 41.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 1195200  |
---------------------------------
Eval num_timesteps=1197192, episode_reward=-179.26 +/- 166.76
Episode length: 624.00 +/- 122.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 1197192  |
---------------------------------
Eval num_timesteps=1199184, episode_reward=-285.07 +/- 35.71
Episode length: 545.20 +/- 156.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 1199184  |
---------------------------------
Eval num_timesteps=1201176, episode_reward=-279.18 +/- 45.12
Episode length: 500.60 +/- 71.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 1201176  |
---------------------------------
Eval num_timesteps=1203168, episode_reward=-254.64 +/- 68.99
Episode length: 508.20 +/- 70.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1203168  |
---------------------------------
Eval num_timesteps=1205160, episode_reward=-301.09 +/- 17.57
Episode length: 465.00 +/- 130.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 465      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 1205160  |
---------------------------------
Eval num_timesteps=1207152, episode_reward=-237.65 +/- 85.28
Episode length: 563.40 +/- 99.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1207152  |
---------------------------------
Eval num_timesteps=1209144, episode_reward=-236.46 +/- 99.99
Episode length: 496.60 +/- 76.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1209144  |
---------------------------------
Eval num_timesteps=1211136, episode_reward=-43.71 +/- 455.19
Episode length: 544.40 +/- 64.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | -43.7    |
| time/              |          |
|    total_timesteps | 1211136  |
---------------------------------
Eval num_timesteps=1213128, episode_reward=-236.88 +/- 91.40
Episode length: 492.80 +/- 72.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 1213128  |
---------------------------------
Eval num_timesteps=1215120, episode_reward=-227.19 +/- 121.41
Episode length: 537.00 +/- 130.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 1215120  |
---------------------------------
Eval num_timesteps=1217112, episode_reward=-299.18 +/- 34.09
Episode length: 469.60 +/- 109.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 1217112  |
---------------------------------
Eval num_timesteps=1219104, episode_reward=-304.25 +/- 23.02
Episode length: 471.20 +/- 64.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1219104  |
---------------------------------
Eval num_timesteps=1221096, episode_reward=-166.13 +/- 242.66
Episode length: 489.40 +/- 39.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 1221096  |
---------------------------------
Eval num_timesteps=1223088, episode_reward=-251.90 +/- 72.39
Episode length: 574.40 +/- 45.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1223088  |
---------------------------------
Eval num_timesteps=1225080, episode_reward=-165.19 +/- 231.89
Episode length: 552.80 +/- 20.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 1225080  |
---------------------------------
Eval num_timesteps=1227072, episode_reward=-297.55 +/- 32.83
Episode length: 450.40 +/- 60.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 1227072  |
---------------------------------
Eval num_timesteps=1229064, episode_reward=-272.52 +/- 30.54
Episode length: 562.60 +/- 117.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 563          |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 1229064      |
| train/                  |              |
|    approx_kl            | 0.0056145024 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0834       |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.12         |
|    value_loss           | 0.306        |
------------------------------------------
Eval num_timesteps=1231056, episode_reward=-281.29 +/- 42.08
Episode length: 496.20 +/- 140.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 1231056  |
---------------------------------
Eval num_timesteps=1233048, episode_reward=-292.37 +/- 31.35
Episode length: 497.60 +/- 104.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 1233048  |
---------------------------------
Eval num_timesteps=1235040, episode_reward=-312.26 +/- 17.29
Episode length: 421.00 +/- 48.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 1235040  |
---------------------------------
Eval num_timesteps=1237032, episode_reward=-233.25 +/- 126.65
Episode length: 505.00 +/- 100.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 1237032  |
---------------------------------
Eval num_timesteps=1239024, episode_reward=-173.64 +/- 235.56
Episode length: 545.60 +/- 176.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 1239024  |
---------------------------------
Eval num_timesteps=1241016, episode_reward=-301.08 +/- 7.27
Episode length: 461.20 +/- 86.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 1241016  |
---------------------------------
Eval num_timesteps=1243008, episode_reward=-304.27 +/- 23.39
Episode length: 444.40 +/- 76.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1243008  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=-213.61 +/- 107.26
Episode length: 549.80 +/- 142.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
Eval num_timesteps=1246992, episode_reward=-179.07 +/- 187.09
Episode length: 556.20 +/- 128.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 1246992  |
---------------------------------
Eval num_timesteps=1248984, episode_reward=-287.00 +/- 14.80
Episode length: 480.60 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 1248984  |
---------------------------------
Eval num_timesteps=1250976, episode_reward=-167.00 +/- 199.24
Episode length: 582.60 +/- 133.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1250976  |
---------------------------------
Eval num_timesteps=1252968, episode_reward=-268.39 +/- 32.89
Episode length: 583.40 +/- 82.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 1252968  |
---------------------------------
Eval num_timesteps=1254960, episode_reward=-116.17 +/- 371.19
Episode length: 497.80 +/- 112.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 1254960  |
---------------------------------
Eval num_timesteps=1256952, episode_reward=-289.00 +/- 20.57
Episode length: 512.80 +/- 138.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 1256952  |
---------------------------------
Eval num_timesteps=1258944, episode_reward=-279.99 +/- 55.24
Episode length: 537.20 +/- 210.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 1258944  |
---------------------------------
Eval num_timesteps=1260936, episode_reward=210.04 +/- 607.14
Episode length: 621.20 +/- 151.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 1260936  |
---------------------------------
Eval num_timesteps=1262928, episode_reward=-271.24 +/- 21.85
Episode length: 529.00 +/- 70.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 1262928  |
---------------------------------
Eval num_timesteps=1264920, episode_reward=-308.04 +/- 18.41
Episode length: 479.20 +/- 20.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 479      |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 1264920  |
---------------------------------
Eval num_timesteps=1266912, episode_reward=-86.15 +/- 238.05
Episode length: 619.20 +/- 115.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | -86.1    |
| time/              |          |
|    total_timesteps | 1266912  |
---------------------------------
Eval num_timesteps=1268904, episode_reward=-52.05 +/- 476.37
Episode length: 502.20 +/- 58.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 502      |
|    mean_reward     | -52      |
| time/              |          |
|    total_timesteps | 1268904  |
---------------------------------
Eval num_timesteps=1270896, episode_reward=-140.69 +/- 314.50
Episode length: 470.60 +/- 181.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 1270896  |
---------------------------------
Eval num_timesteps=1272888, episode_reward=-214.40 +/- 126.95
Episode length: 596.00 +/- 150.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 1272888  |
---------------------------------
Eval num_timesteps=1274880, episode_reward=-267.57 +/- 37.93
Episode length: 537.00 +/- 63.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 1274880  |
---------------------------------
Eval num_timesteps=1276872, episode_reward=-287.95 +/- 20.15
Episode length: 476.00 +/- 88.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 1276872  |
---------------------------------
Eval num_timesteps=1278864, episode_reward=-11.08 +/- 510.10
Episode length: 677.60 +/- 217.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 678         |
|    mean_reward          | -11.1       |
| time/                   |             |
|    total_timesteps      | 1278864     |
| train/                  |             |
|    approx_kl            | 0.005577768 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0891      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00154    |
|    std                  | 1.12        |
|    value_loss           | 0.321       |
-----------------------------------------
Eval num_timesteps=1280856, episode_reward=-140.49 +/- 265.21
Episode length: 624.80 +/- 134.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 1280856  |
---------------------------------
Eval num_timesteps=1282848, episode_reward=-32.67 +/- 500.47
Episode length: 582.60 +/- 152.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | -32.7    |
| time/              |          |
|    total_timesteps | 1282848  |
---------------------------------
Eval num_timesteps=1284840, episode_reward=-33.89 +/- 260.95
Episode length: 729.60 +/- 137.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | -33.9    |
| time/              |          |
|    total_timesteps | 1284840  |
---------------------------------
Eval num_timesteps=1286832, episode_reward=-284.07 +/- 53.95
Episode length: 598.40 +/- 141.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 1286832  |
---------------------------------
Eval num_timesteps=1288824, episode_reward=-153.23 +/- 259.24
Episode length: 538.00 +/- 158.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 1288824  |
---------------------------------
Eval num_timesteps=1290816, episode_reward=-259.52 +/- 69.51
Episode length: 586.20 +/- 158.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 1290816  |
---------------------------------
Eval num_timesteps=1292808, episode_reward=19.31 +/- 519.46
Episode length: 555.00 +/- 128.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 1292808  |
---------------------------------
Eval num_timesteps=1294800, episode_reward=-274.63 +/- 22.07
Episode length: 489.20 +/- 47.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1294800  |
---------------------------------
Eval num_timesteps=1296792, episode_reward=2.81 +/- 530.58
Episode length: 589.40 +/- 127.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 589      |
|    mean_reward     | 2.81     |
| time/              |          |
|    total_timesteps | 1296792  |
---------------------------------
Eval num_timesteps=1298784, episode_reward=-248.33 +/- 52.60
Episode length: 623.40 +/- 85.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1298784  |
---------------------------------
Eval num_timesteps=1300776, episode_reward=-79.61 +/- 416.31
Episode length: 566.80 +/- 88.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | -79.6    |
| time/              |          |
|    total_timesteps | 1300776  |
---------------------------------
Eval num_timesteps=1302768, episode_reward=-236.39 +/- 78.43
Episode length: 587.20 +/- 137.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1302768  |
---------------------------------
Eval num_timesteps=1304760, episode_reward=-223.67 +/- 122.75
Episode length: 514.20 +/- 127.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 1304760  |
---------------------------------
Eval num_timesteps=1306752, episode_reward=-254.99 +/- 26.38
Episode length: 631.40 +/- 105.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1306752  |
---------------------------------
Eval num_timesteps=1308744, episode_reward=-276.24 +/- 36.58
Episode length: 567.00 +/- 150.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 1308744  |
---------------------------------
Eval num_timesteps=1310736, episode_reward=-264.48 +/- 50.77
Episode length: 575.00 +/- 131.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 1310736  |
---------------------------------
Eval num_timesteps=1312728, episode_reward=-282.55 +/- 32.22
Episode length: 592.60 +/- 84.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 1312728  |
---------------------------------
Eval num_timesteps=1314720, episode_reward=-299.55 +/- 17.04
Episode length: 465.60 +/- 46.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1314720  |
---------------------------------
Eval num_timesteps=1316712, episode_reward=-258.64 +/- 17.10
Episode length: 546.60 +/- 51.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 1316712  |
---------------------------------
Eval num_timesteps=1318704, episode_reward=-295.93 +/- 27.10
Episode length: 473.60 +/- 80.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 1318704  |
---------------------------------
Eval num_timesteps=1320696, episode_reward=-279.20 +/- 68.58
Episode length: 564.20 +/- 134.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 1320696  |
---------------------------------
Eval num_timesteps=1322688, episode_reward=108.36 +/- 528.50
Episode length: 666.00 +/- 136.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 1322688  |
---------------------------------
Eval num_timesteps=1324680, episode_reward=-237.22 +/- 132.05
Episode length: 521.60 +/- 84.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 1324680  |
---------------------------------
Eval num_timesteps=1326672, episode_reward=-244.50 +/- 62.15
Episode length: 494.80 +/- 95.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 1326672  |
---------------------------------
Eval num_timesteps=1328664, episode_reward=83.95 +/- 505.03
Episode length: 614.40 +/- 125.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 614          |
|    mean_reward          | 84           |
| time/                   |              |
|    total_timesteps      | 1328664      |
| train/                  |              |
|    approx_kl            | 0.0046932437 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.14        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 0.101        |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.13         |
|    value_loss           | 0.343        |
------------------------------------------
Eval num_timesteps=1330656, episode_reward=29.00 +/- 448.39
Episode length: 619.20 +/- 134.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 1330656  |
---------------------------------
Eval num_timesteps=1332648, episode_reward=-247.19 +/- 27.40
Episode length: 577.00 +/- 42.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 1332648  |
---------------------------------
Eval num_timesteps=1334640, episode_reward=-86.84 +/- 219.94
Episode length: 675.00 +/- 130.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | -86.8    |
| time/              |          |
|    total_timesteps | 1334640  |
---------------------------------
Eval num_timesteps=1336632, episode_reward=-276.84 +/- 60.93
Episode length: 572.60 +/- 171.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 1336632  |
---------------------------------
Eval num_timesteps=1338624, episode_reward=158.35 +/- 532.34
Episode length: 551.60 +/- 67.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 1338624  |
---------------------------------
Eval num_timesteps=1340616, episode_reward=-25.72 +/- 414.93
Episode length: 555.20 +/- 109.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -25.7    |
| time/              |          |
|    total_timesteps | 1340616  |
---------------------------------
Eval num_timesteps=1342608, episode_reward=-268.90 +/- 42.53
Episode length: 578.80 +/- 157.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 1342608  |
---------------------------------
Eval num_timesteps=1344600, episode_reward=151.14 +/- 698.26
Episode length: 688.60 +/- 127.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 1344600  |
---------------------------------
Eval num_timesteps=1346592, episode_reward=-208.18 +/- 82.39
Episode length: 568.00 +/- 106.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 1346592  |
---------------------------------
Eval num_timesteps=1348584, episode_reward=-275.09 +/- 56.97
Episode length: 558.60 +/- 107.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1348584  |
---------------------------------
Eval num_timesteps=1350576, episode_reward=-166.41 +/- 256.28
Episode length: 515.60 +/- 76.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 1350576  |
---------------------------------
Eval num_timesteps=1352568, episode_reward=-287.49 +/- 24.08
Episode length: 514.20 +/- 35.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 1352568  |
---------------------------------
Eval num_timesteps=1354560, episode_reward=-166.75 +/- 100.65
Episode length: 717.20 +/- 138.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1354560  |
---------------------------------
Eval num_timesteps=1356552, episode_reward=-275.39 +/- 36.23
Episode length: 601.40 +/- 122.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 1356552  |
---------------------------------
Eval num_timesteps=1358544, episode_reward=70.22 +/- 521.08
Episode length: 636.00 +/- 148.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 70.2     |
| time/              |          |
|    total_timesteps | 1358544  |
---------------------------------
Eval num_timesteps=1360536, episode_reward=-184.64 +/- 131.62
Episode length: 574.80 +/- 117.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 1360536  |
---------------------------------
Eval num_timesteps=1362528, episode_reward=346.22 +/- 574.34
Episode length: 621.20 +/- 106.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 1362528  |
---------------------------------
New best mean reward!
Eval num_timesteps=1364520, episode_reward=-303.88 +/- 15.49
Episode length: 481.40 +/- 93.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1364520  |
---------------------------------
Eval num_timesteps=1366512, episode_reward=-274.25 +/- 51.12
Episode length: 546.20 +/- 62.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 1366512  |
---------------------------------
Eval num_timesteps=1368504, episode_reward=-249.70 +/- 73.65
Episode length: 612.40 +/- 137.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 1368504  |
---------------------------------
Eval num_timesteps=1370496, episode_reward=-290.88 +/- 12.40
Episode length: 560.80 +/- 91.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 1370496  |
---------------------------------
Eval num_timesteps=1372488, episode_reward=-268.40 +/- 33.49
Episode length: 561.60 +/- 160.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 1372488  |
---------------------------------
Eval num_timesteps=1374480, episode_reward=-207.45 +/- 129.26
Episode length: 618.60 +/- 217.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 1374480  |
---------------------------------
Eval num_timesteps=1376472, episode_reward=487.84 +/- 411.30
Episode length: 626.80 +/- 84.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 627          |
|    mean_reward          | 488          |
| time/                   |              |
|    total_timesteps      | 1376472      |
| train/                  |              |
|    approx_kl            | 0.0064237537 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.18        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0931       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.14         |
|    value_loss           | 0.33         |
------------------------------------------
New best mean reward!
Eval num_timesteps=1378464, episode_reward=562.57 +/- 219.47
Episode length: 621.40 +/- 103.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 563      |
| time/              |          |
|    total_timesteps | 1378464  |
---------------------------------
New best mean reward!
Eval num_timesteps=1380456, episode_reward=1045.47 +/- 445.04
Episode length: 728.60 +/- 85.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 1380456  |
---------------------------------
New best mean reward!
Eval num_timesteps=1382448, episode_reward=182.56 +/- 367.63
Episode length: 698.00 +/- 107.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 1382448  |
---------------------------------
Eval num_timesteps=1384440, episode_reward=339.96 +/- 524.62
Episode length: 555.60 +/- 83.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 1384440  |
---------------------------------
Eval num_timesteps=1386432, episode_reward=257.15 +/- 508.67
Episode length: 646.20 +/- 203.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 1386432  |
---------------------------------
Eval num_timesteps=1388424, episode_reward=368.30 +/- 333.71
Episode length: 537.80 +/- 97.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 1388424  |
---------------------------------
Eval num_timesteps=1390416, episode_reward=265.48 +/- 380.88
Episode length: 573.40 +/- 155.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 1390416  |
---------------------------------
Eval num_timesteps=1392408, episode_reward=430.62 +/- 675.13
Episode length: 519.00 +/- 129.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 1392408  |
---------------------------------
Eval num_timesteps=1394400, episode_reward=317.65 +/- 425.44
Episode length: 626.00 +/- 104.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 1394400  |
---------------------------------
Eval num_timesteps=1396392, episode_reward=328.16 +/- 372.86
Episode length: 680.60 +/- 103.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 1396392  |
---------------------------------
Eval num_timesteps=1398384, episode_reward=605.87 +/- 1126.37
Episode length: 745.40 +/- 159.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 1398384  |
---------------------------------
Eval num_timesteps=1400376, episode_reward=791.77 +/- 725.31
Episode length: 655.20 +/- 88.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 1400376  |
---------------------------------
Eval num_timesteps=1402368, episode_reward=95.60 +/- 617.29
Episode length: 617.60 +/- 197.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 95.6     |
| time/              |          |
|    total_timesteps | 1402368  |
---------------------------------
Eval num_timesteps=1404360, episode_reward=196.39 +/- 370.42
Episode length: 647.80 +/- 74.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 1404360  |
---------------------------------
Eval num_timesteps=1406352, episode_reward=300.55 +/- 414.39
Episode length: 599.80 +/- 144.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 1406352  |
---------------------------------
Eval num_timesteps=1408344, episode_reward=319.84 +/- 371.65
Episode length: 725.80 +/- 189.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 1408344  |
---------------------------------
Eval num_timesteps=1410336, episode_reward=208.14 +/- 372.58
Episode length: 655.00 +/- 171.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 1410336  |
---------------------------------
Eval num_timesteps=1412328, episode_reward=464.05 +/- 458.37
Episode length: 665.40 +/- 154.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 1412328  |
---------------------------------
Eval num_timesteps=1414320, episode_reward=383.94 +/- 278.80
Episode length: 687.20 +/- 104.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 1414320  |
---------------------------------
Eval num_timesteps=1416312, episode_reward=254.13 +/- 368.07
Episode length: 679.20 +/- 139.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 1416312  |
---------------------------------
Eval num_timesteps=1418304, episode_reward=808.13 +/- 519.42
Episode length: 579.20 +/- 90.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 1418304  |
---------------------------------
Eval num_timesteps=1420296, episode_reward=529.05 +/- 543.89
Episode length: 617.20 +/- 136.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 1420296  |
---------------------------------
Eval num_timesteps=1422288, episode_reward=497.98 +/- 855.58
Episode length: 667.60 +/- 138.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 498      |
| time/              |          |
|    total_timesteps | 1422288  |
---------------------------------
Eval num_timesteps=1424280, episode_reward=194.76 +/- 525.80
Episode length: 627.60 +/- 103.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 1424280  |
---------------------------------
Eval num_timesteps=1426272, episode_reward=481.09 +/- 468.09
Episode length: 577.60 +/- 140.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 578          |
|    mean_reward          | 481          |
| time/                   |              |
|    total_timesteps      | 1426272      |
| train/                  |              |
|    approx_kl            | 0.0069150627 |
|    clip_fraction        | 0.059        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.21        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.001        |
|    loss                 | 0.361        |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00051     |
|    std                  | 1.15         |
|    value_loss           | 0.887        |
------------------------------------------
Eval num_timesteps=1428264, episode_reward=377.16 +/- 393.44
Episode length: 573.20 +/- 95.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 1428264  |
---------------------------------
Eval num_timesteps=1430256, episode_reward=123.97 +/- 430.43
Episode length: 690.40 +/- 173.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 1430256  |
---------------------------------
Eval num_timesteps=1432248, episode_reward=213.20 +/- 272.82
Episode length: 603.60 +/- 133.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 1432248  |
---------------------------------
Eval num_timesteps=1434240, episode_reward=340.97 +/- 582.27
Episode length: 654.00 +/- 148.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 1434240  |
---------------------------------
Eval num_timesteps=1436232, episode_reward=165.97 +/- 298.80
Episode length: 669.00 +/- 131.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 1436232  |
---------------------------------
Eval num_timesteps=1438224, episode_reward=681.04 +/- 479.27
Episode length: 624.80 +/- 67.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 1438224  |
---------------------------------
Eval num_timesteps=1440216, episode_reward=247.03 +/- 438.93
Episode length: 627.20 +/- 111.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 247      |
| time/              |          |
|    total_timesteps | 1440216  |
---------------------------------
Eval num_timesteps=1442208, episode_reward=372.44 +/- 492.50
Episode length: 601.40 +/- 124.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 1442208  |
---------------------------------
Eval num_timesteps=1444200, episode_reward=414.08 +/- 314.28
Episode length: 525.60 +/- 90.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 1444200  |
---------------------------------
Eval num_timesteps=1446192, episode_reward=621.48 +/- 146.95
Episode length: 551.00 +/- 46.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 1446192  |
---------------------------------
Eval num_timesteps=1448184, episode_reward=438.20 +/- 391.14
Episode length: 592.40 +/- 53.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 1448184  |
---------------------------------
Eval num_timesteps=1450176, episode_reward=400.09 +/- 542.65
Episode length: 603.40 +/- 107.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 1450176  |
---------------------------------
Eval num_timesteps=1452168, episode_reward=33.74 +/- 302.33
Episode length: 552.80 +/- 113.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 33.7     |
| time/              |          |
|    total_timesteps | 1452168  |
---------------------------------
Eval num_timesteps=1454160, episode_reward=436.81 +/- 378.46
Episode length: 565.40 +/- 156.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 1454160  |
---------------------------------
Eval num_timesteps=1456152, episode_reward=229.49 +/- 394.34
Episode length: 529.20 +/- 124.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 1456152  |
---------------------------------
Eval num_timesteps=1458144, episode_reward=348.29 +/- 482.83
Episode length: 623.60 +/- 46.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 1458144  |
---------------------------------
Eval num_timesteps=1460136, episode_reward=275.16 +/- 514.07
Episode length: 744.80 +/- 63.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 1460136  |
---------------------------------
Eval num_timesteps=1462128, episode_reward=761.65 +/- 152.82
Episode length: 652.20 +/- 95.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 1462128  |
---------------------------------
Eval num_timesteps=1464120, episode_reward=301.62 +/- 375.06
Episode length: 531.60 +/- 36.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 1464120  |
---------------------------------
Eval num_timesteps=1466112, episode_reward=294.74 +/- 385.23
Episode length: 669.00 +/- 148.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 1466112  |
---------------------------------
Eval num_timesteps=1468104, episode_reward=554.76 +/- 510.13
Episode length: 598.80 +/- 145.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 1468104  |
---------------------------------
Eval num_timesteps=1470096, episode_reward=311.20 +/- 433.19
Episode length: 508.00 +/- 108.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 1470096  |
---------------------------------
Eval num_timesteps=1472088, episode_reward=750.02 +/- 311.99
Episode length: 660.20 +/- 90.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 1472088  |
---------------------------------
Eval num_timesteps=1474080, episode_reward=152.14 +/- 379.58
Episode length: 565.20 +/- 108.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 1474080  |
---------------------------------
Eval num_timesteps=1476072, episode_reward=85.65 +/- 335.89
Episode length: 525.40 +/- 52.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 85.6        |
| time/                   |             |
|    total_timesteps      | 1476072     |
| train/                  |             |
|    approx_kl            | 0.006196632 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.001       |
|    loss                 | 0.385       |
|    n_updates            | 300         |
|    policy_gradient_loss | 0.000204    |
|    std                  | 1.15        |
|    value_loss           | 0.948       |
-----------------------------------------
Eval num_timesteps=1478064, episode_reward=656.94 +/- 564.38
Episode length: 614.20 +/- 50.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 1478064  |
---------------------------------
Eval num_timesteps=1480056, episode_reward=286.87 +/- 402.99
Episode length: 664.40 +/- 157.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 1480056  |
---------------------------------
Eval num_timesteps=1482048, episode_reward=222.56 +/- 421.17
Episode length: 628.00 +/- 143.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 1482048  |
---------------------------------
Eval num_timesteps=1484040, episode_reward=455.22 +/- 371.98
Episode length: 651.00 +/- 164.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 1484040  |
---------------------------------
Eval num_timesteps=1486032, episode_reward=508.49 +/- 599.41
Episode length: 609.20 +/- 178.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 1486032  |
---------------------------------
Eval num_timesteps=1488024, episode_reward=683.06 +/- 116.60
Episode length: 579.80 +/- 106.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 1488024  |
---------------------------------
Eval num_timesteps=1490016, episode_reward=365.55 +/- 357.85
Episode length: 573.80 +/- 49.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 1490016  |
---------------------------------
Eval num_timesteps=1492008, episode_reward=764.74 +/- 532.63
Episode length: 611.00 +/- 87.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 1492008  |
---------------------------------
Eval num_timesteps=1494000, episode_reward=457.16 +/- 414.63
Episode length: 582.00 +/- 92.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1495992, episode_reward=230.40 +/- 381.07
Episode length: 550.60 +/- 92.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 1495992  |
---------------------------------
Eval num_timesteps=1497984, episode_reward=320.06 +/- 448.26
Episode length: 613.40 +/- 132.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 1497984  |
---------------------------------
Eval num_timesteps=1499976, episode_reward=360.54 +/- 395.86
Episode length: 603.80 +/- 28.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 1499976  |
---------------------------------
Eval num_timesteps=1501968, episode_reward=433.90 +/- 393.24
Episode length: 637.80 +/- 128.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 1501968  |
---------------------------------
Eval num_timesteps=1503960, episode_reward=42.22 +/- 277.29
Episode length: 463.20 +/- 191.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 42.2     |
| time/              |          |
|    total_timesteps | 1503960  |
---------------------------------
Eval num_timesteps=1505952, episode_reward=479.72 +/- 342.02
Episode length: 603.20 +/- 115.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 1505952  |
---------------------------------
Eval num_timesteps=1507944, episode_reward=618.84 +/- 355.56
Episode length: 542.80 +/- 56.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 1507944  |
---------------------------------
Eval num_timesteps=1509936, episode_reward=784.34 +/- 43.70
Episode length: 594.00 +/- 45.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 1509936  |
---------------------------------
Eval num_timesteps=1511928, episode_reward=392.77 +/- 343.69
Episode length: 575.40 +/- 100.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 1511928  |
---------------------------------
Eval num_timesteps=1513920, episode_reward=570.66 +/- 397.85
Episode length: 662.00 +/- 149.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 1513920  |
---------------------------------
Eval num_timesteps=1515912, episode_reward=842.10 +/- 627.06
Episode length: 656.60 +/- 140.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 1515912  |
---------------------------------
Eval num_timesteps=1517904, episode_reward=278.09 +/- 497.27
Episode length: 660.60 +/- 56.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 1517904  |
---------------------------------
Eval num_timesteps=1519896, episode_reward=743.43 +/- 479.06
Episode length: 603.40 +/- 58.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 1519896  |
---------------------------------
Eval num_timesteps=1521888, episode_reward=282.56 +/- 359.72
Episode length: 704.60 +/- 99.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 1521888  |
---------------------------------
Eval num_timesteps=1523880, episode_reward=532.30 +/- 219.64
Episode length: 534.40 +/- 30.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 534         |
|    mean_reward          | 532         |
| time/                   |             |
|    total_timesteps      | 1523880     |
| train/                  |             |
|    approx_kl            | 0.008890075 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.001       |
|    loss                 | 0.384       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.000447   |
|    std                  | 1.16        |
|    value_loss           | 0.953       |
-----------------------------------------
Eval num_timesteps=1525872, episode_reward=474.61 +/- 334.57
Episode length: 533.00 +/- 178.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 1525872  |
---------------------------------
Eval num_timesteps=1527864, episode_reward=215.38 +/- 316.60
Episode length: 472.60 +/- 55.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 1527864  |
---------------------------------
Eval num_timesteps=1529856, episode_reward=408.06 +/- 284.05
Episode length: 429.80 +/- 26.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 1529856  |
---------------------------------
Eval num_timesteps=1531848, episode_reward=273.97 +/- 298.87
Episode length: 478.00 +/- 53.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 1531848  |
---------------------------------
Eval num_timesteps=1533840, episode_reward=553.40 +/- 416.60
Episode length: 519.80 +/- 69.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 1533840  |
---------------------------------
Eval num_timesteps=1535832, episode_reward=242.13 +/- 378.98
Episode length: 482.20 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 1535832  |
---------------------------------
Eval num_timesteps=1537824, episode_reward=435.18 +/- 257.93
Episode length: 470.20 +/- 40.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 1537824  |
---------------------------------
Eval num_timesteps=1539816, episode_reward=345.77 +/- 391.59
Episode length: 487.80 +/- 65.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 1539816  |
---------------------------------
Eval num_timesteps=1541808, episode_reward=256.79 +/- 275.83
Episode length: 484.40 +/- 49.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 1541808  |
---------------------------------
Eval num_timesteps=1543800, episode_reward=372.73 +/- 357.69
Episode length: 435.20 +/- 59.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 1543800  |
---------------------------------
Eval num_timesteps=1545792, episode_reward=571.51 +/- 293.57
Episode length: 509.80 +/- 50.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 1545792  |
---------------------------------
Eval num_timesteps=1547784, episode_reward=519.89 +/- 184.88
Episode length: 495.20 +/- 75.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 1547784  |
---------------------------------
Eval num_timesteps=1549776, episode_reward=340.76 +/- 230.68
Episode length: 432.60 +/- 66.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 1549776  |
---------------------------------
Eval num_timesteps=1551768, episode_reward=36.09 +/- 197.62
Episode length: 463.00 +/- 74.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 36.1     |
| time/              |          |
|    total_timesteps | 1551768  |
---------------------------------
Eval num_timesteps=1553760, episode_reward=392.36 +/- 197.55
Episode length: 444.20 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 1553760  |
---------------------------------
Eval num_timesteps=1555752, episode_reward=451.77 +/- 260.63
Episode length: 490.80 +/- 76.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 1555752  |
---------------------------------
Eval num_timesteps=1557744, episode_reward=321.93 +/- 470.52
Episode length: 566.40 +/- 267.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 1557744  |
---------------------------------
Eval num_timesteps=1559736, episode_reward=274.71 +/- 454.89
Episode length: 511.60 +/- 148.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 1559736  |
---------------------------------
Eval num_timesteps=1561728, episode_reward=505.62 +/- 307.37
Episode length: 611.20 +/- 105.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 1561728  |
---------------------------------
Eval num_timesteps=1563720, episode_reward=126.71 +/- 198.47
Episode length: 414.40 +/- 40.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 127      |
| time/              |          |
|    total_timesteps | 1563720  |
---------------------------------
Eval num_timesteps=1565712, episode_reward=268.17 +/- 333.28
Episode length: 437.80 +/- 98.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 1565712  |
---------------------------------
Eval num_timesteps=1567704, episode_reward=595.69 +/- 510.84
Episode length: 548.20 +/- 119.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 1567704  |
---------------------------------
Eval num_timesteps=1569696, episode_reward=154.24 +/- 334.27
Episode length: 547.80 +/- 66.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 154      |
| time/              |          |
|    total_timesteps | 1569696  |
---------------------------------
Eval num_timesteps=1571688, episode_reward=410.83 +/- 308.15
Episode length: 486.60 +/- 26.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 1571688  |
---------------------------------
Eval num_timesteps=1573680, episode_reward=388.65 +/- 312.85
Episode length: 426.40 +/- 42.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 1573680     |
| train/                  |             |
|    approx_kl            | 0.009061861 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.28       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.001       |
|    loss                 | 0.238       |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.000415    |
|    std                  | 1.17        |
|    value_loss           | 0.658       |
-----------------------------------------
Eval num_timesteps=1575672, episode_reward=483.45 +/- 200.08
Episode length: 443.80 +/- 46.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 1575672  |
---------------------------------
Eval num_timesteps=1577664, episode_reward=416.10 +/- 249.29
Episode length: 411.00 +/- 56.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 1577664  |
---------------------------------
Eval num_timesteps=1579656, episode_reward=502.72 +/- 179.50
Episode length: 513.80 +/- 66.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 1579656  |
---------------------------------
Eval num_timesteps=1581648, episode_reward=338.62 +/- 357.11
Episode length: 447.60 +/- 41.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 1581648  |
---------------------------------
Eval num_timesteps=1583640, episode_reward=310.52 +/- 333.04
Episode length: 408.20 +/- 47.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 1583640  |
---------------------------------
Eval num_timesteps=1585632, episode_reward=477.83 +/- 236.92
Episode length: 438.00 +/- 51.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 1585632  |
---------------------------------
Eval num_timesteps=1587624, episode_reward=368.22 +/- 265.98
Episode length: 430.20 +/- 38.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 1587624  |
---------------------------------
Eval num_timesteps=1589616, episode_reward=318.43 +/- 363.22
Episode length: 401.20 +/- 86.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 1589616  |
---------------------------------
Eval num_timesteps=1591608, episode_reward=321.25 +/- 202.79
Episode length: 439.20 +/- 53.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 439      |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 1591608  |
---------------------------------
Eval num_timesteps=1593600, episode_reward=543.71 +/- 202.37
Episode length: 435.60 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 436      |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 1593600  |
---------------------------------
Eval num_timesteps=1595592, episode_reward=666.09 +/- 132.86
Episode length: 449.40 +/- 29.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 449      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 1595592  |
---------------------------------
Eval num_timesteps=1597584, episode_reward=405.02 +/- 351.18
Episode length: 398.20 +/- 107.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 398      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 1597584  |
---------------------------------
Eval num_timesteps=1599576, episode_reward=445.86 +/- 413.87
Episode length: 402.60 +/- 47.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 403      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 1599576  |
---------------------------------
Eval num_timesteps=1601568, episode_reward=228.48 +/- 280.04
Episode length: 383.20 +/- 87.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 1601568  |
---------------------------------
Eval num_timesteps=1603560, episode_reward=262.95 +/- 255.62
Episode length: 426.40 +/- 28.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 426      |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 1603560  |
---------------------------------
Eval num_timesteps=1605552, episode_reward=680.19 +/- 100.62
Episode length: 554.00 +/- 99.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 1605552  |
---------------------------------
Eval num_timesteps=1607544, episode_reward=424.20 +/- 306.07
Episode length: 445.20 +/- 28.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 445      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 1607544  |
---------------------------------
Eval num_timesteps=1609536, episode_reward=382.08 +/- 229.71
Episode length: 408.00 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | 382      |
| time/              |          |
|    total_timesteps | 1609536  |
---------------------------------
Eval num_timesteps=1611528, episode_reward=244.25 +/- 284.68
Episode length: 381.00 +/- 79.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 1611528  |
---------------------------------
Eval num_timesteps=1613520, episode_reward=386.13 +/- 253.98
Episode length: 411.00 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 1613520  |
---------------------------------
Eval num_timesteps=1615512, episode_reward=493.63 +/- 291.95
Episode length: 457.00 +/- 67.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 1615512  |
---------------------------------
Eval num_timesteps=1617504, episode_reward=138.02 +/- 229.90
Episode length: 413.60 +/- 25.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 1617504  |
---------------------------------
Eval num_timesteps=1619496, episode_reward=344.51 +/- 192.74
Episode length: 407.00 +/- 37.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 407      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 1619496  |
---------------------------------
Eval num_timesteps=1621488, episode_reward=641.40 +/- 107.12
Episode length: 459.00 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 1621488  |
---------------------------------
Eval num_timesteps=1623480, episode_reward=94.86 +/- 145.41
Episode length: 374.60 +/- 21.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 375         |
|    mean_reward          | 94.9        |
| time/                   |             |
|    total_timesteps      | 1623480     |
| train/                  |             |
|    approx_kl            | 0.004127852 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.001       |
|    loss                 | 0.193       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00142    |
|    std                  | 1.17        |
|    value_loss           | 0.56        |
-----------------------------------------
Eval num_timesteps=1625472, episode_reward=206.54 +/- 170.79
Episode length: 388.60 +/- 26.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 1625472  |
---------------------------------
Eval num_timesteps=1627464, episode_reward=352.77 +/- 192.30
Episode length: 389.20 +/- 27.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 1627464  |
---------------------------------
Eval num_timesteps=1629456, episode_reward=243.55 +/- 216.85
Episode length: 369.00 +/- 49.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 1629456  |
---------------------------------
Eval num_timesteps=1631448, episode_reward=353.11 +/- 217.31
Episode length: 393.80 +/- 26.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 394      |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 1631448  |
---------------------------------
Eval num_timesteps=1633440, episode_reward=216.86 +/- 276.26
Episode length: 388.20 +/- 38.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 1633440  |
---------------------------------
Eval num_timesteps=1635432, episode_reward=273.34 +/- 337.93
Episode length: 385.20 +/- 86.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 1635432  |
---------------------------------
Eval num_timesteps=1637424, episode_reward=86.45 +/- 274.43
Episode length: 372.20 +/- 48.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 372      |
|    mean_reward     | 86.5     |
| time/              |          |
|    total_timesteps | 1637424  |
---------------------------------
Eval num_timesteps=1639416, episode_reward=401.30 +/- 293.58
Episode length: 384.00 +/- 58.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 1639416  |
---------------------------------
Eval num_timesteps=1641408, episode_reward=222.68 +/- 126.72
Episode length: 381.20 +/- 27.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 1641408  |
---------------------------------
Eval num_timesteps=1643400, episode_reward=225.17 +/- 350.94
Episode length: 437.80 +/- 65.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 1643400  |
---------------------------------
Eval num_timesteps=1645392, episode_reward=209.32 +/- 185.51
Episode length: 385.20 +/- 28.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 1645392  |
---------------------------------
Eval num_timesteps=1647384, episode_reward=244.75 +/- 123.32
Episode length: 401.80 +/- 31.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 1647384  |
---------------------------------
Eval num_timesteps=1649376, episode_reward=435.22 +/- 249.71
Episode length: 399.40 +/- 37.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 1649376  |
---------------------------------
Eval num_timesteps=1651368, episode_reward=48.38 +/- 192.71
Episode length: 351.20 +/- 44.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | 48.4     |
| time/              |          |
|    total_timesteps | 1651368  |
---------------------------------
Eval num_timesteps=1653360, episode_reward=113.04 +/- 178.27
Episode length: 374.20 +/- 32.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 374      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 1653360  |
---------------------------------
Eval num_timesteps=1655352, episode_reward=123.65 +/- 361.88
Episode length: 312.40 +/- 122.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 312      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 1655352  |
---------------------------------
Eval num_timesteps=1657344, episode_reward=278.62 +/- 213.00
Episode length: 398.80 +/- 59.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 1657344  |
---------------------------------
Eval num_timesteps=1659336, episode_reward=212.70 +/- 288.79
Episode length: 373.60 +/- 52.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 374      |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 1659336  |
---------------------------------
Eval num_timesteps=1661328, episode_reward=296.38 +/- 233.66
Episode length: 381.60 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 382      |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 1661328  |
---------------------------------
Eval num_timesteps=1663320, episode_reward=145.34 +/- 233.53
Episode length: 376.40 +/- 46.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | 145      |
| time/              |          |
|    total_timesteps | 1663320  |
---------------------------------
Eval num_timesteps=1665312, episode_reward=330.79 +/- 97.93
Episode length: 383.40 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 1665312  |
---------------------------------
Eval num_timesteps=1667304, episode_reward=288.38 +/- 229.11
Episode length: 399.60 +/- 42.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 1667304  |
---------------------------------
Eval num_timesteps=1669296, episode_reward=159.70 +/- 98.87
Episode length: 368.60 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 1669296  |
---------------------------------
Eval num_timesteps=1671288, episode_reward=238.70 +/- 225.44
Episode length: 371.00 +/- 37.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 1671288      |
| train/                  |              |
|    approx_kl            | 0.0032783356 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.31        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 0.099        |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.17         |
|    value_loss           | 0.359        |
------------------------------------------
Eval num_timesteps=1673280, episode_reward=484.91 +/- 328.88
Episode length: 414.20 +/- 83.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 1673280  |
---------------------------------
Eval num_timesteps=1675272, episode_reward=111.44 +/- 305.66
Episode length: 339.80 +/- 68.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 340      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 1675272  |
---------------------------------
Eval num_timesteps=1677264, episode_reward=472.35 +/- 149.94
Episode length: 406.80 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 407      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 1677264  |
---------------------------------
Eval num_timesteps=1679256, episode_reward=281.48 +/- 298.15
Episode length: 387.80 +/- 49.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 1679256  |
---------------------------------
Eval num_timesteps=1681248, episode_reward=256.92 +/- 201.72
Episode length: 376.60 +/- 29.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 1681248  |
---------------------------------
Eval num_timesteps=1683240, episode_reward=395.25 +/- 282.90
Episode length: 405.40 +/- 37.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 1683240  |
---------------------------------
Eval num_timesteps=1685232, episode_reward=413.07 +/- 141.36
Episode length: 412.20 +/- 31.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 1685232  |
---------------------------------
Eval num_timesteps=1687224, episode_reward=356.45 +/- 302.39
Episode length: 386.60 +/- 79.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 1687224  |
---------------------------------
Eval num_timesteps=1689216, episode_reward=200.95 +/- 217.04
Episode length: 377.60 +/- 36.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 1689216  |
---------------------------------
Eval num_timesteps=1691208, episode_reward=421.60 +/- 211.95
Episode length: 391.80 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 1691208  |
---------------------------------
Eval num_timesteps=1693200, episode_reward=192.22 +/- 282.24
Episode length: 338.60 +/- 79.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 1693200  |
---------------------------------
Eval num_timesteps=1695192, episode_reward=357.77 +/- 176.44
Episode length: 419.80 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 1695192  |
---------------------------------
Eval num_timesteps=1697184, episode_reward=193.42 +/- 229.68
Episode length: 401.80 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 1697184  |
---------------------------------
Eval num_timesteps=1699176, episode_reward=261.51 +/- 295.14
Episode length: 379.00 +/- 56.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 1699176  |
---------------------------------
Eval num_timesteps=1701168, episode_reward=240.51 +/- 159.50
Episode length: 412.60 +/- 44.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 1701168  |
---------------------------------
Eval num_timesteps=1703160, episode_reward=195.19 +/- 254.88
Episode length: 360.60 +/- 34.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 361      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 1703160  |
---------------------------------
Eval num_timesteps=1705152, episode_reward=384.21 +/- 258.88
Episode length: 384.40 +/- 42.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 1705152  |
---------------------------------
Eval num_timesteps=1707144, episode_reward=180.12 +/- 143.37
Episode length: 383.40 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 1707144  |
---------------------------------
Eval num_timesteps=1709136, episode_reward=286.86 +/- 255.94
Episode length: 378.00 +/- 40.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 1709136  |
---------------------------------
Eval num_timesteps=1711128, episode_reward=277.61 +/- 184.00
Episode length: 381.00 +/- 44.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 1711128  |
---------------------------------
Eval num_timesteps=1713120, episode_reward=256.55 +/- 210.01
Episode length: 359.60 +/- 62.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 1713120  |
---------------------------------
Eval num_timesteps=1715112, episode_reward=241.71 +/- 270.02
Episode length: 370.80 +/- 92.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 1715112  |
---------------------------------
Eval num_timesteps=1717104, episode_reward=266.14 +/- 310.81
Episode length: 415.80 +/- 38.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 416      |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 1717104  |
---------------------------------
Eval num_timesteps=1719096, episode_reward=359.57 +/- 330.10
Episode length: 412.80 +/- 42.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 1719096  |
---------------------------------
Eval num_timesteps=1721088, episode_reward=384.34 +/- 321.04
Episode length: 424.80 +/- 70.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 1721088      |
| train/                  |              |
|    approx_kl            | 0.0034004159 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0553       |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.18         |
|    value_loss           | 0.267        |
------------------------------------------
Eval num_timesteps=1723080, episode_reward=394.47 +/- 201.19
Episode length: 396.00 +/- 24.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 1723080  |
---------------------------------
Eval num_timesteps=1725072, episode_reward=109.72 +/- 146.85
Episode length: 386.20 +/- 22.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | 110      |
| time/              |          |
|    total_timesteps | 1725072  |
---------------------------------
Eval num_timesteps=1727064, episode_reward=547.12 +/- 176.21
Episode length: 400.20 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 1727064  |
---------------------------------
Eval num_timesteps=1729056, episode_reward=180.23 +/- 198.13
Episode length: 384.00 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 1729056  |
---------------------------------
Eval num_timesteps=1731048, episode_reward=462.14 +/- 201.98
Episode length: 420.60 +/- 33.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 1731048  |
---------------------------------
Eval num_timesteps=1733040, episode_reward=275.92 +/- 203.89
Episode length: 389.40 +/- 35.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 1733040  |
---------------------------------
Eval num_timesteps=1735032, episode_reward=502.61 +/- 234.65
Episode length: 413.20 +/- 35.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 1735032  |
---------------------------------
Eval num_timesteps=1737024, episode_reward=444.70 +/- 225.76
Episode length: 397.20 +/- 31.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 397      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 1737024  |
---------------------------------
Eval num_timesteps=1739016, episode_reward=363.93 +/- 232.08
Episode length: 390.20 +/- 28.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 1739016  |
---------------------------------
Eval num_timesteps=1741008, episode_reward=435.34 +/- 150.18
Episode length: 412.40 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 1741008  |
---------------------------------
Eval num_timesteps=1743000, episode_reward=609.76 +/- 74.10
Episode length: 416.40 +/- 13.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 416      |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 1743000  |
---------------------------------
Eval num_timesteps=1744992, episode_reward=514.61 +/- 161.26
Episode length: 425.80 +/- 26.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 426      |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 1744992  |
---------------------------------
Eval num_timesteps=1746984, episode_reward=157.38 +/- 171.00
Episode length: 362.00 +/- 77.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 362      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 1746984  |
---------------------------------
Eval num_timesteps=1748976, episode_reward=437.95 +/- 161.39
Episode length: 405.60 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 1748976  |
---------------------------------
Eval num_timesteps=1750968, episode_reward=299.31 +/- 295.49
Episode length: 384.40 +/- 49.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 1750968  |
---------------------------------
Eval num_timesteps=1752960, episode_reward=212.10 +/- 233.87
Episode length: 382.20 +/- 35.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 382      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 1752960  |
---------------------------------
Eval num_timesteps=1754952, episode_reward=279.69 +/- 244.09
Episode length: 372.80 +/- 29.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 373      |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 1754952  |
---------------------------------
Eval num_timesteps=1756944, episode_reward=366.05 +/- 199.30
Episode length: 391.80 +/- 32.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 1756944  |
---------------------------------
Eval num_timesteps=1758936, episode_reward=440.96 +/- 170.32
Episode length: 396.00 +/- 22.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 1758936  |
---------------------------------
Eval num_timesteps=1760928, episode_reward=421.20 +/- 234.48
Episode length: 408.40 +/- 37.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 1760928  |
---------------------------------
Eval num_timesteps=1762920, episode_reward=487.75 +/- 192.54
Episode length: 421.20 +/- 45.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 1762920  |
---------------------------------
Eval num_timesteps=1764912, episode_reward=364.44 +/- 268.38
Episode length: 385.00 +/- 33.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 1764912  |
---------------------------------
Eval num_timesteps=1766904, episode_reward=403.33 +/- 305.91
Episode length: 408.40 +/- 62.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 1766904  |
---------------------------------
Eval num_timesteps=1768896, episode_reward=219.06 +/- 222.06
Episode length: 374.80 +/- 29.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 1768896  |
---------------------------------
Eval num_timesteps=1770888, episode_reward=316.82 +/- 160.10
Episode length: 383.00 +/- 24.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 1770888      |
| train/                  |              |
|    approx_kl            | 0.0046787127 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.34        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0543       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.18         |
|    value_loss           | 0.264        |
------------------------------------------
Eval num_timesteps=1772880, episode_reward=136.52 +/- 222.12
Episode length: 349.20 +/- 48.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | 137      |
| time/              |          |
|    total_timesteps | 1772880  |
---------------------------------
Eval num_timesteps=1774872, episode_reward=248.02 +/- 153.38
Episode length: 371.00 +/- 13.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 1774872  |
---------------------------------
Eval num_timesteps=1776864, episode_reward=294.38 +/- 148.29
Episode length: 362.40 +/- 21.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 362      |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 1776864  |
---------------------------------
Eval num_timesteps=1778856, episode_reward=246.28 +/- 130.48
Episode length: 371.40 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 1778856  |
---------------------------------
Eval num_timesteps=1780848, episode_reward=299.20 +/- 127.19
Episode length: 390.40 +/- 22.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 1780848  |
---------------------------------
Eval num_timesteps=1782840, episode_reward=279.21 +/- 246.99
Episode length: 354.40 +/- 45.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 354      |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 1782840  |
---------------------------------
Eval num_timesteps=1784832, episode_reward=118.66 +/- 129.49
Episode length: 355.40 +/- 23.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | 119      |
| time/              |          |
|    total_timesteps | 1784832  |
---------------------------------
Eval num_timesteps=1786824, episode_reward=191.87 +/- 240.38
Episode length: 360.00 +/- 39.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 1786824  |
---------------------------------
Eval num_timesteps=1788816, episode_reward=176.93 +/- 134.25
Episode length: 369.40 +/- 27.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 1788816  |
---------------------------------
Eval num_timesteps=1790808, episode_reward=285.46 +/- 170.24
Episode length: 371.40 +/- 11.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 1790808  |
---------------------------------
Eval num_timesteps=1792800, episode_reward=43.36 +/- 142.35
Episode length: 323.00 +/- 55.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 43.4     |
| time/              |          |
|    total_timesteps | 1792800  |
---------------------------------
Eval num_timesteps=1794792, episode_reward=242.33 +/- 135.98
Episode length: 370.60 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 1794792  |
---------------------------------
Eval num_timesteps=1796784, episode_reward=232.38 +/- 244.84
Episode length: 379.00 +/- 74.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 1796784  |
---------------------------------
Eval num_timesteps=1798776, episode_reward=417.22 +/- 366.36
Episode length: 393.20 +/- 55.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 1798776  |
---------------------------------
Eval num_timesteps=1800768, episode_reward=122.41 +/- 85.66
Episode length: 359.20 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 359      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 1800768  |
---------------------------------
Eval num_timesteps=1802760, episode_reward=134.92 +/- 160.79
Episode length: 380.20 +/- 42.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | 135      |
| time/              |          |
|    total_timesteps | 1802760  |
---------------------------------
Eval num_timesteps=1804752, episode_reward=519.16 +/- 392.10
Episode length: 466.60 +/- 165.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 1804752  |
---------------------------------
Eval num_timesteps=1806744, episode_reward=276.88 +/- 199.33
Episode length: 372.00 +/- 19.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 372      |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 1806744  |
---------------------------------
Eval num_timesteps=1808736, episode_reward=147.74 +/- 92.70
Episode length: 349.60 +/- 14.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 1808736  |
---------------------------------
Eval num_timesteps=1810728, episode_reward=292.09 +/- 181.73
Episode length: 377.60 +/- 12.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 1810728  |
---------------------------------
Eval num_timesteps=1812720, episode_reward=207.42 +/- 297.94
Episode length: 336.60 +/- 59.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 1812720  |
---------------------------------
Eval num_timesteps=1814712, episode_reward=295.70 +/- 280.74
Episode length: 383.00 +/- 51.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 1814712  |
---------------------------------
Eval num_timesteps=1816704, episode_reward=175.97 +/- 173.04
Episode length: 363.80 +/- 23.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 364      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 1816704  |
---------------------------------
Eval num_timesteps=1818696, episode_reward=366.83 +/- 122.99
Episode length: 396.40 +/- 15.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 367         |
| time/                   |             |
|    total_timesteps      | 1818696     |
| train/                  |             |
|    approx_kl            | 0.004463818 |
|    clip_fraction        | 0.0651      |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0657      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00304    |
|    std                  | 1.19        |
|    value_loss           | 0.292       |
-----------------------------------------
Eval num_timesteps=1820688, episode_reward=621.73 +/- 223.87
Episode length: 430.00 +/- 28.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 1820688  |
---------------------------------
Eval num_timesteps=1822680, episode_reward=407.06 +/- 148.78
Episode length: 405.00 +/- 45.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 1822680  |
---------------------------------
Eval num_timesteps=1824672, episode_reward=320.59 +/- 249.63
Episode length: 437.80 +/- 51.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 1824672  |
---------------------------------
Eval num_timesteps=1826664, episode_reward=556.62 +/- 219.99
Episode length: 406.40 +/- 20.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | 557      |
| time/              |          |
|    total_timesteps | 1826664  |
---------------------------------
Eval num_timesteps=1828656, episode_reward=430.11 +/- 173.89
Episode length: 452.20 +/- 101.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 1828656  |
---------------------------------
Eval num_timesteps=1830648, episode_reward=590.06 +/- 184.05
Episode length: 438.20 +/- 38.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1830648  |
---------------------------------
Eval num_timesteps=1832640, episode_reward=503.83 +/- 278.72
Episode length: 432.20 +/- 60.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 1832640  |
---------------------------------
Eval num_timesteps=1834632, episode_reward=719.91 +/- 484.00
Episode length: 450.60 +/- 56.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 451      |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 1834632  |
---------------------------------
Eval num_timesteps=1836624, episode_reward=337.12 +/- 249.36
Episode length: 423.80 +/- 91.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 424      |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 1836624  |
---------------------------------
Eval num_timesteps=1838616, episode_reward=296.46 +/- 285.15
Episode length: 411.60 +/- 34.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 1838616  |
---------------------------------
Eval num_timesteps=1840608, episode_reward=536.01 +/- 275.58
Episode length: 426.40 +/- 55.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 426      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 1840608  |
---------------------------------
Eval num_timesteps=1842600, episode_reward=419.62 +/- 145.27
Episode length: 416.00 +/- 55.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 416      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 1842600  |
---------------------------------
Eval num_timesteps=1844592, episode_reward=464.23 +/- 240.78
Episode length: 422.20 +/- 34.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 1844592  |
---------------------------------
Eval num_timesteps=1846584, episode_reward=439.60 +/- 230.04
Episode length: 422.40 +/- 34.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 1846584  |
---------------------------------
Eval num_timesteps=1848576, episode_reward=657.55 +/- 164.71
Episode length: 490.20 +/- 115.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 1848576  |
---------------------------------
Eval num_timesteps=1850568, episode_reward=564.69 +/- 95.98
Episode length: 448.40 +/- 54.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 1850568  |
---------------------------------
Eval num_timesteps=1852560, episode_reward=353.20 +/- 257.42
Episode length: 402.40 +/- 37.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 1852560  |
---------------------------------
Eval num_timesteps=1854552, episode_reward=367.38 +/- 167.49
Episode length: 387.80 +/- 25.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 1854552  |
---------------------------------
Eval num_timesteps=1856544, episode_reward=534.83 +/- 155.46
Episode length: 396.00 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 1856544  |
---------------------------------
Eval num_timesteps=1858536, episode_reward=283.82 +/- 190.51
Episode length: 394.20 +/- 20.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 394      |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 1858536  |
---------------------------------
Eval num_timesteps=1860528, episode_reward=440.75 +/- 188.41
Episode length: 440.40 +/- 58.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 1860528  |
---------------------------------
Eval num_timesteps=1862520, episode_reward=383.12 +/- 167.39
Episode length: 412.40 +/- 63.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 1862520  |
---------------------------------
Eval num_timesteps=1864512, episode_reward=537.93 +/- 153.22
Episode length: 446.80 +/- 76.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 1864512  |
---------------------------------
Eval num_timesteps=1866504, episode_reward=467.43 +/- 245.46
Episode length: 463.60 +/- 76.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 1866504  |
---------------------------------
Eval num_timesteps=1868496, episode_reward=518.00 +/- 126.94
Episode length: 459.60 +/- 31.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 518          |
| time/                   |              |
|    total_timesteps      | 1868496      |
| train/                  |              |
|    approx_kl            | 0.0046461937 |
|    clip_fraction        | 0.0697       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.36        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0734       |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.19         |
|    value_loss           | 0.295        |
------------------------------------------
Eval num_timesteps=1870488, episode_reward=592.21 +/- 136.09
Episode length: 543.80 +/- 127.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 1870488  |
---------------------------------
Eval num_timesteps=1872480, episode_reward=437.30 +/- 291.97
Episode length: 507.00 +/- 127.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 1872480  |
---------------------------------
Eval num_timesteps=1874472, episode_reward=422.77 +/- 309.52
Episode length: 447.20 +/- 55.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 1874472  |
---------------------------------
Eval num_timesteps=1876464, episode_reward=551.04 +/- 319.33
Episode length: 576.60 +/- 184.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 1876464  |
---------------------------------
Eval num_timesteps=1878456, episode_reward=570.46 +/- 266.37
Episode length: 518.80 +/- 149.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 1878456  |
---------------------------------
Eval num_timesteps=1880448, episode_reward=736.37 +/- 109.78
Episode length: 468.60 +/- 32.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 1880448  |
---------------------------------
Eval num_timesteps=1882440, episode_reward=651.83 +/- 365.01
Episode length: 529.60 +/- 123.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 1882440  |
---------------------------------
Eval num_timesteps=1884432, episode_reward=539.86 +/- 280.53
Episode length: 514.40 +/- 50.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 1884432  |
---------------------------------
Eval num_timesteps=1886424, episode_reward=655.27 +/- 250.58
Episode length: 521.20 +/- 30.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 1886424  |
---------------------------------
Eval num_timesteps=1888416, episode_reward=945.43 +/- 433.27
Episode length: 577.00 +/- 84.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 1888416  |
---------------------------------
Eval num_timesteps=1890408, episode_reward=424.48 +/- 182.02
Episode length: 471.40 +/- 70.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 1890408  |
---------------------------------
Eval num_timesteps=1892400, episode_reward=535.21 +/- 145.24
Episode length: 428.20 +/- 65.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 1892400  |
---------------------------------
Eval num_timesteps=1894392, episode_reward=656.85 +/- 139.17
Episode length: 498.00 +/- 79.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 1894392  |
---------------------------------
Eval num_timesteps=1896384, episode_reward=459.91 +/- 250.13
Episode length: 430.00 +/- 62.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 1896384  |
---------------------------------
Eval num_timesteps=1898376, episode_reward=396.39 +/- 280.77
Episode length: 415.20 +/- 59.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 1898376  |
---------------------------------
Eval num_timesteps=1900368, episode_reward=419.78 +/- 254.02
Episode length: 492.00 +/- 108.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 1900368  |
---------------------------------
Eval num_timesteps=1902360, episode_reward=515.72 +/- 241.57
Episode length: 447.80 +/- 42.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 1902360  |
---------------------------------
Eval num_timesteps=1904352, episode_reward=575.23 +/- 372.15
Episode length: 418.80 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 1904352  |
---------------------------------
Eval num_timesteps=1906344, episode_reward=477.04 +/- 100.03
Episode length: 441.20 +/- 28.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 1906344  |
---------------------------------
Eval num_timesteps=1908336, episode_reward=624.03 +/- 26.49
Episode length: 543.00 +/- 89.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 1908336  |
---------------------------------
Eval num_timesteps=1910328, episode_reward=668.26 +/- 201.90
Episode length: 510.60 +/- 66.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 1910328  |
---------------------------------
Eval num_timesteps=1912320, episode_reward=419.53 +/- 481.71
Episode length: 433.40 +/- 57.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 1912320  |
---------------------------------
Eval num_timesteps=1914312, episode_reward=418.43 +/- 247.32
Episode length: 427.20 +/- 60.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 1914312  |
---------------------------------
Eval num_timesteps=1916304, episode_reward=660.24 +/- 642.57
Episode length: 511.20 +/- 142.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 1916304  |
---------------------------------
Eval num_timesteps=1918296, episode_reward=659.31 +/- 207.23
Episode length: 594.20 +/- 105.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 594          |
|    mean_reward          | 659          |
| time/                   |              |
|    total_timesteps      | 1918296      |
| train/                  |              |
|    approx_kl            | 0.0088002775 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.37        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 0.0835       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.19         |
|    value_loss           | 0.318        |
------------------------------------------
Eval num_timesteps=1920288, episode_reward=918.21 +/- 290.73
Episode length: 600.20 +/- 80.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 1920288  |
---------------------------------
Eval num_timesteps=1922280, episode_reward=397.01 +/- 327.68
Episode length: 605.00 +/- 127.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 1922280  |
---------------------------------
Eval num_timesteps=1924272, episode_reward=583.91 +/- 416.34
Episode length: 656.60 +/- 225.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 1924272  |
---------------------------------
Eval num_timesteps=1926264, episode_reward=686.09 +/- 342.39
Episode length: 680.40 +/- 235.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 1926264  |
---------------------------------
Eval num_timesteps=1928256, episode_reward=331.71 +/- 166.28
Episode length: 483.40 +/- 36.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 1928256  |
---------------------------------
Eval num_timesteps=1930248, episode_reward=440.87 +/- 262.11
Episode length: 539.80 +/- 45.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 1930248  |
---------------------------------
Eval num_timesteps=1932240, episode_reward=678.22 +/- 392.35
Episode length: 644.40 +/- 156.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 1932240  |
---------------------------------
Eval num_timesteps=1934232, episode_reward=532.70 +/- 387.89
Episode length: 825.20 +/- 335.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 1934232  |
---------------------------------
Eval num_timesteps=1936224, episode_reward=806.99 +/- 248.96
Episode length: 656.80 +/- 117.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 1936224  |
---------------------------------
Eval num_timesteps=1938216, episode_reward=836.23 +/- 425.90
Episode length: 613.20 +/- 95.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 1938216  |
---------------------------------
Eval num_timesteps=1940208, episode_reward=557.80 +/- 275.27
Episode length: 613.80 +/- 93.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 1940208  |
---------------------------------
Eval num_timesteps=1942200, episode_reward=403.78 +/- 421.57
Episode length: 538.00 +/- 33.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 1942200  |
---------------------------------
Eval num_timesteps=1944192, episode_reward=345.04 +/- 278.52
Episode length: 515.80 +/- 74.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 1944192  |
---------------------------------
Eval num_timesteps=1946184, episode_reward=412.88 +/- 394.55
Episode length: 506.60 +/- 85.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 1946184  |
---------------------------------
Eval num_timesteps=1948176, episode_reward=402.28 +/- 390.71
Episode length: 622.60 +/- 35.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 1948176  |
---------------------------------
Eval num_timesteps=1950168, episode_reward=730.18 +/- 367.57
Episode length: 534.00 +/- 38.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 1950168  |
---------------------------------
Eval num_timesteps=1952160, episode_reward=621.01 +/- 390.60
Episode length: 553.20 +/- 100.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 1952160  |
---------------------------------
Eval num_timesteps=1954152, episode_reward=406.78 +/- 417.96
Episode length: 638.60 +/- 79.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 1954152  |
---------------------------------
Eval num_timesteps=1956144, episode_reward=232.99 +/- 363.35
Episode length: 635.60 +/- 84.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 1956144  |
---------------------------------
Eval num_timesteps=1958136, episode_reward=351.41 +/- 481.00
Episode length: 568.20 +/- 53.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 1958136  |
---------------------------------
Eval num_timesteps=1960128, episode_reward=558.75 +/- 331.14
Episode length: 673.40 +/- 161.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 1960128  |
---------------------------------
Eval num_timesteps=1962120, episode_reward=566.00 +/- 253.58
Episode length: 671.20 +/- 136.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 1962120  |
---------------------------------
Eval num_timesteps=1964112, episode_reward=612.43 +/- 216.03
Episode length: 585.20 +/- 115.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 1964112  |
---------------------------------
Eval num_timesteps=1966104, episode_reward=93.75 +/- 190.54
Episode length: 559.80 +/- 61.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 560          |
|    mean_reward          | 93.7         |
| time/                   |              |
|    total_timesteps      | 1966104      |
| train/                  |              |
|    approx_kl            | 0.0047651287 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.4         |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.001        |
|    loss                 | 0.0537       |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.2          |
|    value_loss           | 0.264        |
------------------------------------------
Eval num_timesteps=1968096, episode_reward=686.28 +/- 402.23
Episode length: 555.40 +/- 82.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 1968096  |
---------------------------------
Eval num_timesteps=1970088, episode_reward=405.13 +/- 287.95
Episode length: 546.00 +/- 44.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 1970088  |
---------------------------------
Eval num_timesteps=1972080, episode_reward=382.52 +/- 546.39
Episode length: 615.80 +/- 104.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 1972080  |
---------------------------------
Eval num_timesteps=1974072, episode_reward=690.86 +/- 616.57
Episode length: 563.20 +/- 57.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 1974072  |
---------------------------------
Eval num_timesteps=1976064, episode_reward=449.99 +/- 279.17
Episode length: 536.60 +/- 54.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 1976064  |
---------------------------------
Eval num_timesteps=1978056, episode_reward=287.02 +/- 119.63
Episode length: 541.20 +/- 67.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 1978056  |
---------------------------------
Eval num_timesteps=1980048, episode_reward=399.61 +/- 476.48
Episode length: 592.20 +/- 87.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 1980048  |
---------------------------------
Eval num_timesteps=1982040, episode_reward=753.63 +/- 389.24
Episode length: 598.80 +/- 84.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 1982040  |
---------------------------------
Eval num_timesteps=1984032, episode_reward=570.08 +/- 362.32
Episode length: 564.00 +/- 65.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 1984032  |
---------------------------------
Eval num_timesteps=1986024, episode_reward=575.33 +/- 293.08
Episode length: 644.80 +/- 81.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 1986024  |
---------------------------------
Eval num_timesteps=1988016, episode_reward=316.71 +/- 284.17
Episode length: 624.40 +/- 65.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 1988016  |
---------------------------------
Eval num_timesteps=1990008, episode_reward=535.09 +/- 495.14
Episode length: 585.20 +/- 113.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 1990008  |
---------------------------------
Eval num_timesteps=1992000, episode_reward=552.47 +/- 508.53
Episode length: 574.00 +/- 83.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
Eval num_timesteps=1993992, episode_reward=494.88 +/- 235.41
Episode length: 615.80 +/- 49.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 1993992  |
---------------------------------
Eval num_timesteps=1995984, episode_reward=753.03 +/- 85.51
Episode length: 626.00 +/- 94.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 1995984  |
---------------------------------
Eval num_timesteps=1997976, episode_reward=469.25 +/- 368.11
Episode length: 550.40 +/- 73.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 1997976  |
---------------------------------
Eval num_timesteps=1999968, episode_reward=219.11 +/- 274.06
Episode length: 620.00 +/- 80.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 1999968  |
---------------------------------
Eval num_timesteps=2001960, episode_reward=551.76 +/- 366.04
Episode length: 523.20 +/- 62.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 2001960  |
---------------------------------
Eval num_timesteps=2003952, episode_reward=282.63 +/- 341.93
Episode length: 557.20 +/- 92.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 2003952  |
---------------------------------
Eval num_timesteps=2005944, episode_reward=572.18 +/- 199.06
Episode length: 540.00 +/- 64.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 2005944  |
---------------------------------
Eval num_timesteps=2007936, episode_reward=283.02 +/- 346.40
Episode length: 577.40 +/- 95.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 2007936  |
---------------------------------
Eval num_timesteps=2009928, episode_reward=225.03 +/- 220.12
Episode length: 591.60 +/- 88.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 2009928  |
---------------------------------
Eval num_timesteps=2011920, episode_reward=130.98 +/- 285.42
Episode length: 513.00 +/- 58.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 2011920  |
---------------------------------
Eval num_timesteps=2013912, episode_reward=515.07 +/- 274.03
Episode length: 583.00 +/- 64.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 2013912  |
---------------------------------
Eval num_timesteps=2015904, episode_reward=424.63 +/- 136.82
Episode length: 601.20 +/- 143.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 601         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 2015904     |
| train/                  |             |
|    approx_kl            | 0.005428645 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0147      |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00176    |
|    std                  | 1.21        |
|    value_loss           | 0.18        |
-----------------------------------------
Eval num_timesteps=2017896, episode_reward=534.81 +/- 303.99
Episode length: 537.40 +/- 46.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 2017896  |
---------------------------------
Eval num_timesteps=2019888, episode_reward=464.03 +/- 316.27
Episode length: 583.80 +/- 47.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 2019888  |
---------------------------------
Eval num_timesteps=2021880, episode_reward=496.55 +/- 347.18
Episode length: 618.40 +/- 107.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 2021880  |
---------------------------------
Eval num_timesteps=2023872, episode_reward=474.85 +/- 299.66
Episode length: 586.00 +/- 99.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 2023872  |
---------------------------------
Eval num_timesteps=2025864, episode_reward=570.23 +/- 205.13
Episode length: 662.40 +/- 74.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 2025864  |
---------------------------------
Eval num_timesteps=2027856, episode_reward=479.07 +/- 461.78
Episode length: 701.00 +/- 137.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 2027856  |
---------------------------------
Eval num_timesteps=2029848, episode_reward=220.82 +/- 220.73
Episode length: 555.40 +/- 28.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 2029848  |
---------------------------------
Eval num_timesteps=2031840, episode_reward=456.48 +/- 292.70
Episode length: 577.60 +/- 73.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 2031840  |
---------------------------------
Eval num_timesteps=2033832, episode_reward=784.79 +/- 851.88
Episode length: 602.00 +/- 78.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 2033832  |
---------------------------------
Eval num_timesteps=2035824, episode_reward=613.59 +/- 337.50
Episode length: 661.60 +/- 191.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 2035824  |
---------------------------------
Eval num_timesteps=2037816, episode_reward=404.91 +/- 376.32
Episode length: 733.60 +/- 230.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 2037816  |
---------------------------------
Eval num_timesteps=2039808, episode_reward=848.88 +/- 344.01
Episode length: 606.40 +/- 65.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 2039808  |
---------------------------------
Eval num_timesteps=2041800, episode_reward=651.17 +/- 91.93
Episode length: 611.40 +/- 142.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 2041800  |
---------------------------------
Eval num_timesteps=2043792, episode_reward=472.22 +/- 440.38
Episode length: 563.20 +/- 51.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 2043792  |
---------------------------------
Eval num_timesteps=2045784, episode_reward=495.51 +/- 360.17
Episode length: 538.60 +/- 49.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 2045784  |
---------------------------------
Eval num_timesteps=2047776, episode_reward=691.46 +/- 293.74
Episode length: 570.00 +/- 36.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 2047776  |
---------------------------------
Eval num_timesteps=2049768, episode_reward=492.24 +/- 288.64
Episode length: 566.00 +/- 53.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 2049768  |
---------------------------------
Eval num_timesteps=2051760, episode_reward=373.42 +/- 334.65
Episode length: 666.80 +/- 125.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 2051760  |
---------------------------------
Eval num_timesteps=2053752, episode_reward=61.69 +/- 201.06
Episode length: 603.00 +/- 98.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 61.7     |
| time/              |          |
|    total_timesteps | 2053752  |
---------------------------------
Eval num_timesteps=2055744, episode_reward=314.47 +/- 325.88
Episode length: 525.60 +/- 56.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 2055744  |
---------------------------------
Eval num_timesteps=2057736, episode_reward=576.14 +/- 304.17
Episode length: 556.60 +/- 127.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 2057736  |
---------------------------------
Eval num_timesteps=2059728, episode_reward=954.14 +/- 251.70
Episode length: 641.40 +/- 58.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 2059728  |
---------------------------------
Eval num_timesteps=2061720, episode_reward=429.32 +/- 241.04
Episode length: 587.00 +/- 68.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 2061720  |
---------------------------------
Eval num_timesteps=2063712, episode_reward=472.41 +/- 158.14
Episode length: 801.60 +/- 328.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 2063712  |
---------------------------------
Eval num_timesteps=2065704, episode_reward=823.17 +/- 211.90
Episode length: 646.00 +/- 146.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 646          |
|    mean_reward          | 823          |
| time/                   |              |
|    total_timesteps      | 2065704      |
| train/                  |              |
|    approx_kl            | 0.0044140867 |
|    clip_fraction        | 0.0524       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.46        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 0.018        |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1.22         |
|    value_loss           | 0.182        |
------------------------------------------
Eval num_timesteps=2067696, episode_reward=639.67 +/- 159.35
Episode length: 577.40 +/- 139.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 2067696  |
---------------------------------
Eval num_timesteps=2069688, episode_reward=956.91 +/- 361.64
Episode length: 624.80 +/- 53.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 2069688  |
---------------------------------
Eval num_timesteps=2071680, episode_reward=716.95 +/- 332.54
Episode length: 507.00 +/- 29.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 2071680  |
---------------------------------
Eval num_timesteps=2073672, episode_reward=685.32 +/- 496.76
Episode length: 562.80 +/- 90.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 2073672  |
---------------------------------
Eval num_timesteps=2075664, episode_reward=905.50 +/- 470.03
Episode length: 665.40 +/- 186.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 2075664  |
---------------------------------
Eval num_timesteps=2077656, episode_reward=804.85 +/- 297.99
Episode length: 555.00 +/- 92.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 2077656  |
---------------------------------
Eval num_timesteps=2079648, episode_reward=632.69 +/- 331.04
Episode length: 600.20 +/- 118.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 2079648  |
---------------------------------
Eval num_timesteps=2081640, episode_reward=739.51 +/- 272.39
Episode length: 540.20 +/- 77.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 2081640  |
---------------------------------
Eval num_timesteps=2083632, episode_reward=598.99 +/- 299.04
Episode length: 556.60 +/- 144.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 2083632  |
---------------------------------
Eval num_timesteps=2085624, episode_reward=563.75 +/- 244.59
Episode length: 550.20 +/- 91.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 2085624  |
---------------------------------
Eval num_timesteps=2087616, episode_reward=668.30 +/- 208.64
Episode length: 544.60 +/- 47.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 2087616  |
---------------------------------
Eval num_timesteps=2089608, episode_reward=746.16 +/- 584.90
Episode length: 582.20 +/- 68.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 2089608  |
---------------------------------
Eval num_timesteps=2091600, episode_reward=431.05 +/- 408.81
Episode length: 562.80 +/- 97.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 2091600  |
---------------------------------
Eval num_timesteps=2093592, episode_reward=467.48 +/- 330.27
Episode length: 567.40 +/- 33.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 2093592  |
---------------------------------
Eval num_timesteps=2095584, episode_reward=567.02 +/- 276.16
Episode length: 586.20 +/- 76.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 2095584  |
---------------------------------
Eval num_timesteps=2097576, episode_reward=599.97 +/- 463.20
Episode length: 557.00 +/- 123.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 2097576  |
---------------------------------
Eval num_timesteps=2099568, episode_reward=679.41 +/- 416.45
Episode length: 542.80 +/- 47.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 2099568  |
---------------------------------
Eval num_timesteps=2101560, episode_reward=865.17 +/- 936.88
Episode length: 523.00 +/- 111.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 2101560  |
---------------------------------
Eval num_timesteps=2103552, episode_reward=583.50 +/- 285.36
Episode length: 617.60 +/- 84.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 2103552  |
---------------------------------
Eval num_timesteps=2105544, episode_reward=886.95 +/- 551.91
Episode length: 575.00 +/- 129.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 2105544  |
---------------------------------
Eval num_timesteps=2107536, episode_reward=904.82 +/- 362.66
Episode length: 598.00 +/- 90.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 2107536  |
---------------------------------
Eval num_timesteps=2109528, episode_reward=713.22 +/- 589.94
Episode length: 548.00 +/- 124.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 2109528  |
---------------------------------
Eval num_timesteps=2111520, episode_reward=623.00 +/- 208.90
Episode length: 521.60 +/- 89.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 2111520  |
---------------------------------
Eval num_timesteps=2113512, episode_reward=486.46 +/- 337.57
Episode length: 548.80 +/- 96.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 2113512  |
---------------------------------
Eval num_timesteps=2115504, episode_reward=338.70 +/- 401.20
Episode length: 490.20 +/- 117.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 490         |
|    mean_reward          | 339         |
| time/                   |             |
|    total_timesteps      | 2115504     |
| train/                  |             |
|    approx_kl            | 0.006478209 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.001       |
|    loss                 | 0.048       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00145    |
|    std                  | 1.22        |
|    value_loss           | 0.243       |
-----------------------------------------
Eval num_timesteps=2117496, episode_reward=803.77 +/- 774.88
Episode length: 497.00 +/- 142.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 2117496  |
---------------------------------
Eval num_timesteps=2119488, episode_reward=179.87 +/- 324.66
Episode length: 462.80 +/- 183.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 2119488  |
---------------------------------
Eval num_timesteps=2121480, episode_reward=271.57 +/- 277.06
Episode length: 419.60 +/- 64.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 2121480  |
---------------------------------
Eval num_timesteps=2123472, episode_reward=774.31 +/- 309.71
Episode length: 548.00 +/- 78.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 2123472  |
---------------------------------
Eval num_timesteps=2125464, episode_reward=552.01 +/- 311.62
Episode length: 498.60 +/- 132.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 2125464  |
---------------------------------
Eval num_timesteps=2127456, episode_reward=150.41 +/- 126.63
Episode length: 380.60 +/- 39.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 2127456  |
---------------------------------
Eval num_timesteps=2129448, episode_reward=433.40 +/- 274.08
Episode length: 454.40 +/- 68.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 2129448  |
---------------------------------
Eval num_timesteps=2131440, episode_reward=214.06 +/- 387.17
Episode length: 491.00 +/- 107.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 2131440  |
---------------------------------
Eval num_timesteps=2133432, episode_reward=415.59 +/- 343.29
Episode length: 418.80 +/- 93.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 2133432  |
---------------------------------
Eval num_timesteps=2135424, episode_reward=448.91 +/- 244.12
Episode length: 456.20 +/- 88.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 2135424  |
---------------------------------
Eval num_timesteps=2137416, episode_reward=252.14 +/- 420.93
Episode length: 392.20 +/- 115.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 2137416  |
---------------------------------
Eval num_timesteps=2139408, episode_reward=-63.90 +/- 108.01
Episode length: 333.80 +/- 40.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | -63.9    |
| time/              |          |
|    total_timesteps | 2139408  |
---------------------------------
Eval num_timesteps=2141400, episode_reward=472.82 +/- 316.01
Episode length: 442.80 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 443      |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 2141400  |
---------------------------------
Eval num_timesteps=2143392, episode_reward=182.40 +/- 197.79
Episode length: 381.00 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 2143392  |
---------------------------------
Eval num_timesteps=2145384, episode_reward=366.41 +/- 537.44
Episode length: 482.20 +/- 250.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 2145384  |
---------------------------------
Eval num_timesteps=2147376, episode_reward=645.05 +/- 469.02
Episode length: 529.20 +/- 91.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 2147376  |
---------------------------------
Eval num_timesteps=2149368, episode_reward=630.78 +/- 530.73
Episode length: 489.80 +/- 130.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 2149368  |
---------------------------------
Eval num_timesteps=2151360, episode_reward=474.74 +/- 132.38
Episode length: 518.20 +/- 107.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 2151360  |
---------------------------------
Eval num_timesteps=2153352, episode_reward=-33.50 +/- 110.87
Episode length: 340.80 +/- 41.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 341      |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 2153352  |
---------------------------------
Eval num_timesteps=2155344, episode_reward=375.25 +/- 316.00
Episode length: 400.80 +/- 33.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 2155344  |
---------------------------------
Eval num_timesteps=2157336, episode_reward=338.50 +/- 137.00
Episode length: 428.00 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 2157336  |
---------------------------------
Eval num_timesteps=2159328, episode_reward=401.69 +/- 362.37
Episode length: 470.60 +/- 141.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 2159328  |
---------------------------------
Eval num_timesteps=2161320, episode_reward=12.75 +/- 43.35
Episode length: 346.80 +/- 27.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 2161320  |
---------------------------------
Eval num_timesteps=2163312, episode_reward=-47.70 +/- 107.74
Episode length: 429.60 +/- 44.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | -47.7       |
| time/                   |             |
|    total_timesteps      | 2163312     |
| train/                  |             |
|    approx_kl            | 0.003493419 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0401      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00158    |
|    std                  | 1.23        |
|    value_loss           | 0.229       |
-----------------------------------------
Eval num_timesteps=2165304, episode_reward=428.89 +/- 428.77
Episode length: 439.60 +/- 32.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 2165304  |
---------------------------------
Eval num_timesteps=2167296, episode_reward=529.39 +/- 334.40
Episode length: 427.00 +/- 42.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 2167296  |
---------------------------------
Eval num_timesteps=2169288, episode_reward=155.53 +/- 295.21
Episode length: 433.00 +/- 166.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 2169288  |
---------------------------------
Eval num_timesteps=2171280, episode_reward=761.65 +/- 511.13
Episode length: 491.80 +/- 83.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 2171280  |
---------------------------------
Eval num_timesteps=2173272, episode_reward=845.60 +/- 355.83
Episode length: 489.60 +/- 63.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 2173272  |
---------------------------------
Eval num_timesteps=2175264, episode_reward=170.61 +/- 203.51
Episode length: 356.40 +/- 71.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 2175264  |
---------------------------------
Eval num_timesteps=2177256, episode_reward=188.23 +/- 280.09
Episode length: 469.20 +/- 87.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 2177256  |
---------------------------------
Eval num_timesteps=2179248, episode_reward=352.41 +/- 592.34
Episode length: 431.20 +/- 121.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 2179248  |
---------------------------------
Eval num_timesteps=2181240, episode_reward=87.79 +/- 183.88
Episode length: 388.60 +/- 50.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 87.8     |
| time/              |          |
|    total_timesteps | 2181240  |
---------------------------------
Eval num_timesteps=2183232, episode_reward=242.77 +/- 313.46
Episode length: 394.80 +/- 74.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | 243      |
| time/              |          |
|    total_timesteps | 2183232  |
---------------------------------
Eval num_timesteps=2185224, episode_reward=-63.99 +/- 106.93
Episode length: 373.20 +/- 24.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 373      |
|    mean_reward     | -64      |
| time/              |          |
|    total_timesteps | 2185224  |
---------------------------------
Eval num_timesteps=2187216, episode_reward=282.21 +/- 328.85
Episode length: 394.20 +/- 114.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 394      |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 2187216  |
---------------------------------
Eval num_timesteps=2189208, episode_reward=34.27 +/- 166.36
Episode length: 383.40 +/- 142.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 34.3     |
| time/              |          |
|    total_timesteps | 2189208  |
---------------------------------
Eval num_timesteps=2191200, episode_reward=221.09 +/- 327.95
Episode length: 373.80 +/- 94.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 374      |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 2191200  |
---------------------------------
Eval num_timesteps=2193192, episode_reward=723.18 +/- 788.63
Episode length: 480.60 +/- 83.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 2193192  |
---------------------------------
Eval num_timesteps=2195184, episode_reward=16.51 +/- 115.26
Episode length: 381.20 +/- 54.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 2195184  |
---------------------------------
Eval num_timesteps=2197176, episode_reward=153.97 +/- 284.54
Episode length: 494.60 +/- 140.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 154      |
| time/              |          |
|    total_timesteps | 2197176  |
---------------------------------
Eval num_timesteps=2199168, episode_reward=608.61 +/- 732.64
Episode length: 523.60 +/- 146.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 2199168  |
---------------------------------
Eval num_timesteps=2201160, episode_reward=245.70 +/- 217.25
Episode length: 420.60 +/- 74.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 2201160  |
---------------------------------
Eval num_timesteps=2203152, episode_reward=343.16 +/- 457.15
Episode length: 507.20 +/- 105.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 2203152  |
---------------------------------
Eval num_timesteps=2205144, episode_reward=331.31 +/- 435.09
Episode length: 409.00 +/- 68.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 409      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 2205144  |
---------------------------------
Eval num_timesteps=2207136, episode_reward=335.26 +/- 187.54
Episode length: 400.00 +/- 29.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 2207136  |
---------------------------------
Eval num_timesteps=2209128, episode_reward=281.09 +/- 233.78
Episode length: 401.20 +/- 51.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 2209128  |
---------------------------------
Eval num_timesteps=2211120, episode_reward=70.87 +/- 162.90
Episode length: 358.80 +/- 72.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 359      |
|    mean_reward     | 70.9     |
| time/              |          |
|    total_timesteps | 2211120  |
---------------------------------
Eval num_timesteps=2213112, episode_reward=656.26 +/- 344.72
Episode length: 433.60 +/- 29.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 656          |
| time/                   |              |
|    total_timesteps      | 2213112      |
| train/                  |              |
|    approx_kl            | 0.0054542953 |
|    clip_fraction        | 0.0557       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 0.0317       |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.24         |
|    value_loss           | 0.216        |
------------------------------------------
Eval num_timesteps=2215104, episode_reward=233.84 +/- 143.89
Episode length: 406.60 +/- 37.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 407      |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 2215104  |
---------------------------------
Eval num_timesteps=2217096, episode_reward=458.79 +/- 154.55
Episode length: 480.40 +/- 27.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 2217096  |
---------------------------------
Eval num_timesteps=2219088, episode_reward=427.57 +/- 353.58
Episode length: 440.20 +/- 101.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 2219088  |
---------------------------------
Eval num_timesteps=2221080, episode_reward=392.91 +/- 223.41
Episode length: 477.40 +/- 95.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 2221080  |
---------------------------------
Eval num_timesteps=2223072, episode_reward=233.74 +/- 328.84
Episode length: 380.20 +/- 98.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 2223072  |
---------------------------------
Eval num_timesteps=2225064, episode_reward=543.38 +/- 436.64
Episode length: 419.80 +/- 76.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 2225064  |
---------------------------------
Eval num_timesteps=2227056, episode_reward=672.08 +/- 306.72
Episode length: 429.20 +/- 38.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2227056  |
---------------------------------
Eval num_timesteps=2229048, episode_reward=320.94 +/- 510.86
Episode length: 370.80 +/- 87.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 2229048  |
---------------------------------
Eval num_timesteps=2231040, episode_reward=642.29 +/- 347.94
Episode length: 446.60 +/- 49.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 2231040  |
---------------------------------
Eval num_timesteps=2233032, episode_reward=457.61 +/- 443.25
Episode length: 423.00 +/- 73.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 423      |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 2233032  |
---------------------------------
Eval num_timesteps=2235024, episode_reward=473.97 +/- 249.50
Episode length: 440.00 +/- 55.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 2235024  |
---------------------------------
Eval num_timesteps=2237016, episode_reward=398.70 +/- 383.50
Episode length: 429.40 +/- 91.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 2237016  |
---------------------------------
Eval num_timesteps=2239008, episode_reward=90.17 +/- 271.06
Episode length: 301.80 +/- 84.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 90.2     |
| time/              |          |
|    total_timesteps | 2239008  |
---------------------------------
Eval num_timesteps=2241000, episode_reward=451.98 +/- 297.42
Episode length: 451.00 +/- 74.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 451      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 2241000  |
---------------------------------
Eval num_timesteps=2242992, episode_reward=516.39 +/- 243.33
Episode length: 432.00 +/- 65.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 2242992  |
---------------------------------
Eval num_timesteps=2244984, episode_reward=419.50 +/- 371.07
Episode length: 456.20 +/- 87.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 2244984  |
---------------------------------
Eval num_timesteps=2246976, episode_reward=373.53 +/- 158.83
Episode length: 428.80 +/- 49.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 2246976  |
---------------------------------
Eval num_timesteps=2248968, episode_reward=215.26 +/- 277.52
Episode length: 376.00 +/- 102.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 2248968  |
---------------------------------
Eval num_timesteps=2250960, episode_reward=618.27 +/- 278.15
Episode length: 477.60 +/- 50.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 2250960  |
---------------------------------
Eval num_timesteps=2252952, episode_reward=437.94 +/- 401.76
Episode length: 580.00 +/- 273.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 2252952  |
---------------------------------
Eval num_timesteps=2254944, episode_reward=329.61 +/- 265.12
Episode length: 401.00 +/- 56.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 2254944  |
---------------------------------
Eval num_timesteps=2256936, episode_reward=745.25 +/- 269.13
Episode length: 457.40 +/- 26.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 2256936  |
---------------------------------
Eval num_timesteps=2258928, episode_reward=440.23 +/- 385.69
Episode length: 422.40 +/- 27.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 2258928  |
---------------------------------
Eval num_timesteps=2260920, episode_reward=772.01 +/- 288.57
Episode length: 500.80 +/- 89.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 2260920  |
---------------------------------
Eval num_timesteps=2262912, episode_reward=465.68 +/- 312.22
Episode length: 545.20 +/- 123.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 545          |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 2262912      |
| train/                  |              |
|    approx_kl            | 0.0057239155 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.54        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0255       |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.25         |
|    value_loss           | 0.201        |
------------------------------------------
Eval num_timesteps=2264904, episode_reward=851.27 +/- 885.44
Episode length: 529.20 +/- 118.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 2264904  |
---------------------------------
Eval num_timesteps=2266896, episode_reward=723.69 +/- 399.59
Episode length: 536.60 +/- 110.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 2266896  |
---------------------------------
Eval num_timesteps=2268888, episode_reward=828.65 +/- 204.15
Episode length: 559.80 +/- 129.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 2268888  |
---------------------------------
Eval num_timesteps=2270880, episode_reward=1137.80 +/- 946.98
Episode length: 675.80 +/- 178.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2270880  |
---------------------------------
New best mean reward!
Eval num_timesteps=2272872, episode_reward=605.52 +/- 225.04
Episode length: 486.40 +/- 73.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 2272872  |
---------------------------------
Eval num_timesteps=2274864, episode_reward=672.13 +/- 382.66
Episode length: 532.20 +/- 115.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2274864  |
---------------------------------
Eval num_timesteps=2276856, episode_reward=684.86 +/- 353.14
Episode length: 509.60 +/- 151.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 2276856  |
---------------------------------
Eval num_timesteps=2278848, episode_reward=1086.06 +/- 369.93
Episode length: 702.40 +/- 183.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2278848  |
---------------------------------
Eval num_timesteps=2280840, episode_reward=921.19 +/- 659.59
Episode length: 535.80 +/- 152.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 2280840  |
---------------------------------
Eval num_timesteps=2282832, episode_reward=641.49 +/- 177.74
Episode length: 534.00 +/- 116.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 2282832  |
---------------------------------
Eval num_timesteps=2284824, episode_reward=1053.88 +/- 117.52
Episode length: 539.80 +/- 95.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2284824  |
---------------------------------
Eval num_timesteps=2286816, episode_reward=717.12 +/- 580.50
Episode length: 568.80 +/- 172.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 2286816  |
---------------------------------
Eval num_timesteps=2288808, episode_reward=568.89 +/- 289.39
Episode length: 593.40 +/- 208.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 2288808  |
---------------------------------
Eval num_timesteps=2290800, episode_reward=632.96 +/- 449.29
Episode length: 602.60 +/- 140.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 2290800  |
---------------------------------
Eval num_timesteps=2292792, episode_reward=685.99 +/- 234.80
Episode length: 520.80 +/- 131.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 2292792  |
---------------------------------
Eval num_timesteps=2294784, episode_reward=909.31 +/- 436.72
Episode length: 602.60 +/- 124.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 2294784  |
---------------------------------
Eval num_timesteps=2296776, episode_reward=777.93 +/- 191.16
Episode length: 567.40 +/- 119.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 2296776  |
---------------------------------
Eval num_timesteps=2298768, episode_reward=826.11 +/- 518.12
Episode length: 548.20 +/- 143.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 2298768  |
---------------------------------
Eval num_timesteps=2300760, episode_reward=868.83 +/- 549.36
Episode length: 532.20 +/- 107.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 2300760  |
---------------------------------
Eval num_timesteps=2302752, episode_reward=330.72 +/- 309.93
Episode length: 497.60 +/- 172.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 2302752  |
---------------------------------
Eval num_timesteps=2304744, episode_reward=466.03 +/- 303.53
Episode length: 479.20 +/- 128.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 479      |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 2304744  |
---------------------------------
Eval num_timesteps=2306736, episode_reward=676.76 +/- 380.44
Episode length: 554.60 +/- 163.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 2306736  |
---------------------------------
Eval num_timesteps=2308728, episode_reward=554.22 +/- 368.40
Episode length: 536.60 +/- 214.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | 554      |
| time/              |          |
|    total_timesteps | 2308728  |
---------------------------------
Eval num_timesteps=2310720, episode_reward=1116.26 +/- 683.95
Episode length: 616.00 +/- 80.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 616          |
|    mean_reward          | 1.12e+03     |
| time/                   |              |
|    total_timesteps      | 2310720      |
| train/                  |              |
|    approx_kl            | 0.0060222163 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.56        |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 0.0336       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00196     |
|    std                  | 1.25         |
|    value_loss           | 0.217        |
------------------------------------------
Eval num_timesteps=2312712, episode_reward=928.65 +/- 183.97
Episode length: 629.00 +/- 103.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 2312712  |
---------------------------------
Eval num_timesteps=2314704, episode_reward=671.67 +/- 433.41
Episode length: 568.80 +/- 137.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2314704  |
---------------------------------
Eval num_timesteps=2316696, episode_reward=1125.07 +/- 207.33
Episode length: 614.80 +/- 158.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2316696  |
---------------------------------
Eval num_timesteps=2318688, episode_reward=714.16 +/- 572.22
Episode length: 526.20 +/- 74.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 2318688  |
---------------------------------
Eval num_timesteps=2320680, episode_reward=914.61 +/- 439.81
Episode length: 583.40 +/- 91.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 2320680  |
---------------------------------
Eval num_timesteps=2322672, episode_reward=587.89 +/- 585.71
Episode length: 544.80 +/- 151.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 2322672  |
---------------------------------
Eval num_timesteps=2324664, episode_reward=586.40 +/- 468.89
Episode length: 562.00 +/- 93.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 586      |
| time/              |          |
|    total_timesteps | 2324664  |
---------------------------------
Eval num_timesteps=2326656, episode_reward=603.08 +/- 300.01
Episode length: 518.60 +/- 133.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 2326656  |
---------------------------------
Eval num_timesteps=2328648, episode_reward=906.76 +/- 521.38
Episode length: 565.80 +/- 115.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 2328648  |
---------------------------------
Eval num_timesteps=2330640, episode_reward=1111.76 +/- 743.55
Episode length: 616.80 +/- 190.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2330640  |
---------------------------------
Eval num_timesteps=2332632, episode_reward=733.93 +/- 397.98
Episode length: 586.00 +/- 172.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 2332632  |
---------------------------------
Eval num_timesteps=2334624, episode_reward=1032.85 +/- 528.24
Episode length: 569.20 +/- 145.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2334624  |
---------------------------------
Eval num_timesteps=2336616, episode_reward=534.74 +/- 306.15
Episode length: 601.60 +/- 138.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 2336616  |
---------------------------------
Eval num_timesteps=2338608, episode_reward=715.60 +/- 151.53
Episode length: 816.60 +/- 323.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 2338608  |
---------------------------------
Eval num_timesteps=2340600, episode_reward=806.05 +/- 196.44
Episode length: 655.80 +/- 96.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 2340600  |
---------------------------------
Eval num_timesteps=2342592, episode_reward=784.22 +/- 252.47
Episode length: 549.60 +/- 151.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 2342592  |
---------------------------------
Eval num_timesteps=2344584, episode_reward=1329.28 +/- 704.06
Episode length: 738.60 +/- 200.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2344584  |
---------------------------------
New best mean reward!
Eval num_timesteps=2346576, episode_reward=653.96 +/- 265.50
Episode length: 544.80 +/- 125.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 2346576  |
---------------------------------
Eval num_timesteps=2348568, episode_reward=1172.87 +/- 424.14
Episode length: 574.20 +/- 63.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2348568  |
---------------------------------
Eval num_timesteps=2350560, episode_reward=818.94 +/- 96.08
Episode length: 557.20 +/- 53.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 2350560  |
---------------------------------
Eval num_timesteps=2352552, episode_reward=609.95 +/- 285.44
Episode length: 515.80 +/- 107.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 2352552  |
---------------------------------
Eval num_timesteps=2354544, episode_reward=667.91 +/- 549.81
Episode length: 533.00 +/- 111.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 2354544  |
---------------------------------
Eval num_timesteps=2356536, episode_reward=773.60 +/- 367.77
Episode length: 523.20 +/- 103.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 2356536  |
---------------------------------
Eval num_timesteps=2358528, episode_reward=767.22 +/- 644.34
Episode length: 603.00 +/- 130.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 2358528  |
---------------------------------
Eval num_timesteps=2360520, episode_reward=438.74 +/- 484.08
Episode length: 718.60 +/- 165.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 719          |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 2360520      |
| train/                  |              |
|    approx_kl            | 0.0051154955 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.57        |
|    explained_variance   | 0.92         |
|    learning_rate        | 0.001        |
|    loss                 | 0.0168       |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.26         |
|    value_loss           | 0.18         |
------------------------------------------
Eval num_timesteps=2362512, episode_reward=762.33 +/- 204.59
Episode length: 612.40 +/- 127.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 2362512  |
---------------------------------
Eval num_timesteps=2364504, episode_reward=855.40 +/- 365.50
Episode length: 590.80 +/- 80.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 2364504  |
---------------------------------
Eval num_timesteps=2366496, episode_reward=711.14 +/- 456.19
Episode length: 557.40 +/- 176.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 2366496  |
---------------------------------
Eval num_timesteps=2368488, episode_reward=701.81 +/- 404.84
Episode length: 552.20 +/- 61.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 2368488  |
---------------------------------
Eval num_timesteps=2370480, episode_reward=522.92 +/- 349.32
Episode length: 646.20 +/- 137.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 2370480  |
---------------------------------
Eval num_timesteps=2372472, episode_reward=612.38 +/- 332.60
Episode length: 618.60 +/- 45.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 2372472  |
---------------------------------
Eval num_timesteps=2374464, episode_reward=637.68 +/- 382.81
Episode length: 651.80 +/- 217.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 2374464  |
---------------------------------
Eval num_timesteps=2376456, episode_reward=870.93 +/- 206.83
Episode length: 599.40 +/- 80.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 2376456  |
---------------------------------
Eval num_timesteps=2378448, episode_reward=988.26 +/- 630.45
Episode length: 642.80 +/- 54.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 2378448  |
---------------------------------
Eval num_timesteps=2380440, episode_reward=724.65 +/- 172.96
Episode length: 647.20 +/- 138.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 2380440  |
---------------------------------
Eval num_timesteps=2382432, episode_reward=1582.39 +/- 1087.29
Episode length: 727.20 +/- 135.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 2382432  |
---------------------------------
New best mean reward!
Eval num_timesteps=2384424, episode_reward=553.72 +/- 393.67
Episode length: 591.80 +/- 80.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 554      |
| time/              |          |
|    total_timesteps | 2384424  |
---------------------------------
Eval num_timesteps=2386416, episode_reward=474.78 +/- 207.44
Episode length: 495.60 +/- 58.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 2386416  |
---------------------------------
Eval num_timesteps=2388408, episode_reward=409.43 +/- 297.74
Episode length: 593.40 +/- 106.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 2388408  |
---------------------------------
Eval num_timesteps=2390400, episode_reward=337.49 +/- 405.61
Episode length: 485.60 +/- 190.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 2390400  |
---------------------------------
Eval num_timesteps=2392392, episode_reward=934.91 +/- 375.37
Episode length: 676.40 +/- 106.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 2392392  |
---------------------------------
Eval num_timesteps=2394384, episode_reward=833.72 +/- 161.39
Episode length: 573.80 +/- 115.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 2394384  |
---------------------------------
Eval num_timesteps=2396376, episode_reward=926.85 +/- 185.02
Episode length: 700.40 +/- 139.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 2396376  |
---------------------------------
Eval num_timesteps=2398368, episode_reward=560.01 +/- 340.33
Episode length: 685.40 +/- 102.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 560      |
| time/              |          |
|    total_timesteps | 2398368  |
---------------------------------
Eval num_timesteps=2400360, episode_reward=745.15 +/- 247.57
Episode length: 568.60 +/- 41.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 2400360  |
---------------------------------
Eval num_timesteps=2402352, episode_reward=573.35 +/- 698.23
Episode length: 839.80 +/- 224.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 2402352  |
---------------------------------
Eval num_timesteps=2404344, episode_reward=651.45 +/- 241.05
Episode length: 601.20 +/- 70.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 2404344  |
---------------------------------
Eval num_timesteps=2406336, episode_reward=616.90 +/- 417.39
Episode length: 648.40 +/- 82.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 2406336  |
---------------------------------
Eval num_timesteps=2408328, episode_reward=953.23 +/- 948.93
Episode length: 701.80 +/- 52.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 2408328  |
---------------------------------
Eval num_timesteps=2410320, episode_reward=872.97 +/- 94.70
Episode length: 633.00 +/- 47.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 633         |
|    mean_reward          | 873         |
| time/                   |             |
|    total_timesteps      | 2410320     |
| train/                  |             |
|    approx_kl            | 0.004817361 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00333     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00124    |
|    std                  | 1.26        |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=2412312, episode_reward=485.26 +/- 489.24
Episode length: 615.60 +/- 138.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 2412312  |
---------------------------------
Eval num_timesteps=2414304, episode_reward=155.59 +/- 108.90
Episode length: 722.60 +/- 129.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 2414304  |
---------------------------------
Eval num_timesteps=2416296, episode_reward=842.50 +/- 510.07
Episode length: 636.80 +/- 86.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 2416296  |
---------------------------------
Eval num_timesteps=2418288, episode_reward=782.08 +/- 295.20
Episode length: 622.60 +/- 203.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 2418288  |
---------------------------------
Eval num_timesteps=2420280, episode_reward=133.95 +/- 133.80
Episode length: 685.40 +/- 137.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 2420280  |
---------------------------------
Eval num_timesteps=2422272, episode_reward=846.30 +/- 132.94
Episode length: 735.40 +/- 150.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 2422272  |
---------------------------------
Eval num_timesteps=2424264, episode_reward=520.80 +/- 281.89
Episode length: 561.40 +/- 72.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 2424264  |
---------------------------------
Eval num_timesteps=2426256, episode_reward=314.76 +/- 258.87
Episode length: 621.40 +/- 179.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 2426256  |
---------------------------------
Eval num_timesteps=2428248, episode_reward=785.46 +/- 452.69
Episode length: 676.40 +/- 88.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 2428248  |
---------------------------------
Eval num_timesteps=2430240, episode_reward=624.99 +/- 365.57
Episode length: 607.40 +/- 106.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 2430240  |
---------------------------------
Eval num_timesteps=2432232, episode_reward=415.62 +/- 362.32
Episode length: 722.40 +/- 157.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 2432232  |
---------------------------------
Eval num_timesteps=2434224, episode_reward=522.29 +/- 320.96
Episode length: 607.20 +/- 127.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 2434224  |
---------------------------------
Eval num_timesteps=2436216, episode_reward=576.58 +/- 532.71
Episode length: 625.00 +/- 159.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 2436216  |
---------------------------------
Eval num_timesteps=2438208, episode_reward=617.13 +/- 632.90
Episode length: 801.00 +/- 226.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 2438208  |
---------------------------------
Eval num_timesteps=2440200, episode_reward=425.67 +/- 461.07
Episode length: 558.00 +/- 50.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 2440200  |
---------------------------------
Eval num_timesteps=2442192, episode_reward=884.07 +/- 262.21
Episode length: 615.00 +/- 97.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 2442192  |
---------------------------------
Eval num_timesteps=2444184, episode_reward=619.75 +/- 399.45
Episode length: 632.80 +/- 84.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 2444184  |
---------------------------------
Eval num_timesteps=2446176, episode_reward=384.31 +/- 468.78
Episode length: 653.40 +/- 136.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 2446176  |
---------------------------------
Eval num_timesteps=2448168, episode_reward=561.46 +/- 218.69
Episode length: 643.00 +/- 121.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 2448168  |
---------------------------------
Eval num_timesteps=2450160, episode_reward=1345.18 +/- 591.77
Episode length: 897.00 +/- 322.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2450160  |
---------------------------------
Eval num_timesteps=2452152, episode_reward=644.49 +/- 487.66
Episode length: 653.40 +/- 175.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 2452152  |
---------------------------------
Eval num_timesteps=2454144, episode_reward=517.27 +/- 497.11
Episode length: 633.80 +/- 120.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 2454144  |
---------------------------------
Eval num_timesteps=2456136, episode_reward=552.63 +/- 372.32
Episode length: 648.20 +/- 132.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 2456136  |
---------------------------------
Eval num_timesteps=2458128, episode_reward=477.40 +/- 334.99
Episode length: 673.60 +/- 133.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 674          |
|    mean_reward          | 477          |
| time/                   |              |
|    total_timesteps      | 2458128      |
| train/                  |              |
|    approx_kl            | 0.0059700795 |
|    clip_fraction        | 0.0532       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.61        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0199      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.27         |
|    value_loss           | 0.106        |
------------------------------------------
Eval num_timesteps=2460120, episode_reward=562.51 +/- 642.24
Episode length: 608.60 +/- 122.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 563      |
| time/              |          |
|    total_timesteps | 2460120  |
---------------------------------
Eval num_timesteps=2462112, episode_reward=401.38 +/- 178.97
Episode length: 599.60 +/- 146.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 2462112  |
---------------------------------
Eval num_timesteps=2464104, episode_reward=532.04 +/- 271.57
Episode length: 568.80 +/- 66.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 2464104  |
---------------------------------
Eval num_timesteps=2466096, episode_reward=217.91 +/- 275.72
Episode length: 658.20 +/- 128.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 2466096  |
---------------------------------
Eval num_timesteps=2468088, episode_reward=738.22 +/- 413.13
Episode length: 677.20 +/- 123.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 2468088  |
---------------------------------
Eval num_timesteps=2470080, episode_reward=1196.51 +/- 796.08
Episode length: 702.40 +/- 101.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2470080  |
---------------------------------
Eval num_timesteps=2472072, episode_reward=884.82 +/- 782.30
Episode length: 635.00 +/- 86.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 2472072  |
---------------------------------
Eval num_timesteps=2474064, episode_reward=458.66 +/- 274.41
Episode length: 576.20 +/- 65.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 2474064  |
---------------------------------
Eval num_timesteps=2476056, episode_reward=712.79 +/- 255.04
Episode length: 643.40 +/- 43.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 2476056  |
---------------------------------
Eval num_timesteps=2478048, episode_reward=138.95 +/- 172.04
Episode length: 601.60 +/- 80.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 2478048  |
---------------------------------
Eval num_timesteps=2480040, episode_reward=550.09 +/- 580.78
Episode length: 616.40 +/- 46.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 2480040  |
---------------------------------
Eval num_timesteps=2482032, episode_reward=161.19 +/- 105.97
Episode length: 602.80 +/- 209.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 2482032  |
---------------------------------
Eval num_timesteps=2484024, episode_reward=564.05 +/- 311.92
Episode length: 614.00 +/- 140.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 2484024  |
---------------------------------
Eval num_timesteps=2486016, episode_reward=563.55 +/- 344.91
Episode length: 603.20 +/- 83.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 2486016  |
---------------------------------
Eval num_timesteps=2488008, episode_reward=549.58 +/- 347.40
Episode length: 593.00 +/- 68.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 2488008  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=417.42 +/- 282.94
Episode length: 692.00 +/- 41.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 2490000  |
---------------------------------
Eval num_timesteps=2491992, episode_reward=904.97 +/- 255.54
Episode length: 680.20 +/- 76.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 2491992  |
---------------------------------
Eval num_timesteps=2493984, episode_reward=82.22 +/- 142.94
Episode length: 592.40 +/- 66.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 82.2     |
| time/              |          |
|    total_timesteps | 2493984  |
---------------------------------
Eval num_timesteps=2495976, episode_reward=279.99 +/- 405.70
Episode length: 660.40 +/- 123.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 2495976  |
---------------------------------
Eval num_timesteps=2497968, episode_reward=491.55 +/- 189.26
Episode length: 520.60 +/- 121.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 2497968  |
---------------------------------
Eval num_timesteps=2499960, episode_reward=473.71 +/- 516.67
Episode length: 592.20 +/- 84.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 2499960  |
---------------------------------
Eval num_timesteps=2501952, episode_reward=732.91 +/- 530.02
Episode length: 611.40 +/- 110.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 2501952  |
---------------------------------
Eval num_timesteps=2503944, episode_reward=451.50 +/- 312.76
Episode length: 559.80 +/- 83.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 2503944  |
---------------------------------
Eval num_timesteps=2505936, episode_reward=604.20 +/- 276.57
Episode length: 554.20 +/- 79.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 2505936  |
---------------------------------
Eval num_timesteps=2507928, episode_reward=885.94 +/- 525.65
Episode length: 759.60 +/- 155.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 760          |
|    mean_reward          | 886          |
| time/                   |              |
|    total_timesteps      | 2507928      |
| train/                  |              |
|    approx_kl            | 0.0057016197 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.64        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0335      |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.28         |
|    value_loss           | 0.0776       |
------------------------------------------
Eval num_timesteps=2509920, episode_reward=622.41 +/- 379.07
Episode length: 578.00 +/- 143.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 2509920  |
---------------------------------
Eval num_timesteps=2511912, episode_reward=757.41 +/- 141.85
Episode length: 680.60 +/- 170.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 2511912  |
---------------------------------
Eval num_timesteps=2513904, episode_reward=991.44 +/- 627.59
Episode length: 719.80 +/- 136.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 2513904  |
---------------------------------
Eval num_timesteps=2515896, episode_reward=220.70 +/- 184.90
Episode length: 607.80 +/- 160.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 2515896  |
---------------------------------
Eval num_timesteps=2517888, episode_reward=591.00 +/- 245.30
Episode length: 684.80 +/- 115.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 2517888  |
---------------------------------
Eval num_timesteps=2519880, episode_reward=1081.54 +/- 634.10
Episode length: 700.80 +/- 126.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2519880  |
---------------------------------
Eval num_timesteps=2521872, episode_reward=658.03 +/- 568.54
Episode length: 769.60 +/- 165.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 2521872  |
---------------------------------
Eval num_timesteps=2523864, episode_reward=822.15 +/- 180.27
Episode length: 695.40 +/- 215.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 2523864  |
---------------------------------
Eval num_timesteps=2525856, episode_reward=757.07 +/- 121.35
Episode length: 651.20 +/- 151.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 2525856  |
---------------------------------
Eval num_timesteps=2527848, episode_reward=746.71 +/- 442.19
Episode length: 645.00 +/- 188.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 2527848  |
---------------------------------
Eval num_timesteps=2529840, episode_reward=794.80 +/- 129.40
Episode length: 539.40 +/- 33.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 2529840  |
---------------------------------
Eval num_timesteps=2531832, episode_reward=725.07 +/- 141.34
Episode length: 634.00 +/- 108.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 2531832  |
---------------------------------
Eval num_timesteps=2533824, episode_reward=524.68 +/- 675.89
Episode length: 830.20 +/- 202.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 2533824  |
---------------------------------
Eval num_timesteps=2535816, episode_reward=763.79 +/- 420.08
Episode length: 645.40 +/- 99.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 2535816  |
---------------------------------
Eval num_timesteps=2537808, episode_reward=746.47 +/- 323.65
Episode length: 700.40 +/- 183.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 2537808  |
---------------------------------
Eval num_timesteps=2539800, episode_reward=1071.60 +/- 153.85
Episode length: 945.40 +/- 175.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 945      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2539800  |
---------------------------------
Eval num_timesteps=2541792, episode_reward=654.87 +/- 267.29
Episode length: 597.00 +/- 151.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 2541792  |
---------------------------------
Eval num_timesteps=2543784, episode_reward=949.38 +/- 700.55
Episode length: 701.00 +/- 65.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 2543784  |
---------------------------------
Eval num_timesteps=2545776, episode_reward=707.61 +/- 565.18
Episode length: 503.40 +/- 116.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 503      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 2545776  |
---------------------------------
Eval num_timesteps=2547768, episode_reward=711.43 +/- 490.57
Episode length: 654.60 +/- 117.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 2547768  |
---------------------------------
Eval num_timesteps=2549760, episode_reward=881.76 +/- 176.62
Episode length: 612.60 +/- 50.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 2549760  |
---------------------------------
Eval num_timesteps=2551752, episode_reward=722.65 +/- 261.62
Episode length: 618.60 +/- 100.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 2551752  |
---------------------------------
Eval num_timesteps=2553744, episode_reward=1010.94 +/- 461.59
Episode length: 761.80 +/- 137.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2553744  |
---------------------------------
Eval num_timesteps=2555736, episode_reward=668.51 +/- 158.64
Episode length: 552.60 +/- 130.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 2555736  |
---------------------------------
Eval num_timesteps=2557728, episode_reward=632.26 +/- 460.45
Episode length: 655.20 +/- 204.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 655          |
|    mean_reward          | 632          |
| time/                   |              |
|    total_timesteps      | 2557728      |
| train/                  |              |
|    approx_kl            | 0.0050515938 |
|    clip_fraction        | 0.0576       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.67        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0131      |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.29         |
|    value_loss           | 0.121        |
------------------------------------------
Eval num_timesteps=2559720, episode_reward=318.62 +/- 326.83
Episode length: 570.20 +/- 172.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 2559720  |
---------------------------------
Eval num_timesteps=2561712, episode_reward=834.69 +/- 341.49
Episode length: 631.20 +/- 64.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 2561712  |
---------------------------------
Eval num_timesteps=2563704, episode_reward=1024.09 +/- 688.68
Episode length: 567.80 +/- 113.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2563704  |
---------------------------------
Eval num_timesteps=2565696, episode_reward=626.78 +/- 353.62
Episode length: 685.00 +/- 126.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 627      |
| time/              |          |
|    total_timesteps | 2565696  |
---------------------------------
Eval num_timesteps=2567688, episode_reward=829.70 +/- 200.07
Episode length: 620.00 +/- 106.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 2567688  |
---------------------------------
Eval num_timesteps=2569680, episode_reward=446.11 +/- 335.31
Episode length: 615.20 +/- 99.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 2569680  |
---------------------------------
Eval num_timesteps=2571672, episode_reward=710.66 +/- 196.14
Episode length: 577.80 +/- 95.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 2571672  |
---------------------------------
Eval num_timesteps=2573664, episode_reward=791.99 +/- 506.33
Episode length: 580.40 +/- 150.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 2573664  |
---------------------------------
Eval num_timesteps=2575656, episode_reward=511.76 +/- 183.97
Episode length: 526.40 +/- 53.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 526      |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 2575656  |
---------------------------------
Eval num_timesteps=2577648, episode_reward=593.12 +/- 469.68
Episode length: 578.20 +/- 207.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 2577648  |
---------------------------------
Eval num_timesteps=2579640, episode_reward=608.23 +/- 356.29
Episode length: 571.20 +/- 145.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 2579640  |
---------------------------------
Eval num_timesteps=2581632, episode_reward=525.65 +/- 385.05
Episode length: 585.60 +/- 153.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 2581632  |
---------------------------------
Eval num_timesteps=2583624, episode_reward=798.92 +/- 138.82
Episode length: 551.40 +/- 96.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 2583624  |
---------------------------------
Eval num_timesteps=2585616, episode_reward=724.15 +/- 180.59
Episode length: 637.20 +/- 126.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 2585616  |
---------------------------------
Eval num_timesteps=2587608, episode_reward=959.90 +/- 565.00
Episode length: 540.80 +/- 45.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 2587608  |
---------------------------------
Eval num_timesteps=2589600, episode_reward=940.80 +/- 620.91
Episode length: 662.80 +/- 161.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 2589600  |
---------------------------------
Eval num_timesteps=2591592, episode_reward=821.00 +/- 151.47
Episode length: 683.80 +/- 204.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 2591592  |
---------------------------------
Eval num_timesteps=2593584, episode_reward=562.05 +/- 283.55
Episode length: 539.40 +/- 91.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 2593584  |
---------------------------------
Eval num_timesteps=2595576, episode_reward=1173.44 +/- 509.17
Episode length: 597.40 +/- 108.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2595576  |
---------------------------------
Eval num_timesteps=2597568, episode_reward=826.84 +/- 325.35
Episode length: 804.00 +/- 237.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 2597568  |
---------------------------------
Eval num_timesteps=2599560, episode_reward=740.49 +/- 473.09
Episode length: 701.20 +/- 239.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 2599560  |
---------------------------------
Eval num_timesteps=2601552, episode_reward=826.46 +/- 465.37
Episode length: 543.00 +/- 121.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 2601552  |
---------------------------------
Eval num_timesteps=2603544, episode_reward=558.39 +/- 331.02
Episode length: 630.20 +/- 129.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 2603544  |
---------------------------------
Eval num_timesteps=2605536, episode_reward=887.04 +/- 276.98
Episode length: 662.20 +/- 158.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 662          |
|    mean_reward          | 887          |
| time/                   |              |
|    total_timesteps      | 2605536      |
| train/                  |              |
|    approx_kl            | 0.0048957416 |
|    clip_fraction        | 0.0447       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.68        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | -0.00428     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.28         |
|    value_loss           | 0.139        |
------------------------------------------
Eval num_timesteps=2607528, episode_reward=698.75 +/- 217.23
Episode length: 583.20 +/- 109.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 2607528  |
---------------------------------
Eval num_timesteps=2609520, episode_reward=744.37 +/- 211.21
Episode length: 732.80 +/- 178.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 2609520  |
---------------------------------
Eval num_timesteps=2611512, episode_reward=590.05 +/- 176.30
Episode length: 515.20 +/- 71.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 2611512  |
---------------------------------
Eval num_timesteps=2613504, episode_reward=531.23 +/- 206.40
Episode length: 540.60 +/- 131.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 2613504  |
---------------------------------
Eval num_timesteps=2615496, episode_reward=661.21 +/- 283.61
Episode length: 543.40 +/- 116.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 2615496  |
---------------------------------
Eval num_timesteps=2617488, episode_reward=784.34 +/- 398.57
Episode length: 596.60 +/- 102.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 2617488  |
---------------------------------
Eval num_timesteps=2619480, episode_reward=596.28 +/- 235.52
Episode length: 620.60 +/- 140.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 2619480  |
---------------------------------
Eval num_timesteps=2621472, episode_reward=319.24 +/- 123.69
Episode length: 496.40 +/- 137.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 2621472  |
---------------------------------
Eval num_timesteps=2623464, episode_reward=733.74 +/- 227.47
Episode length: 643.40 +/- 118.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 2623464  |
---------------------------------
Eval num_timesteps=2625456, episode_reward=986.68 +/- 310.82
Episode length: 724.40 +/- 140.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 2625456  |
---------------------------------
Eval num_timesteps=2627448, episode_reward=1133.81 +/- 626.15
Episode length: 618.80 +/- 101.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2627448  |
---------------------------------
Eval num_timesteps=2629440, episode_reward=641.65 +/- 91.21
Episode length: 566.40 +/- 45.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 2629440  |
---------------------------------
Eval num_timesteps=2631432, episode_reward=528.28 +/- 251.88
Episode length: 541.20 +/- 99.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 2631432  |
---------------------------------
Eval num_timesteps=2633424, episode_reward=454.68 +/- 298.02
Episode length: 499.00 +/- 121.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 2633424  |
---------------------------------
Eval num_timesteps=2635416, episode_reward=953.69 +/- 513.37
Episode length: 715.00 +/- 174.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 2635416  |
---------------------------------
Eval num_timesteps=2637408, episode_reward=662.50 +/- 504.75
Episode length: 530.40 +/- 98.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 2637408  |
---------------------------------
Eval num_timesteps=2639400, episode_reward=785.36 +/- 272.85
Episode length: 640.40 +/- 224.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 2639400  |
---------------------------------
Eval num_timesteps=2641392, episode_reward=552.45 +/- 122.01
Episode length: 550.80 +/- 167.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 2641392  |
---------------------------------
Eval num_timesteps=2643384, episode_reward=623.88 +/- 313.33
Episode length: 567.40 +/- 205.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 2643384  |
---------------------------------
Eval num_timesteps=2645376, episode_reward=294.49 +/- 426.79
Episode length: 510.40 +/- 158.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 2645376  |
---------------------------------
Eval num_timesteps=2647368, episode_reward=849.63 +/- 703.24
Episode length: 562.20 +/- 166.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 2647368  |
---------------------------------
Eval num_timesteps=2649360, episode_reward=630.94 +/- 414.82
Episode length: 521.40 +/- 45.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 2649360  |
---------------------------------
Eval num_timesteps=2651352, episode_reward=501.88 +/- 270.76
Episode length: 487.20 +/- 119.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 2651352  |
---------------------------------
Eval num_timesteps=2653344, episode_reward=763.98 +/- 351.93
Episode length: 546.40 +/- 112.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 2653344  |
---------------------------------
Eval num_timesteps=2655336, episode_reward=555.33 +/- 493.19
Episode length: 587.40 +/- 198.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 587         |
|    mean_reward          | 555         |
| time/                   |             |
|    total_timesteps      | 2655336     |
| train/                  |             |
|    approx_kl            | 0.008116293 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.67       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00781     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.000928   |
|    std                  | 1.28        |
|    value_loss           | 0.166       |
-----------------------------------------
Eval num_timesteps=2657328, episode_reward=626.61 +/- 357.61
Episode length: 450.40 +/- 60.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | 627      |
| time/              |          |
|    total_timesteps | 2657328  |
---------------------------------
Eval num_timesteps=2659320, episode_reward=456.71 +/- 226.50
Episode length: 579.40 +/- 194.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 2659320  |
---------------------------------
Eval num_timesteps=2661312, episode_reward=617.14 +/- 419.96
Episode length: 477.00 +/- 84.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 2661312  |
---------------------------------
Eval num_timesteps=2663304, episode_reward=617.19 +/- 302.82
Episode length: 742.60 +/- 232.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 2663304  |
---------------------------------
Eval num_timesteps=2665296, episode_reward=693.25 +/- 835.52
Episode length: 536.40 +/- 218.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 2665296  |
---------------------------------
Eval num_timesteps=2667288, episode_reward=692.94 +/- 342.51
Episode length: 596.20 +/- 157.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 2667288  |
---------------------------------
Eval num_timesteps=2669280, episode_reward=605.63 +/- 316.48
Episode length: 597.00 +/- 166.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 2669280  |
---------------------------------
Eval num_timesteps=2671272, episode_reward=361.87 +/- 629.04
Episode length: 422.60 +/- 110.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 423      |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 2671272  |
---------------------------------
Eval num_timesteps=2673264, episode_reward=591.93 +/- 672.32
Episode length: 551.20 +/- 218.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 2673264  |
---------------------------------
Eval num_timesteps=2675256, episode_reward=855.04 +/- 405.17
Episode length: 664.80 +/- 259.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 2675256  |
---------------------------------
Eval num_timesteps=2677248, episode_reward=913.65 +/- 663.98
Episode length: 610.00 +/- 282.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 2677248  |
---------------------------------
Eval num_timesteps=2679240, episode_reward=835.35 +/- 281.13
Episode length: 546.60 +/- 88.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 2679240  |
---------------------------------
Eval num_timesteps=2681232, episode_reward=773.78 +/- 333.07
Episode length: 691.60 +/- 176.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 2681232  |
---------------------------------
Eval num_timesteps=2683224, episode_reward=338.36 +/- 359.28
Episode length: 463.60 +/- 133.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 2683224  |
---------------------------------
Eval num_timesteps=2685216, episode_reward=783.98 +/- 153.40
Episode length: 642.20 +/- 226.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 2685216  |
---------------------------------
Eval num_timesteps=2687208, episode_reward=866.48 +/- 142.28
Episode length: 538.20 +/- 111.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 2687208  |
---------------------------------
Eval num_timesteps=2689200, episode_reward=923.28 +/- 164.30
Episode length: 683.60 +/- 161.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 2689200  |
---------------------------------
Eval num_timesteps=2691192, episode_reward=1472.47 +/- 842.54
Episode length: 841.60 +/- 328.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2691192  |
---------------------------------
Eval num_timesteps=2693184, episode_reward=1059.82 +/- 293.48
Episode length: 753.00 +/- 214.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2693184  |
---------------------------------
Eval num_timesteps=2695176, episode_reward=708.97 +/- 283.11
Episode length: 695.20 +/- 217.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 2695176  |
---------------------------------
Eval num_timesteps=2697168, episode_reward=685.10 +/- 260.10
Episode length: 677.00 +/- 176.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 2697168  |
---------------------------------
Eval num_timesteps=2699160, episode_reward=637.40 +/- 359.78
Episode length: 581.40 +/- 139.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 2699160  |
---------------------------------
Eval num_timesteps=2701152, episode_reward=459.50 +/- 372.18
Episode length: 483.40 +/- 139.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 2701152  |
---------------------------------
Eval num_timesteps=2703144, episode_reward=447.36 +/- 250.39
Episode length: 484.20 +/- 81.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 2703144  |
---------------------------------
Eval num_timesteps=2705136, episode_reward=922.05 +/- 313.80
Episode length: 605.20 +/- 163.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 605         |
|    mean_reward          | 922         |
| time/                   |             |
|    total_timesteps      | 2705136     |
| train/                  |             |
|    approx_kl            | 0.006325583 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.68       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0197      |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0007     |
|    std                  | 1.29        |
|    value_loss           | 0.19        |
-----------------------------------------
Eval num_timesteps=2707128, episode_reward=710.39 +/- 296.66
Episode length: 770.60 +/- 276.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 2707128  |
---------------------------------
Eval num_timesteps=2709120, episode_reward=632.47 +/- 251.85
Episode length: 688.00 +/- 216.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 2709120  |
---------------------------------
Eval num_timesteps=2711112, episode_reward=1108.68 +/- 541.87
Episode length: 835.60 +/- 288.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2711112  |
---------------------------------
Eval num_timesteps=2713104, episode_reward=646.55 +/- 279.98
Episode length: 653.00 +/- 125.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 2713104  |
---------------------------------
Eval num_timesteps=2715096, episode_reward=716.38 +/- 329.06
Episode length: 818.80 +/- 169.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 2715096  |
---------------------------------
Eval num_timesteps=2717088, episode_reward=756.32 +/- 279.48
Episode length: 706.00 +/- 157.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 2717088  |
---------------------------------
Eval num_timesteps=2719080, episode_reward=688.30 +/- 334.81
Episode length: 751.00 +/- 146.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 2719080  |
---------------------------------
Eval num_timesteps=2721072, episode_reward=724.21 +/- 362.73
Episode length: 909.20 +/- 365.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 2721072  |
---------------------------------
Eval num_timesteps=2723064, episode_reward=650.21 +/- 327.52
Episode length: 500.60 +/- 74.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 2723064  |
---------------------------------
Eval num_timesteps=2725056, episode_reward=624.33 +/- 265.78
Episode length: 569.20 +/- 116.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 2725056  |
---------------------------------
Eval num_timesteps=2727048, episode_reward=624.07 +/- 376.42
Episode length: 536.40 +/- 135.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 2727048  |
---------------------------------
Eval num_timesteps=2729040, episode_reward=716.57 +/- 114.34
Episode length: 659.40 +/- 186.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 2729040  |
---------------------------------
Eval num_timesteps=2731032, episode_reward=1116.14 +/- 576.16
Episode length: 683.40 +/- 155.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2731032  |
---------------------------------
Eval num_timesteps=2733024, episode_reward=916.98 +/- 199.73
Episode length: 804.00 +/- 217.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 2733024  |
---------------------------------
Eval num_timesteps=2735016, episode_reward=627.60 +/- 267.12
Episode length: 750.00 +/- 301.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 2735016  |
---------------------------------
Eval num_timesteps=2737008, episode_reward=851.40 +/- 289.73
Episode length: 621.80 +/- 150.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 2737008  |
---------------------------------
Eval num_timesteps=2739000, episode_reward=911.79 +/- 420.84
Episode length: 804.80 +/- 230.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 2739000  |
---------------------------------
Eval num_timesteps=2740992, episode_reward=1224.55 +/- 319.46
Episode length: 919.40 +/- 237.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2740992  |
---------------------------------
Eval num_timesteps=2742984, episode_reward=996.30 +/- 403.32
Episode length: 586.80 +/- 133.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 2742984  |
---------------------------------
Eval num_timesteps=2744976, episode_reward=1419.60 +/- 554.30
Episode length: 710.40 +/- 236.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2744976  |
---------------------------------
Eval num_timesteps=2746968, episode_reward=1199.61 +/- 757.87
Episode length: 799.20 +/- 199.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2746968  |
---------------------------------
Eval num_timesteps=2748960, episode_reward=771.83 +/- 464.47
Episode length: 776.20 +/- 318.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 2748960  |
---------------------------------
Eval num_timesteps=2750952, episode_reward=807.74 +/- 747.87
Episode length: 655.60 +/- 100.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 2750952  |
---------------------------------
Eval num_timesteps=2752944, episode_reward=502.83 +/- 345.18
Episode length: 796.40 +/- 313.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 796         |
|    mean_reward          | 503         |
| time/                   |             |
|    total_timesteps      | 2752944     |
| train/                  |             |
|    approx_kl            | 0.004049947 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.71       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0079      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000877   |
|    std                  | 1.3         |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=2754936, episode_reward=977.75 +/- 652.25
Episode length: 752.20 +/- 183.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 2754936  |
---------------------------------
Eval num_timesteps=2756928, episode_reward=1141.48 +/- 493.88
Episode length: 813.00 +/- 267.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2756928  |
---------------------------------
Eval num_timesteps=2758920, episode_reward=465.96 +/- 185.24
Episode length: 515.60 +/- 107.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 2758920  |
---------------------------------
Eval num_timesteps=2760912, episode_reward=921.40 +/- 190.43
Episode length: 831.80 +/- 189.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 2760912  |
---------------------------------
Eval num_timesteps=2762904, episode_reward=1541.42 +/- 921.37
Episode length: 727.00 +/- 107.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2762904  |
---------------------------------
Eval num_timesteps=2764896, episode_reward=670.97 +/- 368.23
Episode length: 615.80 +/- 130.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 2764896  |
---------------------------------
Eval num_timesteps=2766888, episode_reward=877.35 +/- 511.23
Episode length: 661.00 +/- 202.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 2766888  |
---------------------------------
Eval num_timesteps=2768880, episode_reward=847.21 +/- 543.34
Episode length: 823.60 +/- 136.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 2768880  |
---------------------------------
Eval num_timesteps=2770872, episode_reward=696.25 +/- 363.81
Episode length: 653.20 +/- 161.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 2770872  |
---------------------------------
Eval num_timesteps=2772864, episode_reward=963.88 +/- 528.87
Episode length: 656.80 +/- 177.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 2772864  |
---------------------------------
Eval num_timesteps=2774856, episode_reward=540.20 +/- 569.05
Episode length: 562.20 +/- 98.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 2774856  |
---------------------------------
Eval num_timesteps=2776848, episode_reward=982.14 +/- 455.08
Episode length: 679.80 +/- 68.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 2776848  |
---------------------------------
Eval num_timesteps=2778840, episode_reward=503.31 +/- 235.61
Episode length: 591.80 +/- 181.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 2778840  |
---------------------------------
Eval num_timesteps=2780832, episode_reward=1121.80 +/- 487.71
Episode length: 796.00 +/- 284.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2780832  |
---------------------------------
Eval num_timesteps=2782824, episode_reward=1022.76 +/- 971.47
Episode length: 684.60 +/- 243.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2782824  |
---------------------------------
Eval num_timesteps=2784816, episode_reward=784.69 +/- 369.27
Episode length: 762.00 +/- 272.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 2784816  |
---------------------------------
Eval num_timesteps=2786808, episode_reward=521.64 +/- 454.97
Episode length: 594.00 +/- 202.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 2786808  |
---------------------------------
Eval num_timesteps=2788800, episode_reward=1325.59 +/- 1123.16
Episode length: 926.40 +/- 154.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2788800  |
---------------------------------
Eval num_timesteps=2790792, episode_reward=1150.80 +/- 651.67
Episode length: 817.80 +/- 232.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2790792  |
---------------------------------
Eval num_timesteps=2792784, episode_reward=636.51 +/- 245.36
Episode length: 766.00 +/- 297.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 2792784  |
---------------------------------
Eval num_timesteps=2794776, episode_reward=1445.55 +/- 624.86
Episode length: 856.60 +/- 270.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 2794776  |
---------------------------------
Eval num_timesteps=2796768, episode_reward=713.37 +/- 477.47
Episode length: 532.00 +/- 177.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 2796768  |
---------------------------------
Eval num_timesteps=2798760, episode_reward=960.14 +/- 840.21
Episode length: 621.00 +/- 178.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 2798760  |
---------------------------------
Eval num_timesteps=2800752, episode_reward=774.28 +/- 400.16
Episode length: 754.40 +/- 295.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 2800752  |
---------------------------------
Eval num_timesteps=2802744, episode_reward=873.95 +/- 208.34
Episode length: 892.40 +/- 258.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 892          |
|    mean_reward          | 874          |
| time/                   |              |
|    total_timesteps      | 2802744      |
| train/                  |              |
|    approx_kl            | 0.0054621096 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.73        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | -0.018       |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.3          |
|    value_loss           | 0.111        |
------------------------------------------
Eval num_timesteps=2804736, episode_reward=1109.55 +/- 575.98
Episode length: 837.80 +/- 215.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2804736  |
---------------------------------
Eval num_timesteps=2806728, episode_reward=580.82 +/- 361.99
Episode length: 555.40 +/- 189.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 2806728  |
---------------------------------
Eval num_timesteps=2808720, episode_reward=1117.10 +/- 560.16
Episode length: 840.60 +/- 202.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2808720  |
---------------------------------
Eval num_timesteps=2810712, episode_reward=1057.85 +/- 479.18
Episode length: 799.80 +/- 275.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2810712  |
---------------------------------
Eval num_timesteps=2812704, episode_reward=893.41 +/- 723.96
Episode length: 565.20 +/- 179.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 2812704  |
---------------------------------
Eval num_timesteps=2814696, episode_reward=658.39 +/- 460.58
Episode length: 730.60 +/- 279.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 2814696  |
---------------------------------
Eval num_timesteps=2816688, episode_reward=708.94 +/- 235.61
Episode length: 651.20 +/- 131.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 2816688  |
---------------------------------
Eval num_timesteps=2818680, episode_reward=541.02 +/- 380.65
Episode length: 689.80 +/- 309.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 2818680  |
---------------------------------
Eval num_timesteps=2820672, episode_reward=701.08 +/- 616.74
Episode length: 676.60 +/- 295.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 2820672  |
---------------------------------
Eval num_timesteps=2822664, episode_reward=783.85 +/- 181.64
Episode length: 720.60 +/- 137.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 2822664  |
---------------------------------
Eval num_timesteps=2824656, episode_reward=582.41 +/- 698.36
Episode length: 523.60 +/- 148.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 2824656  |
---------------------------------
Eval num_timesteps=2826648, episode_reward=918.62 +/- 1404.54
Episode length: 540.60 +/- 188.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 2826648  |
---------------------------------
Eval num_timesteps=2828640, episode_reward=670.42 +/- 296.13
Episode length: 738.60 +/- 200.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 2828640  |
---------------------------------
Eval num_timesteps=2830632, episode_reward=787.67 +/- 550.64
Episode length: 561.40 +/- 180.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 2830632  |
---------------------------------
Eval num_timesteps=2832624, episode_reward=688.50 +/- 185.71
Episode length: 689.40 +/- 173.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 2832624  |
---------------------------------
Eval num_timesteps=2834616, episode_reward=708.18 +/- 312.76
Episode length: 691.00 +/- 108.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 2834616  |
---------------------------------
Eval num_timesteps=2836608, episode_reward=850.80 +/- 517.59
Episode length: 781.00 +/- 196.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 2836608  |
---------------------------------
Eval num_timesteps=2838600, episode_reward=1162.54 +/- 251.73
Episode length: 708.20 +/- 176.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2838600  |
---------------------------------
Eval num_timesteps=2840592, episode_reward=801.14 +/- 597.96
Episode length: 596.80 +/- 194.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 2840592  |
---------------------------------
Eval num_timesteps=2842584, episode_reward=1139.89 +/- 773.17
Episode length: 866.00 +/- 222.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 866      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2842584  |
---------------------------------
Eval num_timesteps=2844576, episode_reward=1244.99 +/- 550.75
Episode length: 881.40 +/- 107.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2844576  |
---------------------------------
Eval num_timesteps=2846568, episode_reward=852.08 +/- 716.69
Episode length: 745.60 +/- 273.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 2846568  |
---------------------------------
Eval num_timesteps=2848560, episode_reward=554.58 +/- 484.93
Episode length: 633.40 +/- 262.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 2848560  |
---------------------------------
Eval num_timesteps=2850552, episode_reward=326.24 +/- 210.01
Episode length: 524.60 +/- 128.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 2850552  |
---------------------------------
Eval num_timesteps=2852544, episode_reward=586.56 +/- 456.16
Episode length: 592.80 +/- 202.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 593         |
|    mean_reward          | 587         |
| time/                   |             |
|    total_timesteps      | 2852544     |
| train/                  |             |
|    approx_kl            | 0.004453562 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.75       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0082     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00154    |
|    std                  | 1.31        |
|    value_loss           | 0.134       |
-----------------------------------------
Eval num_timesteps=2854536, episode_reward=1023.97 +/- 463.28
Episode length: 879.00 +/- 120.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2854536  |
---------------------------------
Eval num_timesteps=2856528, episode_reward=1247.05 +/- 581.85
Episode length: 793.60 +/- 279.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2856528  |
---------------------------------
Eval num_timesteps=2858520, episode_reward=898.49 +/- 690.01
Episode length: 773.00 +/- 282.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 2858520  |
---------------------------------
Eval num_timesteps=2860512, episode_reward=746.85 +/- 497.73
Episode length: 636.20 +/- 367.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 2860512  |
---------------------------------
Eval num_timesteps=2862504, episode_reward=749.95 +/- 782.80
Episode length: 630.40 +/- 314.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 2862504  |
---------------------------------
Eval num_timesteps=2864496, episode_reward=807.08 +/- 692.66
Episode length: 637.80 +/- 193.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 2864496  |
---------------------------------
Eval num_timesteps=2866488, episode_reward=726.84 +/- 545.31
Episode length: 707.80 +/- 314.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 2866488  |
---------------------------------
Eval num_timesteps=2868480, episode_reward=1198.47 +/- 652.36
Episode length: 675.40 +/- 60.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2868480  |
---------------------------------
Eval num_timesteps=2870472, episode_reward=840.15 +/- 616.30
Episode length: 594.20 +/- 194.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 2870472  |
---------------------------------
Eval num_timesteps=2872464, episode_reward=714.23 +/- 390.96
Episode length: 631.60 +/- 259.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 2872464  |
---------------------------------
Eval num_timesteps=2874456, episode_reward=831.47 +/- 383.17
Episode length: 684.80 +/- 274.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 2874456  |
---------------------------------
Eval num_timesteps=2876448, episode_reward=666.27 +/- 527.42
Episode length: 550.60 +/- 72.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 2876448  |
---------------------------------
Eval num_timesteps=2878440, episode_reward=816.42 +/- 675.26
Episode length: 772.20 +/- 229.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 2878440  |
---------------------------------
Eval num_timesteps=2880432, episode_reward=841.08 +/- 579.33
Episode length: 823.00 +/- 321.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 2880432  |
---------------------------------
Eval num_timesteps=2882424, episode_reward=518.48 +/- 351.78
Episode length: 626.60 +/- 137.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 2882424  |
---------------------------------
Eval num_timesteps=2884416, episode_reward=695.31 +/- 343.56
Episode length: 665.00 +/- 58.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 2884416  |
---------------------------------
Eval num_timesteps=2886408, episode_reward=1034.06 +/- 908.70
Episode length: 719.40 +/- 338.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2886408  |
---------------------------------
Eval num_timesteps=2888400, episode_reward=1058.84 +/- 679.55
Episode length: 740.80 +/- 439.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2888400  |
---------------------------------
Eval num_timesteps=2890392, episode_reward=724.64 +/- 327.39
Episode length: 597.60 +/- 139.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 2890392  |
---------------------------------
Eval num_timesteps=2892384, episode_reward=1289.80 +/- 984.17
Episode length: 732.80 +/- 227.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2892384  |
---------------------------------
Eval num_timesteps=2894376, episode_reward=1055.12 +/- 762.35
Episode length: 679.80 +/- 232.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2894376  |
---------------------------------
Eval num_timesteps=2896368, episode_reward=1061.08 +/- 900.50
Episode length: 757.00 +/- 300.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2896368  |
---------------------------------
Eval num_timesteps=2898360, episode_reward=1390.13 +/- 934.86
Episode length: 895.60 +/- 168.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2898360  |
---------------------------------
Eval num_timesteps=2900352, episode_reward=996.18 +/- 785.98
Episode length: 548.20 +/- 167.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 548          |
|    mean_reward          | 996          |
| time/                   |              |
|    total_timesteps      | 2900352      |
| train/                  |              |
|    approx_kl            | 0.0066435486 |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0119      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.32         |
|    value_loss           | 0.125        |
------------------------------------------
Eval num_timesteps=2902344, episode_reward=878.05 +/- 587.92
Episode length: 705.00 +/- 192.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 2902344  |
---------------------------------
Eval num_timesteps=2904336, episode_reward=940.77 +/- 829.80
Episode length: 725.40 +/- 257.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 2904336  |
---------------------------------
Eval num_timesteps=2906328, episode_reward=1283.67 +/- 404.67
Episode length: 699.40 +/- 192.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2906328  |
---------------------------------
Eval num_timesteps=2908320, episode_reward=767.63 +/- 448.98
Episode length: 625.40 +/- 158.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 2908320  |
---------------------------------
Eval num_timesteps=2910312, episode_reward=1064.82 +/- 804.80
Episode length: 741.40 +/- 176.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2910312  |
---------------------------------
Eval num_timesteps=2912304, episode_reward=445.79 +/- 396.86
Episode length: 439.40 +/- 138.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 439      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 2912304  |
---------------------------------
Eval num_timesteps=2914296, episode_reward=547.86 +/- 568.19
Episode length: 699.00 +/- 332.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 2914296  |
---------------------------------
Eval num_timesteps=2916288, episode_reward=1618.80 +/- 713.36
Episode length: 841.60 +/- 284.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2916288  |
---------------------------------
New best mean reward!
Eval num_timesteps=2918280, episode_reward=997.54 +/- 439.02
Episode length: 881.20 +/- 200.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 2918280  |
---------------------------------
Eval num_timesteps=2920272, episode_reward=958.07 +/- 642.38
Episode length: 676.20 +/- 180.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 2920272  |
---------------------------------
Eval num_timesteps=2922264, episode_reward=1056.88 +/- 671.11
Episode length: 705.40 +/- 244.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2922264  |
---------------------------------
Eval num_timesteps=2924256, episode_reward=998.24 +/- 625.70
Episode length: 717.80 +/- 260.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 2924256  |
---------------------------------
Eval num_timesteps=2926248, episode_reward=1012.10 +/- 848.95
Episode length: 646.20 +/- 295.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2926248  |
---------------------------------
Eval num_timesteps=2928240, episode_reward=1324.04 +/- 555.74
Episode length: 947.20 +/- 271.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2928240  |
---------------------------------
Eval num_timesteps=2930232, episode_reward=857.34 +/- 774.97
Episode length: 612.40 +/- 142.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 2930232  |
---------------------------------
Eval num_timesteps=2932224, episode_reward=949.46 +/- 530.24
Episode length: 910.00 +/- 275.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 2932224  |
---------------------------------
Eval num_timesteps=2934216, episode_reward=1132.38 +/- 591.01
Episode length: 909.60 +/- 246.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2934216  |
---------------------------------
Eval num_timesteps=2936208, episode_reward=1238.74 +/- 800.98
Episode length: 897.80 +/- 344.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2936208  |
---------------------------------
Eval num_timesteps=2938200, episode_reward=2139.40 +/- 574.76
Episode length: 844.40 +/- 142.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 2938200  |
---------------------------------
New best mean reward!
Eval num_timesteps=2940192, episode_reward=1208.62 +/- 558.88
Episode length: 931.00 +/- 210.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2940192  |
---------------------------------
Eval num_timesteps=2942184, episode_reward=1007.40 +/- 755.86
Episode length: 816.40 +/- 240.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2942184  |
---------------------------------
Eval num_timesteps=2944176, episode_reward=1235.94 +/- 802.44
Episode length: 741.20 +/- 235.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2944176  |
---------------------------------
Eval num_timesteps=2946168, episode_reward=921.56 +/- 621.71
Episode length: 925.20 +/- 410.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 2946168  |
---------------------------------
Eval num_timesteps=2948160, episode_reward=994.97 +/- 488.85
Episode length: 779.20 +/- 150.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 2948160  |
---------------------------------
Eval num_timesteps=2950152, episode_reward=854.03 +/- 549.22
Episode length: 680.20 +/- 165.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 680          |
|    mean_reward          | 854          |
| time/                   |              |
|    total_timesteps      | 2950152      |
| train/                  |              |
|    approx_kl            | 0.0050079674 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.79        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0195      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.32         |
|    value_loss           | 0.108        |
------------------------------------------
Eval num_timesteps=2952144, episode_reward=808.68 +/- 326.41
Episode length: 653.20 +/- 134.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 2952144  |
---------------------------------
Eval num_timesteps=2954136, episode_reward=671.43 +/- 544.27
Episode length: 604.80 +/- 152.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 2954136  |
---------------------------------
Eval num_timesteps=2956128, episode_reward=963.39 +/- 527.27
Episode length: 680.60 +/- 205.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 2956128  |
---------------------------------
Eval num_timesteps=2958120, episode_reward=1413.96 +/- 755.20
Episode length: 883.60 +/- 193.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2958120  |
---------------------------------
Eval num_timesteps=2960112, episode_reward=764.13 +/- 252.99
Episode length: 753.00 +/- 239.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 2960112  |
---------------------------------
Eval num_timesteps=2962104, episode_reward=797.25 +/- 516.84
Episode length: 979.60 +/- 445.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 2962104  |
---------------------------------
Eval num_timesteps=2964096, episode_reward=1037.70 +/- 433.89
Episode length: 768.80 +/- 113.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2964096  |
---------------------------------
Eval num_timesteps=2966088, episode_reward=1100.28 +/- 559.81
Episode length: 730.60 +/- 184.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2966088  |
---------------------------------
Eval num_timesteps=2968080, episode_reward=849.11 +/- 386.58
Episode length: 907.20 +/- 279.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 2968080  |
---------------------------------
Eval num_timesteps=2970072, episode_reward=577.53 +/- 379.15
Episode length: 553.40 +/- 165.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 2970072  |
---------------------------------
Eval num_timesteps=2972064, episode_reward=393.86 +/- 334.50
Episode length: 621.20 +/- 271.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 2972064  |
---------------------------------
Eval num_timesteps=2974056, episode_reward=855.77 +/- 807.65
Episode length: 596.60 +/- 244.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 2974056  |
---------------------------------
Eval num_timesteps=2976048, episode_reward=860.51 +/- 746.66
Episode length: 717.60 +/- 304.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 2976048  |
---------------------------------
Eval num_timesteps=2978040, episode_reward=1103.12 +/- 402.67
Episode length: 711.20 +/- 170.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2978040  |
---------------------------------
Eval num_timesteps=2980032, episode_reward=1104.10 +/- 683.69
Episode length: 747.00 +/- 159.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2980032  |
---------------------------------
Eval num_timesteps=2982024, episode_reward=819.18 +/- 257.47
Episode length: 883.40 +/- 251.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 2982024  |
---------------------------------
Eval num_timesteps=2984016, episode_reward=1609.56 +/- 1036.64
Episode length: 908.00 +/- 372.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2984016  |
---------------------------------
Eval num_timesteps=2986008, episode_reward=576.98 +/- 486.00
Episode length: 713.00 +/- 225.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 2986008  |
---------------------------------
Eval num_timesteps=2988000, episode_reward=1421.09 +/- 556.31
Episode length: 709.00 +/- 216.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2989992, episode_reward=1196.69 +/- 801.84
Episode length: 900.60 +/- 171.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2989992  |
---------------------------------
Eval num_timesteps=2991984, episode_reward=1083.41 +/- 862.92
Episode length: 728.00 +/- 202.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2991984  |
---------------------------------
Eval num_timesteps=2993976, episode_reward=1006.10 +/- 482.14
Episode length: 873.60 +/- 276.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2993976  |
---------------------------------
Eval num_timesteps=2995968, episode_reward=1014.76 +/- 401.02
Episode length: 859.80 +/- 176.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 860      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2995968  |
---------------------------------
Eval num_timesteps=2997960, episode_reward=631.01 +/- 618.59
Episode length: 457.00 +/- 143.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 2997960  |
---------------------------------
Eval num_timesteps=2999952, episode_reward=542.24 +/- 472.71
Episode length: 583.80 +/- 150.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 584          |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 2999952      |
| train/                  |              |
|    approx_kl            | 0.0050362567 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0117      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.33         |
|    value_loss           | 0.126        |
------------------------------------------
Eval num_timesteps=3001944, episode_reward=1560.40 +/- 544.32
Episode length: 966.20 +/- 268.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3001944  |
---------------------------------
Eval num_timesteps=3003936, episode_reward=871.35 +/- 290.77
Episode length: 754.00 +/- 216.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 3003936  |
---------------------------------
Eval num_timesteps=3005928, episode_reward=1077.01 +/- 1208.86
Episode length: 564.40 +/- 341.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3005928  |
---------------------------------
Eval num_timesteps=3007920, episode_reward=758.99 +/- 718.24
Episode length: 666.00 +/- 212.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 3007920  |
---------------------------------
Eval num_timesteps=3009912, episode_reward=815.41 +/- 402.25
Episode length: 803.80 +/- 296.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 3009912  |
---------------------------------
Eval num_timesteps=3011904, episode_reward=963.27 +/- 544.93
Episode length: 736.40 +/- 316.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 3011904  |
---------------------------------
Eval num_timesteps=3013896, episode_reward=1132.50 +/- 1049.04
Episode length: 914.00 +/- 430.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3013896  |
---------------------------------
Eval num_timesteps=3015888, episode_reward=1038.04 +/- 710.12
Episode length: 778.00 +/- 244.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3015888  |
---------------------------------
Eval num_timesteps=3017880, episode_reward=503.92 +/- 433.40
Episode length: 730.40 +/- 218.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 3017880  |
---------------------------------
Eval num_timesteps=3019872, episode_reward=1033.50 +/- 237.69
Episode length: 855.20 +/- 236.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3019872  |
---------------------------------
Eval num_timesteps=3021864, episode_reward=884.92 +/- 970.89
Episode length: 814.20 +/- 351.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 3021864  |
---------------------------------
Eval num_timesteps=3023856, episode_reward=1038.34 +/- 517.38
Episode length: 741.80 +/- 283.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3023856  |
---------------------------------
Eval num_timesteps=3025848, episode_reward=825.60 +/- 906.95
Episode length: 744.60 +/- 364.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 3025848  |
---------------------------------
Eval num_timesteps=3027840, episode_reward=1101.32 +/- 618.51
Episode length: 748.20 +/- 195.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3027840  |
---------------------------------
Eval num_timesteps=3029832, episode_reward=1235.95 +/- 1011.93
Episode length: 912.60 +/- 266.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3029832  |
---------------------------------
Eval num_timesteps=3031824, episode_reward=261.15 +/- 241.40
Episode length: 507.20 +/- 108.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 3031824  |
---------------------------------
Eval num_timesteps=3033816, episode_reward=1491.18 +/- 639.20
Episode length: 961.20 +/- 277.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 3033816  |
---------------------------------
Eval num_timesteps=3035808, episode_reward=568.14 +/- 651.71
Episode length: 576.00 +/- 235.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 3035808  |
---------------------------------
Eval num_timesteps=3037800, episode_reward=1373.37 +/- 616.65
Episode length: 794.60 +/- 287.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3037800  |
---------------------------------
Eval num_timesteps=3039792, episode_reward=1297.30 +/- 567.42
Episode length: 754.80 +/- 248.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3039792  |
---------------------------------
Eval num_timesteps=3041784, episode_reward=1086.93 +/- 577.46
Episode length: 877.80 +/- 242.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3041784  |
---------------------------------
Eval num_timesteps=3043776, episode_reward=967.27 +/- 579.45
Episode length: 813.40 +/- 327.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 3043776  |
---------------------------------
Eval num_timesteps=3045768, episode_reward=824.98 +/- 540.80
Episode length: 632.80 +/- 235.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 3045768  |
---------------------------------
Eval num_timesteps=3047760, episode_reward=1230.67 +/- 282.98
Episode length: 913.80 +/- 235.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 914         |
|    mean_reward          | 1.23e+03    |
| time/                   |             |
|    total_timesteps      | 3047760     |
| train/                  |             |
|    approx_kl            | 0.006121484 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.81       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0133     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00165    |
|    std                  | 1.33        |
|    value_loss           | 0.119       |
-----------------------------------------
Eval num_timesteps=3049752, episode_reward=1181.56 +/- 436.05
Episode length: 856.60 +/- 165.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3049752  |
---------------------------------
Eval num_timesteps=3051744, episode_reward=710.87 +/- 266.30
Episode length: 561.60 +/- 66.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 3051744  |
---------------------------------
Eval num_timesteps=3053736, episode_reward=1203.70 +/- 470.64
Episode length: 961.80 +/- 290.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 962      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3053736  |
---------------------------------
Eval num_timesteps=3055728, episode_reward=1020.32 +/- 572.59
Episode length: 763.40 +/- 282.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3055728  |
---------------------------------
Eval num_timesteps=3057720, episode_reward=624.67 +/- 139.92
Episode length: 744.20 +/- 178.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 3057720  |
---------------------------------
Eval num_timesteps=3059712, episode_reward=641.30 +/- 781.38
Episode length: 705.00 +/- 298.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 3059712  |
---------------------------------
Eval num_timesteps=3061704, episode_reward=519.16 +/- 324.36
Episode length: 645.20 +/- 236.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 3061704  |
---------------------------------
Eval num_timesteps=3063696, episode_reward=1061.99 +/- 402.53
Episode length: 758.00 +/- 98.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3063696  |
---------------------------------
Eval num_timesteps=3065688, episode_reward=948.74 +/- 298.28
Episode length: 809.00 +/- 137.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 3065688  |
---------------------------------
Eval num_timesteps=3067680, episode_reward=1231.16 +/- 701.23
Episode length: 560.80 +/- 176.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3067680  |
---------------------------------
Eval num_timesteps=3069672, episode_reward=1012.56 +/- 256.07
Episode length: 892.00 +/- 236.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3069672  |
---------------------------------
Eval num_timesteps=3071664, episode_reward=870.19 +/- 316.03
Episode length: 813.20 +/- 286.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 3071664  |
---------------------------------
Eval num_timesteps=3073656, episode_reward=1065.68 +/- 896.48
Episode length: 773.00 +/- 239.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3073656  |
---------------------------------
Eval num_timesteps=3075648, episode_reward=1263.51 +/- 674.93
Episode length: 829.60 +/- 239.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3075648  |
---------------------------------
Eval num_timesteps=3077640, episode_reward=1001.88 +/- 239.37
Episode length: 970.80 +/- 129.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 3077640  |
---------------------------------
Eval num_timesteps=3079632, episode_reward=767.08 +/- 416.18
Episode length: 619.00 +/- 153.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 3079632  |
---------------------------------
Eval num_timesteps=3081624, episode_reward=877.61 +/- 407.51
Episode length: 1074.80 +/- 389.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 3081624  |
---------------------------------
Eval num_timesteps=3083616, episode_reward=1237.76 +/- 347.56
Episode length: 909.20 +/- 130.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3083616  |
---------------------------------
Eval num_timesteps=3085608, episode_reward=1020.76 +/- 408.70
Episode length: 864.40 +/- 292.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3085608  |
---------------------------------
Eval num_timesteps=3087600, episode_reward=1131.35 +/- 977.21
Episode length: 801.40 +/- 309.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3087600  |
---------------------------------
Eval num_timesteps=3089592, episode_reward=1107.54 +/- 369.60
Episode length: 844.00 +/- 136.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3089592  |
---------------------------------
Eval num_timesteps=3091584, episode_reward=1359.51 +/- 429.11
Episode length: 818.40 +/- 257.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3091584  |
---------------------------------
Eval num_timesteps=3093576, episode_reward=1265.18 +/- 663.33
Episode length: 777.80 +/- 242.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3093576  |
---------------------------------
Eval num_timesteps=3095568, episode_reward=973.88 +/- 211.83
Episode length: 785.40 +/- 137.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 3095568  |
---------------------------------
Eval num_timesteps=3097560, episode_reward=774.11 +/- 49.55
Episode length: 714.20 +/- 156.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 714          |
|    mean_reward          | 774          |
| time/                   |              |
|    total_timesteps      | 3097560      |
| train/                  |              |
|    approx_kl            | 0.0065379343 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.82        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0183      |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.33         |
|    value_loss           | 0.111        |
------------------------------------------
Eval num_timesteps=3099552, episode_reward=852.11 +/- 177.66
Episode length: 678.60 +/- 148.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 3099552  |
---------------------------------
Eval num_timesteps=3101544, episode_reward=681.02 +/- 107.24
Episode length: 730.20 +/- 179.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 3101544  |
---------------------------------
Eval num_timesteps=3103536, episode_reward=819.41 +/- 176.07
Episode length: 810.20 +/- 86.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 3103536  |
---------------------------------
Eval num_timesteps=3105528, episode_reward=957.66 +/- 395.53
Episode length: 767.00 +/- 27.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 3105528  |
---------------------------------
Eval num_timesteps=3107520, episode_reward=948.91 +/- 340.88
Episode length: 891.40 +/- 260.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 3107520  |
---------------------------------
Eval num_timesteps=3109512, episode_reward=1038.51 +/- 441.34
Episode length: 1007.40 +/- 94.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3109512  |
---------------------------------
Eval num_timesteps=3111504, episode_reward=932.95 +/- 268.59
Episode length: 815.40 +/- 85.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 3111504  |
---------------------------------
Eval num_timesteps=3113496, episode_reward=953.54 +/- 390.77
Episode length: 854.00 +/- 231.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 3113496  |
---------------------------------
Eval num_timesteps=3115488, episode_reward=829.33 +/- 240.73
Episode length: 778.20 +/- 94.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 3115488  |
---------------------------------
Eval num_timesteps=3117480, episode_reward=1327.76 +/- 366.34
Episode length: 1147.80 +/- 218.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3117480  |
---------------------------------
Eval num_timesteps=3119472, episode_reward=1241.10 +/- 323.68
Episode length: 833.20 +/- 206.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3119472  |
---------------------------------
Eval num_timesteps=3121464, episode_reward=712.09 +/- 30.25
Episode length: 794.40 +/- 99.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 3121464  |
---------------------------------
Eval num_timesteps=3123456, episode_reward=1065.46 +/- 173.07
Episode length: 918.40 +/- 153.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3123456  |
---------------------------------
Eval num_timesteps=3125448, episode_reward=934.39 +/- 568.79
Episode length: 886.00 +/- 324.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 3125448  |
---------------------------------
Eval num_timesteps=3127440, episode_reward=1113.57 +/- 463.68
Episode length: 832.80 +/- 232.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3127440  |
---------------------------------
Eval num_timesteps=3129432, episode_reward=964.72 +/- 123.41
Episode length: 819.20 +/- 143.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 3129432  |
---------------------------------
Eval num_timesteps=3131424, episode_reward=865.56 +/- 127.35
Episode length: 878.20 +/- 217.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 3131424  |
---------------------------------
Eval num_timesteps=3133416, episode_reward=1025.70 +/- 216.96
Episode length: 810.60 +/- 136.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3133416  |
---------------------------------
Eval num_timesteps=3135408, episode_reward=736.53 +/- 481.86
Episode length: 860.40 +/- 172.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 860      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 3135408  |
---------------------------------
Eval num_timesteps=3137400, episode_reward=930.27 +/- 275.99
Episode length: 817.20 +/- 300.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 3137400  |
---------------------------------
Eval num_timesteps=3139392, episode_reward=1664.18 +/- 1154.91
Episode length: 885.00 +/- 251.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3139392  |
---------------------------------
Eval num_timesteps=3141384, episode_reward=1284.94 +/- 598.82
Episode length: 829.80 +/- 92.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3141384  |
---------------------------------
Eval num_timesteps=3143376, episode_reward=789.06 +/- 131.89
Episode length: 758.60 +/- 84.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 3143376  |
---------------------------------
Eval num_timesteps=3145368, episode_reward=785.49 +/- 246.93
Episode length: 674.00 +/- 134.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 3145368  |
---------------------------------
Eval num_timesteps=3147360, episode_reward=1262.06 +/- 537.62
Episode length: 892.20 +/- 129.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 892         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 3147360     |
| train/                  |             |
|    approx_kl            | 0.005628351 |
|    clip_fraction        | 0.0372      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.82       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0168     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 1.33        |
|    value_loss           | 0.113       |
-----------------------------------------
Eval num_timesteps=3149352, episode_reward=1357.06 +/- 526.07
Episode length: 812.00 +/- 107.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3149352  |
---------------------------------
Eval num_timesteps=3151344, episode_reward=745.24 +/- 170.75
Episode length: 622.00 +/- 125.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 3151344  |
---------------------------------
Eval num_timesteps=3153336, episode_reward=1466.75 +/- 457.24
Episode length: 627.80 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3153336  |
---------------------------------
Eval num_timesteps=3155328, episode_reward=802.39 +/- 98.94
Episode length: 781.00 +/- 159.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 3155328  |
---------------------------------
Eval num_timesteps=3157320, episode_reward=1247.93 +/- 418.68
Episode length: 821.60 +/- 140.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3157320  |
---------------------------------
Eval num_timesteps=3159312, episode_reward=1037.94 +/- 441.97
Episode length: 975.00 +/- 142.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3159312  |
---------------------------------
Eval num_timesteps=3161304, episode_reward=905.76 +/- 126.60
Episode length: 808.40 +/- 147.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 3161304  |
---------------------------------
Eval num_timesteps=3163296, episode_reward=651.20 +/- 562.67
Episode length: 869.00 +/- 217.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 3163296  |
---------------------------------
Eval num_timesteps=3165288, episode_reward=1423.75 +/- 951.15
Episode length: 894.20 +/- 172.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3165288  |
---------------------------------
Eval num_timesteps=3167280, episode_reward=834.58 +/- 247.31
Episode length: 983.60 +/- 202.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 984      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 3167280  |
---------------------------------
Eval num_timesteps=3169272, episode_reward=1089.52 +/- 428.13
Episode length: 972.20 +/- 185.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 972      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3169272  |
---------------------------------
Eval num_timesteps=3171264, episode_reward=1265.61 +/- 540.85
Episode length: 724.60 +/- 99.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3171264  |
---------------------------------
Eval num_timesteps=3173256, episode_reward=1275.40 +/- 277.30
Episode length: 848.20 +/- 217.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3173256  |
---------------------------------
Eval num_timesteps=3175248, episode_reward=1320.75 +/- 392.97
Episode length: 834.80 +/- 280.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3175248  |
---------------------------------
Eval num_timesteps=3177240, episode_reward=858.07 +/- 118.92
Episode length: 853.20 +/- 214.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 3177240  |
---------------------------------
Eval num_timesteps=3179232, episode_reward=1196.43 +/- 640.41
Episode length: 800.00 +/- 89.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3179232  |
---------------------------------
Eval num_timesteps=3181224, episode_reward=999.20 +/- 483.72
Episode length: 915.00 +/- 136.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 3181224  |
---------------------------------
Eval num_timesteps=3183216, episode_reward=1392.56 +/- 184.19
Episode length: 1048.00 +/- 189.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3183216  |
---------------------------------
Eval num_timesteps=3185208, episode_reward=1584.29 +/- 1087.48
Episode length: 903.60 +/- 169.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3185208  |
---------------------------------
Eval num_timesteps=3187200, episode_reward=1086.66 +/- 244.10
Episode length: 972.80 +/- 260.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3187200  |
---------------------------------
Eval num_timesteps=3189192, episode_reward=1454.92 +/- 798.54
Episode length: 981.40 +/- 167.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3189192  |
---------------------------------
Eval num_timesteps=3191184, episode_reward=960.08 +/- 365.62
Episode length: 1019.60 +/- 206.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 3191184  |
---------------------------------
Eval num_timesteps=3193176, episode_reward=1430.53 +/- 575.94
Episode length: 803.60 +/- 162.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3193176  |
---------------------------------
Eval num_timesteps=3195168, episode_reward=941.24 +/- 350.57
Episode length: 981.00 +/- 158.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 981         |
|    mean_reward          | 941         |
| time/                   |             |
|    total_timesteps      | 3195168     |
| train/                  |             |
|    approx_kl            | 0.005643489 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.83       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | -0.024      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00133    |
|    std                  | 1.34        |
|    value_loss           | 0.0982      |
-----------------------------------------
Eval num_timesteps=3197160, episode_reward=1189.15 +/- 553.04
Episode length: 761.40 +/- 132.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3197160  |
---------------------------------
Eval num_timesteps=3199152, episode_reward=920.76 +/- 302.26
Episode length: 898.40 +/- 159.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 3199152  |
---------------------------------
Eval num_timesteps=3201144, episode_reward=1061.51 +/- 493.82
Episode length: 789.40 +/- 161.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3201144  |
---------------------------------
Eval num_timesteps=3203136, episode_reward=891.22 +/- 179.93
Episode length: 918.40 +/- 150.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 3203136  |
---------------------------------
Eval num_timesteps=3205128, episode_reward=971.53 +/- 83.20
Episode length: 947.00 +/- 160.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 3205128  |
---------------------------------
Eval num_timesteps=3207120, episode_reward=870.66 +/- 601.44
Episode length: 741.60 +/- 176.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 3207120  |
---------------------------------
Eval num_timesteps=3209112, episode_reward=1301.31 +/- 422.94
Episode length: 982.00 +/- 142.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3209112  |
---------------------------------
Eval num_timesteps=3211104, episode_reward=1273.78 +/- 339.10
Episode length: 808.80 +/- 173.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3211104  |
---------------------------------
Eval num_timesteps=3213096, episode_reward=1087.86 +/- 419.65
Episode length: 903.80 +/- 188.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3213096  |
---------------------------------
Eval num_timesteps=3215088, episode_reward=925.15 +/- 601.94
Episode length: 757.60 +/- 173.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 3215088  |
---------------------------------
Eval num_timesteps=3217080, episode_reward=1505.00 +/- 515.51
Episode length: 732.80 +/- 83.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 3217080  |
---------------------------------
Eval num_timesteps=3219072, episode_reward=1296.88 +/- 709.02
Episode length: 852.20 +/- 201.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3219072  |
---------------------------------
Eval num_timesteps=3221064, episode_reward=1215.93 +/- 472.75
Episode length: 898.20 +/- 168.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3221064  |
---------------------------------
Eval num_timesteps=3223056, episode_reward=1238.63 +/- 936.72
Episode length: 987.40 +/- 248.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 987      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3223056  |
---------------------------------
Eval num_timesteps=3225048, episode_reward=1213.31 +/- 660.96
Episode length: 978.40 +/- 236.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3225048  |
---------------------------------
Eval num_timesteps=3227040, episode_reward=974.31 +/- 694.66
Episode length: 833.00 +/- 170.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 3227040  |
---------------------------------
Eval num_timesteps=3229032, episode_reward=1197.91 +/- 541.42
Episode length: 948.60 +/- 315.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3229032  |
---------------------------------
Eval num_timesteps=3231024, episode_reward=1053.84 +/- 915.05
Episode length: 822.80 +/- 145.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3231024  |
---------------------------------
Eval num_timesteps=3233016, episode_reward=918.05 +/- 650.95
Episode length: 701.40 +/- 195.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 3233016  |
---------------------------------
Eval num_timesteps=3235008, episode_reward=968.63 +/- 522.18
Episode length: 816.60 +/- 146.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 3235008  |
---------------------------------
Eval num_timesteps=3237000, episode_reward=1104.45 +/- 701.97
Episode length: 928.40 +/- 212.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3237000  |
---------------------------------
Eval num_timesteps=3238992, episode_reward=976.50 +/- 329.42
Episode length: 746.60 +/- 146.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 3238992  |
---------------------------------
Eval num_timesteps=3240984, episode_reward=890.18 +/- 727.79
Episode length: 931.60 +/- 239.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 3240984  |
---------------------------------
Eval num_timesteps=3242976, episode_reward=1129.13 +/- 130.32
Episode length: 748.20 +/- 130.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3242976  |
---------------------------------
Eval num_timesteps=3244968, episode_reward=678.01 +/- 173.12
Episode length: 787.00 +/- 179.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 787          |
|    mean_reward          | 678          |
| time/                   |              |
|    total_timesteps      | 3244968      |
| train/                  |              |
|    approx_kl            | 0.0051738797 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.85        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | -0.0312      |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.34         |
|    value_loss           | 0.0829       |
------------------------------------------
Eval num_timesteps=3246960, episode_reward=982.77 +/- 281.04
Episode length: 953.40 +/- 155.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 3246960  |
---------------------------------
Eval num_timesteps=3248952, episode_reward=1152.34 +/- 320.34
Episode length: 900.00 +/- 175.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3248952  |
---------------------------------
Eval num_timesteps=3250944, episode_reward=908.39 +/- 236.20
Episode length: 780.80 +/- 103.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 3250944  |
---------------------------------
Eval num_timesteps=3252936, episode_reward=1472.71 +/- 665.63
Episode length: 863.20 +/- 77.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3252936  |
---------------------------------
Eval num_timesteps=3254928, episode_reward=964.26 +/- 363.56
Episode length: 897.80 +/- 128.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 3254928  |
---------------------------------
Eval num_timesteps=3256920, episode_reward=1390.06 +/- 644.56
Episode length: 990.80 +/- 241.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 991      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3256920  |
---------------------------------
Eval num_timesteps=3258912, episode_reward=900.63 +/- 195.46
Episode length: 1024.00 +/- 139.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 3258912  |
---------------------------------
Eval num_timesteps=3260904, episode_reward=1017.91 +/- 297.41
Episode length: 768.40 +/- 93.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3260904  |
---------------------------------
Eval num_timesteps=3262896, episode_reward=721.80 +/- 165.89
Episode length: 745.00 +/- 63.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 3262896  |
---------------------------------
Eval num_timesteps=3264888, episode_reward=1169.00 +/- 640.86
Episode length: 907.20 +/- 169.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3264888  |
---------------------------------
Eval num_timesteps=3266880, episode_reward=870.81 +/- 473.93
Episode length: 961.00 +/- 114.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 3266880  |
---------------------------------
Eval num_timesteps=3268872, episode_reward=602.11 +/- 442.48
Episode length: 720.00 +/- 208.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 602      |
| time/              |          |
|    total_timesteps | 3268872  |
---------------------------------
Eval num_timesteps=3270864, episode_reward=1654.55 +/- 563.74
Episode length: 940.80 +/- 162.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3270864  |
---------------------------------
Eval num_timesteps=3272856, episode_reward=906.90 +/- 85.36
Episode length: 821.00 +/- 132.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 3272856  |
---------------------------------
Eval num_timesteps=3274848, episode_reward=1289.26 +/- 657.34
Episode length: 981.60 +/- 276.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3274848  |
---------------------------------
Eval num_timesteps=3276840, episode_reward=962.62 +/- 579.13
Episode length: 788.40 +/- 151.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 3276840  |
---------------------------------
Eval num_timesteps=3278832, episode_reward=615.20 +/- 466.06
Episode length: 817.80 +/- 277.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 3278832  |
---------------------------------
Eval num_timesteps=3280824, episode_reward=1018.59 +/- 239.40
Episode length: 862.60 +/- 68.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3280824  |
---------------------------------
Eval num_timesteps=3282816, episode_reward=841.18 +/- 403.55
Episode length: 833.20 +/- 148.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 3282816  |
---------------------------------
Eval num_timesteps=3284808, episode_reward=906.99 +/- 209.72
Episode length: 857.80 +/- 134.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 3284808  |
---------------------------------
Eval num_timesteps=3286800, episode_reward=992.31 +/- 227.98
Episode length: 920.80 +/- 252.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 3286800  |
---------------------------------
Eval num_timesteps=3288792, episode_reward=1077.17 +/- 313.29
Episode length: 942.60 +/- 244.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3288792  |
---------------------------------
Eval num_timesteps=3290784, episode_reward=1344.15 +/- 483.15
Episode length: 820.80 +/- 92.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3290784  |
---------------------------------
Eval num_timesteps=3292776, episode_reward=1471.43 +/- 776.43
Episode length: 823.40 +/- 212.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3292776  |
---------------------------------
Eval num_timesteps=3294768, episode_reward=1200.30 +/- 802.99
Episode length: 815.20 +/- 266.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | 1.2e+03     |
| time/                   |             |
|    total_timesteps      | 3294768     |
| train/                  |             |
|    approx_kl            | 0.005310844 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0295     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00135    |
|    std                  | 1.35        |
|    value_loss           | 0.0859      |
-----------------------------------------
Eval num_timesteps=3296760, episode_reward=947.92 +/- 595.96
Episode length: 837.00 +/- 141.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 3296760  |
---------------------------------
Eval num_timesteps=3298752, episode_reward=1067.62 +/- 377.72
Episode length: 819.40 +/- 72.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3298752  |
---------------------------------
Eval num_timesteps=3300744, episode_reward=1014.20 +/- 293.19
Episode length: 799.40 +/- 89.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3300744  |
---------------------------------
Eval num_timesteps=3302736, episode_reward=1379.63 +/- 736.95
Episode length: 877.80 +/- 214.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3302736  |
---------------------------------
Eval num_timesteps=3304728, episode_reward=2152.43 +/- 925.03
Episode length: 872.20 +/- 211.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3304728  |
---------------------------------
New best mean reward!
Eval num_timesteps=3306720, episode_reward=830.92 +/- 156.91
Episode length: 820.60 +/- 161.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 3306720  |
---------------------------------
Eval num_timesteps=3308712, episode_reward=946.89 +/- 242.21
Episode length: 903.00 +/- 225.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 3308712  |
---------------------------------
Eval num_timesteps=3310704, episode_reward=946.43 +/- 214.42
Episode length: 828.00 +/- 91.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 3310704  |
---------------------------------
Eval num_timesteps=3312696, episode_reward=1308.53 +/- 586.20
Episode length: 762.20 +/- 111.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3312696  |
---------------------------------
Eval num_timesteps=3314688, episode_reward=1265.18 +/- 616.86
Episode length: 835.40 +/- 112.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3314688  |
---------------------------------
Eval num_timesteps=3316680, episode_reward=942.77 +/- 390.08
Episode length: 863.20 +/- 220.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 3316680  |
---------------------------------
Eval num_timesteps=3318672, episode_reward=1091.48 +/- 463.57
Episode length: 918.60 +/- 144.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3318672  |
---------------------------------
Eval num_timesteps=3320664, episode_reward=1239.60 +/- 754.70
Episode length: 855.60 +/- 275.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3320664  |
---------------------------------
Eval num_timesteps=3322656, episode_reward=838.44 +/- 200.30
Episode length: 860.60 +/- 147.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 3322656  |
---------------------------------
Eval num_timesteps=3324648, episode_reward=1006.80 +/- 312.83
Episode length: 796.20 +/- 79.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3324648  |
---------------------------------
Eval num_timesteps=3326640, episode_reward=834.93 +/- 233.69
Episode length: 829.60 +/- 125.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 3326640  |
---------------------------------
Eval num_timesteps=3328632, episode_reward=1110.97 +/- 298.24
Episode length: 948.80 +/- 190.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3328632  |
---------------------------------
Eval num_timesteps=3330624, episode_reward=1539.95 +/- 707.99
Episode length: 803.40 +/- 101.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3330624  |
---------------------------------
Eval num_timesteps=3332616, episode_reward=814.44 +/- 117.19
Episode length: 795.00 +/- 50.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 3332616  |
---------------------------------
Eval num_timesteps=3334608, episode_reward=756.62 +/- 395.20
Episode length: 789.40 +/- 255.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 3334608  |
---------------------------------
Eval num_timesteps=3336600, episode_reward=778.90 +/- 183.81
Episode length: 789.40 +/- 161.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 3336600  |
---------------------------------
Eval num_timesteps=3338592, episode_reward=942.71 +/- 288.45
Episode length: 839.80 +/- 87.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 3338592  |
---------------------------------
Eval num_timesteps=3340584, episode_reward=788.07 +/- 133.30
Episode length: 794.20 +/- 59.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 3340584  |
---------------------------------
Eval num_timesteps=3342576, episode_reward=986.90 +/- 588.79
Episode length: 901.40 +/- 138.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 901          |
|    mean_reward          | 987          |
| time/                   |              |
|    total_timesteps      | 3342576      |
| train/                  |              |
|    approx_kl            | 0.0057685683 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0299      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.36         |
|    value_loss           | 0.0875       |
------------------------------------------
Eval num_timesteps=3344568, episode_reward=1245.31 +/- 1095.85
Episode length: 724.00 +/- 270.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3344568  |
---------------------------------
Eval num_timesteps=3346560, episode_reward=1259.18 +/- 546.16
Episode length: 849.20 +/- 102.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3346560  |
---------------------------------
Eval num_timesteps=3348552, episode_reward=1513.72 +/- 533.20
Episode length: 924.40 +/- 240.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3348552  |
---------------------------------
Eval num_timesteps=3350544, episode_reward=1792.21 +/- 635.84
Episode length: 791.00 +/- 142.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3350544  |
---------------------------------
Eval num_timesteps=3352536, episode_reward=1021.60 +/- 390.00
Episode length: 710.40 +/- 136.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3352536  |
---------------------------------
Eval num_timesteps=3354528, episode_reward=1305.46 +/- 437.69
Episode length: 700.00 +/- 108.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3354528  |
---------------------------------
Eval num_timesteps=3356520, episode_reward=657.91 +/- 212.35
Episode length: 816.00 +/- 104.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 3356520  |
---------------------------------
Eval num_timesteps=3358512, episode_reward=1202.45 +/- 722.76
Episode length: 822.20 +/- 88.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3358512  |
---------------------------------
Eval num_timesteps=3360504, episode_reward=1471.94 +/- 781.30
Episode length: 949.40 +/- 227.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3360504  |
---------------------------------
Eval num_timesteps=3362496, episode_reward=1509.48 +/- 337.49
Episode length: 798.20 +/- 108.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3362496  |
---------------------------------
Eval num_timesteps=3364488, episode_reward=1202.39 +/- 353.71
Episode length: 813.80 +/- 140.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3364488  |
---------------------------------
Eval num_timesteps=3366480, episode_reward=1100.23 +/- 874.55
Episode length: 780.80 +/- 151.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3366480  |
---------------------------------
Eval num_timesteps=3368472, episode_reward=1241.40 +/- 598.66
Episode length: 831.20 +/- 182.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3368472  |
---------------------------------
Eval num_timesteps=3370464, episode_reward=1097.87 +/- 366.57
Episode length: 816.00 +/- 122.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3370464  |
---------------------------------
Eval num_timesteps=3372456, episode_reward=1581.29 +/- 639.54
Episode length: 841.40 +/- 141.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3372456  |
---------------------------------
Eval num_timesteps=3374448, episode_reward=1413.91 +/- 389.18
Episode length: 805.00 +/- 132.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3374448  |
---------------------------------
Eval num_timesteps=3376440, episode_reward=1371.07 +/- 538.89
Episode length: 760.60 +/- 195.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3376440  |
---------------------------------
Eval num_timesteps=3378432, episode_reward=1219.50 +/- 380.51
Episode length: 854.60 +/- 232.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3378432  |
---------------------------------
Eval num_timesteps=3380424, episode_reward=1340.05 +/- 414.40
Episode length: 924.00 +/- 118.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3380424  |
---------------------------------
Eval num_timesteps=3382416, episode_reward=991.09 +/- 237.51
Episode length: 790.80 +/- 150.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 3382416  |
---------------------------------
Eval num_timesteps=3384408, episode_reward=864.35 +/- 272.01
Episode length: 829.60 +/- 104.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 3384408  |
---------------------------------
Eval num_timesteps=3386400, episode_reward=1328.51 +/- 803.34
Episode length: 769.40 +/- 84.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3386400  |
---------------------------------
Eval num_timesteps=3388392, episode_reward=1006.48 +/- 550.40
Episode length: 834.80 +/- 231.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3388392  |
---------------------------------
Eval num_timesteps=3390384, episode_reward=934.04 +/- 316.84
Episode length: 745.40 +/- 54.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 3390384  |
---------------------------------
Eval num_timesteps=3392376, episode_reward=1654.10 +/- 392.50
Episode length: 629.60 +/- 67.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 630         |
|    mean_reward          | 1.65e+03    |
| time/                   |             |
|    total_timesteps      | 3392376     |
| train/                  |             |
|    approx_kl            | 0.005175868 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0292     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00127    |
|    std                  | 1.36        |
|    value_loss           | 0.0882      |
-----------------------------------------
Eval num_timesteps=3394368, episode_reward=1433.15 +/- 383.69
Episode length: 686.40 +/- 122.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3394368  |
---------------------------------
Eval num_timesteps=3396360, episode_reward=1321.54 +/- 677.95
Episode length: 732.60 +/- 119.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3396360  |
---------------------------------
Eval num_timesteps=3398352, episode_reward=1059.52 +/- 664.44
Episode length: 777.40 +/- 289.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3398352  |
---------------------------------
Eval num_timesteps=3400344, episode_reward=943.50 +/- 367.04
Episode length: 769.20 +/- 163.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 3400344  |
---------------------------------
Eval num_timesteps=3402336, episode_reward=1170.74 +/- 670.07
Episode length: 817.60 +/- 187.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3402336  |
---------------------------------
Eval num_timesteps=3404328, episode_reward=790.82 +/- 107.50
Episode length: 665.40 +/- 52.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 3404328  |
---------------------------------
Eval num_timesteps=3406320, episode_reward=1637.59 +/- 731.21
Episode length: 796.80 +/- 162.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3406320  |
---------------------------------
Eval num_timesteps=3408312, episode_reward=1313.04 +/- 447.20
Episode length: 787.40 +/- 105.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3408312  |
---------------------------------
Eval num_timesteps=3410304, episode_reward=1345.12 +/- 534.89
Episode length: 676.40 +/- 75.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3410304  |
---------------------------------
Eval num_timesteps=3412296, episode_reward=994.82 +/- 608.91
Episode length: 745.00 +/- 172.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 3412296  |
---------------------------------
Eval num_timesteps=3414288, episode_reward=1306.42 +/- 501.64
Episode length: 708.20 +/- 116.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3414288  |
---------------------------------
Eval num_timesteps=3416280, episode_reward=1034.93 +/- 419.81
Episode length: 614.20 +/- 66.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3416280  |
---------------------------------
Eval num_timesteps=3418272, episode_reward=1389.48 +/- 351.34
Episode length: 825.60 +/- 94.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3418272  |
---------------------------------
Eval num_timesteps=3420264, episode_reward=1364.45 +/- 541.53
Episode length: 860.80 +/- 179.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3420264  |
---------------------------------
Eval num_timesteps=3422256, episode_reward=1020.96 +/- 483.17
Episode length: 813.40 +/- 253.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3422256  |
---------------------------------
Eval num_timesteps=3424248, episode_reward=693.27 +/- 154.87
Episode length: 549.20 +/- 153.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 3424248  |
---------------------------------
Eval num_timesteps=3426240, episode_reward=1777.28 +/- 651.75
Episode length: 730.20 +/- 159.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3426240  |
---------------------------------
Eval num_timesteps=3428232, episode_reward=950.87 +/- 389.48
Episode length: 681.00 +/- 156.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 3428232  |
---------------------------------
Eval num_timesteps=3430224, episode_reward=1608.86 +/- 677.65
Episode length: 684.60 +/- 96.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3430224  |
---------------------------------
Eval num_timesteps=3432216, episode_reward=1328.01 +/- 611.54
Episode length: 698.00 +/- 55.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3432216  |
---------------------------------
Eval num_timesteps=3434208, episode_reward=1353.43 +/- 592.44
Episode length: 672.60 +/- 52.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3434208  |
---------------------------------
Eval num_timesteps=3436200, episode_reward=1488.99 +/- 783.40
Episode length: 674.40 +/- 101.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 3436200  |
---------------------------------
Eval num_timesteps=3438192, episode_reward=1110.68 +/- 783.07
Episode length: 779.20 +/- 91.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3438192  |
---------------------------------
Eval num_timesteps=3440184, episode_reward=1508.80 +/- 378.32
Episode length: 684.40 +/- 39.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3440184  |
---------------------------------
Eval num_timesteps=3442176, episode_reward=1670.29 +/- 786.55
Episode length: 763.00 +/- 157.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 763         |
|    mean_reward          | 1.67e+03    |
| time/                   |             |
|    total_timesteps      | 3442176     |
| train/                  |             |
|    approx_kl            | 0.005508862 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0319     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00135    |
|    std                  | 1.37        |
|    value_loss           | 0.0837      |
-----------------------------------------
Eval num_timesteps=3444168, episode_reward=1039.28 +/- 389.54
Episode length: 647.20 +/- 60.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3444168  |
---------------------------------
Eval num_timesteps=3446160, episode_reward=1623.13 +/- 807.94
Episode length: 735.00 +/- 209.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3446160  |
---------------------------------
Eval num_timesteps=3448152, episode_reward=1570.73 +/- 857.04
Episode length: 693.60 +/- 122.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3448152  |
---------------------------------
Eval num_timesteps=3450144, episode_reward=1626.27 +/- 426.05
Episode length: 787.00 +/- 188.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3450144  |
---------------------------------
Eval num_timesteps=3452136, episode_reward=685.04 +/- 242.03
Episode length: 689.00 +/- 117.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 3452136  |
---------------------------------
Eval num_timesteps=3454128, episode_reward=1157.62 +/- 472.94
Episode length: 678.00 +/- 183.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3454128  |
---------------------------------
Eval num_timesteps=3456120, episode_reward=1434.77 +/- 360.62
Episode length: 719.20 +/- 103.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3456120  |
---------------------------------
Eval num_timesteps=3458112, episode_reward=1479.82 +/- 822.52
Episode length: 635.60 +/- 137.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 3458112  |
---------------------------------
Eval num_timesteps=3460104, episode_reward=1194.45 +/- 452.07
Episode length: 707.60 +/- 37.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3460104  |
---------------------------------
Eval num_timesteps=3462096, episode_reward=904.62 +/- 199.31
Episode length: 645.20 +/- 75.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 3462096  |
---------------------------------
Eval num_timesteps=3464088, episode_reward=1006.91 +/- 367.32
Episode length: 776.20 +/- 117.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3464088  |
---------------------------------
Eval num_timesteps=3466080, episode_reward=1313.29 +/- 640.02
Episode length: 710.80 +/- 66.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3466080  |
---------------------------------
Eval num_timesteps=3468072, episode_reward=1283.60 +/- 730.19
Episode length: 633.40 +/- 123.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3468072  |
---------------------------------
Eval num_timesteps=3470064, episode_reward=1417.05 +/- 367.04
Episode length: 759.20 +/- 164.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3470064  |
---------------------------------
Eval num_timesteps=3472056, episode_reward=1224.14 +/- 659.92
Episode length: 733.60 +/- 218.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3472056  |
---------------------------------
Eval num_timesteps=3474048, episode_reward=667.23 +/- 533.52
Episode length: 514.40 +/- 151.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 3474048  |
---------------------------------
Eval num_timesteps=3476040, episode_reward=1045.69 +/- 512.54
Episode length: 623.60 +/- 169.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3476040  |
---------------------------------
Eval num_timesteps=3478032, episode_reward=1428.35 +/- 265.52
Episode length: 650.60 +/- 69.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3478032  |
---------------------------------
Eval num_timesteps=3480024, episode_reward=1114.62 +/- 750.44
Episode length: 630.20 +/- 133.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3480024  |
---------------------------------
Eval num_timesteps=3482016, episode_reward=1323.60 +/- 627.09
Episode length: 709.20 +/- 196.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3482016  |
---------------------------------
Eval num_timesteps=3484008, episode_reward=1278.63 +/- 354.37
Episode length: 725.20 +/- 110.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3484008  |
---------------------------------
Eval num_timesteps=3486000, episode_reward=1045.56 +/- 789.53
Episode length: 500.00 +/- 100.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3486000  |
---------------------------------
Eval num_timesteps=3487992, episode_reward=1425.06 +/- 736.15
Episode length: 627.80 +/- 163.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3487992  |
---------------------------------
Eval num_timesteps=3489984, episode_reward=1232.49 +/- 359.98
Episode length: 617.20 +/- 30.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 617          |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 3489984      |
| train/                  |              |
|    approx_kl            | 0.0052605933 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0315      |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1.37         |
|    value_loss           | 0.0839       |
------------------------------------------
Eval num_timesteps=3491976, episode_reward=688.87 +/- 165.28
Episode length: 660.80 +/- 191.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 3491976  |
---------------------------------
Eval num_timesteps=3493968, episode_reward=1198.31 +/- 483.44
Episode length: 805.00 +/- 201.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3493968  |
---------------------------------
Eval num_timesteps=3495960, episode_reward=1111.39 +/- 678.16
Episode length: 790.60 +/- 172.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3495960  |
---------------------------------
Eval num_timesteps=3497952, episode_reward=1902.49 +/- 749.80
Episode length: 902.00 +/- 158.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3497952  |
---------------------------------
Eval num_timesteps=3499944, episode_reward=1109.05 +/- 163.01
Episode length: 628.60 +/- 50.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3499944  |
---------------------------------
Eval num_timesteps=3501936, episode_reward=906.54 +/- 332.99
Episode length: 770.80 +/- 222.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 3501936  |
---------------------------------
Eval num_timesteps=3503928, episode_reward=994.38 +/- 402.10
Episode length: 723.80 +/- 120.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 3503928  |
---------------------------------
Eval num_timesteps=3505920, episode_reward=1080.99 +/- 615.33
Episode length: 656.40 +/- 228.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3505920  |
---------------------------------
Eval num_timesteps=3507912, episode_reward=905.79 +/- 226.68
Episode length: 763.80 +/- 140.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 3507912  |
---------------------------------
Eval num_timesteps=3509904, episode_reward=1153.91 +/- 400.26
Episode length: 732.00 +/- 113.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3509904  |
---------------------------------
Eval num_timesteps=3511896, episode_reward=1111.89 +/- 783.12
Episode length: 559.20 +/- 137.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3511896  |
---------------------------------
Eval num_timesteps=3513888, episode_reward=992.25 +/- 254.72
Episode length: 745.40 +/- 155.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 3513888  |
---------------------------------
Eval num_timesteps=3515880, episode_reward=996.61 +/- 254.90
Episode length: 679.80 +/- 138.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 3515880  |
---------------------------------
Eval num_timesteps=3517872, episode_reward=1960.48 +/- 919.44
Episode length: 802.80 +/- 117.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3517872  |
---------------------------------
Eval num_timesteps=3519864, episode_reward=759.82 +/- 76.96
Episode length: 761.60 +/- 73.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 3519864  |
---------------------------------
Eval num_timesteps=3521856, episode_reward=1079.64 +/- 423.77
Episode length: 659.00 +/- 114.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3521856  |
---------------------------------
Eval num_timesteps=3523848, episode_reward=1157.64 +/- 403.44
Episode length: 756.80 +/- 94.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3523848  |
---------------------------------
Eval num_timesteps=3525840, episode_reward=1113.06 +/- 452.11
Episode length: 596.20 +/- 144.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3525840  |
---------------------------------
Eval num_timesteps=3527832, episode_reward=590.56 +/- 75.45
Episode length: 663.40 +/- 59.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 3527832  |
---------------------------------
Eval num_timesteps=3529824, episode_reward=1755.20 +/- 384.68
Episode length: 720.00 +/- 140.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3529824  |
---------------------------------
Eval num_timesteps=3531816, episode_reward=762.36 +/- 305.19
Episode length: 686.40 +/- 158.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 3531816  |
---------------------------------
Eval num_timesteps=3533808, episode_reward=1270.12 +/- 555.81
Episode length: 758.20 +/- 160.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3533808  |
---------------------------------
Eval num_timesteps=3535800, episode_reward=1306.29 +/- 676.45
Episode length: 614.20 +/- 34.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3535800  |
---------------------------------
Eval num_timesteps=3537792, episode_reward=1009.11 +/- 370.37
Episode length: 626.20 +/- 56.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3537792  |
---------------------------------
Eval num_timesteps=3539784, episode_reward=709.55 +/- 85.76
Episode length: 653.00 +/- 42.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 653       |
|    mean_reward          | 710       |
| time/                   |           |
|    total_timesteps      | 3539784   |
| train/                  |           |
|    approx_kl            | 0.0054695 |
|    clip_fraction        | 0.0323    |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.94     |
|    explained_variance   | 0.961     |
|    learning_rate        | 0.001     |
|    loss                 | -0.0346   |
|    n_updates            | 720       |
|    policy_gradient_loss | -0.00124  |
|    std                  | 1.38      |
|    value_loss           | 0.0768    |
---------------------------------------
Eval num_timesteps=3541776, episode_reward=991.43 +/- 668.98
Episode length: 674.00 +/- 119.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 3541776  |
---------------------------------
Eval num_timesteps=3543768, episode_reward=1027.63 +/- 572.95
Episode length: 655.60 +/- 125.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3543768  |
---------------------------------
Eval num_timesteps=3545760, episode_reward=845.32 +/- 176.20
Episode length: 663.40 +/- 102.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 3545760  |
---------------------------------
Eval num_timesteps=3547752, episode_reward=723.66 +/- 61.17
Episode length: 665.00 +/- 57.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 3547752  |
---------------------------------
Eval num_timesteps=3549744, episode_reward=762.40 +/- 101.33
Episode length: 659.40 +/- 131.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 3549744  |
---------------------------------
Eval num_timesteps=3551736, episode_reward=943.97 +/- 225.96
Episode length: 654.20 +/- 148.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 3551736  |
---------------------------------
Eval num_timesteps=3553728, episode_reward=1098.14 +/- 564.15
Episode length: 669.40 +/- 87.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3553728  |
---------------------------------
Eval num_timesteps=3555720, episode_reward=938.44 +/- 714.51
Episode length: 576.20 +/- 112.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 3555720  |
---------------------------------
Eval num_timesteps=3557712, episode_reward=1157.57 +/- 541.17
Episode length: 666.60 +/- 48.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3557712  |
---------------------------------
Eval num_timesteps=3559704, episode_reward=853.28 +/- 681.65
Episode length: 604.00 +/- 183.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 3559704  |
---------------------------------
Eval num_timesteps=3561696, episode_reward=886.33 +/- 383.65
Episode length: 673.80 +/- 90.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 3561696  |
---------------------------------
Eval num_timesteps=3563688, episode_reward=975.64 +/- 416.14
Episode length: 628.60 +/- 33.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 3563688  |
---------------------------------
Eval num_timesteps=3565680, episode_reward=1245.96 +/- 530.06
Episode length: 710.40 +/- 147.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3565680  |
---------------------------------
Eval num_timesteps=3567672, episode_reward=1350.52 +/- 547.89
Episode length: 629.20 +/- 48.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3567672  |
---------------------------------
Eval num_timesteps=3569664, episode_reward=833.71 +/- 217.33
Episode length: 671.80 +/- 87.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 3569664  |
---------------------------------
Eval num_timesteps=3571656, episode_reward=1052.96 +/- 510.66
Episode length: 543.80 +/- 53.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3571656  |
---------------------------------
Eval num_timesteps=3573648, episode_reward=727.16 +/- 586.14
Episode length: 514.80 +/- 106.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 3573648  |
---------------------------------
Eval num_timesteps=3575640, episode_reward=1009.42 +/- 535.71
Episode length: 712.00 +/- 89.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3575640  |
---------------------------------
Eval num_timesteps=3577632, episode_reward=694.53 +/- 105.61
Episode length: 636.60 +/- 131.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 3577632  |
---------------------------------
Eval num_timesteps=3579624, episode_reward=932.56 +/- 414.53
Episode length: 745.20 +/- 197.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 3579624  |
---------------------------------
Eval num_timesteps=3581616, episode_reward=1003.62 +/- 588.08
Episode length: 843.00 +/- 173.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 3581616  |
---------------------------------
Eval num_timesteps=3583608, episode_reward=863.31 +/- 194.54
Episode length: 647.60 +/- 89.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 3583608  |
---------------------------------
Eval num_timesteps=3585600, episode_reward=1352.12 +/- 442.33
Episode length: 751.60 +/- 181.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3585600  |
---------------------------------
Eval num_timesteps=3587592, episode_reward=778.47 +/- 219.03
Episode length: 633.20 +/- 134.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 3587592  |
---------------------------------
Eval num_timesteps=3589584, episode_reward=941.75 +/- 495.47
Episode length: 797.60 +/- 165.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 798       |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 3589584   |
| train/                  |           |
|    approx_kl            | 0.0047479 |
|    clip_fraction        | 0.0255    |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.96     |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.001     |
|    loss                 | -0.0349   |
|    n_updates            | 730       |
|    policy_gradient_loss | -0.00128  |
|    std                  | 1.38      |
|    value_loss           | 0.0777    |
---------------------------------------
Eval num_timesteps=3591576, episode_reward=838.08 +/- 152.12
Episode length: 715.20 +/- 133.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 3591576  |
---------------------------------
Eval num_timesteps=3593568, episode_reward=1014.60 +/- 563.85
Episode length: 723.40 +/- 150.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3593568  |
---------------------------------
Eval num_timesteps=3595560, episode_reward=911.30 +/- 96.64
Episode length: 769.60 +/- 65.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 3595560  |
---------------------------------
Eval num_timesteps=3597552, episode_reward=733.91 +/- 58.53
Episode length: 698.60 +/- 48.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 3597552  |
---------------------------------
Eval num_timesteps=3599544, episode_reward=1552.06 +/- 461.08
Episode length: 769.00 +/- 206.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3599544  |
---------------------------------
Eval num_timesteps=3601536, episode_reward=997.56 +/- 304.11
Episode length: 831.80 +/- 123.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 3601536  |
---------------------------------
Eval num_timesteps=3603528, episode_reward=1157.17 +/- 810.46
Episode length: 705.00 +/- 93.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3603528  |
---------------------------------
Eval num_timesteps=3605520, episode_reward=1079.11 +/- 488.72
Episode length: 810.40 +/- 87.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3605520  |
---------------------------------
Eval num_timesteps=3607512, episode_reward=990.34 +/- 169.01
Episode length: 670.00 +/- 114.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 3607512  |
---------------------------------
Eval num_timesteps=3609504, episode_reward=898.13 +/- 154.73
Episode length: 660.40 +/- 47.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 3609504  |
---------------------------------
Eval num_timesteps=3611496, episode_reward=869.86 +/- 157.75
Episode length: 765.60 +/- 96.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 3611496  |
---------------------------------
Eval num_timesteps=3613488, episode_reward=1079.69 +/- 626.01
Episode length: 645.40 +/- 44.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3613488  |
---------------------------------
Eval num_timesteps=3615480, episode_reward=949.32 +/- 353.39
Episode length: 724.20 +/- 115.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 3615480  |
---------------------------------
Eval num_timesteps=3617472, episode_reward=1033.21 +/- 526.17
Episode length: 729.60 +/- 51.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3617472  |
---------------------------------
Eval num_timesteps=3619464, episode_reward=920.16 +/- 244.76
Episode length: 667.80 +/- 34.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 3619464  |
---------------------------------
Eval num_timesteps=3621456, episode_reward=1204.96 +/- 389.75
Episode length: 643.60 +/- 36.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3621456  |
---------------------------------
Eval num_timesteps=3623448, episode_reward=844.07 +/- 174.43
Episode length: 678.40 +/- 75.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 3623448  |
---------------------------------
Eval num_timesteps=3625440, episode_reward=1108.39 +/- 361.15
Episode length: 636.20 +/- 104.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3625440  |
---------------------------------
Eval num_timesteps=3627432, episode_reward=737.39 +/- 75.87
Episode length: 776.80 +/- 147.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 3627432  |
---------------------------------
Eval num_timesteps=3629424, episode_reward=988.97 +/- 450.03
Episode length: 745.40 +/- 55.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 3629424  |
---------------------------------
Eval num_timesteps=3631416, episode_reward=825.07 +/- 116.20
Episode length: 675.60 +/- 85.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 3631416  |
---------------------------------
Eval num_timesteps=3633408, episode_reward=1067.88 +/- 410.00
Episode length: 657.60 +/- 20.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3633408  |
---------------------------------
Eval num_timesteps=3635400, episode_reward=1432.40 +/- 933.67
Episode length: 731.60 +/- 79.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3635400  |
---------------------------------
Eval num_timesteps=3637392, episode_reward=1611.49 +/- 604.96
Episode length: 649.20 +/- 96.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 649         |
|    mean_reward          | 1.61e+03    |
| time/                   |             |
|    total_timesteps      | 3637392     |
| train/                  |             |
|    approx_kl            | 0.004719806 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0329     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00104    |
|    std                  | 1.38        |
|    value_loss           | 0.0805      |
-----------------------------------------
Eval num_timesteps=3639384, episode_reward=834.88 +/- 171.84
Episode length: 702.60 +/- 140.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 3639384  |
---------------------------------
Eval num_timesteps=3641376, episode_reward=1177.54 +/- 377.65
Episode length: 723.80 +/- 187.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3641376  |
---------------------------------
Eval num_timesteps=3643368, episode_reward=947.47 +/- 336.26
Episode length: 777.20 +/- 86.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 3643368  |
---------------------------------
Eval num_timesteps=3645360, episode_reward=1099.93 +/- 364.80
Episode length: 751.60 +/- 108.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3645360  |
---------------------------------
Eval num_timesteps=3647352, episode_reward=912.74 +/- 208.53
Episode length: 699.80 +/- 86.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 3647352  |
---------------------------------
Eval num_timesteps=3649344, episode_reward=910.64 +/- 220.32
Episode length: 681.60 +/- 81.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 3649344  |
---------------------------------
Eval num_timesteps=3651336, episode_reward=798.03 +/- 81.66
Episode length: 742.80 +/- 134.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 3651336  |
---------------------------------
Eval num_timesteps=3653328, episode_reward=967.99 +/- 393.45
Episode length: 723.00 +/- 60.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 3653328  |
---------------------------------
Eval num_timesteps=3655320, episode_reward=922.99 +/- 278.00
Episode length: 670.80 +/- 80.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 3655320  |
---------------------------------
Eval num_timesteps=3657312, episode_reward=1009.17 +/- 318.01
Episode length: 719.60 +/- 111.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3657312  |
---------------------------------
Eval num_timesteps=3659304, episode_reward=867.79 +/- 291.06
Episode length: 668.80 +/- 74.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 3659304  |
---------------------------------
Eval num_timesteps=3661296, episode_reward=881.00 +/- 228.34
Episode length: 674.40 +/- 86.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 3661296  |
---------------------------------
Eval num_timesteps=3663288, episode_reward=639.92 +/- 194.44
Episode length: 687.00 +/- 116.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 3663288  |
---------------------------------
Eval num_timesteps=3665280, episode_reward=1229.32 +/- 470.02
Episode length: 717.20 +/- 139.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3665280  |
---------------------------------
Eval num_timesteps=3667272, episode_reward=930.16 +/- 602.06
Episode length: 670.60 +/- 102.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 3667272  |
---------------------------------
Eval num_timesteps=3669264, episode_reward=826.00 +/- 527.59
Episode length: 678.80 +/- 64.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 3669264  |
---------------------------------
Eval num_timesteps=3671256, episode_reward=881.00 +/- 297.54
Episode length: 758.20 +/- 95.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 3671256  |
---------------------------------
Eval num_timesteps=3673248, episode_reward=920.25 +/- 245.32
Episode length: 671.00 +/- 81.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 3673248  |
---------------------------------
Eval num_timesteps=3675240, episode_reward=1198.15 +/- 405.51
Episode length: 776.20 +/- 67.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3675240  |
---------------------------------
Eval num_timesteps=3677232, episode_reward=1436.18 +/- 624.59
Episode length: 683.00 +/- 47.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3677232  |
---------------------------------
Eval num_timesteps=3679224, episode_reward=792.24 +/- 314.58
Episode length: 630.80 +/- 86.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 3679224  |
---------------------------------
Eval num_timesteps=3681216, episode_reward=951.06 +/- 359.36
Episode length: 730.00 +/- 60.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 3681216  |
---------------------------------
Eval num_timesteps=3683208, episode_reward=1028.13 +/- 379.77
Episode length: 764.60 +/- 92.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3683208  |
---------------------------------
Eval num_timesteps=3685200, episode_reward=1161.48 +/- 357.45
Episode length: 779.20 +/- 66.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3685200  |
---------------------------------
Eval num_timesteps=3687192, episode_reward=879.75 +/- 333.30
Episode length: 702.00 +/- 103.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 702         |
|    mean_reward          | 880         |
| time/                   |             |
|    total_timesteps      | 3687192     |
| train/                  |             |
|    approx_kl            | 0.003437775 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0312     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 1.39        |
|    value_loss           | 0.084       |
-----------------------------------------
Eval num_timesteps=3689184, episode_reward=1183.78 +/- 757.49
Episode length: 639.20 +/- 63.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3689184  |
---------------------------------
Eval num_timesteps=3691176, episode_reward=1286.87 +/- 502.25
Episode length: 605.40 +/- 63.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3691176  |
---------------------------------
Eval num_timesteps=3693168, episode_reward=1038.49 +/- 207.30
Episode length: 638.40 +/- 64.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3693168  |
---------------------------------
Eval num_timesteps=3695160, episode_reward=1273.23 +/- 470.25
Episode length: 647.80 +/- 80.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3695160  |
---------------------------------
Eval num_timesteps=3697152, episode_reward=1571.57 +/- 393.17
Episode length: 697.00 +/- 119.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3697152  |
---------------------------------
Eval num_timesteps=3699144, episode_reward=1058.54 +/- 464.63
Episode length: 708.40 +/- 165.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3699144  |
---------------------------------
Eval num_timesteps=3701136, episode_reward=1478.04 +/- 606.70
Episode length: 642.60 +/- 43.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 3701136  |
---------------------------------
Eval num_timesteps=3703128, episode_reward=1055.29 +/- 389.20
Episode length: 668.40 +/- 52.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3703128  |
---------------------------------
Eval num_timesteps=3705120, episode_reward=989.29 +/- 596.26
Episode length: 708.20 +/- 108.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 3705120  |
---------------------------------
Eval num_timesteps=3707112, episode_reward=1358.71 +/- 348.05
Episode length: 714.60 +/- 100.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3707112  |
---------------------------------
Eval num_timesteps=3709104, episode_reward=1051.74 +/- 393.24
Episode length: 701.80 +/- 95.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3709104  |
---------------------------------
Eval num_timesteps=3711096, episode_reward=808.23 +/- 113.35
Episode length: 750.00 +/- 70.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 3711096  |
---------------------------------
Eval num_timesteps=3713088, episode_reward=990.53 +/- 459.21
Episode length: 587.80 +/- 59.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 3713088  |
---------------------------------
Eval num_timesteps=3715080, episode_reward=1178.75 +/- 389.59
Episode length: 617.20 +/- 35.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3715080  |
---------------------------------
Eval num_timesteps=3717072, episode_reward=959.97 +/- 401.69
Episode length: 710.80 +/- 123.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 3717072  |
---------------------------------
Eval num_timesteps=3719064, episode_reward=989.73 +/- 247.94
Episode length: 603.00 +/- 70.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 3719064  |
---------------------------------
Eval num_timesteps=3721056, episode_reward=1051.99 +/- 499.18
Episode length: 675.60 +/- 84.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3721056  |
---------------------------------
Eval num_timesteps=3723048, episode_reward=1281.62 +/- 482.57
Episode length: 708.20 +/- 112.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3723048  |
---------------------------------
Eval num_timesteps=3725040, episode_reward=1112.08 +/- 362.95
Episode length: 672.20 +/- 58.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3725040  |
---------------------------------
Eval num_timesteps=3727032, episode_reward=819.53 +/- 218.73
Episode length: 639.80 +/- 108.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 3727032  |
---------------------------------
Eval num_timesteps=3729024, episode_reward=924.08 +/- 510.09
Episode length: 631.40 +/- 101.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 3729024  |
---------------------------------
Eval num_timesteps=3731016, episode_reward=962.49 +/- 316.00
Episode length: 674.60 +/- 141.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 3731016  |
---------------------------------
Eval num_timesteps=3733008, episode_reward=1280.14 +/- 442.26
Episode length: 735.80 +/- 124.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3733008  |
---------------------------------
Eval num_timesteps=3735000, episode_reward=1058.45 +/- 310.94
Episode length: 586.40 +/- 51.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3735000  |
---------------------------------
Eval num_timesteps=3736992, episode_reward=813.83 +/- 313.16
Episode length: 579.80 +/- 112.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 580          |
|    mean_reward          | 814          |
| time/                   |              |
|    total_timesteps      | 3736992      |
| train/                  |              |
|    approx_kl            | 0.0068035275 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7           |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0343      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.4          |
|    value_loss           | 0.0799       |
------------------------------------------
Eval num_timesteps=3738984, episode_reward=923.92 +/- 299.83
Episode length: 696.00 +/- 114.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 3738984  |
---------------------------------
Eval num_timesteps=3740976, episode_reward=1347.92 +/- 393.40
Episode length: 737.40 +/- 113.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3740976  |
---------------------------------
Eval num_timesteps=3742968, episode_reward=925.90 +/- 445.73
Episode length: 642.60 +/- 95.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 3742968  |
---------------------------------
Eval num_timesteps=3744960, episode_reward=1676.27 +/- 532.98
Episode length: 738.60 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3744960  |
---------------------------------
Eval num_timesteps=3746952, episode_reward=1087.73 +/- 187.73
Episode length: 697.00 +/- 86.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3746952  |
---------------------------------
Eval num_timesteps=3748944, episode_reward=1281.67 +/- 715.55
Episode length: 668.60 +/- 31.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3748944  |
---------------------------------
Eval num_timesteps=3750936, episode_reward=1032.60 +/- 462.76
Episode length: 776.80 +/- 72.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3750936  |
---------------------------------
Eval num_timesteps=3752928, episode_reward=1517.23 +/- 763.67
Episode length: 670.20 +/- 86.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3752928  |
---------------------------------
Eval num_timesteps=3754920, episode_reward=996.71 +/- 360.09
Episode length: 784.80 +/- 155.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 3754920  |
---------------------------------
Eval num_timesteps=3756912, episode_reward=1126.52 +/- 497.15
Episode length: 648.80 +/- 91.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3756912  |
---------------------------------
Eval num_timesteps=3758904, episode_reward=1206.13 +/- 428.71
Episode length: 713.40 +/- 58.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3758904  |
---------------------------------
Eval num_timesteps=3760896, episode_reward=1066.66 +/- 676.88
Episode length: 617.00 +/- 116.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3760896  |
---------------------------------
Eval num_timesteps=3762888, episode_reward=1240.85 +/- 667.90
Episode length: 596.20 +/- 150.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3762888  |
---------------------------------
Eval num_timesteps=3764880, episode_reward=1712.77 +/- 457.06
Episode length: 841.80 +/- 197.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 3764880  |
---------------------------------
Eval num_timesteps=3766872, episode_reward=1195.03 +/- 603.61
Episode length: 690.20 +/- 108.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3766872  |
---------------------------------
Eval num_timesteps=3768864, episode_reward=1087.15 +/- 474.10
Episode length: 750.60 +/- 102.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3768864  |
---------------------------------
Eval num_timesteps=3770856, episode_reward=1580.67 +/- 561.35
Episode length: 760.20 +/- 166.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3770856  |
---------------------------------
Eval num_timesteps=3772848, episode_reward=1545.26 +/- 298.90
Episode length: 837.60 +/- 106.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3772848  |
---------------------------------
Eval num_timesteps=3774840, episode_reward=907.91 +/- 340.12
Episode length: 718.80 +/- 139.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 3774840  |
---------------------------------
Eval num_timesteps=3776832, episode_reward=1141.86 +/- 574.31
Episode length: 601.00 +/- 134.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3776832  |
---------------------------------
Eval num_timesteps=3778824, episode_reward=2019.04 +/- 438.20
Episode length: 791.40 +/- 47.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3778824  |
---------------------------------
Eval num_timesteps=3780816, episode_reward=1125.65 +/- 526.57
Episode length: 738.80 +/- 146.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3780816  |
---------------------------------
Eval num_timesteps=3782808, episode_reward=1316.41 +/- 440.11
Episode length: 629.60 +/- 109.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3782808  |
---------------------------------
Eval num_timesteps=3784800, episode_reward=1244.20 +/- 588.48
Episode length: 806.80 +/- 141.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | 1.24e+03    |
| time/                   |             |
|    total_timesteps      | 3784800     |
| train/                  |             |
|    approx_kl            | 0.004240592 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0316     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00138    |
|    std                  | 1.41        |
|    value_loss           | 0.0866      |
-----------------------------------------
Eval num_timesteps=3786792, episode_reward=1400.10 +/- 625.99
Episode length: 693.60 +/- 38.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3786792  |
---------------------------------
Eval num_timesteps=3788784, episode_reward=848.00 +/- 770.02
Episode length: 629.20 +/- 154.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 3788784  |
---------------------------------
Eval num_timesteps=3790776, episode_reward=963.60 +/- 271.44
Episode length: 741.20 +/- 104.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 3790776  |
---------------------------------
Eval num_timesteps=3792768, episode_reward=907.81 +/- 408.97
Episode length: 725.80 +/- 38.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 3792768  |
---------------------------------
Eval num_timesteps=3794760, episode_reward=1039.34 +/- 685.51
Episode length: 578.20 +/- 152.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3794760  |
---------------------------------
Eval num_timesteps=3796752, episode_reward=880.18 +/- 484.75
Episode length: 686.60 +/- 105.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 3796752  |
---------------------------------
Eval num_timesteps=3798744, episode_reward=1735.10 +/- 581.99
Episode length: 802.80 +/- 172.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3798744  |
---------------------------------
Eval num_timesteps=3800736, episode_reward=1062.58 +/- 673.64
Episode length: 650.80 +/- 156.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3800736  |
---------------------------------
Eval num_timesteps=3802728, episode_reward=996.89 +/- 544.76
Episode length: 690.60 +/- 160.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 3802728  |
---------------------------------
Eval num_timesteps=3804720, episode_reward=1408.57 +/- 548.34
Episode length: 729.80 +/- 95.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3804720  |
---------------------------------
Eval num_timesteps=3806712, episode_reward=1226.34 +/- 449.88
Episode length: 787.80 +/- 324.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3806712  |
---------------------------------
Eval num_timesteps=3808704, episode_reward=1148.85 +/- 666.25
Episode length: 704.60 +/- 213.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3808704  |
---------------------------------
Eval num_timesteps=3810696, episode_reward=1038.29 +/- 406.37
Episode length: 696.80 +/- 111.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3810696  |
---------------------------------
Eval num_timesteps=3812688, episode_reward=972.14 +/- 439.08
Episode length: 672.40 +/- 140.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 3812688  |
---------------------------------
Eval num_timesteps=3814680, episode_reward=1513.22 +/- 640.38
Episode length: 769.20 +/- 137.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3814680  |
---------------------------------
Eval num_timesteps=3816672, episode_reward=1162.53 +/- 603.01
Episode length: 716.80 +/- 78.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3816672  |
---------------------------------
Eval num_timesteps=3818664, episode_reward=1558.11 +/- 599.72
Episode length: 732.40 +/- 119.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3818664  |
---------------------------------
Eval num_timesteps=3820656, episode_reward=1626.05 +/- 969.43
Episode length: 820.80 +/- 277.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3820656  |
---------------------------------
Eval num_timesteps=3822648, episode_reward=1100.05 +/- 542.07
Episode length: 759.00 +/- 227.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3822648  |
---------------------------------
Eval num_timesteps=3824640, episode_reward=1069.84 +/- 340.23
Episode length: 758.20 +/- 122.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3824640  |
---------------------------------
Eval num_timesteps=3826632, episode_reward=1848.90 +/- 273.96
Episode length: 772.00 +/- 83.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3826632  |
---------------------------------
Eval num_timesteps=3828624, episode_reward=820.46 +/- 436.37
Episode length: 722.60 +/- 289.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 3828624  |
---------------------------------
Eval num_timesteps=3830616, episode_reward=1385.97 +/- 824.33
Episode length: 751.00 +/- 175.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3830616  |
---------------------------------
Eval num_timesteps=3832608, episode_reward=1845.33 +/- 722.45
Episode length: 814.00 +/- 231.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3832608  |
---------------------------------
Eval num_timesteps=3834600, episode_reward=1392.29 +/- 535.81
Episode length: 800.60 +/- 87.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 801          |
|    mean_reward          | 1.39e+03     |
| time/                   |              |
|    total_timesteps      | 3834600      |
| train/                  |              |
|    approx_kl            | 0.0040849405 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | -0.034       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000996    |
|    std                  | 1.41         |
|    value_loss           | 0.0793       |
------------------------------------------
Eval num_timesteps=3836592, episode_reward=1380.61 +/- 430.78
Episode length: 757.60 +/- 137.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3836592  |
---------------------------------
Eval num_timesteps=3838584, episode_reward=1507.39 +/- 867.15
Episode length: 829.00 +/- 275.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3838584  |
---------------------------------
Eval num_timesteps=3840576, episode_reward=1128.31 +/- 347.93
Episode length: 655.00 +/- 153.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3840576  |
---------------------------------
Eval num_timesteps=3842568, episode_reward=1635.47 +/- 120.91
Episode length: 705.20 +/- 118.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3842568  |
---------------------------------
Eval num_timesteps=3844560, episode_reward=1448.74 +/- 556.07
Episode length: 828.20 +/- 56.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3844560  |
---------------------------------
Eval num_timesteps=3846552, episode_reward=1189.97 +/- 659.30
Episode length: 679.00 +/- 252.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3846552  |
---------------------------------
Eval num_timesteps=3848544, episode_reward=1795.43 +/- 679.39
Episode length: 888.60 +/- 142.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3848544  |
---------------------------------
Eval num_timesteps=3850536, episode_reward=1057.18 +/- 284.51
Episode length: 665.20 +/- 131.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3850536  |
---------------------------------
Eval num_timesteps=3852528, episode_reward=1591.53 +/- 1228.18
Episode length: 846.00 +/- 260.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3852528  |
---------------------------------
Eval num_timesteps=3854520, episode_reward=1541.45 +/- 897.17
Episode length: 833.20 +/- 264.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3854520  |
---------------------------------
Eval num_timesteps=3856512, episode_reward=1172.68 +/- 681.09
Episode length: 661.40 +/- 170.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3856512  |
---------------------------------
Eval num_timesteps=3858504, episode_reward=1347.44 +/- 663.87
Episode length: 704.00 +/- 78.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3858504  |
---------------------------------
Eval num_timesteps=3860496, episode_reward=1119.92 +/- 565.45
Episode length: 759.20 +/- 205.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3860496  |
---------------------------------
Eval num_timesteps=3862488, episode_reward=1471.99 +/- 280.72
Episode length: 723.80 +/- 98.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3862488  |
---------------------------------
Eval num_timesteps=3864480, episode_reward=1253.41 +/- 247.31
Episode length: 673.60 +/- 104.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3864480  |
---------------------------------
Eval num_timesteps=3866472, episode_reward=1087.68 +/- 428.24
Episode length: 897.20 +/- 272.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3866472  |
---------------------------------
Eval num_timesteps=3868464, episode_reward=1344.54 +/- 522.50
Episode length: 771.20 +/- 112.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3868464  |
---------------------------------
Eval num_timesteps=3870456, episode_reward=908.89 +/- 520.22
Episode length: 697.80 +/- 138.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 3870456  |
---------------------------------
Eval num_timesteps=3872448, episode_reward=1631.62 +/- 1162.35
Episode length: 684.40 +/- 163.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3872448  |
---------------------------------
Eval num_timesteps=3874440, episode_reward=1567.21 +/- 524.09
Episode length: 793.80 +/- 96.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3874440  |
---------------------------------
Eval num_timesteps=3876432, episode_reward=1462.77 +/- 532.57
Episode length: 702.80 +/- 84.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3876432  |
---------------------------------
Eval num_timesteps=3878424, episode_reward=1924.88 +/- 317.41
Episode length: 764.60 +/- 154.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3878424  |
---------------------------------
Eval num_timesteps=3880416, episode_reward=1107.94 +/- 492.86
Episode length: 733.00 +/- 173.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3880416  |
---------------------------------
Eval num_timesteps=3882408, episode_reward=1378.47 +/- 556.76
Episode length: 824.40 +/- 82.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3882408  |
---------------------------------
Eval num_timesteps=3884400, episode_reward=1253.64 +/- 657.39
Episode length: 690.20 +/- 212.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 690          |
|    mean_reward          | 1.25e+03     |
| time/                   |              |
|    total_timesteps      | 3884400      |
| train/                  |              |
|    approx_kl            | 0.0052470583 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.06        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | -0.033       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.42         |
|    value_loss           | 0.0845       |
------------------------------------------
Eval num_timesteps=3886392, episode_reward=1255.55 +/- 675.12
Episode length: 707.40 +/- 127.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3886392  |
---------------------------------
Eval num_timesteps=3888384, episode_reward=1208.20 +/- 673.30
Episode length: 580.20 +/- 112.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3888384  |
---------------------------------
Eval num_timesteps=3890376, episode_reward=1534.49 +/- 343.50
Episode length: 773.00 +/- 135.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3890376  |
---------------------------------
Eval num_timesteps=3892368, episode_reward=1449.03 +/- 494.90
Episode length: 720.20 +/- 163.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3892368  |
---------------------------------
Eval num_timesteps=3894360, episode_reward=1048.34 +/- 304.30
Episode length: 729.80 +/- 154.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3894360  |
---------------------------------
Eval num_timesteps=3896352, episode_reward=1093.79 +/- 360.91
Episode length: 751.20 +/- 122.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3896352  |
---------------------------------
Eval num_timesteps=3898344, episode_reward=1149.40 +/- 320.57
Episode length: 704.40 +/- 107.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3898344  |
---------------------------------
Eval num_timesteps=3900336, episode_reward=1600.39 +/- 582.04
Episode length: 708.60 +/- 120.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3900336  |
---------------------------------
Eval num_timesteps=3902328, episode_reward=1084.72 +/- 365.39
Episode length: 758.80 +/- 176.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3902328  |
---------------------------------
Eval num_timesteps=3904320, episode_reward=1077.88 +/- 227.18
Episode length: 643.60 +/- 72.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3904320  |
---------------------------------
Eval num_timesteps=3906312, episode_reward=1365.19 +/- 325.80
Episode length: 654.60 +/- 100.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3906312  |
---------------------------------
Eval num_timesteps=3908304, episode_reward=1290.15 +/- 355.29
Episode length: 792.00 +/- 263.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3908304  |
---------------------------------
Eval num_timesteps=3910296, episode_reward=1192.22 +/- 499.18
Episode length: 721.80 +/- 38.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3910296  |
---------------------------------
Eval num_timesteps=3912288, episode_reward=1033.60 +/- 341.64
Episode length: 676.60 +/- 76.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3912288  |
---------------------------------
Eval num_timesteps=3914280, episode_reward=957.15 +/- 542.85
Episode length: 538.00 +/- 147.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 3914280  |
---------------------------------
Eval num_timesteps=3916272, episode_reward=903.81 +/- 107.20
Episode length: 656.20 +/- 156.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 3916272  |
---------------------------------
Eval num_timesteps=3918264, episode_reward=1543.61 +/- 626.40
Episode length: 764.20 +/- 125.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3918264  |
---------------------------------
Eval num_timesteps=3920256, episode_reward=1164.43 +/- 392.90
Episode length: 687.20 +/- 107.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3920256  |
---------------------------------
Eval num_timesteps=3922248, episode_reward=1148.25 +/- 407.81
Episode length: 688.20 +/- 123.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3922248  |
---------------------------------
Eval num_timesteps=3924240, episode_reward=1117.54 +/- 384.26
Episode length: 680.60 +/- 93.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3924240  |
---------------------------------
Eval num_timesteps=3926232, episode_reward=813.78 +/- 121.97
Episode length: 696.20 +/- 92.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 3926232  |
---------------------------------
Eval num_timesteps=3928224, episode_reward=1242.30 +/- 443.85
Episode length: 712.00 +/- 123.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3928224  |
---------------------------------
Eval num_timesteps=3930216, episode_reward=912.85 +/- 378.83
Episode length: 643.00 +/- 52.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 3930216  |
---------------------------------
Eval num_timesteps=3932208, episode_reward=892.37 +/- 198.16
Episode length: 638.80 +/- 49.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 639         |
|    mean_reward          | 892         |
| time/                   |             |
|    total_timesteps      | 3932208     |
| train/                  |             |
|    approx_kl            | 0.005949981 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.07       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0325     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.000138   |
|    std                  | 1.42        |
|    value_loss           | 0.0833      |
-----------------------------------------
Eval num_timesteps=3934200, episode_reward=1550.74 +/- 549.72
Episode length: 722.80 +/- 222.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3934200  |
---------------------------------
Eval num_timesteps=3936192, episode_reward=970.10 +/- 266.62
Episode length: 578.40 +/- 56.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 3936192  |
---------------------------------
Eval num_timesteps=3938184, episode_reward=1333.75 +/- 368.15
Episode length: 637.60 +/- 67.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3938184  |
---------------------------------
Eval num_timesteps=3940176, episode_reward=1161.15 +/- 555.54
Episode length: 763.60 +/- 134.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3940176  |
---------------------------------
Eval num_timesteps=3942168, episode_reward=844.87 +/- 192.37
Episode length: 711.40 +/- 134.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 3942168  |
---------------------------------
Eval num_timesteps=3944160, episode_reward=757.02 +/- 185.99
Episode length: 633.80 +/- 45.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 3944160  |
---------------------------------
Eval num_timesteps=3946152, episode_reward=794.54 +/- 66.94
Episode length: 642.20 +/- 31.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 3946152  |
---------------------------------
Eval num_timesteps=3948144, episode_reward=1060.28 +/- 407.14
Episode length: 621.20 +/- 105.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3948144  |
---------------------------------
Eval num_timesteps=3950136, episode_reward=1123.71 +/- 508.44
Episode length: 653.40 +/- 105.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3950136  |
---------------------------------
Eval num_timesteps=3952128, episode_reward=631.69 +/- 436.28
Episode length: 581.00 +/- 117.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 3952128  |
---------------------------------
Eval num_timesteps=3954120, episode_reward=1657.95 +/- 806.35
Episode length: 863.80 +/- 172.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3954120  |
---------------------------------
Eval num_timesteps=3956112, episode_reward=868.56 +/- 109.21
Episode length: 702.80 +/- 66.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 3956112  |
---------------------------------
Eval num_timesteps=3958104, episode_reward=907.57 +/- 308.07
Episode length: 631.00 +/- 29.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 3958104  |
---------------------------------
Eval num_timesteps=3960096, episode_reward=1189.58 +/- 466.21
Episode length: 685.80 +/- 97.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3960096  |
---------------------------------
Eval num_timesteps=3962088, episode_reward=737.90 +/- 64.21
Episode length: 610.80 +/- 72.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 3962088  |
---------------------------------
Eval num_timesteps=3964080, episode_reward=993.29 +/- 448.11
Episode length: 618.20 +/- 24.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 3964080  |
---------------------------------
Eval num_timesteps=3966072, episode_reward=1233.84 +/- 413.87
Episode length: 654.00 +/- 128.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3966072  |
---------------------------------
Eval num_timesteps=3968064, episode_reward=853.46 +/- 186.44
Episode length: 694.80 +/- 107.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 3968064  |
---------------------------------
Eval num_timesteps=3970056, episode_reward=1125.47 +/- 364.56
Episode length: 636.80 +/- 69.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3970056  |
---------------------------------
Eval num_timesteps=3972048, episode_reward=1078.09 +/- 258.95
Episode length: 665.40 +/- 180.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3972048  |
---------------------------------
Eval num_timesteps=3974040, episode_reward=1044.75 +/- 281.92
Episode length: 595.80 +/- 114.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3974040  |
---------------------------------
Eval num_timesteps=3976032, episode_reward=1334.03 +/- 722.02
Episode length: 766.20 +/- 219.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3976032  |
---------------------------------
Eval num_timesteps=3978024, episode_reward=901.17 +/- 191.93
Episode length: 609.80 +/- 87.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 3978024  |
---------------------------------
Eval num_timesteps=3980016, episode_reward=996.78 +/- 453.93
Episode length: 664.60 +/- 53.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 3980016  |
---------------------------------
Eval num_timesteps=3982008, episode_reward=969.94 +/- 835.85
Episode length: 590.80 +/- 248.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 591          |
|    mean_reward          | 970          |
| time/                   |              |
|    total_timesteps      | 3982008      |
| train/                  |              |
|    approx_kl            | 0.0057347706 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.08        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0333      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.43         |
|    value_loss           | 0.0818       |
------------------------------------------
Eval num_timesteps=3984000, episode_reward=1546.75 +/- 447.22
Episode length: 731.60 +/- 171.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3984000  |
---------------------------------
Eval num_timesteps=3985992, episode_reward=1045.83 +/- 444.83
Episode length: 637.40 +/- 180.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3985992  |
---------------------------------
Eval num_timesteps=3987984, episode_reward=1178.13 +/- 671.00
Episode length: 710.40 +/- 201.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3987984  |
---------------------------------
Eval num_timesteps=3989976, episode_reward=1552.84 +/- 590.42
Episode length: 796.80 +/- 230.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3989976  |
---------------------------------
Eval num_timesteps=3991968, episode_reward=1196.54 +/- 450.88
Episode length: 639.60 +/- 49.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3991968  |
---------------------------------
Eval num_timesteps=3993960, episode_reward=1213.35 +/- 277.05
Episode length: 678.20 +/- 130.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3993960  |
---------------------------------
Eval num_timesteps=3995952, episode_reward=1136.84 +/- 568.51
Episode length: 720.40 +/- 193.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3995952  |
---------------------------------
Eval num_timesteps=3997944, episode_reward=1046.41 +/- 336.92
Episode length: 714.20 +/- 73.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3997944  |
---------------------------------
Eval num_timesteps=3999936, episode_reward=1049.44 +/- 417.58
Episode length: 607.80 +/- 88.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3999936  |
---------------------------------
Eval num_timesteps=4001928, episode_reward=1615.88 +/- 297.48
Episode length: 687.00 +/- 109.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4001928  |
---------------------------------
Eval num_timesteps=4003920, episode_reward=2221.64 +/- 1184.33
Episode length: 874.00 +/- 169.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 4003920  |
---------------------------------
New best mean reward!
Eval num_timesteps=4005912, episode_reward=1085.94 +/- 377.41
Episode length: 738.20 +/- 126.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4005912  |
---------------------------------
Eval num_timesteps=4007904, episode_reward=1629.12 +/- 545.69
Episode length: 845.60 +/- 219.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 4007904  |
---------------------------------
Eval num_timesteps=4009896, episode_reward=1032.78 +/- 354.69
Episode length: 657.00 +/- 51.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4009896  |
---------------------------------
Eval num_timesteps=4011888, episode_reward=1279.33 +/- 394.32
Episode length: 773.80 +/- 136.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4011888  |
---------------------------------
Eval num_timesteps=4013880, episode_reward=1409.88 +/- 482.13
Episode length: 732.60 +/- 134.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4013880  |
---------------------------------
Eval num_timesteps=4015872, episode_reward=937.36 +/- 523.12
Episode length: 689.40 +/- 158.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 4015872  |
---------------------------------
Eval num_timesteps=4017864, episode_reward=1347.22 +/- 586.72
Episode length: 712.40 +/- 171.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4017864  |
---------------------------------
Eval num_timesteps=4019856, episode_reward=1819.60 +/- 545.06
Episode length: 754.60 +/- 185.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4019856  |
---------------------------------
Eval num_timesteps=4021848, episode_reward=1093.34 +/- 600.96
Episode length: 654.20 +/- 105.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4021848  |
---------------------------------
Eval num_timesteps=4023840, episode_reward=1565.65 +/- 375.19
Episode length: 723.80 +/- 79.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4023840  |
---------------------------------
Eval num_timesteps=4025832, episode_reward=1470.51 +/- 543.25
Episode length: 763.60 +/- 99.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4025832  |
---------------------------------
Eval num_timesteps=4027824, episode_reward=962.25 +/- 302.00
Episode length: 635.40 +/- 86.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 4027824  |
---------------------------------
Eval num_timesteps=4029816, episode_reward=1507.49 +/- 539.99
Episode length: 770.80 +/- 156.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4029816  |
---------------------------------
Eval num_timesteps=4031808, episode_reward=1297.74 +/- 533.66
Episode length: 589.60 +/- 119.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 590          |
|    mean_reward          | 1.3e+03      |
| time/                   |              |
|    total_timesteps      | 4031808      |
| train/                  |              |
|    approx_kl            | 0.0047764713 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.1         |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0316      |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.0012      |
|    std                  | 1.43         |
|    value_loss           | 0.0865       |
------------------------------------------
Eval num_timesteps=4033800, episode_reward=1492.15 +/- 400.69
Episode length: 722.20 +/- 135.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 4033800  |
---------------------------------
Eval num_timesteps=4035792, episode_reward=1701.98 +/- 125.22
Episode length: 644.40 +/- 118.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 4035792  |
---------------------------------
Eval num_timesteps=4037784, episode_reward=1306.62 +/- 581.65
Episode length: 737.00 +/- 124.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4037784  |
---------------------------------
Eval num_timesteps=4039776, episode_reward=1316.39 +/- 362.39
Episode length: 686.00 +/- 68.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4039776  |
---------------------------------
Eval num_timesteps=4041768, episode_reward=1207.28 +/- 502.37
Episode length: 812.20 +/- 148.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4041768  |
---------------------------------
Eval num_timesteps=4043760, episode_reward=1176.23 +/- 217.51
Episode length: 713.20 +/- 67.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4043760  |
---------------------------------
Eval num_timesteps=4045752, episode_reward=1206.87 +/- 399.06
Episode length: 616.40 +/- 73.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4045752  |
---------------------------------
Eval num_timesteps=4047744, episode_reward=1118.69 +/- 288.61
Episode length: 661.00 +/- 100.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4047744  |
---------------------------------
Eval num_timesteps=4049736, episode_reward=1526.07 +/- 367.90
Episode length: 656.80 +/- 89.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4049736  |
---------------------------------
Eval num_timesteps=4051728, episode_reward=1238.88 +/- 430.38
Episode length: 691.20 +/- 89.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4051728  |
---------------------------------
Eval num_timesteps=4053720, episode_reward=1211.49 +/- 301.38
Episode length: 690.20 +/- 131.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4053720  |
---------------------------------
Eval num_timesteps=4055712, episode_reward=1277.76 +/- 678.89
Episode length: 771.00 +/- 163.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4055712  |
---------------------------------
Eval num_timesteps=4057704, episode_reward=1551.58 +/- 502.66
Episode length: 713.20 +/- 125.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 4057704  |
---------------------------------
Eval num_timesteps=4059696, episode_reward=1419.62 +/- 405.94
Episode length: 794.40 +/- 101.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4059696  |
---------------------------------
Eval num_timesteps=4061688, episode_reward=1112.35 +/- 187.28
Episode length: 635.00 +/- 68.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4061688  |
---------------------------------
Eval num_timesteps=4063680, episode_reward=843.07 +/- 468.72
Episode length: 655.60 +/- 58.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 4063680  |
---------------------------------
Eval num_timesteps=4065672, episode_reward=1539.04 +/- 773.17
Episode length: 883.40 +/- 193.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 4065672  |
---------------------------------
Eval num_timesteps=4067664, episode_reward=1073.90 +/- 318.25
Episode length: 746.20 +/- 122.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4067664  |
---------------------------------
Eval num_timesteps=4069656, episode_reward=1288.35 +/- 156.42
Episode length: 689.80 +/- 92.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4069656  |
---------------------------------
Eval num_timesteps=4071648, episode_reward=1239.23 +/- 286.42
Episode length: 694.20 +/- 125.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4071648  |
---------------------------------
Eval num_timesteps=4073640, episode_reward=1182.44 +/- 349.04
Episode length: 679.00 +/- 97.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4073640  |
---------------------------------
Eval num_timesteps=4075632, episode_reward=1309.76 +/- 419.79
Episode length: 795.40 +/- 295.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4075632  |
---------------------------------
Eval num_timesteps=4077624, episode_reward=1583.23 +/- 269.08
Episode length: 639.80 +/- 99.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4077624  |
---------------------------------
Eval num_timesteps=4079616, episode_reward=1260.23 +/- 349.33
Episode length: 645.60 +/- 45.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4079616  |
---------------------------------
Eval num_timesteps=4081608, episode_reward=1019.89 +/- 644.29
Episode length: 681.00 +/- 198.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 681          |
|    mean_reward          | 1.02e+03     |
| time/                   |              |
|    total_timesteps      | 4081608      |
| train/                  |              |
|    approx_kl            | 0.0035199756 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | -0.033       |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.45         |
|    value_loss           | 0.0842       |
------------------------------------------
Eval num_timesteps=4083600, episode_reward=1541.84 +/- 195.62
Episode length: 665.40 +/- 82.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 4083600  |
---------------------------------
Eval num_timesteps=4085592, episode_reward=1356.19 +/- 430.49
Episode length: 657.60 +/- 98.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4085592  |
---------------------------------
Eval num_timesteps=4087584, episode_reward=1254.13 +/- 361.90
Episode length: 752.40 +/- 140.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4087584  |
---------------------------------
Eval num_timesteps=4089576, episode_reward=1340.33 +/- 431.95
Episode length: 677.60 +/- 42.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4089576  |
---------------------------------
Eval num_timesteps=4091568, episode_reward=1556.27 +/- 792.65
Episode length: 828.40 +/- 158.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4091568  |
---------------------------------
Eval num_timesteps=4093560, episode_reward=1207.36 +/- 331.57
Episode length: 679.60 +/- 124.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4093560  |
---------------------------------
Eval num_timesteps=4095552, episode_reward=1094.75 +/- 488.44
Episode length: 631.00 +/- 78.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4095552  |
---------------------------------
Eval num_timesteps=4097544, episode_reward=1565.98 +/- 513.44
Episode length: 725.20 +/- 178.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4097544  |
---------------------------------
Eval num_timesteps=4099536, episode_reward=1333.78 +/- 336.91
Episode length: 764.80 +/- 169.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4099536  |
---------------------------------
Eval num_timesteps=4101528, episode_reward=1301.33 +/- 348.20
Episode length: 743.40 +/- 160.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4101528  |
---------------------------------
Eval num_timesteps=4103520, episode_reward=1050.81 +/- 287.60
Episode length: 650.80 +/- 119.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4103520  |
---------------------------------
Eval num_timesteps=4105512, episode_reward=787.81 +/- 435.62
Episode length: 652.80 +/- 154.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 4105512  |
---------------------------------
Eval num_timesteps=4107504, episode_reward=1415.76 +/- 497.63
Episode length: 758.20 +/- 171.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4107504  |
---------------------------------
Eval num_timesteps=4109496, episode_reward=1305.36 +/- 184.75
Episode length: 799.80 +/- 170.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4109496  |
---------------------------------
Eval num_timesteps=4111488, episode_reward=1162.66 +/- 444.04
Episode length: 707.20 +/- 98.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4111488  |
---------------------------------
Eval num_timesteps=4113480, episode_reward=1174.58 +/- 300.23
Episode length: 698.00 +/- 168.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4113480  |
---------------------------------
Eval num_timesteps=4115472, episode_reward=1209.31 +/- 442.45
Episode length: 728.80 +/- 141.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4115472  |
---------------------------------
Eval num_timesteps=4117464, episode_reward=1472.41 +/- 322.42
Episode length: 648.00 +/- 129.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4117464  |
---------------------------------
Eval num_timesteps=4119456, episode_reward=1372.98 +/- 291.56
Episode length: 833.80 +/- 80.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4119456  |
---------------------------------
Eval num_timesteps=4121448, episode_reward=1016.58 +/- 243.95
Episode length: 748.20 +/- 170.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4121448  |
---------------------------------
Eval num_timesteps=4123440, episode_reward=1193.21 +/- 349.04
Episode length: 671.00 +/- 115.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4123440  |
---------------------------------
Eval num_timesteps=4125432, episode_reward=1278.04 +/- 184.20
Episode length: 734.20 +/- 156.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4125432  |
---------------------------------
Eval num_timesteps=4127424, episode_reward=1453.49 +/- 571.38
Episode length: 786.20 +/- 104.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4127424  |
---------------------------------
Eval num_timesteps=4129416, episode_reward=1327.46 +/- 361.42
Episode length: 732.40 +/- 112.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 732         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 4129416     |
| train/                  |             |
|    approx_kl            | 0.004908232 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | -0.032      |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.000849   |
|    std                  | 1.46        |
|    value_loss           | 0.0846      |
-----------------------------------------
Eval num_timesteps=4131408, episode_reward=1116.27 +/- 264.80
Episode length: 703.20 +/- 27.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4131408  |
---------------------------------
Eval num_timesteps=4133400, episode_reward=1360.39 +/- 452.44
Episode length: 741.40 +/- 175.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4133400  |
---------------------------------
Eval num_timesteps=4135392, episode_reward=1929.43 +/- 768.28
Episode length: 856.00 +/- 43.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 4135392  |
---------------------------------
Eval num_timesteps=4137384, episode_reward=1238.30 +/- 582.36
Episode length: 625.60 +/- 171.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4137384  |
---------------------------------
Eval num_timesteps=4139376, episode_reward=1136.06 +/- 557.57
Episode length: 648.80 +/- 159.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4139376  |
---------------------------------
Eval num_timesteps=4141368, episode_reward=1636.51 +/- 291.57
Episode length: 761.80 +/- 187.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 4141368  |
---------------------------------
Eval num_timesteps=4143360, episode_reward=1064.09 +/- 565.30
Episode length: 706.80 +/- 79.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 4143360  |
---------------------------------
Eval num_timesteps=4145352, episode_reward=1680.80 +/- 332.70
Episode length: 667.60 +/- 59.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 4145352  |
---------------------------------
Eval num_timesteps=4147344, episode_reward=1182.19 +/- 288.01
Episode length: 707.80 +/- 98.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4147344  |
---------------------------------
Eval num_timesteps=4149336, episode_reward=1120.70 +/- 278.16
Episode length: 671.60 +/- 147.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4149336  |
---------------------------------
Eval num_timesteps=4151328, episode_reward=974.74 +/- 587.84
Episode length: 609.60 +/- 107.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 4151328  |
---------------------------------
Eval num_timesteps=4153320, episode_reward=1117.56 +/- 249.68
Episode length: 786.80 +/- 113.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4153320  |
---------------------------------
Eval num_timesteps=4155312, episode_reward=1508.60 +/- 577.85
Episode length: 777.20 +/- 167.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4155312  |
---------------------------------
Eval num_timesteps=4157304, episode_reward=1256.43 +/- 400.22
Episode length: 699.20 +/- 140.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4157304  |
---------------------------------
Eval num_timesteps=4159296, episode_reward=1265.29 +/- 307.24
Episode length: 762.40 +/- 143.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4159296  |
---------------------------------
Eval num_timesteps=4161288, episode_reward=1268.70 +/- 513.23
Episode length: 741.00 +/- 106.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4161288  |
---------------------------------
Eval num_timesteps=4163280, episode_reward=1156.01 +/- 455.28
Episode length: 630.20 +/- 67.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4163280  |
---------------------------------
Eval num_timesteps=4165272, episode_reward=1268.14 +/- 572.42
Episode length: 857.60 +/- 108.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4165272  |
---------------------------------
Eval num_timesteps=4167264, episode_reward=1301.30 +/- 267.98
Episode length: 645.80 +/- 61.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4167264  |
---------------------------------
Eval num_timesteps=4169256, episode_reward=1140.13 +/- 298.94
Episode length: 752.20 +/- 118.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4169256  |
---------------------------------
Eval num_timesteps=4171248, episode_reward=1803.57 +/- 669.55
Episode length: 828.00 +/- 191.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 4171248  |
---------------------------------
Eval num_timesteps=4173240, episode_reward=1104.11 +/- 439.46
Episode length: 750.80 +/- 141.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4173240  |
---------------------------------
Eval num_timesteps=4175232, episode_reward=1387.03 +/- 379.54
Episode length: 703.20 +/- 115.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4175232  |
---------------------------------
Eval num_timesteps=4177224, episode_reward=1236.67 +/- 311.83
Episode length: 674.60 +/- 82.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4177224  |
---------------------------------
Eval num_timesteps=4179216, episode_reward=1167.15 +/- 413.88
Episode length: 731.40 +/- 143.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 731          |
|    mean_reward          | 1.17e+03     |
| time/                   |              |
|    total_timesteps      | 4179216      |
| train/                  |              |
|    approx_kl            | 0.0064491904 |
|    clip_fraction        | 0.0447       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.18        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0352      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.46         |
|    value_loss           | 0.0812       |
------------------------------------------
Eval num_timesteps=4181208, episode_reward=1376.68 +/- 499.03
Episode length: 803.60 +/- 130.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4181208  |
---------------------------------
Eval num_timesteps=4183200, episode_reward=975.26 +/- 823.60
Episode length: 663.00 +/- 143.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 4183200  |
---------------------------------
Eval num_timesteps=4185192, episode_reward=1176.64 +/- 440.40
Episode length: 732.40 +/- 65.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4185192  |
---------------------------------
Eval num_timesteps=4187184, episode_reward=1074.48 +/- 374.44
Episode length: 727.80 +/- 90.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4187184  |
---------------------------------
Eval num_timesteps=4189176, episode_reward=1102.06 +/- 369.25
Episode length: 715.00 +/- 79.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4189176  |
---------------------------------
Eval num_timesteps=4191168, episode_reward=1128.28 +/- 345.48
Episode length: 629.00 +/- 102.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4191168  |
---------------------------------
Eval num_timesteps=4193160, episode_reward=1799.84 +/- 394.91
Episode length: 685.40 +/- 93.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 4193160  |
---------------------------------
Eval num_timesteps=4195152, episode_reward=1104.17 +/- 225.87
Episode length: 769.00 +/- 104.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4195152  |
---------------------------------
Eval num_timesteps=4197144, episode_reward=1474.47 +/- 833.51
Episode length: 819.80 +/- 183.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4197144  |
---------------------------------
Eval num_timesteps=4199136, episode_reward=1727.24 +/- 660.27
Episode length: 705.00 +/- 172.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4199136  |
---------------------------------
Eval num_timesteps=4201128, episode_reward=1173.31 +/- 280.80
Episode length: 692.80 +/- 160.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4201128  |
---------------------------------
Eval num_timesteps=4203120, episode_reward=1297.26 +/- 465.00
Episode length: 668.20 +/- 80.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4203120  |
---------------------------------
Eval num_timesteps=4205112, episode_reward=1282.83 +/- 429.01
Episode length: 746.80 +/- 174.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4205112  |
---------------------------------
Eval num_timesteps=4207104, episode_reward=1135.29 +/- 297.54
Episode length: 621.60 +/- 46.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4207104  |
---------------------------------
Eval num_timesteps=4209096, episode_reward=1350.87 +/- 361.10
Episode length: 826.40 +/- 101.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4209096  |
---------------------------------
Eval num_timesteps=4211088, episode_reward=1178.81 +/- 345.68
Episode length: 766.20 +/- 156.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4211088  |
---------------------------------
Eval num_timesteps=4213080, episode_reward=1161.30 +/- 351.14
Episode length: 730.60 +/- 95.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4213080  |
---------------------------------
Eval num_timesteps=4215072, episode_reward=1194.47 +/- 400.37
Episode length: 804.40 +/- 91.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4215072  |
---------------------------------
Eval num_timesteps=4217064, episode_reward=1631.64 +/- 721.93
Episode length: 764.60 +/- 164.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 4217064  |
---------------------------------
Eval num_timesteps=4219056, episode_reward=873.10 +/- 330.57
Episode length: 674.60 +/- 93.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 4219056  |
---------------------------------
Eval num_timesteps=4221048, episode_reward=1107.94 +/- 382.79
Episode length: 658.40 +/- 43.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4221048  |
---------------------------------
Eval num_timesteps=4223040, episode_reward=1155.50 +/- 443.89
Episode length: 630.20 +/- 62.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4223040  |
---------------------------------
Eval num_timesteps=4225032, episode_reward=1374.63 +/- 503.04
Episode length: 672.60 +/- 75.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4225032  |
---------------------------------
Eval num_timesteps=4227024, episode_reward=1204.64 +/- 303.91
Episode length: 664.20 +/- 83.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 4227024  |
---------------------------------
Eval num_timesteps=4229016, episode_reward=1068.03 +/- 258.45
Episode length: 674.80 +/- 74.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 1.07e+03     |
| time/                   |              |
|    total_timesteps      | 4229016      |
| train/                  |              |
|    approx_kl            | 0.0036173926 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0423      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.47         |
|    value_loss           | 0.0684       |
------------------------------------------
Eval num_timesteps=4231008, episode_reward=946.89 +/- 240.59
Episode length: 665.00 +/- 39.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 4231008  |
---------------------------------
Eval num_timesteps=4233000, episode_reward=1433.05 +/- 394.80
Episode length: 666.40 +/- 68.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4233000  |
---------------------------------
Eval num_timesteps=4234992, episode_reward=1678.31 +/- 560.28
Episode length: 747.60 +/- 128.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 4234992  |
---------------------------------
Eval num_timesteps=4236984, episode_reward=1403.64 +/- 559.16
Episode length: 725.40 +/- 78.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4236984  |
---------------------------------
Eval num_timesteps=4238976, episode_reward=1396.81 +/- 305.72
Episode length: 708.60 +/- 104.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4238976  |
---------------------------------
Eval num_timesteps=4240968, episode_reward=973.91 +/- 533.62
Episode length: 810.20 +/- 87.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 4240968  |
---------------------------------
Eval num_timesteps=4242960, episode_reward=1071.21 +/- 358.71
Episode length: 748.40 +/- 127.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4242960  |
---------------------------------
Eval num_timesteps=4244952, episode_reward=1381.90 +/- 533.84
Episode length: 748.80 +/- 170.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4244952  |
---------------------------------
Eval num_timesteps=4246944, episode_reward=1456.22 +/- 340.60
Episode length: 792.20 +/- 180.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4246944  |
---------------------------------
Eval num_timesteps=4248936, episode_reward=1465.60 +/- 599.78
Episode length: 725.20 +/- 103.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4248936  |
---------------------------------
Eval num_timesteps=4250928, episode_reward=958.26 +/- 155.55
Episode length: 607.20 +/- 83.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 4250928  |
---------------------------------
Eval num_timesteps=4252920, episode_reward=977.56 +/- 301.81
Episode length: 679.20 +/- 27.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 4252920  |
---------------------------------
Eval num_timesteps=4254912, episode_reward=933.52 +/- 499.54
Episode length: 672.20 +/- 186.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 4254912  |
---------------------------------
Eval num_timesteps=4256904, episode_reward=1719.62 +/- 461.19
Episode length: 663.80 +/- 74.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4256904  |
---------------------------------
Eval num_timesteps=4258896, episode_reward=997.05 +/- 372.26
Episode length: 750.00 +/- 90.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 4258896  |
---------------------------------
Eval num_timesteps=4260888, episode_reward=1360.55 +/- 392.48
Episode length: 711.60 +/- 57.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4260888  |
---------------------------------
Eval num_timesteps=4262880, episode_reward=1073.18 +/- 381.94
Episode length: 676.20 +/- 82.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4262880  |
---------------------------------
Eval num_timesteps=4264872, episode_reward=1400.61 +/- 505.46
Episode length: 775.40 +/- 141.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4264872  |
---------------------------------
Eval num_timesteps=4266864, episode_reward=1406.93 +/- 519.62
Episode length: 853.40 +/- 130.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4266864  |
---------------------------------
Eval num_timesteps=4268856, episode_reward=1017.72 +/- 252.02
Episode length: 791.60 +/- 117.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4268856  |
---------------------------------
Eval num_timesteps=4270848, episode_reward=1373.64 +/- 443.13
Episode length: 850.00 +/- 170.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4270848  |
---------------------------------
Eval num_timesteps=4272840, episode_reward=1271.17 +/- 394.74
Episode length: 751.80 +/- 158.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4272840  |
---------------------------------
Eval num_timesteps=4274832, episode_reward=1223.81 +/- 266.73
Episode length: 758.20 +/- 130.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 4274832  |
---------------------------------
Eval num_timesteps=4276824, episode_reward=1134.12 +/- 267.64
Episode length: 773.20 +/- 252.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 773          |
|    mean_reward          | 1.13e+03     |
| time/                   |              |
|    total_timesteps      | 4276824      |
| train/                  |              |
|    approx_kl            | 0.0058783214 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.23        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0385      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.48         |
|    value_loss           | 0.0759       |
------------------------------------------
Eval num_timesteps=4278816, episode_reward=1194.89 +/- 571.16
Episode length: 693.00 +/- 106.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4278816  |
---------------------------------
Eval num_timesteps=4280808, episode_reward=1548.42 +/- 553.89
Episode length: 749.20 +/- 128.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 4280808  |
---------------------------------
Eval num_timesteps=4282800, episode_reward=1323.18 +/- 456.39
Episode length: 737.80 +/- 121.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4282800  |
---------------------------------
Eval num_timesteps=4284792, episode_reward=1271.62 +/- 353.13
Episode length: 745.00 +/- 81.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4284792  |
---------------------------------
Eval num_timesteps=4286784, episode_reward=1571.68 +/- 331.42
Episode length: 848.60 +/- 153.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4286784  |
---------------------------------
Eval num_timesteps=4288776, episode_reward=1105.63 +/- 462.68
Episode length: 736.20 +/- 144.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4288776  |
---------------------------------
Eval num_timesteps=4290768, episode_reward=1166.59 +/- 439.79
Episode length: 700.40 +/- 76.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4290768  |
---------------------------------
Eval num_timesteps=4292760, episode_reward=1308.20 +/- 289.09
Episode length: 596.20 +/- 35.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4292760  |
---------------------------------
Eval num_timesteps=4294752, episode_reward=1073.26 +/- 731.18
Episode length: 724.80 +/- 144.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4294752  |
---------------------------------
Eval num_timesteps=4296744, episode_reward=1319.42 +/- 443.69
Episode length: 690.00 +/- 179.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4296744  |
---------------------------------
Eval num_timesteps=4298736, episode_reward=1108.97 +/- 420.09
Episode length: 852.40 +/- 117.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4298736  |
---------------------------------
Eval num_timesteps=4300728, episode_reward=1271.53 +/- 553.98
Episode length: 627.80 +/- 67.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4300728  |
---------------------------------
Eval num_timesteps=4302720, episode_reward=1526.61 +/- 382.45
Episode length: 698.00 +/- 66.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4302720  |
---------------------------------
Eval num_timesteps=4304712, episode_reward=1135.08 +/- 246.25
Episode length: 724.20 +/- 118.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4304712  |
---------------------------------
Eval num_timesteps=4306704, episode_reward=1380.84 +/- 280.28
Episode length: 620.20 +/- 63.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4306704  |
---------------------------------
Eval num_timesteps=4308696, episode_reward=835.18 +/- 400.29
Episode length: 636.60 +/- 142.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 4308696  |
---------------------------------
Eval num_timesteps=4310688, episode_reward=1043.87 +/- 392.88
Episode length: 695.80 +/- 186.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 4310688  |
---------------------------------
Eval num_timesteps=4312680, episode_reward=1059.79 +/- 639.25
Episode length: 698.00 +/- 125.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 4312680  |
---------------------------------
Eval num_timesteps=4314672, episode_reward=1339.19 +/- 525.64
Episode length: 681.40 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4314672  |
---------------------------------
Eval num_timesteps=4316664, episode_reward=1600.76 +/- 407.38
Episode length: 674.40 +/- 47.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4316664  |
---------------------------------
Eval num_timesteps=4318656, episode_reward=1187.62 +/- 452.86
Episode length: 719.40 +/- 51.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4318656  |
---------------------------------
Eval num_timesteps=4320648, episode_reward=1272.41 +/- 362.64
Episode length: 744.40 +/- 76.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4320648  |
---------------------------------
Eval num_timesteps=4322640, episode_reward=1333.97 +/- 367.68
Episode length: 648.00 +/- 42.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4322640  |
---------------------------------
Eval num_timesteps=4324632, episode_reward=910.75 +/- 535.51
Episode length: 720.20 +/- 175.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 4324632  |
---------------------------------
Eval num_timesteps=4326624, episode_reward=966.81 +/- 447.19
Episode length: 594.80 +/- 102.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 595          |
|    mean_reward          | 967          |
| time/                   |              |
|    total_timesteps      | 4326624      |
| train/                  |              |
|    approx_kl            | 0.0034750607 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0355      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.49         |
|    value_loss           | 0.0798       |
------------------------------------------
Eval num_timesteps=4328616, episode_reward=1574.44 +/- 402.41
Episode length: 633.60 +/- 65.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4328616  |
---------------------------------
Eval num_timesteps=4330608, episode_reward=1484.27 +/- 353.65
Episode length: 602.80 +/- 71.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4330608  |
---------------------------------
Eval num_timesteps=4332600, episode_reward=1177.97 +/- 373.39
Episode length: 593.20 +/- 60.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4332600  |
---------------------------------
Eval num_timesteps=4334592, episode_reward=1258.29 +/- 667.44
Episode length: 675.80 +/- 198.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4334592  |
---------------------------------
Eval num_timesteps=4336584, episode_reward=1223.47 +/- 450.50
Episode length: 673.80 +/- 81.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 4336584  |
---------------------------------
Eval num_timesteps=4338576, episode_reward=1490.33 +/- 797.96
Episode length: 742.00 +/- 204.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 4338576  |
---------------------------------
Eval num_timesteps=4340568, episode_reward=1421.98 +/- 225.81
Episode length: 677.20 +/- 55.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4340568  |
---------------------------------
Eval num_timesteps=4342560, episode_reward=1087.72 +/- 577.18
Episode length: 656.60 +/- 141.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4342560  |
---------------------------------
Eval num_timesteps=4344552, episode_reward=1461.48 +/- 392.07
Episode length: 680.80 +/- 130.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4344552  |
---------------------------------
Eval num_timesteps=4346544, episode_reward=1581.67 +/- 205.56
Episode length: 671.00 +/- 145.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4346544  |
---------------------------------
Eval num_timesteps=4348536, episode_reward=1168.52 +/- 350.10
Episode length: 614.40 +/- 62.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4348536  |
---------------------------------
Eval num_timesteps=4350528, episode_reward=1418.35 +/- 371.12
Episode length: 713.00 +/- 80.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4350528  |
---------------------------------
Eval num_timesteps=4352520, episode_reward=1369.85 +/- 459.75
Episode length: 640.80 +/- 64.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4352520  |
---------------------------------
Eval num_timesteps=4354512, episode_reward=1031.33 +/- 400.49
Episode length: 684.40 +/- 38.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4354512  |
---------------------------------
Eval num_timesteps=4356504, episode_reward=1575.48 +/- 408.24
Episode length: 726.40 +/- 151.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4356504  |
---------------------------------
Eval num_timesteps=4358496, episode_reward=1028.29 +/- 529.88
Episode length: 674.00 +/- 137.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4358496  |
---------------------------------
Eval num_timesteps=4360488, episode_reward=1106.24 +/- 414.33
Episode length: 729.20 +/- 81.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4360488  |
---------------------------------
Eval num_timesteps=4362480, episode_reward=1514.55 +/- 426.42
Episode length: 718.60 +/- 108.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4362480  |
---------------------------------
Eval num_timesteps=4364472, episode_reward=1439.03 +/- 317.82
Episode length: 667.20 +/- 98.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4364472  |
---------------------------------
Eval num_timesteps=4366464, episode_reward=1473.28 +/- 569.10
Episode length: 685.40 +/- 56.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4366464  |
---------------------------------
Eval num_timesteps=4368456, episode_reward=820.95 +/- 59.48
Episode length: 612.20 +/- 78.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 4368456  |
---------------------------------
Eval num_timesteps=4370448, episode_reward=1156.51 +/- 470.61
Episode length: 581.80 +/- 67.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4370448  |
---------------------------------
Eval num_timesteps=4372440, episode_reward=1180.73 +/- 314.93
Episode length: 736.80 +/- 163.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4372440  |
---------------------------------
Eval num_timesteps=4374432, episode_reward=1186.70 +/- 480.73
Episode length: 653.00 +/- 107.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4374432  |
---------------------------------
Eval num_timesteps=4376424, episode_reward=1493.10 +/- 695.64
Episode length: 835.60 +/- 142.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 836          |
|    mean_reward          | 1.49e+03     |
| time/                   |              |
|    total_timesteps      | 4376424      |
| train/                  |              |
|    approx_kl            | 0.0064361575 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0312      |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.49         |
|    value_loss           | 0.0915       |
------------------------------------------
Eval num_timesteps=4378416, episode_reward=1585.47 +/- 199.16
Episode length: 704.60 +/- 90.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 4378416  |
---------------------------------
Eval num_timesteps=4380408, episode_reward=1307.78 +/- 420.07
Episode length: 736.80 +/- 125.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4380408  |
---------------------------------
Eval num_timesteps=4382400, episode_reward=1166.97 +/- 250.60
Episode length: 727.40 +/- 125.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4382400  |
---------------------------------
Eval num_timesteps=4384392, episode_reward=1143.08 +/- 226.69
Episode length: 653.60 +/- 81.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4384392  |
---------------------------------
Eval num_timesteps=4386384, episode_reward=1051.88 +/- 316.98
Episode length: 703.00 +/- 78.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4386384  |
---------------------------------
Eval num_timesteps=4388376, episode_reward=1602.29 +/- 274.85
Episode length: 665.00 +/- 62.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4388376  |
---------------------------------
Eval num_timesteps=4390368, episode_reward=1295.64 +/- 303.70
Episode length: 669.60 +/- 75.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4390368  |
---------------------------------
Eval num_timesteps=4392360, episode_reward=1739.08 +/- 462.03
Episode length: 706.80 +/- 55.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4392360  |
---------------------------------
Eval num_timesteps=4394352, episode_reward=1214.85 +/- 333.10
Episode length: 629.20 +/- 66.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4394352  |
---------------------------------
Eval num_timesteps=4396344, episode_reward=1328.41 +/- 427.10
Episode length: 740.20 +/- 86.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4396344  |
---------------------------------
Eval num_timesteps=4398336, episode_reward=1384.05 +/- 387.15
Episode length: 694.80 +/- 86.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4398336  |
---------------------------------
Eval num_timesteps=4400328, episode_reward=1611.45 +/- 152.15
Episode length: 700.00 +/- 94.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4400328  |
---------------------------------
Eval num_timesteps=4402320, episode_reward=1635.28 +/- 421.95
Episode length: 745.20 +/- 69.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 4402320  |
---------------------------------
Eval num_timesteps=4404312, episode_reward=1511.05 +/- 472.71
Episode length: 755.20 +/- 144.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4404312  |
---------------------------------
Eval num_timesteps=4406304, episode_reward=1084.64 +/- 542.51
Episode length: 661.00 +/- 70.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4406304  |
---------------------------------
Eval num_timesteps=4408296, episode_reward=1181.51 +/- 460.06
Episode length: 690.40 +/- 90.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4408296  |
---------------------------------
Eval num_timesteps=4410288, episode_reward=1241.52 +/- 440.68
Episode length: 717.00 +/- 183.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4410288  |
---------------------------------
Eval num_timesteps=4412280, episode_reward=1532.64 +/- 453.77
Episode length: 740.60 +/- 135.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4412280  |
---------------------------------
Eval num_timesteps=4414272, episode_reward=1156.41 +/- 540.72
Episode length: 733.40 +/- 58.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4414272  |
---------------------------------
Eval num_timesteps=4416264, episode_reward=944.52 +/- 184.50
Episode length: 592.40 +/- 59.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 4416264  |
---------------------------------
Eval num_timesteps=4418256, episode_reward=1322.14 +/- 481.34
Episode length: 627.20 +/- 71.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4418256  |
---------------------------------
Eval num_timesteps=4420248, episode_reward=1179.00 +/- 276.32
Episode length: 755.60 +/- 144.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4420248  |
---------------------------------
Eval num_timesteps=4422240, episode_reward=1167.78 +/- 363.83
Episode length: 741.80 +/- 143.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4422240  |
---------------------------------
Eval num_timesteps=4424232, episode_reward=1239.71 +/- 467.70
Episode length: 612.20 +/- 57.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 612         |
|    mean_reward          | 1.24e+03    |
| time/                   |             |
|    total_timesteps      | 4424232     |
| train/                  |             |
|    approx_kl            | 0.003987029 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0347     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00103    |
|    std                  | 1.5         |
|    value_loss           | 0.0838      |
-----------------------------------------
Eval num_timesteps=4426224, episode_reward=1077.62 +/- 168.43
Episode length: 747.80 +/- 180.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4426224  |
---------------------------------
Eval num_timesteps=4428216, episode_reward=1404.36 +/- 478.75
Episode length: 730.40 +/- 117.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4428216  |
---------------------------------
Eval num_timesteps=4430208, episode_reward=1397.81 +/- 540.53
Episode length: 709.20 +/- 98.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4430208  |
---------------------------------
Eval num_timesteps=4432200, episode_reward=949.03 +/- 186.75
Episode length: 636.60 +/- 64.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 4432200  |
---------------------------------
Eval num_timesteps=4434192, episode_reward=1175.58 +/- 385.19
Episode length: 566.60 +/- 71.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4434192  |
---------------------------------
Eval num_timesteps=4436184, episode_reward=1200.85 +/- 298.01
Episode length: 748.40 +/- 81.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 4436184  |
---------------------------------
Eval num_timesteps=4438176, episode_reward=1235.34 +/- 385.06
Episode length: 703.60 +/- 112.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4438176  |
---------------------------------
Eval num_timesteps=4440168, episode_reward=1437.30 +/- 382.33
Episode length: 644.00 +/- 117.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4440168  |
---------------------------------
Eval num_timesteps=4442160, episode_reward=1179.81 +/- 624.32
Episode length: 648.20 +/- 72.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4442160  |
---------------------------------
Eval num_timesteps=4444152, episode_reward=1328.77 +/- 562.45
Episode length: 693.80 +/- 113.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4444152  |
---------------------------------
Eval num_timesteps=4446144, episode_reward=1235.95 +/- 260.91
Episode length: 767.20 +/- 99.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4446144  |
---------------------------------
Eval num_timesteps=4448136, episode_reward=986.74 +/- 339.66
Episode length: 682.00 +/- 34.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 4448136  |
---------------------------------
Eval num_timesteps=4450128, episode_reward=1212.51 +/- 197.60
Episode length: 704.00 +/- 135.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4450128  |
---------------------------------
Eval num_timesteps=4452120, episode_reward=1324.77 +/- 394.42
Episode length: 669.60 +/- 24.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4452120  |
---------------------------------
Eval num_timesteps=4454112, episode_reward=1309.46 +/- 315.58
Episode length: 649.60 +/- 87.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4454112  |
---------------------------------
Eval num_timesteps=4456104, episode_reward=1191.43 +/- 352.57
Episode length: 732.80 +/- 162.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4456104  |
---------------------------------
Eval num_timesteps=4458096, episode_reward=1227.33 +/- 207.23
Episode length: 645.80 +/- 184.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4458096  |
---------------------------------
Eval num_timesteps=4460088, episode_reward=1118.10 +/- 374.70
Episode length: 718.60 +/- 47.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4460088  |
---------------------------------
Eval num_timesteps=4462080, episode_reward=1229.80 +/- 379.66
Episode length: 764.80 +/- 100.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4462080  |
---------------------------------
Eval num_timesteps=4464072, episode_reward=1158.50 +/- 306.44
Episode length: 655.60 +/- 32.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4464072  |
---------------------------------
Eval num_timesteps=4466064, episode_reward=1147.11 +/- 433.61
Episode length: 645.00 +/- 122.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4466064  |
---------------------------------
Eval num_timesteps=4468056, episode_reward=1122.16 +/- 432.86
Episode length: 602.80 +/- 29.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4468056  |
---------------------------------
Eval num_timesteps=4470048, episode_reward=1472.61 +/- 429.36
Episode length: 635.40 +/- 74.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4470048  |
---------------------------------
Eval num_timesteps=4472040, episode_reward=1477.07 +/- 422.02
Episode length: 640.80 +/- 65.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4472040  |
---------------------------------
Eval num_timesteps=4474032, episode_reward=1302.41 +/- 402.69
Episode length: 654.60 +/- 66.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 655         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 4474032     |
| train/                  |             |
|    approx_kl            | 0.004650611 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0366     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0011     |
|    std                  | 1.5         |
|    value_loss           | 0.0796      |
-----------------------------------------
Eval num_timesteps=4476024, episode_reward=1286.79 +/- 423.92
Episode length: 684.80 +/- 127.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4476024  |
---------------------------------
Eval num_timesteps=4478016, episode_reward=1408.66 +/- 339.17
Episode length: 732.20 +/- 131.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4478016  |
---------------------------------
Eval num_timesteps=4480008, episode_reward=1482.39 +/- 96.63
Episode length: 621.20 +/- 132.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4480008  |
---------------------------------
Eval num_timesteps=4482000, episode_reward=1313.16 +/- 507.41
Episode length: 743.20 +/- 95.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4482000  |
---------------------------------
Eval num_timesteps=4483992, episode_reward=1029.39 +/- 249.46
Episode length: 745.60 +/- 130.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4483992  |
---------------------------------
Eval num_timesteps=4485984, episode_reward=1206.43 +/- 263.49
Episode length: 640.80 +/- 94.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4485984  |
---------------------------------
Eval num_timesteps=4487976, episode_reward=1442.42 +/- 431.87
Episode length: 755.80 +/- 115.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4487976  |
---------------------------------
Eval num_timesteps=4489968, episode_reward=1443.84 +/- 442.35
Episode length: 720.60 +/- 102.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4489968  |
---------------------------------
Eval num_timesteps=4491960, episode_reward=1158.57 +/- 420.73
Episode length: 692.80 +/- 71.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4491960  |
---------------------------------
Eval num_timesteps=4493952, episode_reward=1421.24 +/- 389.03
Episode length: 628.40 +/- 65.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4493952  |
---------------------------------
Eval num_timesteps=4495944, episode_reward=982.72 +/- 469.43
Episode length: 624.20 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 4495944  |
---------------------------------
Eval num_timesteps=4497936, episode_reward=1194.57 +/- 576.88
Episode length: 633.80 +/- 164.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4497936  |
---------------------------------
Eval num_timesteps=4499928, episode_reward=968.16 +/- 145.46
Episode length: 619.80 +/- 68.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 4499928  |
---------------------------------
Eval num_timesteps=4501920, episode_reward=1349.48 +/- 295.23
Episode length: 642.00 +/- 119.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4501920  |
---------------------------------
Eval num_timesteps=4503912, episode_reward=1310.68 +/- 529.30
Episode length: 645.20 +/- 173.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4503912  |
---------------------------------
Eval num_timesteps=4505904, episode_reward=1386.63 +/- 365.55
Episode length: 746.60 +/- 119.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4505904  |
---------------------------------
Eval num_timesteps=4507896, episode_reward=1105.74 +/- 388.69
Episode length: 572.40 +/- 50.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4507896  |
---------------------------------
Eval num_timesteps=4509888, episode_reward=1352.50 +/- 178.91
Episode length: 685.60 +/- 163.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4509888  |
---------------------------------
Eval num_timesteps=4511880, episode_reward=1513.30 +/- 352.37
Episode length: 686.00 +/- 117.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4511880  |
---------------------------------
Eval num_timesteps=4513872, episode_reward=1260.49 +/- 365.24
Episode length: 617.80 +/- 133.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4513872  |
---------------------------------
Eval num_timesteps=4515864, episode_reward=1338.39 +/- 530.19
Episode length: 667.40 +/- 177.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4515864  |
---------------------------------
Eval num_timesteps=4517856, episode_reward=1233.17 +/- 350.77
Episode length: 618.20 +/- 123.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4517856  |
---------------------------------
Eval num_timesteps=4519848, episode_reward=1324.78 +/- 405.14
Episode length: 636.00 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4519848  |
---------------------------------
Eval num_timesteps=4521840, episode_reward=1567.88 +/- 479.60
Episode length: 701.40 +/- 133.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4521840  |
---------------------------------
Eval num_timesteps=4523832, episode_reward=1867.72 +/- 312.86
Episode length: 851.00 +/- 121.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 851          |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 4523832      |
| train/                  |              |
|    approx_kl            | 0.0055086724 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | -0.036       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.51         |
|    value_loss           | 0.0812       |
------------------------------------------
Eval num_timesteps=4525824, episode_reward=1428.63 +/- 501.44
Episode length: 676.20 +/- 97.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4525824  |
---------------------------------
Eval num_timesteps=4527816, episode_reward=1692.67 +/- 88.54
Episode length: 664.80 +/- 98.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4527816  |
---------------------------------
Eval num_timesteps=4529808, episode_reward=1306.84 +/- 416.28
Episode length: 577.00 +/- 33.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4529808  |
---------------------------------
Eval num_timesteps=4531800, episode_reward=1461.56 +/- 269.52
Episode length: 657.00 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4531800  |
---------------------------------
Eval num_timesteps=4533792, episode_reward=1253.10 +/- 530.79
Episode length: 731.00 +/- 97.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4533792  |
---------------------------------
Eval num_timesteps=4535784, episode_reward=1215.06 +/- 421.86
Episode length: 794.00 +/- 81.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 4535784  |
---------------------------------
Eval num_timesteps=4537776, episode_reward=987.36 +/- 407.50
Episode length: 641.00 +/- 48.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 4537776  |
---------------------------------
Eval num_timesteps=4539768, episode_reward=1184.20 +/- 423.94
Episode length: 641.00 +/- 68.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4539768  |
---------------------------------
Eval num_timesteps=4541760, episode_reward=1299.03 +/- 521.60
Episode length: 704.80 +/- 88.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4541760  |
---------------------------------
Eval num_timesteps=4543752, episode_reward=605.95 +/- 340.46
Episode length: 652.20 +/- 191.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 4543752  |
---------------------------------
Eval num_timesteps=4545744, episode_reward=1184.43 +/- 260.84
Episode length: 628.00 +/- 56.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4545744  |
---------------------------------
Eval num_timesteps=4547736, episode_reward=1717.73 +/- 369.73
Episode length: 696.20 +/- 95.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4547736  |
---------------------------------
Eval num_timesteps=4549728, episode_reward=1333.89 +/- 337.36
Episode length: 759.00 +/- 154.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4549728  |
---------------------------------
Eval num_timesteps=4551720, episode_reward=1452.88 +/- 425.51
Episode length: 729.40 +/- 163.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4551720  |
---------------------------------
Eval num_timesteps=4553712, episode_reward=1495.56 +/- 418.21
Episode length: 698.40 +/- 79.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 4553712  |
---------------------------------
Eval num_timesteps=4555704, episode_reward=1457.85 +/- 320.86
Episode length: 622.20 +/- 112.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4555704  |
---------------------------------
Eval num_timesteps=4557696, episode_reward=1355.33 +/- 154.43
Episode length: 609.60 +/- 118.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4557696  |
---------------------------------
Eval num_timesteps=4559688, episode_reward=1653.15 +/- 108.74
Episode length: 639.80 +/- 79.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4559688  |
---------------------------------
Eval num_timesteps=4561680, episode_reward=1401.63 +/- 374.03
Episode length: 591.40 +/- 59.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4561680  |
---------------------------------
Eval num_timesteps=4563672, episode_reward=1160.52 +/- 384.51
Episode length: 790.80 +/- 207.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4563672  |
---------------------------------
Eval num_timesteps=4565664, episode_reward=1334.90 +/- 485.40
Episode length: 698.60 +/- 63.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4565664  |
---------------------------------
Eval num_timesteps=4567656, episode_reward=1314.73 +/- 462.69
Episode length: 638.00 +/- 54.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4567656  |
---------------------------------
Eval num_timesteps=4569648, episode_reward=1484.33 +/- 94.46
Episode length: 638.20 +/- 70.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4569648  |
---------------------------------
Eval num_timesteps=4571640, episode_reward=1495.23 +/- 291.16
Episode length: 742.80 +/- 99.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 743          |
|    mean_reward          | 1.5e+03      |
| time/                   |              |
|    total_timesteps      | 4571640      |
| train/                  |              |
|    approx_kl            | 0.0050457353 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0354      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.52         |
|    value_loss           | 0.0828       |
------------------------------------------
Eval num_timesteps=4573632, episode_reward=1301.75 +/- 633.24
Episode length: 744.20 +/- 155.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4573632  |
---------------------------------
Eval num_timesteps=4575624, episode_reward=1387.65 +/- 399.67
Episode length: 702.00 +/- 191.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4575624  |
---------------------------------
Eval num_timesteps=4577616, episode_reward=1473.43 +/- 222.54
Episode length: 662.60 +/- 124.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4577616  |
---------------------------------
Eval num_timesteps=4579608, episode_reward=1507.82 +/- 394.77
Episode length: 644.80 +/- 31.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4579608  |
---------------------------------
Eval num_timesteps=4581600, episode_reward=1374.78 +/- 487.02
Episode length: 805.80 +/- 112.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4581600  |
---------------------------------
Eval num_timesteps=4583592, episode_reward=1016.83 +/- 416.00
Episode length: 780.40 +/- 134.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4583592  |
---------------------------------
Eval num_timesteps=4585584, episode_reward=1506.70 +/- 699.39
Episode length: 864.60 +/- 153.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4585584  |
---------------------------------
Eval num_timesteps=4587576, episode_reward=1658.11 +/- 255.04
Episode length: 687.00 +/- 47.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 4587576  |
---------------------------------
Eval num_timesteps=4589568, episode_reward=1507.71 +/- 684.12
Episode length: 676.40 +/- 148.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4589568  |
---------------------------------
Eval num_timesteps=4591560, episode_reward=1345.45 +/- 433.41
Episode length: 616.60 +/- 82.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4591560  |
---------------------------------
Eval num_timesteps=4593552, episode_reward=1256.63 +/- 514.61
Episode length: 825.80 +/- 175.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4593552  |
---------------------------------
Eval num_timesteps=4595544, episode_reward=1115.59 +/- 534.59
Episode length: 730.00 +/- 135.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4595544  |
---------------------------------
Eval num_timesteps=4597536, episode_reward=1254.36 +/- 505.31
Episode length: 705.40 +/- 89.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4597536  |
---------------------------------
Eval num_timesteps=4599528, episode_reward=1575.93 +/- 372.25
Episode length: 791.20 +/- 88.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4599528  |
---------------------------------
Eval num_timesteps=4601520, episode_reward=1761.60 +/- 525.94
Episode length: 742.60 +/- 150.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 4601520  |
---------------------------------
Eval num_timesteps=4603512, episode_reward=1186.56 +/- 329.28
Episode length: 774.80 +/- 143.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4603512  |
---------------------------------
Eval num_timesteps=4605504, episode_reward=960.42 +/- 321.10
Episode length: 582.20 +/- 46.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 4605504  |
---------------------------------
Eval num_timesteps=4607496, episode_reward=1442.20 +/- 442.81
Episode length: 723.20 +/- 163.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4607496  |
---------------------------------
Eval num_timesteps=4609488, episode_reward=1050.62 +/- 493.51
Episode length: 631.60 +/- 85.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4609488  |
---------------------------------
Eval num_timesteps=4611480, episode_reward=1356.16 +/- 763.25
Episode length: 731.20 +/- 252.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4611480  |
---------------------------------
Eval num_timesteps=4613472, episode_reward=1254.15 +/- 311.93
Episode length: 800.60 +/- 119.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4613472  |
---------------------------------
Eval num_timesteps=4615464, episode_reward=1454.14 +/- 613.03
Episode length: 820.80 +/- 180.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4615464  |
---------------------------------
Eval num_timesteps=4617456, episode_reward=1398.15 +/- 357.70
Episode length: 705.00 +/- 157.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4617456  |
---------------------------------
Eval num_timesteps=4619448, episode_reward=1115.02 +/- 424.65
Episode length: 583.80 +/- 122.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4619448  |
---------------------------------
Eval num_timesteps=4621440, episode_reward=1130.97 +/- 238.47
Episode length: 647.60 +/- 107.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 648         |
|    mean_reward          | 1.13e+03    |
| time/                   |             |
|    total_timesteps      | 4621440     |
| train/                  |             |
|    approx_kl            | 0.004717447 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0295     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.000666   |
|    std                  | 1.54        |
|    value_loss           | 0.0936      |
-----------------------------------------
Eval num_timesteps=4623432, episode_reward=1439.13 +/- 364.24
Episode length: 726.40 +/- 99.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4623432  |
---------------------------------
Eval num_timesteps=4625424, episode_reward=1187.89 +/- 484.45
Episode length: 778.60 +/- 147.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4625424  |
---------------------------------
Eval num_timesteps=4627416, episode_reward=728.17 +/- 392.43
Episode length: 643.00 +/- 242.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 4627416  |
---------------------------------
Eval num_timesteps=4629408, episode_reward=1124.02 +/- 366.82
Episode length: 711.00 +/- 110.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4629408  |
---------------------------------
Eval num_timesteps=4631400, episode_reward=1438.80 +/- 416.84
Episode length: 676.00 +/- 130.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4631400  |
---------------------------------
Eval num_timesteps=4633392, episode_reward=865.50 +/- 191.51
Episode length: 621.20 +/- 95.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 4633392  |
---------------------------------
Eval num_timesteps=4635384, episode_reward=1454.19 +/- 477.22
Episode length: 611.40 +/- 64.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4635384  |
---------------------------------
Eval num_timesteps=4637376, episode_reward=1455.49 +/- 475.11
Episode length: 668.00 +/- 128.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4637376  |
---------------------------------
Eval num_timesteps=4639368, episode_reward=1133.27 +/- 303.46
Episode length: 767.60 +/- 164.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4639368  |
---------------------------------
Eval num_timesteps=4641360, episode_reward=1293.00 +/- 260.96
Episode length: 858.60 +/- 92.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4641360  |
---------------------------------
Eval num_timesteps=4643352, episode_reward=1100.28 +/- 297.55
Episode length: 639.20 +/- 62.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4643352  |
---------------------------------
Eval num_timesteps=4645344, episode_reward=1097.30 +/- 309.09
Episode length: 746.00 +/- 84.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4645344  |
---------------------------------
Eval num_timesteps=4647336, episode_reward=989.36 +/- 209.13
Episode length: 657.80 +/- 131.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 4647336  |
---------------------------------
Eval num_timesteps=4649328, episode_reward=1150.51 +/- 384.63
Episode length: 745.60 +/- 134.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4649328  |
---------------------------------
Eval num_timesteps=4651320, episode_reward=1172.63 +/- 264.03
Episode length: 589.60 +/- 72.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4651320  |
---------------------------------
Eval num_timesteps=4653312, episode_reward=1377.75 +/- 196.48
Episode length: 679.20 +/- 55.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4653312  |
---------------------------------
Eval num_timesteps=4655304, episode_reward=1334.54 +/- 267.99
Episode length: 651.00 +/- 117.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4655304  |
---------------------------------
Eval num_timesteps=4657296, episode_reward=1173.69 +/- 432.51
Episode length: 628.00 +/- 95.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4657296  |
---------------------------------
Eval num_timesteps=4659288, episode_reward=1302.87 +/- 380.65
Episode length: 754.80 +/- 136.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4659288  |
---------------------------------
Eval num_timesteps=4661280, episode_reward=1392.88 +/- 438.79
Episode length: 746.40 +/- 121.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4661280  |
---------------------------------
Eval num_timesteps=4663272, episode_reward=1495.11 +/- 266.82
Episode length: 727.20 +/- 52.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 4663272  |
---------------------------------
Eval num_timesteps=4665264, episode_reward=1119.99 +/- 553.66
Episode length: 599.20 +/- 153.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4665264  |
---------------------------------
Eval num_timesteps=4667256, episode_reward=1417.55 +/- 368.12
Episode length: 684.00 +/- 116.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 4667256  |
---------------------------------
Eval num_timesteps=4669248, episode_reward=1003.11 +/- 687.95
Episode length: 732.40 +/- 221.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4669248  |
---------------------------------
Eval num_timesteps=4671240, episode_reward=1036.25 +/- 359.01
Episode length: 718.00 +/- 144.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 718          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 4671240      |
| train/                  |              |
|    approx_kl            | 0.0039096596 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0363      |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000991    |
|    std                  | 1.54         |
|    value_loss           | 0.0816       |
------------------------------------------
Eval num_timesteps=4673232, episode_reward=1154.89 +/- 409.48
Episode length: 675.80 +/- 193.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4673232  |
---------------------------------
Eval num_timesteps=4675224, episode_reward=971.99 +/- 262.32
Episode length: 755.40 +/- 185.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 4675224  |
---------------------------------
Eval num_timesteps=4677216, episode_reward=1206.23 +/- 489.53
Episode length: 782.80 +/- 82.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4677216  |
---------------------------------
Eval num_timesteps=4679208, episode_reward=914.28 +/- 376.38
Episode length: 704.00 +/- 234.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 4679208  |
---------------------------------
Eval num_timesteps=4681200, episode_reward=1167.81 +/- 437.70
Episode length: 784.40 +/- 112.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4681200  |
---------------------------------
Eval num_timesteps=4683192, episode_reward=1134.47 +/- 387.37
Episode length: 669.80 +/- 138.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4683192  |
---------------------------------
Eval num_timesteps=4685184, episode_reward=1265.91 +/- 308.34
Episode length: 738.40 +/- 149.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4685184  |
---------------------------------
Eval num_timesteps=4687176, episode_reward=1531.26 +/- 456.85
Episode length: 826.40 +/- 118.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4687176  |
---------------------------------
Eval num_timesteps=4689168, episode_reward=1029.69 +/- 419.49
Episode length: 751.80 +/- 95.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4689168  |
---------------------------------
Eval num_timesteps=4691160, episode_reward=1282.64 +/- 421.64
Episode length: 763.00 +/- 98.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4691160  |
---------------------------------
Eval num_timesteps=4693152, episode_reward=1276.47 +/- 415.38
Episode length: 646.20 +/- 95.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4693152  |
---------------------------------
Eval num_timesteps=4695144, episode_reward=955.05 +/- 329.91
Episode length: 789.40 +/- 115.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 4695144  |
---------------------------------
Eval num_timesteps=4697136, episode_reward=1006.03 +/- 329.01
Episode length: 818.20 +/- 127.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 4697136  |
---------------------------------
Eval num_timesteps=4699128, episode_reward=1307.00 +/- 387.95
Episode length: 640.20 +/- 80.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4699128  |
---------------------------------
Eval num_timesteps=4701120, episode_reward=1350.94 +/- 399.73
Episode length: 714.60 +/- 141.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4701120  |
---------------------------------
Eval num_timesteps=4703112, episode_reward=1454.96 +/- 423.60
Episode length: 691.80 +/- 114.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4703112  |
---------------------------------
Eval num_timesteps=4705104, episode_reward=1248.12 +/- 479.92
Episode length: 705.20 +/- 116.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4705104  |
---------------------------------
Eval num_timesteps=4707096, episode_reward=1222.91 +/- 463.25
Episode length: 703.80 +/- 142.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 4707096  |
---------------------------------
Eval num_timesteps=4709088, episode_reward=1480.74 +/- 357.94
Episode length: 708.20 +/- 143.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4709088  |
---------------------------------
Eval num_timesteps=4711080, episode_reward=1106.01 +/- 226.51
Episode length: 815.60 +/- 62.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4711080  |
---------------------------------
Eval num_timesteps=4713072, episode_reward=1090.86 +/- 458.35
Episode length: 751.80 +/- 150.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4713072  |
---------------------------------
Eval num_timesteps=4715064, episode_reward=1445.47 +/- 394.04
Episode length: 703.20 +/- 99.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4715064  |
---------------------------------
Eval num_timesteps=4717056, episode_reward=1002.38 +/- 366.38
Episode length: 711.80 +/- 107.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4717056  |
---------------------------------
Eval num_timesteps=4719048, episode_reward=1476.19 +/- 708.26
Episode length: 778.80 +/- 198.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 779         |
|    mean_reward          | 1.48e+03    |
| time/                   |             |
|    total_timesteps      | 4719048     |
| train/                  |             |
|    approx_kl            | 0.004305457 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.41       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0383     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00113    |
|    std                  | 1.55        |
|    value_loss           | 0.0785      |
-----------------------------------------
Eval num_timesteps=4721040, episode_reward=780.39 +/- 88.33
Episode length: 633.40 +/- 119.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 4721040  |
---------------------------------
Eval num_timesteps=4723032, episode_reward=1281.84 +/- 421.15
Episode length: 758.40 +/- 91.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4723032  |
---------------------------------
Eval num_timesteps=4725024, episode_reward=1007.65 +/- 345.27
Episode length: 711.40 +/- 148.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 4725024  |
---------------------------------
Eval num_timesteps=4727016, episode_reward=1300.47 +/- 433.38
Episode length: 616.80 +/- 62.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4727016  |
---------------------------------
Eval num_timesteps=4729008, episode_reward=1062.20 +/- 285.04
Episode length: 757.20 +/- 131.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 4729008  |
---------------------------------
Eval num_timesteps=4731000, episode_reward=1301.38 +/- 490.62
Episode length: 702.40 +/- 178.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4731000  |
---------------------------------
Eval num_timesteps=4732992, episode_reward=1803.56 +/- 321.65
Episode length: 736.00 +/- 117.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 4732992  |
---------------------------------
Eval num_timesteps=4734984, episode_reward=747.24 +/- 46.89
Episode length: 608.40 +/- 75.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 4734984  |
---------------------------------
Eval num_timesteps=4736976, episode_reward=1261.38 +/- 308.54
Episode length: 699.60 +/- 90.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4736976  |
---------------------------------
Eval num_timesteps=4738968, episode_reward=1425.78 +/- 416.90
Episode length: 636.60 +/- 102.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4738968  |
---------------------------------
Eval num_timesteps=4740960, episode_reward=1157.68 +/- 560.41
Episode length: 684.40 +/- 83.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4740960  |
---------------------------------
Eval num_timesteps=4742952, episode_reward=1002.22 +/- 395.74
Episode length: 639.60 +/- 80.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4742952  |
---------------------------------
Eval num_timesteps=4744944, episode_reward=1322.72 +/- 391.12
Episode length: 753.40 +/- 139.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4744944  |
---------------------------------
Eval num_timesteps=4746936, episode_reward=811.82 +/- 97.65
Episode length: 708.80 +/- 70.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 4746936  |
---------------------------------
Eval num_timesteps=4748928, episode_reward=1427.51 +/- 449.08
Episode length: 679.40 +/- 130.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4748928  |
---------------------------------
Eval num_timesteps=4750920, episode_reward=920.99 +/- 258.16
Episode length: 693.20 +/- 114.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 4750920  |
---------------------------------
Eval num_timesteps=4752912, episode_reward=986.61 +/- 404.42
Episode length: 737.60 +/- 154.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 4752912  |
---------------------------------
Eval num_timesteps=4754904, episode_reward=960.94 +/- 307.75
Episode length: 713.00 +/- 104.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 4754904  |
---------------------------------
Eval num_timesteps=4756896, episode_reward=1040.54 +/- 389.01
Episode length: 724.60 +/- 76.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 4756896  |
---------------------------------
Eval num_timesteps=4758888, episode_reward=1028.78 +/- 275.49
Episode length: 564.40 +/- 92.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4758888  |
---------------------------------
Eval num_timesteps=4760880, episode_reward=1147.30 +/- 463.70
Episode length: 664.40 +/- 113.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4760880  |
---------------------------------
Eval num_timesteps=4762872, episode_reward=1065.33 +/- 461.38
Episode length: 679.80 +/- 142.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4762872  |
---------------------------------
Eval num_timesteps=4764864, episode_reward=1146.86 +/- 238.01
Episode length: 748.00 +/- 170.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4764864  |
---------------------------------
Eval num_timesteps=4766856, episode_reward=807.70 +/- 197.27
Episode length: 684.80 +/- 130.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 4766856  |
---------------------------------
Eval num_timesteps=4768848, episode_reward=826.51 +/- 339.18
Episode length: 640.40 +/- 176.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 640          |
|    mean_reward          | 827          |
| time/                   |              |
|    total_timesteps      | 4768848      |
| train/                  |              |
|    approx_kl            | 0.0052108364 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.43        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0428      |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.56         |
|    value_loss           | 0.0714       |
------------------------------------------
Eval num_timesteps=4770840, episode_reward=1409.68 +/- 542.31
Episode length: 655.80 +/- 47.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4770840  |
---------------------------------
Eval num_timesteps=4772832, episode_reward=1156.18 +/- 467.17
Episode length: 654.20 +/- 42.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4772832  |
---------------------------------
Eval num_timesteps=4774824, episode_reward=754.86 +/- 134.00
Episode length: 764.00 +/- 91.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 4774824  |
---------------------------------
Eval num_timesteps=4776816, episode_reward=1149.38 +/- 448.36
Episode length: 717.00 +/- 102.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4776816  |
---------------------------------
Eval num_timesteps=4778808, episode_reward=961.54 +/- 367.21
Episode length: 657.00 +/- 30.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 4778808  |
---------------------------------
Eval num_timesteps=4780800, episode_reward=1178.56 +/- 618.32
Episode length: 709.80 +/- 42.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4780800  |
---------------------------------
Eval num_timesteps=4782792, episode_reward=990.13 +/- 341.53
Episode length: 699.80 +/- 187.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 4782792  |
---------------------------------
Eval num_timesteps=4784784, episode_reward=792.96 +/- 124.47
Episode length: 761.00 +/- 107.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 4784784  |
---------------------------------
Eval num_timesteps=4786776, episode_reward=1033.51 +/- 378.77
Episode length: 736.40 +/- 47.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4786776  |
---------------------------------
Eval num_timesteps=4788768, episode_reward=821.41 +/- 94.17
Episode length: 859.00 +/- 128.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 4788768  |
---------------------------------
Eval num_timesteps=4790760, episode_reward=1148.59 +/- 336.79
Episode length: 708.00 +/- 55.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4790760  |
---------------------------------
Eval num_timesteps=4792752, episode_reward=761.99 +/- 94.57
Episode length: 694.00 +/- 62.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 4792752  |
---------------------------------
Eval num_timesteps=4794744, episode_reward=1117.09 +/- 435.93
Episode length: 778.60 +/- 149.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4794744  |
---------------------------------
Eval num_timesteps=4796736, episode_reward=970.64 +/- 277.66
Episode length: 744.80 +/- 104.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 4796736  |
---------------------------------
Eval num_timesteps=4798728, episode_reward=774.51 +/- 119.65
Episode length: 688.60 +/- 118.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 4798728  |
---------------------------------
Eval num_timesteps=4800720, episode_reward=1354.31 +/- 499.17
Episode length: 726.60 +/- 78.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4800720  |
---------------------------------
Eval num_timesteps=4802712, episode_reward=934.75 +/- 358.92
Episode length: 731.80 +/- 133.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 4802712  |
---------------------------------
Eval num_timesteps=4804704, episode_reward=852.70 +/- 211.85
Episode length: 824.00 +/- 80.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 4804704  |
---------------------------------
Eval num_timesteps=4806696, episode_reward=755.74 +/- 115.60
Episode length: 799.80 +/- 85.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 4806696  |
---------------------------------
Eval num_timesteps=4808688, episode_reward=1258.75 +/- 377.53
Episode length: 674.20 +/- 124.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4808688  |
---------------------------------
Eval num_timesteps=4810680, episode_reward=722.91 +/- 46.28
Episode length: 614.60 +/- 47.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 4810680  |
---------------------------------
Eval num_timesteps=4812672, episode_reward=1133.86 +/- 434.94
Episode length: 702.80 +/- 152.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4812672  |
---------------------------------
Eval num_timesteps=4814664, episode_reward=1040.82 +/- 426.70
Episode length: 773.20 +/- 119.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 4814664  |
---------------------------------
Eval num_timesteps=4816656, episode_reward=1036.06 +/- 464.88
Episode length: 684.40 +/- 67.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 4816656  |
---------------------------------
Eval num_timesteps=4818648, episode_reward=957.11 +/- 396.10
Episode length: 760.20 +/- 220.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 760          |
|    mean_reward          | 957          |
| time/                   |              |
|    total_timesteps      | 4818648      |
| train/                  |              |
|    approx_kl            | 0.0041149724 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0464      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.57         |
|    value_loss           | 0.0633       |
------------------------------------------
Eval num_timesteps=4820640, episode_reward=861.61 +/- 187.03
Episode length: 617.80 +/- 76.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 4820640  |
---------------------------------
Eval num_timesteps=4822632, episode_reward=1054.23 +/- 461.41
Episode length: 780.40 +/- 124.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4822632  |
---------------------------------
Eval num_timesteps=4824624, episode_reward=1027.55 +/- 570.28
Episode length: 675.20 +/- 48.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4824624  |
---------------------------------
Eval num_timesteps=4826616, episode_reward=914.29 +/- 306.87
Episode length: 787.20 +/- 131.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 4826616  |
---------------------------------
Eval num_timesteps=4828608, episode_reward=1143.80 +/- 511.69
Episode length: 764.80 +/- 106.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4828608  |
---------------------------------
Eval num_timesteps=4830600, episode_reward=1242.06 +/- 500.63
Episode length: 712.60 +/- 149.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4830600  |
---------------------------------
Eval num_timesteps=4832592, episode_reward=707.48 +/- 56.77
Episode length: 639.40 +/- 59.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 4832592  |
---------------------------------
Eval num_timesteps=4834584, episode_reward=780.91 +/- 158.30
Episode length: 756.20 +/- 93.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 4834584  |
---------------------------------
Eval num_timesteps=4836576, episode_reward=1354.39 +/- 542.64
Episode length: 729.80 +/- 112.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4836576  |
---------------------------------
Eval num_timesteps=4838568, episode_reward=1412.23 +/- 555.24
Episode length: 750.20 +/- 191.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4838568  |
---------------------------------
Eval num_timesteps=4840560, episode_reward=1114.95 +/- 398.13
Episode length: 757.00 +/- 83.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4840560  |
---------------------------------
Eval num_timesteps=4842552, episode_reward=1087.93 +/- 383.94
Episode length: 741.40 +/- 119.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4842552  |
---------------------------------
Eval num_timesteps=4844544, episode_reward=1165.62 +/- 469.06
Episode length: 644.60 +/- 55.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4844544  |
---------------------------------
Eval num_timesteps=4846536, episode_reward=877.54 +/- 152.57
Episode length: 855.40 +/- 71.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 4846536  |
---------------------------------
Eval num_timesteps=4848528, episode_reward=1434.85 +/- 548.91
Episode length: 662.60 +/- 9.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4848528  |
---------------------------------
Eval num_timesteps=4850520, episode_reward=972.07 +/- 419.55
Episode length: 794.60 +/- 75.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 4850520  |
---------------------------------
Eval num_timesteps=4852512, episode_reward=960.39 +/- 282.08
Episode length: 696.00 +/- 148.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 4852512  |
---------------------------------
Eval num_timesteps=4854504, episode_reward=1130.83 +/- 462.18
Episode length: 717.60 +/- 45.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4854504  |
---------------------------------
Eval num_timesteps=4856496, episode_reward=846.18 +/- 112.28
Episode length: 707.60 +/- 111.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 4856496  |
---------------------------------
Eval num_timesteps=4858488, episode_reward=998.39 +/- 315.35
Episode length: 780.00 +/- 130.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 4858488  |
---------------------------------
Eval num_timesteps=4860480, episode_reward=1101.09 +/- 487.94
Episode length: 764.20 +/- 111.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4860480  |
---------------------------------
Eval num_timesteps=4862472, episode_reward=1068.26 +/- 696.26
Episode length: 714.20 +/- 56.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4862472  |
---------------------------------
Eval num_timesteps=4864464, episode_reward=1110.31 +/- 501.05
Episode length: 820.00 +/- 130.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4864464  |
---------------------------------
Eval num_timesteps=4866456, episode_reward=1031.80 +/- 446.14
Episode length: 740.00 +/- 86.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 740         |
|    mean_reward          | 1.03e+03    |
| time/                   |             |
|    total_timesteps      | 4866456     |
| train/                  |             |
|    approx_kl            | 0.004129426 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.47       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0469     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00139    |
|    std                  | 1.58        |
|    value_loss           | 0.063       |
-----------------------------------------
Eval num_timesteps=4868448, episode_reward=1102.00 +/- 612.10
Episode length: 858.40 +/- 160.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4868448  |
---------------------------------
Eval num_timesteps=4870440, episode_reward=1004.19 +/- 225.34
Episode length: 765.20 +/- 82.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4870440  |
---------------------------------
Eval num_timesteps=4872432, episode_reward=694.37 +/- 64.54
Episode length: 699.40 +/- 71.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 4872432  |
---------------------------------
Eval num_timesteps=4874424, episode_reward=1313.55 +/- 633.60
Episode length: 746.80 +/- 63.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4874424  |
---------------------------------
Eval num_timesteps=4876416, episode_reward=643.51 +/- 145.41
Episode length: 685.60 +/- 78.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 4876416  |
---------------------------------
Eval num_timesteps=4878408, episode_reward=841.61 +/- 43.99
Episode length: 775.20 +/- 39.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 4878408  |
---------------------------------
Eval num_timesteps=4880400, episode_reward=1221.34 +/- 639.71
Episode length: 645.00 +/- 80.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 4880400  |
---------------------------------
Eval num_timesteps=4882392, episode_reward=1093.96 +/- 402.15
Episode length: 777.40 +/- 179.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4882392  |
---------------------------------
Eval num_timesteps=4884384, episode_reward=934.90 +/- 408.66
Episode length: 761.60 +/- 96.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 4884384  |
---------------------------------
Eval num_timesteps=4886376, episode_reward=893.72 +/- 263.08
Episode length: 764.00 +/- 124.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 4886376  |
---------------------------------
Eval num_timesteps=4888368, episode_reward=991.87 +/- 254.22
Episode length: 653.00 +/- 163.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 4888368  |
---------------------------------
Eval num_timesteps=4890360, episode_reward=1050.37 +/- 484.86
Episode length: 675.60 +/- 33.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4890360  |
---------------------------------
Eval num_timesteps=4892352, episode_reward=770.84 +/- 104.80
Episode length: 643.40 +/- 99.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 4892352  |
---------------------------------
Eval num_timesteps=4894344, episode_reward=1157.31 +/- 438.72
Episode length: 897.20 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4894344  |
---------------------------------
Eval num_timesteps=4896336, episode_reward=901.37 +/- 202.39
Episode length: 720.80 +/- 103.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 4896336  |
---------------------------------
Eval num_timesteps=4898328, episode_reward=824.61 +/- 301.06
Episode length: 740.00 +/- 139.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 4898328  |
---------------------------------
Eval num_timesteps=4900320, episode_reward=1522.92 +/- 432.36
Episode length: 667.80 +/- 137.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4900320  |
---------------------------------
Eval num_timesteps=4902312, episode_reward=1529.72 +/- 577.21
Episode length: 879.00 +/- 186.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4902312  |
---------------------------------
Eval num_timesteps=4904304, episode_reward=1229.69 +/- 831.46
Episode length: 703.20 +/- 111.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4904304  |
---------------------------------
Eval num_timesteps=4906296, episode_reward=834.18 +/- 389.14
Episode length: 664.20 +/- 232.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 4906296  |
---------------------------------
Eval num_timesteps=4908288, episode_reward=1237.40 +/- 491.97
Episode length: 739.60 +/- 118.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 4908288  |
---------------------------------
Eval num_timesteps=4910280, episode_reward=966.45 +/- 208.00
Episode length: 765.40 +/- 116.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 4910280  |
---------------------------------
Eval num_timesteps=4912272, episode_reward=1290.10 +/- 645.71
Episode length: 711.60 +/- 82.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4912272  |
---------------------------------
Eval num_timesteps=4914264, episode_reward=916.07 +/- 410.77
Episode length: 709.20 +/- 61.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 4914264  |
---------------------------------
Eval num_timesteps=4916256, episode_reward=1237.37 +/- 450.98
Episode length: 651.60 +/- 78.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 652         |
|    mean_reward          | 1.24e+03    |
| time/                   |             |
|    total_timesteps      | 4916256     |
| train/                  |             |
|    approx_kl            | 0.006201679 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0448     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 1.59        |
|    value_loss           | 0.0684      |
-----------------------------------------
Eval num_timesteps=4918248, episode_reward=1433.91 +/- 482.86
Episode length: 703.20 +/- 119.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4918248  |
---------------------------------
Eval num_timesteps=4920240, episode_reward=1354.95 +/- 515.72
Episode length: 698.60 +/- 48.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4920240  |
---------------------------------
Eval num_timesteps=4922232, episode_reward=1297.61 +/- 549.78
Episode length: 692.40 +/- 109.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 4922232  |
---------------------------------
Eval num_timesteps=4924224, episode_reward=1019.05 +/- 384.05
Episode length: 644.00 +/- 182.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4924224  |
---------------------------------
Eval num_timesteps=4926216, episode_reward=989.89 +/- 545.01
Episode length: 591.00 +/- 140.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 4926216  |
---------------------------------
Eval num_timesteps=4928208, episode_reward=1205.50 +/- 658.31
Episode length: 770.60 +/- 144.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4928208  |
---------------------------------
Eval num_timesteps=4930200, episode_reward=1003.81 +/- 380.71
Episode length: 634.00 +/- 31.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4930200  |
---------------------------------
Eval num_timesteps=4932192, episode_reward=1425.39 +/- 597.82
Episode length: 722.40 +/- 33.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4932192  |
---------------------------------
Eval num_timesteps=4934184, episode_reward=1317.89 +/- 656.91
Episode length: 762.40 +/- 93.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4934184  |
---------------------------------
Eval num_timesteps=4936176, episode_reward=1543.24 +/- 634.92
Episode length: 778.60 +/- 105.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 4936176  |
---------------------------------
Eval num_timesteps=4938168, episode_reward=1312.27 +/- 806.79
Episode length: 718.40 +/- 143.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4938168  |
---------------------------------
Eval num_timesteps=4940160, episode_reward=989.81 +/- 746.77
Episode length: 610.40 +/- 219.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 4940160  |
---------------------------------
Eval num_timesteps=4942152, episode_reward=1566.02 +/- 389.45
Episode length: 633.80 +/- 33.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4942152  |
---------------------------------
Eval num_timesteps=4944144, episode_reward=657.16 +/- 181.38
Episode length: 612.80 +/- 152.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 4944144  |
---------------------------------
Eval num_timesteps=4946136, episode_reward=1602.47 +/- 491.93
Episode length: 718.20 +/- 124.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4946136  |
---------------------------------
Eval num_timesteps=4948128, episode_reward=1742.40 +/- 247.69
Episode length: 700.80 +/- 93.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4948128  |
---------------------------------
Eval num_timesteps=4950120, episode_reward=1273.35 +/- 664.58
Episode length: 668.60 +/- 200.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4950120  |
---------------------------------
Eval num_timesteps=4952112, episode_reward=958.11 +/- 331.10
Episode length: 688.60 +/- 76.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 4952112  |
---------------------------------
Eval num_timesteps=4954104, episode_reward=1275.89 +/- 419.13
Episode length: 731.80 +/- 96.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4954104  |
---------------------------------
Eval num_timesteps=4956096, episode_reward=1395.58 +/- 539.51
Episode length: 744.80 +/- 132.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4956096  |
---------------------------------
Eval num_timesteps=4958088, episode_reward=1770.20 +/- 226.18
Episode length: 796.00 +/- 151.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 4958088  |
---------------------------------
Eval num_timesteps=4960080, episode_reward=1203.55 +/- 652.90
Episode length: 675.60 +/- 176.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 4960080  |
---------------------------------
Eval num_timesteps=4962072, episode_reward=1073.47 +/- 460.80
Episode length: 820.80 +/- 189.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4962072  |
---------------------------------
Eval num_timesteps=4964064, episode_reward=1171.57 +/- 470.79
Episode length: 641.40 +/- 55.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4964064  |
---------------------------------
Eval num_timesteps=4966056, episode_reward=953.36 +/- 345.62
Episode length: 775.00 +/- 116.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 775         |
|    mean_reward          | 953         |
| time/                   |             |
|    total_timesteps      | 4966056     |
| train/                  |             |
|    approx_kl            | 0.005331993 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0429     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.6         |
|    value_loss           | 0.0735      |
-----------------------------------------
Eval num_timesteps=4968048, episode_reward=1564.05 +/- 319.40
Episode length: 810.40 +/- 110.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4968048  |
---------------------------------
Eval num_timesteps=4970040, episode_reward=1171.43 +/- 542.59
Episode length: 740.00 +/- 102.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4970040  |
---------------------------------
Eval num_timesteps=4972032, episode_reward=1460.62 +/- 645.89
Episode length: 783.20 +/- 160.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4972032  |
---------------------------------
Eval num_timesteps=4974024, episode_reward=1022.21 +/- 276.29
Episode length: 724.00 +/- 145.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4974024  |
---------------------------------
Eval num_timesteps=4976016, episode_reward=1523.83 +/- 452.23
Episode length: 666.00 +/- 77.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4976016  |
---------------------------------
Eval num_timesteps=4978008, episode_reward=1508.26 +/- 468.11
Episode length: 771.80 +/- 74.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4978008  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=1620.38 +/- 467.65
Episode length: 827.60 +/- 179.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4981992, episode_reward=1323.34 +/- 485.13
Episode length: 695.80 +/- 217.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4981992  |
---------------------------------
Eval num_timesteps=4983984, episode_reward=1065.84 +/- 620.65
Episode length: 686.60 +/- 182.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4983984  |
---------------------------------
Eval num_timesteps=4985976, episode_reward=919.54 +/- 213.67
Episode length: 728.20 +/- 97.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 4985976  |
---------------------------------
Eval num_timesteps=4987968, episode_reward=1342.39 +/- 307.07
Episode length: 938.40 +/- 177.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4987968  |
---------------------------------
Eval num_timesteps=4989960, episode_reward=1347.16 +/- 464.15
Episode length: 713.00 +/- 286.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4989960  |
---------------------------------
Eval num_timesteps=4991952, episode_reward=1472.65 +/- 283.05
Episode length: 829.20 +/- 157.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4991952  |
---------------------------------
Eval num_timesteps=4993944, episode_reward=1158.65 +/- 609.74
Episode length: 695.80 +/- 269.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4993944  |
---------------------------------
Eval num_timesteps=4995936, episode_reward=1409.47 +/- 551.28
Episode length: 654.80 +/- 179.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4995936  |
---------------------------------
Eval num_timesteps=4997928, episode_reward=1096.73 +/- 464.97
Episode length: 700.80 +/- 209.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4997928  |
---------------------------------
Eval num_timesteps=4999920, episode_reward=1494.18 +/- 465.97
Episode length: 642.00 +/- 92.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 4999920  |
---------------------------------
Eval num_timesteps=5001912, episode_reward=996.45 +/- 492.45
Episode length: 683.80 +/- 84.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 5001912  |
---------------------------------
Eval num_timesteps=5003904, episode_reward=1682.63 +/- 749.78
Episode length: 826.00 +/- 145.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 5003904  |
---------------------------------
Eval num_timesteps=5005896, episode_reward=1408.46 +/- 360.77
Episode length: 634.20 +/- 26.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5005896  |
---------------------------------
Eval num_timesteps=5007888, episode_reward=1475.45 +/- 274.77
Episode length: 854.00 +/- 136.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5007888  |
---------------------------------
Eval num_timesteps=5009880, episode_reward=1317.77 +/- 474.25
Episode length: 782.40 +/- 144.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5009880  |
---------------------------------
Eval num_timesteps=5011872, episode_reward=1115.46 +/- 343.58
Episode length: 793.40 +/- 165.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 5011872  |
---------------------------------
Eval num_timesteps=5013864, episode_reward=1065.25 +/- 397.50
Episode length: 780.40 +/- 128.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 780         |
|    mean_reward          | 1.07e+03    |
| time/                   |             |
|    total_timesteps      | 5013864     |
| train/                  |             |
|    approx_kl            | 0.006440713 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.56       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0387     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.61        |
|    value_loss           | 0.0832      |
-----------------------------------------
Eval num_timesteps=5015856, episode_reward=1234.62 +/- 656.85
Episode length: 637.20 +/- 186.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 5015856  |
---------------------------------
Eval num_timesteps=5017848, episode_reward=1387.71 +/- 615.48
Episode length: 724.20 +/- 187.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5017848  |
---------------------------------
Eval num_timesteps=5019840, episode_reward=1344.17 +/- 595.00
Episode length: 799.40 +/- 134.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5019840  |
---------------------------------
Eval num_timesteps=5021832, episode_reward=1712.42 +/- 405.97
Episode length: 801.80 +/- 87.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 5021832  |
---------------------------------
Eval num_timesteps=5023824, episode_reward=1670.44 +/- 432.00
Episode length: 758.80 +/- 92.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 5023824  |
---------------------------------
Eval num_timesteps=5025816, episode_reward=1476.04 +/- 423.17
Episode length: 697.80 +/- 118.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5025816  |
---------------------------------
Eval num_timesteps=5027808, episode_reward=1328.72 +/- 575.08
Episode length: 706.40 +/- 159.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 5027808  |
---------------------------------
Eval num_timesteps=5029800, episode_reward=1405.39 +/- 527.97
Episode length: 849.00 +/- 140.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5029800  |
---------------------------------
Eval num_timesteps=5031792, episode_reward=1441.91 +/- 574.51
Episode length: 771.20 +/- 179.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5031792  |
---------------------------------
Eval num_timesteps=5033784, episode_reward=1071.87 +/- 515.83
Episode length: 697.40 +/- 200.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 5033784  |
---------------------------------
Eval num_timesteps=5035776, episode_reward=1080.83 +/- 471.53
Episode length: 712.60 +/- 87.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5035776  |
---------------------------------
Eval num_timesteps=5037768, episode_reward=1261.76 +/- 689.36
Episode length: 779.80 +/- 157.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5037768  |
---------------------------------
Eval num_timesteps=5039760, episode_reward=1176.01 +/- 569.71
Episode length: 590.40 +/- 68.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 5039760  |
---------------------------------
Eval num_timesteps=5041752, episode_reward=1302.85 +/- 362.56
Episode length: 750.00 +/- 144.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 5041752  |
---------------------------------
Eval num_timesteps=5043744, episode_reward=1583.05 +/- 655.69
Episode length: 799.40 +/- 111.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5043744  |
---------------------------------
Eval num_timesteps=5045736, episode_reward=1359.88 +/- 402.99
Episode length: 729.60 +/- 70.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5045736  |
---------------------------------
Eval num_timesteps=5047728, episode_reward=1355.06 +/- 535.49
Episode length: 738.60 +/- 217.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5047728  |
---------------------------------
Eval num_timesteps=5049720, episode_reward=1521.54 +/- 593.79
Episode length: 689.00 +/- 129.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 5049720  |
---------------------------------
Eval num_timesteps=5051712, episode_reward=1540.82 +/- 653.49
Episode length: 725.60 +/- 72.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 5051712  |
---------------------------------
Eval num_timesteps=5053704, episode_reward=967.10 +/- 452.16
Episode length: 614.80 +/- 116.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 5053704  |
---------------------------------
Eval num_timesteps=5055696, episode_reward=1440.00 +/- 491.46
Episode length: 692.40 +/- 184.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5055696  |
---------------------------------
Eval num_timesteps=5057688, episode_reward=969.86 +/- 243.28
Episode length: 631.00 +/- 43.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 5057688  |
---------------------------------
Eval num_timesteps=5059680, episode_reward=834.48 +/- 328.96
Episode length: 674.20 +/- 194.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 5059680  |
---------------------------------
Eval num_timesteps=5061672, episode_reward=1275.55 +/- 711.89
Episode length: 707.80 +/- 163.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 5061672  |
---------------------------------
Eval num_timesteps=5063664, episode_reward=1461.21 +/- 604.01
Episode length: 640.80 +/- 44.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 641          |
|    mean_reward          | 1.46e+03     |
| time/                   |              |
|    total_timesteps      | 5063664      |
| train/                  |              |
|    approx_kl            | 0.0056412495 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0415      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 1.62         |
|    value_loss           | 0.0754       |
------------------------------------------
Eval num_timesteps=5065656, episode_reward=1318.96 +/- 445.49
Episode length: 699.60 +/- 123.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5065656  |
---------------------------------
Eval num_timesteps=5067648, episode_reward=1486.91 +/- 306.00
Episode length: 712.40 +/- 150.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 5067648  |
---------------------------------
Eval num_timesteps=5069640, episode_reward=1072.21 +/- 384.20
Episode length: 826.20 +/- 252.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 5069640  |
---------------------------------
Eval num_timesteps=5071632, episode_reward=1511.69 +/- 431.22
Episode length: 640.80 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 5071632  |
---------------------------------
Eval num_timesteps=5073624, episode_reward=1419.02 +/- 294.20
Episode length: 706.20 +/- 107.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 5073624  |
---------------------------------
Eval num_timesteps=5075616, episode_reward=1480.03 +/- 443.98
Episode length: 617.20 +/- 79.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5075616  |
---------------------------------
Eval num_timesteps=5077608, episode_reward=1594.76 +/- 631.04
Episode length: 780.20 +/- 131.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 5077608  |
---------------------------------
Eval num_timesteps=5079600, episode_reward=1442.31 +/- 475.45
Episode length: 906.40 +/- 214.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5079600  |
---------------------------------
Eval num_timesteps=5081592, episode_reward=1653.95 +/- 508.86
Episode length: 668.60 +/- 101.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 5081592  |
---------------------------------
Eval num_timesteps=5083584, episode_reward=957.68 +/- 170.16
Episode length: 708.80 +/- 127.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 5083584  |
---------------------------------
Eval num_timesteps=5085576, episode_reward=1817.44 +/- 379.14
Episode length: 892.60 +/- 205.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 5085576  |
---------------------------------
Eval num_timesteps=5087568, episode_reward=1514.79 +/- 407.26
Episode length: 644.00 +/- 99.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 5087568  |
---------------------------------
Eval num_timesteps=5089560, episode_reward=1370.34 +/- 388.78
Episode length: 757.20 +/- 87.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5089560  |
---------------------------------
Eval num_timesteps=5091552, episode_reward=1431.15 +/- 725.12
Episode length: 729.80 +/- 168.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5091552  |
---------------------------------
Eval num_timesteps=5093544, episode_reward=1407.26 +/- 545.74
Episode length: 660.80 +/- 82.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5093544  |
---------------------------------
Eval num_timesteps=5095536, episode_reward=1877.41 +/- 568.82
Episode length: 926.80 +/- 183.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 5095536  |
---------------------------------
Eval num_timesteps=5097528, episode_reward=1518.81 +/- 669.16
Episode length: 654.40 +/- 64.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 5097528  |
---------------------------------
Eval num_timesteps=5099520, episode_reward=1210.24 +/- 554.48
Episode length: 748.40 +/- 192.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 5099520  |
---------------------------------
Eval num_timesteps=5101512, episode_reward=1541.65 +/- 422.79
Episode length: 723.20 +/- 93.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 5101512  |
---------------------------------
Eval num_timesteps=5103504, episode_reward=1377.63 +/- 747.67
Episode length: 700.60 +/- 204.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5103504  |
---------------------------------
Eval num_timesteps=5105496, episode_reward=2222.34 +/- 710.44
Episode length: 828.60 +/- 119.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 5105496  |
---------------------------------
New best mean reward!
Eval num_timesteps=5107488, episode_reward=925.67 +/- 361.00
Episode length: 764.60 +/- 118.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 5107488  |
---------------------------------
Eval num_timesteps=5109480, episode_reward=1871.62 +/- 335.40
Episode length: 792.60 +/- 206.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 5109480  |
---------------------------------
Eval num_timesteps=5111472, episode_reward=1338.52 +/- 406.99
Episode length: 714.00 +/- 120.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5111472  |
---------------------------------
Eval num_timesteps=5113464, episode_reward=934.52 +/- 360.77
Episode length: 687.00 +/- 49.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 687          |
|    mean_reward          | 935          |
| time/                   |              |
|    total_timesteps      | 5113464      |
| train/                  |              |
|    approx_kl            | 0.0053760037 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0416      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.62         |
|    value_loss           | 0.0759       |
------------------------------------------
Eval num_timesteps=5115456, episode_reward=986.37 +/- 247.64
Episode length: 674.80 +/- 34.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 5115456  |
---------------------------------
Eval num_timesteps=5117448, episode_reward=1221.65 +/- 547.30
Episode length: 805.40 +/- 94.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5117448  |
---------------------------------
Eval num_timesteps=5119440, episode_reward=1382.00 +/- 677.16
Episode length: 777.20 +/- 122.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5119440  |
---------------------------------
Eval num_timesteps=5121432, episode_reward=952.35 +/- 601.24
Episode length: 667.80 +/- 110.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 5121432  |
---------------------------------
Eval num_timesteps=5123424, episode_reward=1476.36 +/- 423.53
Episode length: 621.40 +/- 74.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5123424  |
---------------------------------
Eval num_timesteps=5125416, episode_reward=1015.39 +/- 374.88
Episode length: 628.80 +/- 99.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 5125416  |
---------------------------------
Eval num_timesteps=5127408, episode_reward=1367.44 +/- 374.11
Episode length: 692.20 +/- 82.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5127408  |
---------------------------------
Eval num_timesteps=5129400, episode_reward=1356.36 +/- 666.40
Episode length: 790.20 +/- 266.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5129400  |
---------------------------------
Eval num_timesteps=5131392, episode_reward=1340.03 +/- 722.91
Episode length: 742.00 +/- 129.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5131392  |
---------------------------------
Eval num_timesteps=5133384, episode_reward=1580.06 +/- 481.88
Episode length: 738.20 +/- 67.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5133384  |
---------------------------------
Eval num_timesteps=5135376, episode_reward=1082.57 +/- 674.13
Episode length: 691.40 +/- 244.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5135376  |
---------------------------------
Eval num_timesteps=5137368, episode_reward=1316.63 +/- 758.21
Episode length: 640.20 +/- 208.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5137368  |
---------------------------------
Eval num_timesteps=5139360, episode_reward=1043.67 +/- 324.36
Episode length: 669.00 +/- 59.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5139360  |
---------------------------------
Eval num_timesteps=5141352, episode_reward=757.62 +/- 377.44
Episode length: 531.60 +/- 142.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 5141352  |
---------------------------------
Eval num_timesteps=5143344, episode_reward=1055.08 +/- 409.59
Episode length: 686.20 +/- 103.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 5143344  |
---------------------------------
Eval num_timesteps=5145336, episode_reward=1566.13 +/- 636.97
Episode length: 812.00 +/- 140.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 5145336  |
---------------------------------
Eval num_timesteps=5147328, episode_reward=904.30 +/- 404.56
Episode length: 605.40 +/- 90.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 5147328  |
---------------------------------
Eval num_timesteps=5149320, episode_reward=1127.34 +/- 487.26
Episode length: 617.60 +/- 54.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 5149320  |
---------------------------------
Eval num_timesteps=5151312, episode_reward=1416.41 +/- 505.30
Episode length: 721.20 +/- 69.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 5151312  |
---------------------------------
Eval num_timesteps=5153304, episode_reward=1241.63 +/- 748.27
Episode length: 571.60 +/- 119.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 5153304  |
---------------------------------
Eval num_timesteps=5155296, episode_reward=828.33 +/- 376.37
Episode length: 702.80 +/- 189.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 5155296  |
---------------------------------
Eval num_timesteps=5157288, episode_reward=962.94 +/- 281.21
Episode length: 635.80 +/- 78.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 5157288  |
---------------------------------
Eval num_timesteps=5159280, episode_reward=1447.60 +/- 531.14
Episode length: 765.00 +/- 78.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 5159280  |
---------------------------------
Eval num_timesteps=5161272, episode_reward=1283.35 +/- 538.74
Episode length: 676.60 +/- 30.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 677         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 5161272     |
| train/                  |             |
|    approx_kl            | 0.006953313 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0395     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00148    |
|    std                  | 1.63        |
|    value_loss           | 0.0802      |
-----------------------------------------
Eval num_timesteps=5163264, episode_reward=1093.20 +/- 625.84
Episode length: 718.20 +/- 106.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5163264  |
---------------------------------
Eval num_timesteps=5165256, episode_reward=1325.54 +/- 469.04
Episode length: 615.40 +/- 50.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 5165256  |
---------------------------------
Eval num_timesteps=5167248, episode_reward=787.81 +/- 22.47
Episode length: 780.00 +/- 180.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 5167248  |
---------------------------------
Eval num_timesteps=5169240, episode_reward=866.39 +/- 285.37
Episode length: 646.80 +/- 53.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 5169240  |
---------------------------------
Eval num_timesteps=5171232, episode_reward=918.19 +/- 392.38
Episode length: 622.00 +/- 60.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 5171232  |
---------------------------------
Eval num_timesteps=5173224, episode_reward=1120.48 +/- 415.89
Episode length: 682.00 +/- 126.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 5173224  |
---------------------------------
Eval num_timesteps=5175216, episode_reward=1261.06 +/- 587.44
Episode length: 626.00 +/- 79.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5175216  |
---------------------------------
Eval num_timesteps=5177208, episode_reward=752.51 +/- 66.94
Episode length: 593.60 +/- 49.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 5177208  |
---------------------------------
Eval num_timesteps=5179200, episode_reward=1040.53 +/- 283.07
Episode length: 671.40 +/- 65.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5179200  |
---------------------------------
Eval num_timesteps=5181192, episode_reward=1025.60 +/- 400.53
Episode length: 638.00 +/- 56.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 5181192  |
---------------------------------
Eval num_timesteps=5183184, episode_reward=1017.24 +/- 485.51
Episode length: 768.80 +/- 118.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 5183184  |
---------------------------------
Eval num_timesteps=5185176, episode_reward=1106.53 +/- 513.95
Episode length: 736.40 +/- 108.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5185176  |
---------------------------------
Eval num_timesteps=5187168, episode_reward=987.04 +/- 147.69
Episode length: 660.80 +/- 114.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 5187168  |
---------------------------------
Eval num_timesteps=5189160, episode_reward=1023.97 +/- 407.97
Episode length: 607.80 +/- 103.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 5189160  |
---------------------------------
Eval num_timesteps=5191152, episode_reward=1186.35 +/- 455.32
Episode length: 684.00 +/- 117.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 5191152  |
---------------------------------
Eval num_timesteps=5193144, episode_reward=940.80 +/- 129.58
Episode length: 749.20 +/- 128.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 5193144  |
---------------------------------
Eval num_timesteps=5195136, episode_reward=985.90 +/- 329.59
Episode length: 634.60 +/- 75.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 5195136  |
---------------------------------
Eval num_timesteps=5197128, episode_reward=801.98 +/- 89.86
Episode length: 678.60 +/- 152.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 5197128  |
---------------------------------
Eval num_timesteps=5199120, episode_reward=1022.89 +/- 539.03
Episode length: 667.80 +/- 96.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 5199120  |
---------------------------------
Eval num_timesteps=5201112, episode_reward=940.77 +/- 308.32
Episode length: 644.80 +/- 62.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 5201112  |
---------------------------------
Eval num_timesteps=5203104, episode_reward=1127.25 +/- 455.52
Episode length: 718.20 +/- 93.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 5203104  |
---------------------------------
Eval num_timesteps=5205096, episode_reward=1160.24 +/- 480.74
Episode length: 776.80 +/- 208.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 5205096  |
---------------------------------
Eval num_timesteps=5207088, episode_reward=1181.07 +/- 617.61
Episode length: 717.20 +/- 139.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 5207088  |
---------------------------------
Eval num_timesteps=5209080, episode_reward=1003.50 +/- 427.92
Episode length: 780.00 +/- 121.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 5209080  |
---------------------------------
Eval num_timesteps=5211072, episode_reward=841.83 +/- 105.41
Episode length: 672.60 +/- 85.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 673          |
|    mean_reward          | 842          |
| time/                   |              |
|    total_timesteps      | 5211072      |
| train/                  |              |
|    approx_kl            | 0.0055960864 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0453      |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.000976    |
|    std                  | 1.64         |
|    value_loss           | 0.0685       |
------------------------------------------
Eval num_timesteps=5213064, episode_reward=793.36 +/- 107.44
Episode length: 774.60 +/- 108.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 5213064  |
---------------------------------
Eval num_timesteps=5215056, episode_reward=821.65 +/- 127.09
Episode length: 738.00 +/- 78.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 5215056  |
---------------------------------
Eval num_timesteps=5217048, episode_reward=835.79 +/- 320.27
Episode length: 657.60 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 5217048  |
---------------------------------
Eval num_timesteps=5219040, episode_reward=971.19 +/- 264.06
Episode length: 762.80 +/- 145.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 5219040  |
---------------------------------
Eval num_timesteps=5221032, episode_reward=1104.64 +/- 586.88
Episode length: 733.00 +/- 181.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 5221032  |
---------------------------------
Eval num_timesteps=5223024, episode_reward=939.02 +/- 397.66
Episode length: 673.20 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 5223024  |
---------------------------------
Eval num_timesteps=5225016, episode_reward=1077.51 +/- 519.20
Episode length: 663.60 +/- 85.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5225016  |
---------------------------------
Eval num_timesteps=5227008, episode_reward=802.83 +/- 100.61
Episode length: 731.60 +/- 105.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 5227008  |
---------------------------------
Eval num_timesteps=5229000, episode_reward=1077.83 +/- 486.28
Episode length: 692.80 +/- 66.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5229000  |
---------------------------------
Eval num_timesteps=5230992, episode_reward=880.95 +/- 182.23
Episode length: 774.20 +/- 118.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 5230992  |
---------------------------------
Eval num_timesteps=5232984, episode_reward=993.57 +/- 553.61
Episode length: 854.80 +/- 195.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 5232984  |
---------------------------------
Eval num_timesteps=5234976, episode_reward=807.87 +/- 88.43
Episode length: 676.20 +/- 89.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 5234976  |
---------------------------------
Eval num_timesteps=5236968, episode_reward=933.86 +/- 175.31
Episode length: 805.00 +/- 145.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 5236968  |
---------------------------------
Eval num_timesteps=5238960, episode_reward=745.11 +/- 41.57
Episode length: 710.80 +/- 132.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 5238960  |
---------------------------------
Eval num_timesteps=5240952, episode_reward=966.36 +/- 263.92
Episode length: 688.20 +/- 125.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 5240952  |
---------------------------------
Eval num_timesteps=5242944, episode_reward=775.61 +/- 82.18
Episode length: 677.20 +/- 61.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 5242944  |
---------------------------------
Eval num_timesteps=5244936, episode_reward=1110.68 +/- 568.07
Episode length: 717.60 +/- 57.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5244936  |
---------------------------------
Eval num_timesteps=5246928, episode_reward=984.77 +/- 422.03
Episode length: 720.00 +/- 158.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 985      |
| time/              |          |
|    total_timesteps | 5246928  |
---------------------------------
Eval num_timesteps=5248920, episode_reward=1003.02 +/- 484.59
Episode length: 827.60 +/- 118.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 5248920  |
---------------------------------
Eval num_timesteps=5250912, episode_reward=1427.53 +/- 877.61
Episode length: 777.60 +/- 172.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5250912  |
---------------------------------
Eval num_timesteps=5252904, episode_reward=858.14 +/- 188.47
Episode length: 741.00 +/- 154.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 5252904  |
---------------------------------
Eval num_timesteps=5254896, episode_reward=774.97 +/- 108.30
Episode length: 682.20 +/- 60.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 5254896  |
---------------------------------
Eval num_timesteps=5256888, episode_reward=836.72 +/- 161.94
Episode length: 701.40 +/- 103.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 5256888  |
---------------------------------
Eval num_timesteps=5258880, episode_reward=1406.51 +/- 1066.93
Episode length: 758.00 +/- 157.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5258880  |
---------------------------------
Eval num_timesteps=5260872, episode_reward=836.44 +/- 60.26
Episode length: 709.80 +/- 158.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 710          |
|    mean_reward          | 836          |
| time/                   |              |
|    total_timesteps      | 5260872      |
| train/                  |              |
|    approx_kl            | 0.0040099802 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | -0.055       |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.65         |
|    value_loss           | 0.0514       |
------------------------------------------
Eval num_timesteps=5262864, episode_reward=714.97 +/- 91.49
Episode length: 720.80 +/- 98.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 5262864  |
---------------------------------
Eval num_timesteps=5264856, episode_reward=911.05 +/- 534.55
Episode length: 658.00 +/- 60.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 5264856  |
---------------------------------
Eval num_timesteps=5266848, episode_reward=827.77 +/- 167.92
Episode length: 679.20 +/- 109.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 5266848  |
---------------------------------
Eval num_timesteps=5268840, episode_reward=719.70 +/- 69.52
Episode length: 682.00 +/- 48.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 5268840  |
---------------------------------
Eval num_timesteps=5270832, episode_reward=760.40 +/- 150.52
Episode length: 786.80 +/- 162.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 5270832  |
---------------------------------
Eval num_timesteps=5272824, episode_reward=779.66 +/- 100.10
Episode length: 841.20 +/- 123.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 5272824  |
---------------------------------
Eval num_timesteps=5274816, episode_reward=787.16 +/- 109.12
Episode length: 673.60 +/- 57.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 5274816  |
---------------------------------
Eval num_timesteps=5276808, episode_reward=1011.18 +/- 89.24
Episode length: 698.80 +/- 128.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 5276808  |
---------------------------------
Eval num_timesteps=5278800, episode_reward=683.02 +/- 111.18
Episode length: 628.40 +/- 65.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 5278800  |
---------------------------------
Eval num_timesteps=5280792, episode_reward=803.74 +/- 97.49
Episode length: 747.00 +/- 125.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 5280792  |
---------------------------------
Eval num_timesteps=5282784, episode_reward=789.83 +/- 120.65
Episode length: 621.80 +/- 74.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 5282784  |
---------------------------------
Eval num_timesteps=5284776, episode_reward=789.59 +/- 118.50
Episode length: 718.40 +/- 75.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 5284776  |
---------------------------------
Eval num_timesteps=5286768, episode_reward=740.66 +/- 31.26
Episode length: 770.00 +/- 169.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 5286768  |
---------------------------------
Eval num_timesteps=5288760, episode_reward=945.78 +/- 308.87
Episode length: 704.00 +/- 136.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 5288760  |
---------------------------------
Eval num_timesteps=5290752, episode_reward=1044.17 +/- 689.03
Episode length: 671.40 +/- 40.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5290752  |
---------------------------------
Eval num_timesteps=5292744, episode_reward=680.32 +/- 50.79
Episode length: 678.20 +/- 53.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 5292744  |
---------------------------------
Eval num_timesteps=5294736, episode_reward=767.36 +/- 121.37
Episode length: 732.80 +/- 144.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 5294736  |
---------------------------------
Eval num_timesteps=5296728, episode_reward=807.17 +/- 213.06
Episode length: 717.80 +/- 101.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 5296728  |
---------------------------------
Eval num_timesteps=5298720, episode_reward=1062.71 +/- 343.65
Episode length: 694.20 +/- 130.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 5298720  |
---------------------------------
Eval num_timesteps=5300712, episode_reward=850.48 +/- 55.52
Episode length: 856.80 +/- 141.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 5300712  |
---------------------------------
Eval num_timesteps=5302704, episode_reward=846.35 +/- 160.71
Episode length: 776.60 +/- 80.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 5302704  |
---------------------------------
Eval num_timesteps=5304696, episode_reward=836.00 +/- 365.89
Episode length: 701.80 +/- 70.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 5304696  |
---------------------------------
Eval num_timesteps=5306688, episode_reward=1093.54 +/- 334.83
Episode length: 742.60 +/- 104.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5306688  |
---------------------------------
Eval num_timesteps=5308680, episode_reward=948.90 +/- 213.73
Episode length: 800.40 +/- 138.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 800         |
|    mean_reward          | 949         |
| time/                   |             |
|    total_timesteps      | 5308680     |
| train/                  |             |
|    approx_kl            | 0.005106052 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.69       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0592     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0016     |
|    std                  | 1.67        |
|    value_loss           | 0.0437      |
-----------------------------------------
Eval num_timesteps=5310672, episode_reward=841.73 +/- 105.66
Episode length: 834.80 +/- 96.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 5310672  |
---------------------------------
Eval num_timesteps=5312664, episode_reward=950.18 +/- 202.10
Episode length: 691.00 +/- 61.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 5312664  |
---------------------------------
Eval num_timesteps=5314656, episode_reward=725.42 +/- 65.17
Episode length: 705.60 +/- 109.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 5314656  |
---------------------------------
Eval num_timesteps=5316648, episode_reward=858.32 +/- 189.77
Episode length: 739.40 +/- 122.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 5316648  |
---------------------------------
Eval num_timesteps=5318640, episode_reward=778.49 +/- 105.86
Episode length: 891.00 +/- 154.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 5318640  |
---------------------------------
Eval num_timesteps=5320632, episode_reward=871.65 +/- 250.89
Episode length: 777.20 +/- 123.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 5320632  |
---------------------------------
Eval num_timesteps=5322624, episode_reward=728.34 +/- 74.03
Episode length: 656.80 +/- 53.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 5322624  |
---------------------------------
Eval num_timesteps=5324616, episode_reward=733.34 +/- 95.77
Episode length: 697.60 +/- 52.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 5324616  |
---------------------------------
Eval num_timesteps=5326608, episode_reward=863.74 +/- 179.13
Episode length: 736.20 +/- 78.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 5326608  |
---------------------------------
Eval num_timesteps=5328600, episode_reward=945.35 +/- 212.55
Episode length: 738.00 +/- 60.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 5328600  |
---------------------------------
Eval num_timesteps=5330592, episode_reward=873.82 +/- 51.20
Episode length: 757.40 +/- 98.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 5330592  |
---------------------------------
Eval num_timesteps=5332584, episode_reward=761.83 +/- 54.77
Episode length: 710.60 +/- 124.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 5332584  |
---------------------------------
Eval num_timesteps=5334576, episode_reward=891.80 +/- 223.34
Episode length: 677.40 +/- 29.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 5334576  |
---------------------------------
Eval num_timesteps=5336568, episode_reward=801.30 +/- 80.56
Episode length: 714.00 +/- 143.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 5336568  |
---------------------------------
Eval num_timesteps=5338560, episode_reward=975.15 +/- 556.19
Episode length: 714.80 +/- 151.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 5338560  |
---------------------------------
Eval num_timesteps=5340552, episode_reward=792.28 +/- 75.82
Episode length: 726.80 +/- 125.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 5340552  |
---------------------------------
Eval num_timesteps=5342544, episode_reward=833.06 +/- 117.58
Episode length: 788.40 +/- 137.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 5342544  |
---------------------------------
Eval num_timesteps=5344536, episode_reward=743.36 +/- 71.42
Episode length: 721.40 +/- 74.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 5344536  |
---------------------------------
Eval num_timesteps=5346528, episode_reward=1070.30 +/- 555.70
Episode length: 743.60 +/- 154.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 5346528  |
---------------------------------
Eval num_timesteps=5348520, episode_reward=805.00 +/- 215.68
Episode length: 693.00 +/- 110.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 5348520  |
---------------------------------
Eval num_timesteps=5350512, episode_reward=960.35 +/- 478.69
Episode length: 862.40 +/- 108.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 5350512  |
---------------------------------
Eval num_timesteps=5352504, episode_reward=786.82 +/- 107.62
Episode length: 744.40 +/- 244.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 5352504  |
---------------------------------
Eval num_timesteps=5354496, episode_reward=718.04 +/- 68.75
Episode length: 646.00 +/- 24.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 5354496  |
---------------------------------
Eval num_timesteps=5356488, episode_reward=951.76 +/- 287.61
Episode length: 813.60 +/- 172.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 5356488  |
---------------------------------
Eval num_timesteps=5358480, episode_reward=894.35 +/- 308.42
Episode length: 795.00 +/- 150.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 795          |
|    mean_reward          | 894          |
| time/                   |              |
|    total_timesteps      | 5358480      |
| train/                  |              |
|    approx_kl            | 0.0067310347 |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0573      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.69         |
|    value_loss           | 0.047        |
------------------------------------------
Eval num_timesteps=5360472, episode_reward=1141.91 +/- 507.69
Episode length: 829.40 +/- 134.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 5360472  |
---------------------------------
Eval num_timesteps=5362464, episode_reward=1353.68 +/- 603.59
Episode length: 742.60 +/- 34.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 5362464  |
---------------------------------
Eval num_timesteps=5364456, episode_reward=721.65 +/- 43.55
Episode length: 881.20 +/- 90.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 5364456  |
---------------------------------
Eval num_timesteps=5366448, episode_reward=825.61 +/- 182.23
Episode length: 732.20 +/- 131.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 5366448  |
---------------------------------
Eval num_timesteps=5368440, episode_reward=875.29 +/- 247.96
Episode length: 791.00 +/- 143.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 5368440  |
---------------------------------
Eval num_timesteps=5370432, episode_reward=822.12 +/- 136.43
Episode length: 825.20 +/- 99.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 5370432  |
---------------------------------
Eval num_timesteps=5372424, episode_reward=908.20 +/- 409.80
Episode length: 862.60 +/- 168.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 5372424  |
---------------------------------
Eval num_timesteps=5374416, episode_reward=1014.51 +/- 298.04
Episode length: 822.40 +/- 217.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 5374416  |
---------------------------------
Eval num_timesteps=5376408, episode_reward=900.39 +/- 110.11
Episode length: 816.20 +/- 130.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 5376408  |
---------------------------------
Eval num_timesteps=5378400, episode_reward=816.68 +/- 277.04
Episode length: 971.00 +/- 77.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 5378400  |
---------------------------------
Eval num_timesteps=5380392, episode_reward=1223.07 +/- 911.55
Episode length: 756.60 +/- 117.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5380392  |
---------------------------------
Eval num_timesteps=5382384, episode_reward=1158.08 +/- 553.39
Episode length: 893.60 +/- 98.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 5382384  |
---------------------------------
Eval num_timesteps=5384376, episode_reward=780.31 +/- 106.65
Episode length: 697.80 +/- 117.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 5384376  |
---------------------------------
Eval num_timesteps=5386368, episode_reward=853.21 +/- 160.89
Episode length: 756.60 +/- 224.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 5386368  |
---------------------------------
Eval num_timesteps=5388360, episode_reward=885.02 +/- 151.37
Episode length: 860.60 +/- 101.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 5388360  |
---------------------------------
Eval num_timesteps=5390352, episode_reward=992.13 +/- 298.88
Episode length: 829.40 +/- 266.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 5390352  |
---------------------------------
Eval num_timesteps=5392344, episode_reward=839.47 +/- 139.76
Episode length: 853.40 +/- 141.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 5392344  |
---------------------------------
Eval num_timesteps=5394336, episode_reward=721.31 +/- 81.72
Episode length: 756.40 +/- 170.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 5394336  |
---------------------------------
Eval num_timesteps=5396328, episode_reward=979.43 +/- 303.54
Episode length: 882.20 +/- 91.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 5396328  |
---------------------------------
Eval num_timesteps=5398320, episode_reward=738.36 +/- 384.46
Episode length: 804.00 +/- 140.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 5398320  |
---------------------------------
Eval num_timesteps=5400312, episode_reward=1040.01 +/- 431.22
Episode length: 827.20 +/- 118.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5400312  |
---------------------------------
Eval num_timesteps=5402304, episode_reward=859.93 +/- 182.11
Episode length: 665.80 +/- 64.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 5402304  |
---------------------------------
Eval num_timesteps=5404296, episode_reward=832.06 +/- 237.91
Episode length: 825.00 +/- 123.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 5404296  |
---------------------------------
Eval num_timesteps=5406288, episode_reward=1044.38 +/- 320.85
Episode length: 1024.00 +/- 370.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5406288  |
---------------------------------
Eval num_timesteps=5408280, episode_reward=1098.81 +/- 417.57
Episode length: 767.20 +/- 82.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 767         |
|    mean_reward          | 1.1e+03     |
| time/                   |             |
|    total_timesteps      | 5408280     |
| train/                  |             |
|    approx_kl            | 0.005447989 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0555     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.7         |
|    value_loss           | 0.0538      |
-----------------------------------------
Eval num_timesteps=5410272, episode_reward=1162.66 +/- 233.71
Episode length: 850.80 +/- 82.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 5410272  |
---------------------------------
Eval num_timesteps=5412264, episode_reward=928.15 +/- 222.64
Episode length: 886.00 +/- 131.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 5412264  |
---------------------------------
Eval num_timesteps=5414256, episode_reward=1050.03 +/- 384.51
Episode length: 857.20 +/- 117.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 5414256  |
---------------------------------
Eval num_timesteps=5416248, episode_reward=1248.39 +/- 500.36
Episode length: 788.40 +/- 39.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 5416248  |
---------------------------------
Eval num_timesteps=5418240, episode_reward=1263.05 +/- 532.02
Episode length: 769.80 +/- 120.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5418240  |
---------------------------------
Eval num_timesteps=5420232, episode_reward=861.49 +/- 111.65
Episode length: 726.60 +/- 71.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 5420232  |
---------------------------------
Eval num_timesteps=5422224, episode_reward=703.68 +/- 62.68
Episode length: 787.00 +/- 85.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 5422224  |
---------------------------------
Eval num_timesteps=5424216, episode_reward=902.03 +/- 142.78
Episode length: 869.00 +/- 108.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 5424216  |
---------------------------------
Eval num_timesteps=5426208, episode_reward=932.71 +/- 388.27
Episode length: 887.40 +/- 94.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 5426208  |
---------------------------------
Eval num_timesteps=5428200, episode_reward=941.76 +/- 190.43
Episode length: 887.20 +/- 131.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 5428200  |
---------------------------------
Eval num_timesteps=5430192, episode_reward=1004.88 +/- 351.34
Episode length: 883.60 +/- 156.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 5430192  |
---------------------------------
Eval num_timesteps=5432184, episode_reward=1032.62 +/- 127.63
Episode length: 793.00 +/- 76.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 5432184  |
---------------------------------
Eval num_timesteps=5434176, episode_reward=1086.79 +/- 356.42
Episode length: 866.60 +/- 163.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5434176  |
---------------------------------
Eval num_timesteps=5436168, episode_reward=1055.96 +/- 214.96
Episode length: 944.00 +/- 190.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 944      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 5436168  |
---------------------------------
Eval num_timesteps=5438160, episode_reward=1024.83 +/- 346.64
Episode length: 815.00 +/- 93.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 5438160  |
---------------------------------
Eval num_timesteps=5440152, episode_reward=1079.21 +/- 361.40
Episode length: 818.40 +/- 135.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5440152  |
---------------------------------
Eval num_timesteps=5442144, episode_reward=942.45 +/- 338.49
Episode length: 751.20 +/- 93.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 5442144  |
---------------------------------
Eval num_timesteps=5444136, episode_reward=1290.53 +/- 569.79
Episode length: 823.20 +/- 107.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5444136  |
---------------------------------
Eval num_timesteps=5446128, episode_reward=996.11 +/- 383.64
Episode length: 812.60 +/- 156.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 5446128  |
---------------------------------
Eval num_timesteps=5448120, episode_reward=1045.58 +/- 357.90
Episode length: 971.20 +/- 127.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 5448120  |
---------------------------------
Eval num_timesteps=5450112, episode_reward=845.26 +/- 218.52
Episode length: 781.80 +/- 35.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 5450112  |
---------------------------------
Eval num_timesteps=5452104, episode_reward=919.85 +/- 240.15
Episode length: 790.20 +/- 108.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 5452104  |
---------------------------------
Eval num_timesteps=5454096, episode_reward=787.43 +/- 129.91
Episode length: 877.20 +/- 86.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 5454096  |
---------------------------------
Eval num_timesteps=5456088, episode_reward=1296.61 +/- 253.77
Episode length: 829.20 +/- 54.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 829          |
|    mean_reward          | 1.3e+03      |
| time/                   |              |
|    total_timesteps      | 5456088      |
| train/                  |              |
|    approx_kl            | 0.0051497906 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | -0.0563      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.7          |
|    value_loss           | 0.0519       |
------------------------------------------
Eval num_timesteps=5458080, episode_reward=1191.17 +/- 457.76
Episode length: 889.00 +/- 167.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 5458080  |
---------------------------------
Eval num_timesteps=5460072, episode_reward=1248.74 +/- 404.75
Episode length: 776.60 +/- 104.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 5460072  |
---------------------------------
Eval num_timesteps=5462064, episode_reward=1224.93 +/- 315.85
Episode length: 911.60 +/- 162.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5462064  |
---------------------------------
Eval num_timesteps=5464056, episode_reward=1094.92 +/- 276.62
Episode length: 967.60 +/- 206.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5464056  |
---------------------------------
Eval num_timesteps=5466048, episode_reward=916.18 +/- 185.29
Episode length: 676.20 +/- 126.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 5466048  |
---------------------------------
Eval num_timesteps=5468040, episode_reward=1619.21 +/- 573.08
Episode length: 875.00 +/- 165.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 5468040  |
---------------------------------
Eval num_timesteps=5470032, episode_reward=1202.13 +/- 474.50
Episode length: 724.20 +/- 64.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5470032  |
---------------------------------
Eval num_timesteps=5472024, episode_reward=1430.68 +/- 309.62
Episode length: 863.80 +/- 109.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5472024  |
---------------------------------
Eval num_timesteps=5474016, episode_reward=1044.05 +/- 460.89
Episode length: 885.20 +/- 141.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5474016  |
---------------------------------
Eval num_timesteps=5476008, episode_reward=1578.94 +/- 762.33
Episode length: 895.60 +/- 102.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5476008  |
---------------------------------
Eval num_timesteps=5478000, episode_reward=842.46 +/- 124.30
Episode length: 836.20 +/- 101.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 5478000  |
---------------------------------
Eval num_timesteps=5479992, episode_reward=1093.94 +/- 434.42
Episode length: 868.20 +/- 126.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5479992  |
---------------------------------
Eval num_timesteps=5481984, episode_reward=1252.36 +/- 427.79
Episode length: 924.00 +/- 93.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 5481984  |
---------------------------------
Eval num_timesteps=5483976, episode_reward=1169.17 +/- 528.87
Episode length: 829.20 +/- 65.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5483976  |
---------------------------------
Eval num_timesteps=5485968, episode_reward=1253.94 +/- 540.87
Episode length: 879.80 +/- 69.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 5485968  |
---------------------------------
Eval num_timesteps=5487960, episode_reward=1183.42 +/- 546.90
Episode length: 726.80 +/- 73.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 5487960  |
---------------------------------
Eval num_timesteps=5489952, episode_reward=1201.20 +/- 576.64
Episode length: 743.20 +/- 100.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5489952  |
---------------------------------
Eval num_timesteps=5491944, episode_reward=1161.58 +/- 384.27
Episode length: 835.40 +/- 118.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 5491944  |
---------------------------------
Eval num_timesteps=5493936, episode_reward=955.23 +/- 219.41
Episode length: 982.80 +/- 80.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 5493936  |
---------------------------------
Eval num_timesteps=5495928, episode_reward=1063.37 +/- 698.67
Episode length: 1138.80 +/- 229.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 5495928  |
---------------------------------
Eval num_timesteps=5497920, episode_reward=999.29 +/- 98.83
Episode length: 863.60 +/- 93.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 5497920  |
---------------------------------
Eval num_timesteps=5499912, episode_reward=1032.05 +/- 162.79
Episode length: 852.00 +/- 73.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 5499912  |
---------------------------------
Eval num_timesteps=5501904, episode_reward=1230.12 +/- 602.31
Episode length: 891.20 +/- 129.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 5501904  |
---------------------------------
Eval num_timesteps=5503896, episode_reward=1393.12 +/- 659.66
Episode length: 880.80 +/- 140.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5503896  |
---------------------------------
Eval num_timesteps=5505888, episode_reward=1184.68 +/- 405.41
Episode length: 961.40 +/- 93.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 961          |
|    mean_reward          | 1.18e+03     |
| time/                   |              |
|    total_timesteps      | 5505888      |
| train/                  |              |
|    approx_kl            | 0.0053744535 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0522      |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.71         |
|    value_loss           | 0.0608       |
------------------------------------------
Eval num_timesteps=5507880, episode_reward=1586.77 +/- 490.67
Episode length: 892.40 +/- 110.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 5507880  |
---------------------------------
Eval num_timesteps=5509872, episode_reward=1336.90 +/- 236.40
Episode length: 950.20 +/- 166.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5509872  |
---------------------------------
Eval num_timesteps=5511864, episode_reward=1000.07 +/- 264.61
Episode length: 917.60 +/- 62.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 5511864  |
---------------------------------
Eval num_timesteps=5513856, episode_reward=1386.37 +/- 555.90
Episode length: 925.40 +/- 123.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5513856  |
---------------------------------
Eval num_timesteps=5515848, episode_reward=1597.79 +/- 1023.21
Episode length: 882.20 +/- 89.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 5515848  |
---------------------------------
Eval num_timesteps=5517840, episode_reward=1326.06 +/- 457.82
Episode length: 1112.80 +/- 225.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 5517840  |
---------------------------------
Eval num_timesteps=5519832, episode_reward=1114.21 +/- 266.28
Episode length: 956.00 +/- 114.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 956      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5519832  |
---------------------------------
Eval num_timesteps=5521824, episode_reward=1633.50 +/- 1291.41
Episode length: 909.40 +/- 339.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 5521824  |
---------------------------------
Eval num_timesteps=5523816, episode_reward=1174.28 +/- 285.21
Episode length: 879.80 +/- 117.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5523816  |
---------------------------------
Eval num_timesteps=5525808, episode_reward=1238.39 +/- 434.92
Episode length: 962.60 +/- 132.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 5525808  |
---------------------------------
Eval num_timesteps=5527800, episode_reward=1658.50 +/- 672.56
Episode length: 926.60 +/- 111.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 5527800  |
---------------------------------
Eval num_timesteps=5529792, episode_reward=1169.43 +/- 401.23
Episode length: 939.40 +/- 89.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5529792  |
---------------------------------
Eval num_timesteps=5531784, episode_reward=1099.24 +/- 308.10
Episode length: 815.20 +/- 170.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 5531784  |
---------------------------------
Eval num_timesteps=5533776, episode_reward=1710.43 +/- 766.56
Episode length: 1087.00 +/- 90.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 5533776  |
---------------------------------
Eval num_timesteps=5535768, episode_reward=1260.77 +/- 391.60
Episode length: 854.60 +/- 128.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5535768  |
---------------------------------
Eval num_timesteps=5537760, episode_reward=1322.72 +/- 425.98
Episode length: 1024.20 +/- 80.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5537760  |
---------------------------------
Eval num_timesteps=5539752, episode_reward=1638.90 +/- 547.43
Episode length: 937.40 +/- 215.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 5539752  |
---------------------------------
Eval num_timesteps=5541744, episode_reward=1038.56 +/- 331.02
Episode length: 879.40 +/- 156.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5541744  |
---------------------------------
Eval num_timesteps=5543736, episode_reward=1292.39 +/- 494.27
Episode length: 906.20 +/- 112.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5543736  |
---------------------------------
Eval num_timesteps=5545728, episode_reward=1681.81 +/- 605.88
Episode length: 1091.00 +/- 105.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 5545728  |
---------------------------------
Eval num_timesteps=5547720, episode_reward=1221.41 +/- 177.45
Episode length: 811.00 +/- 143.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5547720  |
---------------------------------
Eval num_timesteps=5549712, episode_reward=1173.27 +/- 327.80
Episode length: 953.80 +/- 268.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5549712  |
---------------------------------
Eval num_timesteps=5551704, episode_reward=1314.10 +/- 456.85
Episode length: 940.80 +/- 93.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5551704  |
---------------------------------
Eval num_timesteps=5553696, episode_reward=1143.93 +/- 318.41
Episode length: 955.00 +/- 102.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 5553696  |
---------------------------------
Eval num_timesteps=5555688, episode_reward=1330.17 +/- 415.56
Episode length: 836.40 +/- 76.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 836         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 5555688     |
| train/                  |             |
|    approx_kl            | 0.006227413 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | -0.053      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 1.71        |
|    value_loss           | 0.0593      |
-----------------------------------------
Eval num_timesteps=5557680, episode_reward=1041.29 +/- 133.10
Episode length: 905.60 +/- 101.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 5557680  |
---------------------------------
Eval num_timesteps=5559672, episode_reward=1723.33 +/- 136.99
Episode length: 917.00 +/- 142.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 917      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 5559672  |
---------------------------------
Eval num_timesteps=5561664, episode_reward=980.30 +/- 530.58
Episode length: 718.60 +/- 277.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 5561664  |
---------------------------------
Eval num_timesteps=5563656, episode_reward=1469.71 +/- 453.50
Episode length: 962.80 +/- 182.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 5563656  |
---------------------------------
Eval num_timesteps=5565648, episode_reward=1953.47 +/- 960.32
Episode length: 1005.60 +/- 286.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 5565648  |
---------------------------------
Eval num_timesteps=5567640, episode_reward=1408.39 +/- 416.48
Episode length: 828.80 +/- 134.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5567640  |
---------------------------------
Eval num_timesteps=5569632, episode_reward=1593.46 +/- 512.07
Episode length: 1019.60 +/- 222.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 5569632  |
---------------------------------
Eval num_timesteps=5571624, episode_reward=1594.30 +/- 641.48
Episode length: 1108.20 +/- 129.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 5571624  |
---------------------------------
Eval num_timesteps=5573616, episode_reward=1464.17 +/- 345.40
Episode length: 1046.60 +/- 133.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 5573616  |
---------------------------------
Eval num_timesteps=5575608, episode_reward=1556.38 +/- 480.79
Episode length: 936.80 +/- 137.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 5575608  |
---------------------------------
Eval num_timesteps=5577600, episode_reward=1030.40 +/- 730.93
Episode length: 748.00 +/- 352.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 5577600  |
---------------------------------
Eval num_timesteps=5579592, episode_reward=1441.91 +/- 456.89
Episode length: 1091.60 +/- 140.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5579592  |
---------------------------------
Eval num_timesteps=5581584, episode_reward=1776.85 +/- 207.31
Episode length: 1062.00 +/- 91.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 5581584  |
---------------------------------
Eval num_timesteps=5583576, episode_reward=1278.04 +/- 387.40
Episode length: 844.20 +/- 165.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 5583576  |
---------------------------------
Eval num_timesteps=5585568, episode_reward=1390.60 +/- 658.82
Episode length: 844.00 +/- 154.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5585568  |
---------------------------------
Eval num_timesteps=5587560, episode_reward=1910.34 +/- 530.46
Episode length: 897.00 +/- 76.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 5587560  |
---------------------------------
Eval num_timesteps=5589552, episode_reward=1120.86 +/- 378.85
Episode length: 891.40 +/- 159.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 5589552  |
---------------------------------
Eval num_timesteps=5591544, episode_reward=840.87 +/- 293.76
Episode length: 893.40 +/- 174.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 5591544  |
---------------------------------
Eval num_timesteps=5593536, episode_reward=1311.77 +/- 173.92
Episode length: 822.60 +/- 118.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5593536  |
---------------------------------
Eval num_timesteps=5595528, episode_reward=1553.36 +/- 1018.21
Episode length: 922.60 +/- 236.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 5595528  |
---------------------------------
Eval num_timesteps=5597520, episode_reward=1451.45 +/- 330.37
Episode length: 1109.80 +/- 63.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 5597520  |
---------------------------------
Eval num_timesteps=5599512, episode_reward=1105.25 +/- 337.73
Episode length: 1034.20 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5599512  |
---------------------------------
Eval num_timesteps=5601504, episode_reward=1362.87 +/- 528.18
Episode length: 939.60 +/- 198.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5601504  |
---------------------------------
Eval num_timesteps=5603496, episode_reward=1795.22 +/- 298.16
Episode length: 1093.80 +/- 47.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.09e+03     |
|    mean_reward          | 1.8e+03      |
| time/                   |              |
|    total_timesteps      | 5603496      |
| train/                  |              |
|    approx_kl            | 0.0055905012 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.83        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0541      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.72         |
|    value_loss           | 0.0584       |
------------------------------------------
Eval num_timesteps=5605488, episode_reward=1319.41 +/- 188.44
Episode length: 1144.80 +/- 121.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5605488  |
---------------------------------
Eval num_timesteps=5607480, episode_reward=1414.50 +/- 250.02
Episode length: 1045.80 +/- 134.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 5607480  |
---------------------------------
Eval num_timesteps=5609472, episode_reward=1386.41 +/- 371.31
Episode length: 862.60 +/- 152.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5609472  |
---------------------------------
Eval num_timesteps=5611464, episode_reward=1604.38 +/- 409.18
Episode length: 967.60 +/- 150.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 5611464  |
---------------------------------
Eval num_timesteps=5613456, episode_reward=1346.29 +/- 559.14
Episode length: 1072.80 +/- 95.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 5613456  |
---------------------------------
Eval num_timesteps=5615448, episode_reward=1520.08 +/- 394.91
Episode length: 876.40 +/- 120.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 5615448  |
---------------------------------
Eval num_timesteps=5617440, episode_reward=1220.36 +/- 694.70
Episode length: 753.00 +/- 233.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5617440  |
---------------------------------
Eval num_timesteps=5619432, episode_reward=1760.55 +/- 585.90
Episode length: 925.80 +/- 158.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 5619432  |
---------------------------------
Eval num_timesteps=5621424, episode_reward=1340.79 +/- 305.89
Episode length: 828.60 +/- 203.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5621424  |
---------------------------------
Eval num_timesteps=5623416, episode_reward=1864.32 +/- 487.32
Episode length: 982.00 +/- 73.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 5623416  |
---------------------------------
Eval num_timesteps=5625408, episode_reward=1447.36 +/- 279.99
Episode length: 933.00 +/- 147.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 5625408  |
---------------------------------
Eval num_timesteps=5627400, episode_reward=1780.66 +/- 443.90
Episode length: 1059.80 +/- 196.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 5627400  |
---------------------------------
Eval num_timesteps=5629392, episode_reward=2360.29 +/- 845.63
Episode length: 1152.20 +/- 130.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 5629392  |
---------------------------------
New best mean reward!
Eval num_timesteps=5631384, episode_reward=1087.85 +/- 335.30
Episode length: 885.40 +/- 284.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 5631384  |
---------------------------------
Eval num_timesteps=5633376, episode_reward=1580.51 +/- 385.34
Episode length: 1076.00 +/- 154.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5633376  |
---------------------------------
Eval num_timesteps=5635368, episode_reward=1384.15 +/- 869.47
Episode length: 900.00 +/- 297.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5635368  |
---------------------------------
Eval num_timesteps=5637360, episode_reward=1384.03 +/- 432.78
Episode length: 931.00 +/- 214.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5637360  |
---------------------------------
Eval num_timesteps=5639352, episode_reward=976.15 +/- 331.98
Episode length: 742.20 +/- 129.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 5639352  |
---------------------------------
Eval num_timesteps=5641344, episode_reward=1451.47 +/- 354.61
Episode length: 968.60 +/- 162.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 5641344  |
---------------------------------
Eval num_timesteps=5643336, episode_reward=1384.99 +/- 872.61
Episode length: 853.20 +/- 176.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5643336  |
---------------------------------
Eval num_timesteps=5645328, episode_reward=1548.37 +/- 349.91
Episode length: 1072.40 +/- 267.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 5645328  |
---------------------------------
Eval num_timesteps=5647320, episode_reward=1272.86 +/- 467.12
Episode length: 939.20 +/- 160.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5647320  |
---------------------------------
Eval num_timesteps=5649312, episode_reward=1602.55 +/- 362.79
Episode length: 1061.20 +/- 183.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 5649312  |
---------------------------------
Eval num_timesteps=5651304, episode_reward=1783.67 +/- 474.02
Episode length: 951.00 +/- 229.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 5651304  |
---------------------------------
Eval num_timesteps=5653296, episode_reward=1788.42 +/- 757.44
Episode length: 1106.80 +/- 185.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.11e+03     |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 5653296      |
| train/                  |              |
|    approx_kl            | 0.0050258907 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.86        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0497      |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.74         |
|    value_loss           | 0.0661       |
------------------------------------------
Eval num_timesteps=5655288, episode_reward=2018.70 +/- 933.31
Episode length: 1070.20 +/- 192.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 5655288  |
---------------------------------
Eval num_timesteps=5657280, episode_reward=1385.62 +/- 372.97
Episode length: 1109.00 +/- 122.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5657280  |
---------------------------------
Eval num_timesteps=5659272, episode_reward=1711.40 +/- 184.34
Episode length: 1017.20 +/- 175.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 5659272  |
---------------------------------
Eval num_timesteps=5661264, episode_reward=1503.74 +/- 510.45
Episode length: 1109.40 +/- 154.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 5661264  |
---------------------------------
Eval num_timesteps=5663256, episode_reward=1722.00 +/- 1309.96
Episode length: 894.00 +/- 291.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 5663256  |
---------------------------------
Eval num_timesteps=5665248, episode_reward=1080.19 +/- 623.23
Episode length: 869.20 +/- 306.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5665248  |
---------------------------------
Eval num_timesteps=5667240, episode_reward=2080.60 +/- 375.34
Episode length: 1120.20 +/- 196.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 5667240  |
---------------------------------
Eval num_timesteps=5669232, episode_reward=1940.45 +/- 371.12
Episode length: 1020.40 +/- 87.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 5669232  |
---------------------------------
Eval num_timesteps=5671224, episode_reward=949.63 +/- 576.45
Episode length: 715.00 +/- 208.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 5671224  |
---------------------------------
Eval num_timesteps=5673216, episode_reward=1186.28 +/- 853.41
Episode length: 857.60 +/- 351.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 5673216  |
---------------------------------
Eval num_timesteps=5675208, episode_reward=1815.31 +/- 504.06
Episode length: 1058.20 +/- 273.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 5675208  |
---------------------------------
Eval num_timesteps=5677200, episode_reward=1080.14 +/- 909.87
Episode length: 783.20 +/- 359.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5677200  |
---------------------------------
Eval num_timesteps=5679192, episode_reward=1810.97 +/- 560.94
Episode length: 1047.60 +/- 52.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 5679192  |
---------------------------------
Eval num_timesteps=5681184, episode_reward=1356.15 +/- 741.50
Episode length: 1020.00 +/- 109.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5681184  |
---------------------------------
Eval num_timesteps=5683176, episode_reward=1072.38 +/- 793.41
Episode length: 853.40 +/- 338.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 5683176  |
---------------------------------
Eval num_timesteps=5685168, episode_reward=1756.83 +/- 1099.73
Episode length: 990.40 +/- 181.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 990      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 5685168  |
---------------------------------
Eval num_timesteps=5687160, episode_reward=1473.41 +/- 850.57
Episode length: 933.20 +/- 293.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 5687160  |
---------------------------------
Eval num_timesteps=5689152, episode_reward=1266.56 +/- 704.06
Episode length: 1016.60 +/- 338.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5689152  |
---------------------------------
Eval num_timesteps=5691144, episode_reward=1489.29 +/- 486.21
Episode length: 920.00 +/- 94.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 5691144  |
---------------------------------
Eval num_timesteps=5693136, episode_reward=1340.44 +/- 214.00
Episode length: 927.00 +/- 123.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5693136  |
---------------------------------
Eval num_timesteps=5695128, episode_reward=1507.79 +/- 698.92
Episode length: 1032.20 +/- 239.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 5695128  |
---------------------------------
Eval num_timesteps=5697120, episode_reward=1705.44 +/- 958.03
Episode length: 920.00 +/- 187.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 5697120  |
---------------------------------
Eval num_timesteps=5699112, episode_reward=1741.99 +/- 170.99
Episode length: 1009.20 +/- 207.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 5699112  |
---------------------------------
Eval num_timesteps=5701104, episode_reward=1244.91 +/- 785.95
Episode length: 734.60 +/- 310.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 5701104  |
---------------------------------
Eval num_timesteps=5703096, episode_reward=1047.94 +/- 310.55
Episode length: 1010.00 +/- 111.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.01e+03     |
|    mean_reward          | 1.05e+03     |
| time/                   |              |
|    total_timesteps      | 5703096      |
| train/                  |              |
|    approx_kl            | 0.0057300474 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0499      |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.75         |
|    value_loss           | 0.068        |
------------------------------------------
Eval num_timesteps=5705088, episode_reward=1419.33 +/- 822.42
Episode length: 910.40 +/- 322.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 5705088  |
---------------------------------
Eval num_timesteps=5707080, episode_reward=1005.43 +/- 661.86
Episode length: 958.20 +/- 352.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 958      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 5707080  |
---------------------------------
Eval num_timesteps=5709072, episode_reward=946.70 +/- 735.91
Episode length: 1025.00 +/- 255.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 5709072  |
---------------------------------
Eval num_timesteps=5711064, episode_reward=1800.84 +/- 366.88
Episode length: 1025.20 +/- 106.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 5711064  |
---------------------------------
Eval num_timesteps=5713056, episode_reward=1149.78 +/- 409.26
Episode length: 905.80 +/- 204.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 5713056  |
---------------------------------
Eval num_timesteps=5715048, episode_reward=1814.70 +/- 504.44
Episode length: 1055.80 +/- 178.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 5715048  |
---------------------------------
Eval num_timesteps=5717040, episode_reward=1935.85 +/- 999.63
Episode length: 1000.00 +/- 214.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 5717040  |
---------------------------------
Eval num_timesteps=5719032, episode_reward=1563.04 +/- 306.61
Episode length: 1022.80 +/- 180.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 5719032  |
---------------------------------
Eval num_timesteps=5721024, episode_reward=1434.96 +/- 389.92
Episode length: 988.40 +/- 146.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5721024  |
---------------------------------
Eval num_timesteps=5723016, episode_reward=1306.05 +/- 1219.08
Episode length: 924.40 +/- 384.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5723016  |
---------------------------------
Eval num_timesteps=5725008, episode_reward=1315.94 +/- 359.67
Episode length: 978.00 +/- 109.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5725008  |
---------------------------------
Eval num_timesteps=5727000, episode_reward=1366.07 +/- 321.57
Episode length: 1041.20 +/- 239.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5727000  |
---------------------------------
Eval num_timesteps=5728992, episode_reward=1678.76 +/- 323.61
Episode length: 1148.40 +/- 212.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 5728992  |
---------------------------------
Eval num_timesteps=5730984, episode_reward=1311.11 +/- 947.10
Episode length: 815.40 +/- 231.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5730984  |
---------------------------------
Eval num_timesteps=5732976, episode_reward=1185.53 +/- 418.87
Episode length: 851.40 +/- 189.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 5732976  |
---------------------------------
Eval num_timesteps=5734968, episode_reward=1589.27 +/- 722.95
Episode length: 1171.00 +/- 120.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 5734968  |
---------------------------------
Eval num_timesteps=5736960, episode_reward=1838.12 +/- 739.45
Episode length: 1113.40 +/- 183.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 5736960  |
---------------------------------
Eval num_timesteps=5738952, episode_reward=1471.01 +/- 419.03
Episode length: 994.20 +/- 108.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 994      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 5738952  |
---------------------------------
Eval num_timesteps=5740944, episode_reward=1238.44 +/- 688.80
Episode length: 781.60 +/- 207.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 5740944  |
---------------------------------
Eval num_timesteps=5742936, episode_reward=1551.67 +/- 358.89
Episode length: 1064.80 +/- 101.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 5742936  |
---------------------------------
Eval num_timesteps=5744928, episode_reward=1817.14 +/- 466.38
Episode length: 1189.20 +/- 138.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 5744928  |
---------------------------------
Eval num_timesteps=5746920, episode_reward=1257.88 +/- 765.56
Episode length: 916.40 +/- 370.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5746920  |
---------------------------------
Eval num_timesteps=5748912, episode_reward=1680.18 +/- 265.74
Episode length: 1112.20 +/- 234.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 5748912  |
---------------------------------
Eval num_timesteps=5750904, episode_reward=1532.84 +/- 1207.46
Episode length: 936.20 +/- 323.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 936          |
|    mean_reward          | 1.53e+03     |
| time/                   |              |
|    total_timesteps      | 5750904      |
| train/                  |              |
|    approx_kl            | 0.0046769464 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.91        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0552      |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 1.76         |
|    value_loss           | 0.0583       |
------------------------------------------
Eval num_timesteps=5752896, episode_reward=1288.98 +/- 564.58
Episode length: 1037.80 +/- 195.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5752896  |
---------------------------------
Eval num_timesteps=5754888, episode_reward=1418.86 +/- 763.87
Episode length: 954.40 +/- 244.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 5754888  |
---------------------------------
Eval num_timesteps=5756880, episode_reward=1357.69 +/- 314.33
Episode length: 948.60 +/- 191.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5756880  |
---------------------------------
Eval num_timesteps=5758872, episode_reward=1698.12 +/- 331.43
Episode length: 1057.20 +/- 155.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 5758872  |
---------------------------------
Eval num_timesteps=5760864, episode_reward=1336.43 +/- 692.50
Episode length: 890.00 +/- 268.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5760864  |
---------------------------------
Eval num_timesteps=5762856, episode_reward=1532.49 +/- 476.51
Episode length: 1008.00 +/- 74.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 5762856  |
---------------------------------
Eval num_timesteps=5764848, episode_reward=1072.39 +/- 233.81
Episode length: 1023.60 +/- 71.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 5764848  |
---------------------------------
Eval num_timesteps=5766840, episode_reward=2295.52 +/- 876.33
Episode length: 1030.20 +/- 107.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 5766840  |
---------------------------------
Eval num_timesteps=5768832, episode_reward=1616.18 +/- 354.61
Episode length: 1150.20 +/- 198.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 5768832  |
---------------------------------
Eval num_timesteps=5770824, episode_reward=1811.74 +/- 360.77
Episode length: 962.40 +/- 81.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 962      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 5770824  |
---------------------------------
Eval num_timesteps=5772816, episode_reward=2219.66 +/- 737.80
Episode length: 1163.00 +/- 142.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 5772816  |
---------------------------------
Eval num_timesteps=5774808, episode_reward=1323.58 +/- 711.46
Episode length: 895.00 +/- 215.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5774808  |
---------------------------------
Eval num_timesteps=5776800, episode_reward=1301.60 +/- 675.41
Episode length: 808.60 +/- 261.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 5776800  |
---------------------------------
Eval num_timesteps=5778792, episode_reward=1352.77 +/- 418.21
Episode length: 906.00 +/- 241.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 5778792  |
---------------------------------
Eval num_timesteps=5780784, episode_reward=1360.96 +/- 810.57
Episode length: 1012.80 +/- 206.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5780784  |
---------------------------------
Eval num_timesteps=5782776, episode_reward=1299.13 +/- 331.49
Episode length: 982.80 +/- 169.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 5782776  |
---------------------------------
Eval num_timesteps=5784768, episode_reward=1506.96 +/- 780.48
Episode length: 1055.60 +/- 70.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 5784768  |
---------------------------------
Eval num_timesteps=5786760, episode_reward=1168.60 +/- 651.76
Episode length: 920.40 +/- 301.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5786760  |
---------------------------------
Eval num_timesteps=5788752, episode_reward=1338.56 +/- 160.29
Episode length: 917.80 +/- 176.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5788752  |
---------------------------------
Eval num_timesteps=5790744, episode_reward=1914.51 +/- 266.53
Episode length: 1101.40 +/- 153.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 5790744  |
---------------------------------
Eval num_timesteps=5792736, episode_reward=1100.91 +/- 299.26
Episode length: 1169.00 +/- 175.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 5792736  |
---------------------------------
Eval num_timesteps=5794728, episode_reward=1447.22 +/- 225.97
Episode length: 1032.80 +/- 125.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 5794728  |
---------------------------------
Eval num_timesteps=5796720, episode_reward=1230.85 +/- 853.52
Episode length: 1019.20 +/- 201.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 5796720  |
---------------------------------
Eval num_timesteps=5798712, episode_reward=1222.47 +/- 318.16
Episode length: 1044.80 +/- 51.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5798712  |
---------------------------------
Eval num_timesteps=5800704, episode_reward=1647.28 +/- 747.33
Episode length: 1027.60 +/- 208.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.03e+03     |
|    mean_reward          | 1.65e+03     |
| time/                   |              |
|    total_timesteps      | 5800704      |
| train/                  |              |
|    approx_kl            | 0.0057295514 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.94        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0536      |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.77         |
|    value_loss           | 0.061        |
------------------------------------------
Eval num_timesteps=5802696, episode_reward=1721.17 +/- 822.56
Episode length: 905.60 +/- 282.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 5802696  |
---------------------------------
Eval num_timesteps=5804688, episode_reward=1357.63 +/- 460.51
Episode length: 1067.80 +/- 92.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 5804688  |
---------------------------------
Eval num_timesteps=5806680, episode_reward=1105.60 +/- 342.49
Episode length: 900.20 +/- 281.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5806680  |
---------------------------------
Eval num_timesteps=5808672, episode_reward=954.12 +/- 401.66
Episode length: 969.80 +/- 149.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 970      |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 5808672  |
---------------------------------
Eval num_timesteps=5810664, episode_reward=1477.70 +/- 430.86
Episode length: 912.60 +/- 137.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5810664  |
---------------------------------
Eval num_timesteps=5812656, episode_reward=1321.23 +/- 407.62
Episode length: 1065.80 +/- 83.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5812656  |
---------------------------------
Eval num_timesteps=5814648, episode_reward=1439.05 +/- 833.55
Episode length: 1177.40 +/- 188.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5814648  |
---------------------------------
Eval num_timesteps=5816640, episode_reward=1197.23 +/- 429.71
Episode length: 1062.20 +/- 84.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5816640  |
---------------------------------
Eval num_timesteps=5818632, episode_reward=1770.72 +/- 414.70
Episode length: 1086.00 +/- 157.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 5818632  |
---------------------------------
Eval num_timesteps=5820624, episode_reward=1308.34 +/- 365.65
Episode length: 950.60 +/- 192.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5820624  |
---------------------------------
Eval num_timesteps=5822616, episode_reward=1646.58 +/- 652.38
Episode length: 1030.60 +/- 140.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 5822616  |
---------------------------------
Eval num_timesteps=5824608, episode_reward=1820.67 +/- 544.07
Episode length: 986.00 +/- 211.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 5824608  |
---------------------------------
Eval num_timesteps=5826600, episode_reward=1578.79 +/- 718.94
Episode length: 1020.60 +/- 88.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5826600  |
---------------------------------
Eval num_timesteps=5828592, episode_reward=1367.02 +/- 324.73
Episode length: 998.00 +/- 192.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5828592  |
---------------------------------
Eval num_timesteps=5830584, episode_reward=1441.89 +/- 313.07
Episode length: 913.40 +/- 77.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5830584  |
---------------------------------
Eval num_timesteps=5832576, episode_reward=1270.03 +/- 319.81
Episode length: 1093.00 +/- 55.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5832576  |
---------------------------------
Eval num_timesteps=5834568, episode_reward=1146.31 +/- 299.34
Episode length: 1056.80 +/- 144.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 5834568  |
---------------------------------
Eval num_timesteps=5836560, episode_reward=1577.78 +/- 531.45
Episode length: 1110.00 +/- 282.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5836560  |
---------------------------------
Eval num_timesteps=5838552, episode_reward=1208.90 +/- 462.01
Episode length: 903.00 +/- 111.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 5838552  |
---------------------------------
Eval num_timesteps=5840544, episode_reward=1580.25 +/- 309.98
Episode length: 913.40 +/- 150.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 5840544  |
---------------------------------
Eval num_timesteps=5842536, episode_reward=1498.95 +/- 480.60
Episode length: 928.80 +/- 68.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 929      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 5842536  |
---------------------------------
Eval num_timesteps=5844528, episode_reward=1259.83 +/- 221.94
Episode length: 920.80 +/- 80.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5844528  |
---------------------------------
Eval num_timesteps=5846520, episode_reward=1081.17 +/- 476.32
Episode length: 1016.60 +/- 186.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5846520  |
---------------------------------
Eval num_timesteps=5848512, episode_reward=1337.94 +/- 305.16
Episode length: 966.20 +/- 159.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5848512  |
---------------------------------
Eval num_timesteps=5850504, episode_reward=1463.07 +/- 288.92
Episode length: 1068.80 +/- 260.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.07e+03     |
|    mean_reward          | 1.46e+03     |
| time/                   |              |
|    total_timesteps      | 5850504      |
| train/                  |              |
|    approx_kl            | 0.0061092824 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.97        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | -0.0572      |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 1.79         |
|    value_loss           | 0.0552       |
------------------------------------------
Eval num_timesteps=5852496, episode_reward=1365.40 +/- 145.11
Episode length: 1081.00 +/- 329.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5852496  |
---------------------------------
Eval num_timesteps=5854488, episode_reward=1692.15 +/- 577.31
Episode length: 1086.00 +/- 190.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 5854488  |
---------------------------------
Eval num_timesteps=5856480, episode_reward=1293.62 +/- 430.15
Episode length: 887.60 +/- 70.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5856480  |
---------------------------------
Eval num_timesteps=5858472, episode_reward=1003.02 +/- 172.68
Episode length: 984.00 +/- 134.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 984      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 5858472  |
---------------------------------
Eval num_timesteps=5860464, episode_reward=1182.83 +/- 416.64
Episode length: 999.40 +/- 68.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 5860464  |
---------------------------------
Eval num_timesteps=5862456, episode_reward=1313.67 +/- 375.70
Episode length: 877.20 +/- 91.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5862456  |
---------------------------------
Eval num_timesteps=5864448, episode_reward=1626.07 +/- 579.63
Episode length: 900.00 +/- 108.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 5864448  |
---------------------------------
Eval num_timesteps=5866440, episode_reward=1664.77 +/- 488.33
Episode length: 832.60 +/- 170.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 5866440  |
---------------------------------
Eval num_timesteps=5868432, episode_reward=1780.79 +/- 388.21
Episode length: 1085.20 +/- 204.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 5868432  |
---------------------------------
Eval num_timesteps=5870424, episode_reward=1009.39 +/- 228.31
Episode length: 961.20 +/- 133.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 5870424  |
---------------------------------
Eval num_timesteps=5872416, episode_reward=1292.27 +/- 484.50
Episode length: 969.00 +/- 217.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5872416  |
---------------------------------
Eval num_timesteps=5874408, episode_reward=1325.72 +/- 445.07
Episode length: 1150.60 +/- 223.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 5874408  |
---------------------------------
Eval num_timesteps=5876400, episode_reward=952.92 +/- 196.40
Episode length: 1009.80 +/- 136.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 5876400  |
---------------------------------
Eval num_timesteps=5878392, episode_reward=1377.10 +/- 407.02
Episode length: 1195.60 +/- 103.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5878392  |
---------------------------------
Eval num_timesteps=5880384, episode_reward=1490.49 +/- 566.92
Episode length: 887.40 +/- 222.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 5880384  |
---------------------------------
Eval num_timesteps=5882376, episode_reward=1758.58 +/- 769.60
Episode length: 1081.60 +/- 278.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 5882376  |
---------------------------------
Eval num_timesteps=5884368, episode_reward=1483.72 +/- 750.69
Episode length: 1122.00 +/- 181.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 5884368  |
---------------------------------
Eval num_timesteps=5886360, episode_reward=1272.33 +/- 401.59
Episode length: 996.80 +/- 224.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 997      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5886360  |
---------------------------------
Eval num_timesteps=5888352, episode_reward=1633.62 +/- 759.52
Episode length: 1114.00 +/- 143.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 5888352  |
---------------------------------
Eval num_timesteps=5890344, episode_reward=1169.72 +/- 334.05
Episode length: 922.20 +/- 160.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5890344  |
---------------------------------
Eval num_timesteps=5892336, episode_reward=1301.59 +/- 549.91
Episode length: 1012.80 +/- 123.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 5892336  |
---------------------------------
Eval num_timesteps=5894328, episode_reward=2199.90 +/- 1134.13
Episode length: 1179.20 +/- 199.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 5894328  |
---------------------------------
Eval num_timesteps=5896320, episode_reward=1272.26 +/- 294.58
Episode length: 840.80 +/- 76.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5896320  |
---------------------------------
Eval num_timesteps=5898312, episode_reward=1480.63 +/- 241.14
Episode length: 962.40 +/- 218.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 962         |
|    mean_reward          | 1.48e+03    |
| time/                   |             |
|    total_timesteps      | 5898312     |
| train/                  |             |
|    approx_kl            | 0.005166001 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0559     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00211    |
|    std                  | 1.8         |
|    value_loss           | 0.0582      |
-----------------------------------------
Eval num_timesteps=5900304, episode_reward=959.94 +/- 166.89
Episode length: 986.80 +/- 145.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 987      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 5900304  |
---------------------------------
Eval num_timesteps=5902296, episode_reward=1198.70 +/- 434.39
Episode length: 1062.00 +/- 150.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5902296  |
---------------------------------
Eval num_timesteps=5904288, episode_reward=1698.50 +/- 732.22
Episode length: 1085.80 +/- 127.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 5904288  |
---------------------------------
Eval num_timesteps=5906280, episode_reward=975.38 +/- 178.21
Episode length: 796.80 +/- 41.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 5906280  |
---------------------------------
Eval num_timesteps=5908272, episode_reward=1318.14 +/- 268.85
Episode length: 892.60 +/- 109.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5908272  |
---------------------------------
Eval num_timesteps=5910264, episode_reward=1460.63 +/- 170.49
Episode length: 921.00 +/- 126.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 5910264  |
---------------------------------
Eval num_timesteps=5912256, episode_reward=1197.46 +/- 343.96
Episode length: 974.40 +/- 160.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 974      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5912256  |
---------------------------------
Eval num_timesteps=5914248, episode_reward=1430.26 +/- 418.17
Episode length: 840.60 +/- 57.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5914248  |
---------------------------------
Eval num_timesteps=5916240, episode_reward=1201.59 +/- 320.93
Episode length: 955.40 +/- 189.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5916240  |
---------------------------------
Eval num_timesteps=5918232, episode_reward=1367.32 +/- 521.50
Episode length: 982.00 +/- 78.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 5918232  |
---------------------------------
Eval num_timesteps=5920224, episode_reward=1247.84 +/- 336.04
Episode length: 951.60 +/- 145.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 952      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 5920224  |
---------------------------------
Eval num_timesteps=5922216, episode_reward=1340.50 +/- 658.44
Episode length: 909.00 +/- 220.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 5922216  |
---------------------------------
Eval num_timesteps=5924208, episode_reward=1473.39 +/- 234.86
Episode length: 1050.80 +/- 309.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 5924208  |
---------------------------------
Eval num_timesteps=5926200, episode_reward=1682.26 +/- 679.13
Episode length: 1017.00 +/- 236.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 5926200  |
---------------------------------
Eval num_timesteps=5928192, episode_reward=1331.60 +/- 297.22
Episode length: 930.20 +/- 73.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 5928192  |
---------------------------------
Eval num_timesteps=5930184, episode_reward=1290.64 +/- 295.55
Episode length: 1088.80 +/- 121.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5930184  |
---------------------------------
Eval num_timesteps=5932176, episode_reward=1498.48 +/- 703.22
Episode length: 1009.20 +/- 167.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 5932176  |
---------------------------------
Eval num_timesteps=5934168, episode_reward=1167.18 +/- 284.62
Episode length: 1068.20 +/- 244.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5934168  |
---------------------------------
Eval num_timesteps=5936160, episode_reward=861.29 +/- 426.49
Episode length: 942.00 +/- 61.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 5936160  |
---------------------------------
Eval num_timesteps=5938152, episode_reward=1278.28 +/- 253.46
Episode length: 884.40 +/- 144.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 5938152  |
---------------------------------
Eval num_timesteps=5940144, episode_reward=1390.48 +/- 312.58
Episode length: 917.00 +/- 104.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 917      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5940144  |
---------------------------------
Eval num_timesteps=5942136, episode_reward=1324.42 +/- 264.63
Episode length: 939.20 +/- 128.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5942136  |
---------------------------------
Eval num_timesteps=5944128, episode_reward=1314.73 +/- 459.37
Episode length: 1046.20 +/- 169.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 5944128  |
---------------------------------
Eval num_timesteps=5946120, episode_reward=1224.41 +/- 244.00
Episode length: 926.40 +/- 123.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5946120  |
---------------------------------
Eval num_timesteps=5948112, episode_reward=1086.85 +/- 248.74
Episode length: 1004.00 +/- 79.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 1.09e+03    |
| time/                   |             |
|    total_timesteps      | 5948112     |
| train/                  |             |
|    approx_kl            | 0.005384798 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0548     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00192    |
|    std                  | 1.81        |
|    value_loss           | 0.0601      |
-----------------------------------------
Eval num_timesteps=5950104, episode_reward=1171.65 +/- 392.36
Episode length: 1018.20 +/- 217.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5950104  |
---------------------------------
Eval num_timesteps=5952096, episode_reward=1075.48 +/- 481.61
Episode length: 913.60 +/- 82.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5952096  |
---------------------------------
Eval num_timesteps=5954088, episode_reward=1648.30 +/- 501.50
Episode length: 1033.60 +/- 130.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 5954088  |
---------------------------------
Eval num_timesteps=5956080, episode_reward=1216.74 +/- 269.39
Episode length: 950.80 +/- 80.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 5956080  |
---------------------------------
Eval num_timesteps=5958072, episode_reward=1199.98 +/- 626.83
Episode length: 903.40 +/- 89.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 5958072  |
---------------------------------
Eval num_timesteps=5960064, episode_reward=1105.22 +/- 326.54
Episode length: 915.80 +/- 199.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 5960064  |
---------------------------------
Eval num_timesteps=5962056, episode_reward=1232.00 +/- 289.56
Episode length: 976.00 +/- 201.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 5962056  |
---------------------------------
Eval num_timesteps=5964048, episode_reward=1436.12 +/- 734.95
Episode length: 1024.20 +/- 174.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 5964048  |
---------------------------------
Eval num_timesteps=5966040, episode_reward=1256.73 +/- 334.66
Episode length: 921.20 +/- 197.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 5966040  |
---------------------------------
Eval num_timesteps=5968032, episode_reward=1083.63 +/- 332.95
Episode length: 838.40 +/- 128.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 5968032  |
---------------------------------
Eval num_timesteps=5970024, episode_reward=1525.22 +/- 353.07
Episode length: 1087.00 +/- 228.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 5970024  |
---------------------------------
Eval num_timesteps=5972016, episode_reward=1155.78 +/- 591.41
Episode length: 918.20 +/- 92.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 5972016  |
---------------------------------
Eval num_timesteps=5974008, episode_reward=954.76 +/- 261.57
Episode length: 948.20 +/- 222.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 5974008  |
---------------------------------
Eval num_timesteps=5976000, episode_reward=1389.75 +/- 492.37
Episode length: 853.40 +/- 126.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 5976000  |
---------------------------------
Eval num_timesteps=5977992, episode_reward=1283.83 +/- 526.47
Episode length: 960.20 +/- 104.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 5977992  |
---------------------------------
Eval num_timesteps=5979984, episode_reward=1287.14 +/- 501.25
Episode length: 1106.40 +/- 205.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 5979984  |
---------------------------------
Eval num_timesteps=5981976, episode_reward=1178.23 +/- 271.86
Episode length: 907.60 +/- 160.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 5981976  |
---------------------------------
Eval num_timesteps=5983968, episode_reward=1430.89 +/- 888.24
Episode length: 966.20 +/- 260.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 5983968  |
---------------------------------
Eval num_timesteps=5985960, episode_reward=961.11 +/- 339.68
Episode length: 928.20 +/- 172.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 5985960  |
---------------------------------
Eval num_timesteps=5987952, episode_reward=1543.06 +/- 547.66
Episode length: 1000.40 +/- 189.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 5987952  |
---------------------------------
Eval num_timesteps=5989944, episode_reward=1380.30 +/- 521.09
Episode length: 1072.60 +/- 131.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 5989944  |
---------------------------------
Eval num_timesteps=5991936, episode_reward=1516.82 +/- 533.81
Episode length: 883.60 +/- 143.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 5991936  |
---------------------------------
Eval num_timesteps=5993928, episode_reward=1321.02 +/- 250.51
Episode length: 927.00 +/- 215.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 5993928  |
---------------------------------
Eval num_timesteps=5995920, episode_reward=1205.21 +/- 190.65
Episode length: 797.00 +/- 98.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 5995920  |
---------------------------------
Eval num_timesteps=5997912, episode_reward=1781.15 +/- 722.36
Episode length: 1070.20 +/- 154.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.07e+03     |
|    mean_reward          | 1.78e+03     |
| time/                   |              |
|    total_timesteps      | 5997912      |
| train/                  |              |
|    approx_kl            | 0.0055838265 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.05        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0545      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.83         |
|    value_loss           | 0.0603       |
------------------------------------------
Eval num_timesteps=5999904, episode_reward=1027.41 +/- 250.37
Episode length: 954.60 +/- 48.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 5999904  |
---------------------------------
Eval num_timesteps=6001896, episode_reward=1514.22 +/- 471.85
Episode length: 980.40 +/- 179.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6001896  |
---------------------------------
Eval num_timesteps=6003888, episode_reward=1697.48 +/- 705.31
Episode length: 1024.00 +/- 222.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 6003888  |
---------------------------------
Eval num_timesteps=6005880, episode_reward=906.12 +/- 334.45
Episode length: 982.60 +/- 44.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 6005880  |
---------------------------------
Eval num_timesteps=6007872, episode_reward=1497.18 +/- 555.43
Episode length: 993.80 +/- 108.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 994      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 6007872  |
---------------------------------
Eval num_timesteps=6009864, episode_reward=1214.80 +/- 748.83
Episode length: 1282.80 +/- 301.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.28e+03 |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 6009864  |
---------------------------------
Eval num_timesteps=6011856, episode_reward=1641.13 +/- 378.39
Episode length: 806.40 +/- 94.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 6011856  |
---------------------------------
Eval num_timesteps=6013848, episode_reward=1725.34 +/- 698.07
Episode length: 1079.40 +/- 138.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6013848  |
---------------------------------
Eval num_timesteps=6015840, episode_reward=1442.10 +/- 457.37
Episode length: 982.60 +/- 121.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6015840  |
---------------------------------
Eval num_timesteps=6017832, episode_reward=1064.04 +/- 429.87
Episode length: 901.60 +/- 69.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6017832  |
---------------------------------
Eval num_timesteps=6019824, episode_reward=1599.78 +/- 322.74
Episode length: 1136.00 +/- 285.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6019824  |
---------------------------------
Eval num_timesteps=6021816, episode_reward=1816.02 +/- 730.63
Episode length: 1103.00 +/- 67.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 6021816  |
---------------------------------
Eval num_timesteps=6023808, episode_reward=1827.36 +/- 745.16
Episode length: 1051.80 +/- 223.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 6023808  |
---------------------------------
Eval num_timesteps=6025800, episode_reward=1428.54 +/- 370.10
Episode length: 879.60 +/- 103.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 6025800  |
---------------------------------
Eval num_timesteps=6027792, episode_reward=1946.96 +/- 1301.75
Episode length: 1050.40 +/- 121.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 6027792  |
---------------------------------
Eval num_timesteps=6029784, episode_reward=1358.36 +/- 500.67
Episode length: 1023.80 +/- 90.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6029784  |
---------------------------------
Eval num_timesteps=6031776, episode_reward=1150.10 +/- 484.85
Episode length: 995.20 +/- 241.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 995      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6031776  |
---------------------------------
Eval num_timesteps=6033768, episode_reward=1213.86 +/- 225.86
Episode length: 1059.00 +/- 208.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 6033768  |
---------------------------------
Eval num_timesteps=6035760, episode_reward=1247.28 +/- 621.63
Episode length: 950.00 +/- 118.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 6035760  |
---------------------------------
Eval num_timesteps=6037752, episode_reward=1326.99 +/- 288.88
Episode length: 893.20 +/- 145.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 6037752  |
---------------------------------
Eval num_timesteps=6039744, episode_reward=1364.11 +/- 522.44
Episode length: 992.20 +/- 108.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6039744  |
---------------------------------
Eval num_timesteps=6041736, episode_reward=1162.12 +/- 253.61
Episode length: 944.20 +/- 116.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 944      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 6041736  |
---------------------------------
Eval num_timesteps=6043728, episode_reward=1173.30 +/- 158.75
Episode length: 1113.20 +/- 287.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 6043728  |
---------------------------------
Eval num_timesteps=6045720, episode_reward=1279.92 +/- 914.98
Episode length: 802.00 +/- 183.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 802         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 6045720     |
| train/                  |             |
|    approx_kl            | 0.004983756 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0578     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 1.84        |
|    value_loss           | 0.0547      |
-----------------------------------------
Eval num_timesteps=6047712, episode_reward=1975.51 +/- 462.77
Episode length: 897.60 +/- 78.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 6047712  |
---------------------------------
Eval num_timesteps=6049704, episode_reward=1990.47 +/- 692.46
Episode length: 931.60 +/- 114.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 6049704  |
---------------------------------
Eval num_timesteps=6051696, episode_reward=1554.71 +/- 352.80
Episode length: 841.00 +/- 47.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6051696  |
---------------------------------
Eval num_timesteps=6053688, episode_reward=895.76 +/- 133.59
Episode length: 998.40 +/- 144.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 6053688  |
---------------------------------
Eval num_timesteps=6055680, episode_reward=960.02 +/- 233.23
Episode length: 938.00 +/- 207.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 6055680  |
---------------------------------
Eval num_timesteps=6057672, episode_reward=1409.65 +/- 347.02
Episode length: 966.00 +/- 130.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6057672  |
---------------------------------
Eval num_timesteps=6059664, episode_reward=1188.09 +/- 574.26
Episode length: 889.80 +/- 96.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6059664  |
---------------------------------
Eval num_timesteps=6061656, episode_reward=1655.98 +/- 434.29
Episode length: 864.80 +/- 81.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6061656  |
---------------------------------
Eval num_timesteps=6063648, episode_reward=1413.37 +/- 758.05
Episode length: 856.00 +/- 107.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6063648  |
---------------------------------
Eval num_timesteps=6065640, episode_reward=1683.00 +/- 473.67
Episode length: 984.20 +/- 76.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 984      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 6065640  |
---------------------------------
Eval num_timesteps=6067632, episode_reward=2112.86 +/- 218.00
Episode length: 1013.40 +/- 74.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 6067632  |
---------------------------------
Eval num_timesteps=6069624, episode_reward=1368.64 +/- 238.01
Episode length: 871.80 +/- 153.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6069624  |
---------------------------------
Eval num_timesteps=6071616, episode_reward=1622.91 +/- 537.71
Episode length: 1022.40 +/- 145.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6071616  |
---------------------------------
Eval num_timesteps=6073608, episode_reward=1419.60 +/- 820.26
Episode length: 970.20 +/- 340.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 970      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6073608  |
---------------------------------
Eval num_timesteps=6075600, episode_reward=1443.44 +/- 447.58
Episode length: 941.00 +/- 81.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6075600  |
---------------------------------
Eval num_timesteps=6077592, episode_reward=1491.67 +/- 373.67
Episode length: 976.00 +/- 105.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 6077592  |
---------------------------------
Eval num_timesteps=6079584, episode_reward=1784.49 +/- 649.26
Episode length: 1090.40 +/- 187.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 6079584  |
---------------------------------
Eval num_timesteps=6081576, episode_reward=1387.13 +/- 465.37
Episode length: 983.60 +/- 131.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 984      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 6081576  |
---------------------------------
Eval num_timesteps=6083568, episode_reward=1157.93 +/- 704.16
Episode length: 827.20 +/- 347.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 6083568  |
---------------------------------
Eval num_timesteps=6085560, episode_reward=931.25 +/- 440.43
Episode length: 882.80 +/- 225.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 6085560  |
---------------------------------
Eval num_timesteps=6087552, episode_reward=1684.89 +/- 529.21
Episode length: 1079.00 +/- 240.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 6087552  |
---------------------------------
Eval num_timesteps=6089544, episode_reward=1736.78 +/- 448.10
Episode length: 911.60 +/- 111.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 6089544  |
---------------------------------
Eval num_timesteps=6091536, episode_reward=1483.25 +/- 455.85
Episode length: 946.40 +/- 156.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6091536  |
---------------------------------
Eval num_timesteps=6093528, episode_reward=977.54 +/- 414.59
Episode length: 920.20 +/- 50.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 6093528  |
---------------------------------
Eval num_timesteps=6095520, episode_reward=1401.88 +/- 368.82
Episode length: 1158.00 +/- 209.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.16e+03    |
|    mean_reward          | 1.4e+03     |
| time/                   |             |
|    total_timesteps      | 6095520     |
| train/                  |             |
|    approx_kl            | 0.004715122 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0575     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 1.85        |
|    value_loss           | 0.0557      |
-----------------------------------------
Eval num_timesteps=6097512, episode_reward=1696.49 +/- 632.91
Episode length: 1015.00 +/- 115.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 6097512  |
---------------------------------
Eval num_timesteps=6099504, episode_reward=1509.10 +/- 463.97
Episode length: 786.00 +/- 88.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6099504  |
---------------------------------
Eval num_timesteps=6101496, episode_reward=1774.28 +/- 437.72
Episode length: 865.20 +/- 97.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 6101496  |
---------------------------------
Eval num_timesteps=6103488, episode_reward=1536.44 +/- 859.01
Episode length: 1123.20 +/- 232.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 6103488  |
---------------------------------
Eval num_timesteps=6105480, episode_reward=1850.85 +/- 453.03
Episode length: 849.40 +/- 152.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6105480  |
---------------------------------
Eval num_timesteps=6107472, episode_reward=1812.93 +/- 680.54
Episode length: 1079.20 +/- 118.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6107472  |
---------------------------------
Eval num_timesteps=6109464, episode_reward=1236.70 +/- 320.28
Episode length: 894.80 +/- 151.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6109464  |
---------------------------------
Eval num_timesteps=6111456, episode_reward=1214.42 +/- 270.17
Episode length: 1064.80 +/- 313.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 6111456  |
---------------------------------
Eval num_timesteps=6113448, episode_reward=1659.45 +/- 759.26
Episode length: 924.40 +/- 132.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6113448  |
---------------------------------
Eval num_timesteps=6115440, episode_reward=1594.23 +/- 401.61
Episode length: 1022.20 +/- 75.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 6115440  |
---------------------------------
Eval num_timesteps=6117432, episode_reward=1506.75 +/- 452.36
Episode length: 1000.00 +/- 189.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6117432  |
---------------------------------
Eval num_timesteps=6119424, episode_reward=1763.57 +/- 596.70
Episode length: 1019.60 +/- 249.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 6119424  |
---------------------------------
Eval num_timesteps=6121416, episode_reward=1476.12 +/- 818.02
Episode length: 951.00 +/- 133.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6121416  |
---------------------------------
Eval num_timesteps=6123408, episode_reward=1622.94 +/- 590.54
Episode length: 1039.40 +/- 125.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6123408  |
---------------------------------
Eval num_timesteps=6125400, episode_reward=1557.88 +/- 364.34
Episode length: 1025.00 +/- 171.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 6125400  |
---------------------------------
Eval num_timesteps=6127392, episode_reward=1370.07 +/- 876.99
Episode length: 965.20 +/- 317.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 965      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6127392  |
---------------------------------
Eval num_timesteps=6129384, episode_reward=1623.80 +/- 980.87
Episode length: 1189.00 +/- 216.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6129384  |
---------------------------------
Eval num_timesteps=6131376, episode_reward=1124.11 +/- 571.67
Episode length: 1017.20 +/- 295.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 6131376  |
---------------------------------
Eval num_timesteps=6133368, episode_reward=2369.83 +/- 661.40
Episode length: 1040.60 +/- 203.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 6133368  |
---------------------------------
New best mean reward!
Eval num_timesteps=6135360, episode_reward=2027.56 +/- 787.09
Episode length: 1029.80 +/- 177.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 6135360  |
---------------------------------
Eval num_timesteps=6137352, episode_reward=1621.08 +/- 380.36
Episode length: 971.60 +/- 274.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 972      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6137352  |
---------------------------------
Eval num_timesteps=6139344, episode_reward=1528.33 +/- 441.67
Episode length: 1017.20 +/- 148.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6139344  |
---------------------------------
Eval num_timesteps=6141336, episode_reward=1412.38 +/- 347.09
Episode length: 999.60 +/- 110.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6141336  |
---------------------------------
Eval num_timesteps=6143328, episode_reward=1296.54 +/- 628.85
Episode length: 1010.80 +/- 298.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 6143328  |
---------------------------------
Eval num_timesteps=6145320, episode_reward=1757.13 +/- 366.06
Episode length: 1010.20 +/- 156.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.01e+03    |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 6145320     |
| train/                  |             |
|    approx_kl            | 0.005405278 |
|    clip_fraction        | 0.0401      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0598     |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00183    |
|    std                  | 1.86        |
|    value_loss           | 0.0528      |
-----------------------------------------
Eval num_timesteps=6147312, episode_reward=1227.79 +/- 375.06
Episode length: 916.00 +/- 116.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 6147312  |
---------------------------------
Eval num_timesteps=6149304, episode_reward=1407.28 +/- 281.21
Episode length: 848.20 +/- 189.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6149304  |
---------------------------------
Eval num_timesteps=6151296, episode_reward=1263.50 +/- 950.46
Episode length: 828.60 +/- 245.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6151296  |
---------------------------------
Eval num_timesteps=6153288, episode_reward=1476.73 +/- 709.13
Episode length: 1003.00 +/- 209.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6153288  |
---------------------------------
Eval num_timesteps=6155280, episode_reward=1197.38 +/- 793.69
Episode length: 747.00 +/- 211.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 6155280  |
---------------------------------
Eval num_timesteps=6157272, episode_reward=1863.75 +/- 408.69
Episode length: 1049.80 +/- 259.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 6157272  |
---------------------------------
Eval num_timesteps=6159264, episode_reward=1477.43 +/- 809.29
Episode length: 1096.80 +/- 163.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6159264  |
---------------------------------
Eval num_timesteps=6161256, episode_reward=860.97 +/- 874.06
Episode length: 891.40 +/- 130.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 6161256  |
---------------------------------
Eval num_timesteps=6163248, episode_reward=1643.76 +/- 746.49
Episode length: 1015.00 +/- 106.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 6163248  |
---------------------------------
Eval num_timesteps=6165240, episode_reward=1126.46 +/- 572.09
Episode length: 855.80 +/- 263.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6165240  |
---------------------------------
Eval num_timesteps=6167232, episode_reward=1852.35 +/- 443.03
Episode length: 980.60 +/- 223.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6167232  |
---------------------------------
Eval num_timesteps=6169224, episode_reward=1377.54 +/- 727.72
Episode length: 745.80 +/- 176.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6169224  |
---------------------------------
Eval num_timesteps=6171216, episode_reward=1482.57 +/- 311.46
Episode length: 1011.40 +/- 161.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6171216  |
---------------------------------
Eval num_timesteps=6173208, episode_reward=1189.96 +/- 353.42
Episode length: 1026.80 +/- 188.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6173208  |
---------------------------------
Eval num_timesteps=6175200, episode_reward=1360.58 +/- 370.93
Episode length: 1025.00 +/- 135.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6175200  |
---------------------------------
Eval num_timesteps=6177192, episode_reward=1379.05 +/- 404.52
Episode length: 965.80 +/- 112.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6177192  |
---------------------------------
Eval num_timesteps=6179184, episode_reward=1963.99 +/- 710.84
Episode length: 946.00 +/- 200.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 6179184  |
---------------------------------
Eval num_timesteps=6181176, episode_reward=1604.95 +/- 481.10
Episode length: 1127.40 +/- 84.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6181176  |
---------------------------------
Eval num_timesteps=6183168, episode_reward=1569.67 +/- 804.84
Episode length: 1092.00 +/- 313.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 6183168  |
---------------------------------
Eval num_timesteps=6185160, episode_reward=1478.33 +/- 794.77
Episode length: 940.80 +/- 327.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6185160  |
---------------------------------
Eval num_timesteps=6187152, episode_reward=1035.29 +/- 522.22
Episode length: 833.60 +/- 227.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 6187152  |
---------------------------------
Eval num_timesteps=6189144, episode_reward=1285.75 +/- 192.91
Episode length: 842.60 +/- 161.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6189144  |
---------------------------------
Eval num_timesteps=6191136, episode_reward=1397.69 +/- 883.28
Episode length: 807.60 +/- 172.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6191136  |
---------------------------------
Eval num_timesteps=6193128, episode_reward=1731.76 +/- 203.83
Episode length: 1033.80 +/- 332.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6193128  |
---------------------------------
Eval num_timesteps=6195120, episode_reward=1797.39 +/- 698.84
Episode length: 1016.40 +/- 207.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.02e+03    |
|    mean_reward          | 1.8e+03     |
| time/                   |             |
|    total_timesteps      | 6195120     |
| train/                  |             |
|    approx_kl            | 0.006239022 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0596     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 1.87        |
|    value_loss           | 0.0539      |
-----------------------------------------
Eval num_timesteps=6197112, episode_reward=1091.78 +/- 517.19
Episode length: 810.00 +/- 207.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6197112  |
---------------------------------
Eval num_timesteps=6199104, episode_reward=1752.85 +/- 748.54
Episode length: 957.40 +/- 160.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 6199104  |
---------------------------------
Eval num_timesteps=6201096, episode_reward=1846.95 +/- 813.65
Episode length: 1052.00 +/- 165.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6201096  |
---------------------------------
Eval num_timesteps=6203088, episode_reward=1642.73 +/- 713.01
Episode length: 873.00 +/- 158.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 6203088  |
---------------------------------
Eval num_timesteps=6205080, episode_reward=1324.68 +/- 803.56
Episode length: 1129.60 +/- 412.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 6205080  |
---------------------------------
Eval num_timesteps=6207072, episode_reward=1660.80 +/- 683.11
Episode length: 962.60 +/- 149.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6207072  |
---------------------------------
Eval num_timesteps=6209064, episode_reward=1609.88 +/- 594.19
Episode length: 993.60 +/- 151.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 994      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 6209064  |
---------------------------------
Eval num_timesteps=6211056, episode_reward=1059.13 +/- 678.55
Episode length: 821.00 +/- 87.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6211056  |
---------------------------------
Eval num_timesteps=6213048, episode_reward=2047.07 +/- 546.07
Episode length: 1181.40 +/- 325.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 6213048  |
---------------------------------
Eval num_timesteps=6215040, episode_reward=1126.06 +/- 497.47
Episode length: 1089.40 +/- 117.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6215040  |
---------------------------------
Eval num_timesteps=6217032, episode_reward=1020.80 +/- 470.67
Episode length: 931.60 +/- 353.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 6217032  |
---------------------------------
Eval num_timesteps=6219024, episode_reward=1222.20 +/- 464.44
Episode length: 997.80 +/- 276.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 6219024  |
---------------------------------
Eval num_timesteps=6221016, episode_reward=1967.45 +/- 658.15
Episode length: 968.60 +/- 98.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 6221016  |
---------------------------------
Eval num_timesteps=6223008, episode_reward=1356.40 +/- 418.47
Episode length: 1026.60 +/- 232.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6223008  |
---------------------------------
Eval num_timesteps=6225000, episode_reward=1257.26 +/- 750.69
Episode length: 808.60 +/- 268.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6225000  |
---------------------------------
Eval num_timesteps=6226992, episode_reward=804.47 +/- 487.28
Episode length: 885.80 +/- 206.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 6226992  |
---------------------------------
Eval num_timesteps=6228984, episode_reward=1474.78 +/- 376.47
Episode length: 934.40 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 934      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 6228984  |
---------------------------------
Eval num_timesteps=6230976, episode_reward=2311.56 +/- 1124.58
Episode length: 1196.80 +/- 187.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 6230976  |
---------------------------------
Eval num_timesteps=6232968, episode_reward=2253.02 +/- 904.28
Episode length: 1040.20 +/- 152.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 6232968  |
---------------------------------
Eval num_timesteps=6234960, episode_reward=1250.68 +/- 854.99
Episode length: 870.40 +/- 335.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 6234960  |
---------------------------------
Eval num_timesteps=6236952, episode_reward=1708.32 +/- 669.85
Episode length: 1154.00 +/- 277.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 6236952  |
---------------------------------
Eval num_timesteps=6238944, episode_reward=1566.73 +/- 424.85
Episode length: 1116.60 +/- 192.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 6238944  |
---------------------------------
Eval num_timesteps=6240936, episode_reward=1538.70 +/- 558.53
Episode length: 1023.60 +/- 113.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 6240936  |
---------------------------------
Eval num_timesteps=6242928, episode_reward=1490.84 +/- 661.69
Episode length: 895.20 +/- 111.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 895          |
|    mean_reward          | 1.49e+03     |
| time/                   |              |
|    total_timesteps      | 6242928      |
| train/                  |              |
|    approx_kl            | 0.0053709736 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.17        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0596      |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.88         |
|    value_loss           | 0.0525       |
------------------------------------------
Eval num_timesteps=6244920, episode_reward=893.95 +/- 260.98
Episode length: 892.20 +/- 124.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 6244920  |
---------------------------------
Eval num_timesteps=6246912, episode_reward=1363.06 +/- 785.20
Episode length: 1006.60 +/- 283.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6246912  |
---------------------------------
Eval num_timesteps=6248904, episode_reward=1297.91 +/- 416.84
Episode length: 1236.60 +/- 248.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 6248904  |
---------------------------------
Eval num_timesteps=6250896, episode_reward=1420.60 +/- 613.74
Episode length: 946.60 +/- 33.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6250896  |
---------------------------------
Eval num_timesteps=6252888, episode_reward=1718.21 +/- 814.96
Episode length: 1089.20 +/- 219.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 6252888  |
---------------------------------
Eval num_timesteps=6254880, episode_reward=1061.15 +/- 398.89
Episode length: 1147.20 +/- 290.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6254880  |
---------------------------------
Eval num_timesteps=6256872, episode_reward=1993.28 +/- 1103.17
Episode length: 1090.60 +/- 352.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 6256872  |
---------------------------------
Eval num_timesteps=6258864, episode_reward=1933.93 +/- 705.35
Episode length: 976.60 +/- 170.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 977      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 6258864  |
---------------------------------
Eval num_timesteps=6260856, episode_reward=1963.95 +/- 468.67
Episode length: 1233.80 +/- 279.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 6260856  |
---------------------------------
Eval num_timesteps=6262848, episode_reward=897.01 +/- 214.99
Episode length: 1055.40 +/- 170.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 6262848  |
---------------------------------
Eval num_timesteps=6264840, episode_reward=2304.54 +/- 424.95
Episode length: 1012.60 +/- 119.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 6264840  |
---------------------------------
Eval num_timesteps=6266832, episode_reward=1424.49 +/- 425.54
Episode length: 1195.40 +/- 140.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6266832  |
---------------------------------
Eval num_timesteps=6268824, episode_reward=1834.59 +/- 515.35
Episode length: 974.80 +/- 83.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 6268824  |
---------------------------------
Eval num_timesteps=6270816, episode_reward=1215.58 +/- 171.92
Episode length: 941.80 +/- 128.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 6270816  |
---------------------------------
Eval num_timesteps=6272808, episode_reward=1650.78 +/- 593.09
Episode length: 936.40 +/- 98.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 6272808  |
---------------------------------
Eval num_timesteps=6274800, episode_reward=1136.07 +/- 304.88
Episode length: 1116.40 +/- 434.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6274800  |
---------------------------------
Eval num_timesteps=6276792, episode_reward=1986.75 +/- 575.96
Episode length: 918.00 +/- 170.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 6276792  |
---------------------------------
Eval num_timesteps=6278784, episode_reward=1614.49 +/- 531.70
Episode length: 1145.60 +/- 419.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 6278784  |
---------------------------------
Eval num_timesteps=6280776, episode_reward=1120.57 +/- 293.11
Episode length: 902.20 +/- 91.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 6280776  |
---------------------------------
Eval num_timesteps=6282768, episode_reward=1408.34 +/- 692.90
Episode length: 1020.40 +/- 226.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6282768  |
---------------------------------
Eval num_timesteps=6284760, episode_reward=1417.82 +/- 390.36
Episode length: 919.60 +/- 186.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6284760  |
---------------------------------
Eval num_timesteps=6286752, episode_reward=1328.88 +/- 665.20
Episode length: 1102.20 +/- 349.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 6286752  |
---------------------------------
Eval num_timesteps=6288744, episode_reward=1598.18 +/- 669.97
Episode length: 1064.40 +/- 156.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6288744  |
---------------------------------
Eval num_timesteps=6290736, episode_reward=1510.86 +/- 623.37
Episode length: 1070.20 +/- 161.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6290736  |
---------------------------------
Eval num_timesteps=6292728, episode_reward=1699.60 +/- 926.22
Episode length: 867.60 +/- 235.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 868          |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 6292728      |
| train/                  |              |
|    approx_kl            | 0.0049234624 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0556      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 1.88         |
|    value_loss           | 0.0625       |
------------------------------------------
Eval num_timesteps=6294720, episode_reward=1866.25 +/- 986.72
Episode length: 981.00 +/- 159.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 6294720  |
---------------------------------
Eval num_timesteps=6296712, episode_reward=1187.19 +/- 476.38
Episode length: 1021.80 +/- 212.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6296712  |
---------------------------------
Eval num_timesteps=6298704, episode_reward=1380.68 +/- 669.60
Episode length: 904.80 +/- 98.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6298704  |
---------------------------------
Eval num_timesteps=6300696, episode_reward=1285.44 +/- 658.67
Episode length: 952.20 +/- 185.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 952      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6300696  |
---------------------------------
Eval num_timesteps=6302688, episode_reward=1809.72 +/- 591.05
Episode length: 976.20 +/- 170.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6302688  |
---------------------------------
Eval num_timesteps=6304680, episode_reward=1689.68 +/- 489.47
Episode length: 976.00 +/- 172.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6304680  |
---------------------------------
Eval num_timesteps=6306672, episode_reward=979.75 +/- 305.08
Episode length: 895.20 +/- 55.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 6306672  |
---------------------------------
Eval num_timesteps=6308664, episode_reward=1905.79 +/- 1239.10
Episode length: 1008.40 +/- 267.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 6308664  |
---------------------------------
Eval num_timesteps=6310656, episode_reward=1438.23 +/- 244.85
Episode length: 1034.00 +/- 212.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6310656  |
---------------------------------
Eval num_timesteps=6312648, episode_reward=1715.43 +/- 566.99
Episode length: 948.60 +/- 128.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 6312648  |
---------------------------------
Eval num_timesteps=6314640, episode_reward=1223.66 +/- 370.09
Episode length: 989.60 +/- 152.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 990      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 6314640  |
---------------------------------
Eval num_timesteps=6316632, episode_reward=1558.00 +/- 364.50
Episode length: 979.20 +/- 134.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 979      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 6316632  |
---------------------------------
Eval num_timesteps=6318624, episode_reward=1135.04 +/- 668.40
Episode length: 928.40 +/- 315.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6318624  |
---------------------------------
Eval num_timesteps=6320616, episode_reward=2104.34 +/- 668.24
Episode length: 906.20 +/- 135.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 6320616  |
---------------------------------
Eval num_timesteps=6322608, episode_reward=1157.36 +/- 398.74
Episode length: 848.60 +/- 94.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 6322608  |
---------------------------------
Eval num_timesteps=6324600, episode_reward=1715.46 +/- 959.59
Episode length: 984.60 +/- 82.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 985      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 6324600  |
---------------------------------
Eval num_timesteps=6326592, episode_reward=1252.93 +/- 322.14
Episode length: 996.40 +/- 137.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 996      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 6326592  |
---------------------------------
Eval num_timesteps=6328584, episode_reward=1418.63 +/- 392.54
Episode length: 989.00 +/- 172.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 989      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6328584  |
---------------------------------
Eval num_timesteps=6330576, episode_reward=1703.42 +/- 1022.18
Episode length: 1272.80 +/- 249.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 6330576  |
---------------------------------
Eval num_timesteps=6332568, episode_reward=1195.94 +/- 321.55
Episode length: 912.00 +/- 111.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 6332568  |
---------------------------------
Eval num_timesteps=6334560, episode_reward=1374.76 +/- 588.07
Episode length: 910.60 +/- 141.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6334560  |
---------------------------------
Eval num_timesteps=6336552, episode_reward=1813.12 +/- 590.59
Episode length: 1025.00 +/- 165.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6336552  |
---------------------------------
Eval num_timesteps=6338544, episode_reward=1864.05 +/- 851.06
Episode length: 928.80 +/- 129.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 929      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 6338544  |
---------------------------------
Eval num_timesteps=6340536, episode_reward=1853.94 +/- 591.81
Episode length: 1128.60 +/- 355.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6340536  |
---------------------------------
Eval num_timesteps=6342528, episode_reward=1448.12 +/- 573.80
Episode length: 947.40 +/- 92.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 947          |
|    mean_reward          | 1.45e+03     |
| time/                   |              |
|    total_timesteps      | 6342528      |
| train/                  |              |
|    approx_kl            | 0.0044694054 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.2         |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | -0.0571      |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 1.89         |
|    value_loss           | 0.0604       |
------------------------------------------
Eval num_timesteps=6344520, episode_reward=1191.72 +/- 1109.57
Episode length: 816.00 +/- 322.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6344520  |
---------------------------------
Eval num_timesteps=6346512, episode_reward=1382.59 +/- 367.56
Episode length: 826.60 +/- 110.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6346512  |
---------------------------------
Eval num_timesteps=6348504, episode_reward=1370.57 +/- 381.29
Episode length: 1018.40 +/- 289.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6348504  |
---------------------------------
Eval num_timesteps=6350496, episode_reward=1597.61 +/- 718.96
Episode length: 980.20 +/- 141.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6350496  |
---------------------------------
Eval num_timesteps=6352488, episode_reward=1083.01 +/- 239.67
Episode length: 956.00 +/- 145.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 956      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 6352488  |
---------------------------------
Eval num_timesteps=6354480, episode_reward=1508.48 +/- 558.03
Episode length: 942.00 +/- 151.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6354480  |
---------------------------------
Eval num_timesteps=6356472, episode_reward=1436.35 +/- 383.06
Episode length: 969.40 +/- 79.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6356472  |
---------------------------------
Eval num_timesteps=6358464, episode_reward=1592.46 +/- 748.45
Episode length: 1016.00 +/- 185.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 6358464  |
---------------------------------
Eval num_timesteps=6360456, episode_reward=1396.66 +/- 520.26
Episode length: 972.40 +/- 91.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 972      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6360456  |
---------------------------------
Eval num_timesteps=6362448, episode_reward=2269.94 +/- 469.05
Episode length: 950.20 +/- 152.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 6362448  |
---------------------------------
Eval num_timesteps=6364440, episode_reward=1728.86 +/- 750.89
Episode length: 837.20 +/- 84.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6364440  |
---------------------------------
Eval num_timesteps=6366432, episode_reward=1452.95 +/- 351.18
Episode length: 960.00 +/- 241.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6366432  |
---------------------------------
Eval num_timesteps=6368424, episode_reward=1514.98 +/- 648.67
Episode length: 1051.60 +/- 242.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 6368424  |
---------------------------------
Eval num_timesteps=6370416, episode_reward=1468.84 +/- 281.86
Episode length: 809.00 +/- 57.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 6370416  |
---------------------------------
Eval num_timesteps=6372408, episode_reward=1874.37 +/- 221.16
Episode length: 866.40 +/- 117.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 866      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 6372408  |
---------------------------------
Eval num_timesteps=6374400, episode_reward=1312.76 +/- 325.54
Episode length: 906.20 +/- 76.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6374400  |
---------------------------------
Eval num_timesteps=6376392, episode_reward=2205.43 +/- 482.11
Episode length: 936.80 +/- 110.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 6376392  |
---------------------------------
Eval num_timesteps=6378384, episode_reward=1364.61 +/- 336.86
Episode length: 1059.20 +/- 130.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6378384  |
---------------------------------
Eval num_timesteps=6380376, episode_reward=1039.19 +/- 216.58
Episode length: 905.60 +/- 148.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 6380376  |
---------------------------------
Eval num_timesteps=6382368, episode_reward=913.33 +/- 622.76
Episode length: 931.20 +/- 165.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 6382368  |
---------------------------------
Eval num_timesteps=6384360, episode_reward=1406.46 +/- 472.85
Episode length: 897.00 +/- 185.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6384360  |
---------------------------------
Eval num_timesteps=6386352, episode_reward=1430.65 +/- 546.50
Episode length: 930.80 +/- 60.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 6386352  |
---------------------------------
Eval num_timesteps=6388344, episode_reward=1555.36 +/- 656.10
Episode length: 850.80 +/- 91.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 6388344  |
---------------------------------
Eval num_timesteps=6390336, episode_reward=1495.11 +/- 303.77
Episode length: 824.20 +/- 44.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 824          |
|    mean_reward          | 1.5e+03      |
| time/                   |              |
|    total_timesteps      | 6390336      |
| train/                  |              |
|    approx_kl            | 0.0055227573 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0491      |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.89         |
|    value_loss           | 0.0761       |
------------------------------------------
Eval num_timesteps=6392328, episode_reward=1819.08 +/- 604.40
Episode length: 900.20 +/- 177.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 6392328  |
---------------------------------
Eval num_timesteps=6394320, episode_reward=1256.86 +/- 373.33
Episode length: 986.00 +/- 81.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6394320  |
---------------------------------
Eval num_timesteps=6396312, episode_reward=1737.69 +/- 557.64
Episode length: 847.00 +/- 134.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 847      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 6396312  |
---------------------------------
Eval num_timesteps=6398304, episode_reward=843.40 +/- 472.68
Episode length: 773.20 +/- 235.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 6398304  |
---------------------------------
Eval num_timesteps=6400296, episode_reward=1420.12 +/- 203.59
Episode length: 882.40 +/- 141.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6400296  |
---------------------------------
Eval num_timesteps=6402288, episode_reward=1625.79 +/- 934.23
Episode length: 936.60 +/- 235.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 6402288  |
---------------------------------
Eval num_timesteps=6404280, episode_reward=1749.60 +/- 416.24
Episode length: 937.60 +/- 205.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 6404280  |
---------------------------------
Eval num_timesteps=6406272, episode_reward=1193.48 +/- 614.90
Episode length: 798.60 +/- 207.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6406272  |
---------------------------------
Eval num_timesteps=6408264, episode_reward=1442.15 +/- 572.27
Episode length: 933.20 +/- 251.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6408264  |
---------------------------------
Eval num_timesteps=6410256, episode_reward=1550.72 +/- 504.98
Episode length: 858.60 +/- 140.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6410256  |
---------------------------------
Eval num_timesteps=6412248, episode_reward=1216.29 +/- 123.10
Episode length: 985.40 +/- 247.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 985      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 6412248  |
---------------------------------
Eval num_timesteps=6414240, episode_reward=1713.36 +/- 761.34
Episode length: 936.60 +/- 70.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 6414240  |
---------------------------------
Eval num_timesteps=6416232, episode_reward=1175.02 +/- 724.06
Episode length: 798.40 +/- 273.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6416232  |
---------------------------------
Eval num_timesteps=6418224, episode_reward=1055.99 +/- 569.22
Episode length: 896.80 +/- 191.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6418224  |
---------------------------------
Eval num_timesteps=6420216, episode_reward=1530.54 +/- 1124.89
Episode length: 895.40 +/- 116.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6420216  |
---------------------------------
Eval num_timesteps=6422208, episode_reward=1581.69 +/- 965.61
Episode length: 809.80 +/- 282.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 6422208  |
---------------------------------
Eval num_timesteps=6424200, episode_reward=2112.31 +/- 197.21
Episode length: 1003.60 +/- 98.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 6424200  |
---------------------------------
Eval num_timesteps=6426192, episode_reward=1151.14 +/- 762.71
Episode length: 850.40 +/- 237.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6426192  |
---------------------------------
Eval num_timesteps=6428184, episode_reward=1126.45 +/- 294.33
Episode length: 861.60 +/- 35.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6428184  |
---------------------------------
Eval num_timesteps=6430176, episode_reward=1635.41 +/- 617.25
Episode length: 969.80 +/- 88.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 970      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 6430176  |
---------------------------------
Eval num_timesteps=6432168, episode_reward=838.40 +/- 696.28
Episode length: 687.60 +/- 217.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 6432168  |
---------------------------------
Eval num_timesteps=6434160, episode_reward=1840.73 +/- 495.43
Episode length: 1131.20 +/- 177.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 6434160  |
---------------------------------
Eval num_timesteps=6436152, episode_reward=1667.39 +/- 1318.46
Episode length: 926.20 +/- 189.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 6436152  |
---------------------------------
Eval num_timesteps=6438144, episode_reward=1851.76 +/- 1064.34
Episode length: 929.80 +/- 115.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6438144  |
---------------------------------
Eval num_timesteps=6440136, episode_reward=1505.18 +/- 518.07
Episode length: 1078.60 +/- 197.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.08e+03     |
|    mean_reward          | 1.51e+03     |
| time/                   |              |
|    total_timesteps      | 6440136      |
| train/                  |              |
|    approx_kl            | 0.0059834844 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | -0.054       |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.91         |
|    value_loss           | 0.066        |
------------------------------------------
Eval num_timesteps=6442128, episode_reward=869.42 +/- 363.72
Episode length: 915.00 +/- 259.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 6442128  |
---------------------------------
Eval num_timesteps=6444120, episode_reward=1579.02 +/- 591.74
Episode length: 998.20 +/- 173.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 6444120  |
---------------------------------
Eval num_timesteps=6446112, episode_reward=1689.76 +/- 217.07
Episode length: 1214.40 +/- 212.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6446112  |
---------------------------------
Eval num_timesteps=6448104, episode_reward=1805.51 +/- 325.50
Episode length: 927.60 +/- 97.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6448104  |
---------------------------------
Eval num_timesteps=6450096, episode_reward=1816.99 +/- 677.68
Episode length: 877.80 +/- 121.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 6450096  |
---------------------------------
Eval num_timesteps=6452088, episode_reward=1857.70 +/- 426.41
Episode length: 1176.00 +/- 197.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 6452088  |
---------------------------------
Eval num_timesteps=6454080, episode_reward=1552.86 +/- 517.44
Episode length: 882.80 +/- 221.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6454080  |
---------------------------------
Eval num_timesteps=6456072, episode_reward=1356.29 +/- 717.86
Episode length: 1011.20 +/- 240.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6456072  |
---------------------------------
Eval num_timesteps=6458064, episode_reward=1001.40 +/- 272.40
Episode length: 963.00 +/- 368.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 6458064  |
---------------------------------
Eval num_timesteps=6460056, episode_reward=1012.83 +/- 616.07
Episode length: 806.40 +/- 235.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 6460056  |
---------------------------------
Eval num_timesteps=6462048, episode_reward=1656.12 +/- 389.73
Episode length: 857.20 +/- 193.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6462048  |
---------------------------------
Eval num_timesteps=6464040, episode_reward=1394.64 +/- 443.83
Episode length: 924.40 +/- 226.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 6464040  |
---------------------------------
Eval num_timesteps=6466032, episode_reward=1454.03 +/- 558.24
Episode length: 715.20 +/- 102.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6466032  |
---------------------------------
Eval num_timesteps=6468024, episode_reward=1730.32 +/- 866.21
Episode length: 1005.40 +/- 202.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6468024  |
---------------------------------
Eval num_timesteps=6470016, episode_reward=1809.68 +/- 264.51
Episode length: 941.60 +/- 86.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6470016  |
---------------------------------
Eval num_timesteps=6472008, episode_reward=1272.52 +/- 868.17
Episode length: 896.60 +/- 379.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6472008  |
---------------------------------
Eval num_timesteps=6474000, episode_reward=1622.55 +/- 409.27
Episode length: 1060.00 +/- 210.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6474000  |
---------------------------------
Eval num_timesteps=6475992, episode_reward=1582.18 +/- 601.45
Episode length: 1258.20 +/- 331.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 6475992  |
---------------------------------
Eval num_timesteps=6477984, episode_reward=2044.61 +/- 498.16
Episode length: 979.60 +/- 155.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 6477984  |
---------------------------------
Eval num_timesteps=6479976, episode_reward=1155.81 +/- 182.75
Episode length: 1061.00 +/- 248.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 6479976  |
---------------------------------
Eval num_timesteps=6481968, episode_reward=1344.18 +/- 458.93
Episode length: 1023.20 +/- 200.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 6481968  |
---------------------------------
Eval num_timesteps=6483960, episode_reward=1870.61 +/- 937.74
Episode length: 827.60 +/- 207.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 6483960  |
---------------------------------
Eval num_timesteps=6485952, episode_reward=1528.13 +/- 527.56
Episode length: 825.40 +/- 139.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6485952  |
---------------------------------
Eval num_timesteps=6487944, episode_reward=1432.12 +/- 378.65
Episode length: 953.20 +/- 183.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 6487944  |
---------------------------------
Eval num_timesteps=6489936, episode_reward=1622.92 +/- 775.30
Episode length: 1025.00 +/- 148.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.02e+03    |
|    mean_reward          | 1.62e+03    |
| time/                   |             |
|    total_timesteps      | 6489936     |
| train/                  |             |
|    approx_kl            | 0.005480939 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0594     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 1.92        |
|    value_loss           | 0.0555      |
-----------------------------------------
Eval num_timesteps=6491928, episode_reward=1985.29 +/- 642.22
Episode length: 923.80 +/- 127.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 6491928  |
---------------------------------
Eval num_timesteps=6493920, episode_reward=1824.97 +/- 484.35
Episode length: 989.20 +/- 301.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 989      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 6493920  |
---------------------------------
Eval num_timesteps=6495912, episode_reward=1336.60 +/- 448.38
Episode length: 977.80 +/- 90.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 6495912  |
---------------------------------
Eval num_timesteps=6497904, episode_reward=2139.44 +/- 375.48
Episode length: 827.40 +/- 101.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 6497904  |
---------------------------------
Eval num_timesteps=6499896, episode_reward=1546.04 +/- 592.83
Episode length: 843.00 +/- 73.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6499896  |
---------------------------------
Eval num_timesteps=6501888, episode_reward=1703.35 +/- 749.19
Episode length: 761.40 +/- 160.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 6501888  |
---------------------------------
Eval num_timesteps=6503880, episode_reward=1387.23 +/- 587.16
Episode length: 899.40 +/- 185.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 6503880  |
---------------------------------
Eval num_timesteps=6505872, episode_reward=1960.23 +/- 659.82
Episode length: 857.20 +/- 255.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 6505872  |
---------------------------------
Eval num_timesteps=6507864, episode_reward=2116.56 +/- 1031.94
Episode length: 981.00 +/- 264.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 6507864  |
---------------------------------
Eval num_timesteps=6509856, episode_reward=1444.85 +/- 116.27
Episode length: 900.60 +/- 146.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6509856  |
---------------------------------
Eval num_timesteps=6511848, episode_reward=1629.50 +/- 343.80
Episode length: 902.60 +/- 106.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 6511848  |
---------------------------------
Eval num_timesteps=6513840, episode_reward=1662.74 +/- 602.99
Episode length: 861.20 +/- 195.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6513840  |
---------------------------------
Eval num_timesteps=6515832, episode_reward=1591.34 +/- 492.77
Episode length: 791.00 +/- 111.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 6515832  |
---------------------------------
Eval num_timesteps=6517824, episode_reward=1311.42 +/- 818.91
Episode length: 762.60 +/- 216.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6517824  |
---------------------------------
Eval num_timesteps=6519816, episode_reward=1475.85 +/- 889.82
Episode length: 897.20 +/- 226.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6519816  |
---------------------------------
Eval num_timesteps=6521808, episode_reward=1734.34 +/- 693.69
Episode length: 916.40 +/- 283.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6521808  |
---------------------------------
Eval num_timesteps=6523800, episode_reward=1814.73 +/- 555.34
Episode length: 1175.80 +/- 247.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 6523800  |
---------------------------------
Eval num_timesteps=6525792, episode_reward=1693.69 +/- 616.74
Episode length: 859.00 +/- 118.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6525792  |
---------------------------------
Eval num_timesteps=6527784, episode_reward=1730.02 +/- 793.25
Episode length: 908.00 +/- 86.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 6527784  |
---------------------------------
Eval num_timesteps=6529776, episode_reward=1708.22 +/- 278.06
Episode length: 939.80 +/- 242.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 6529776  |
---------------------------------
Eval num_timesteps=6531768, episode_reward=1407.18 +/- 304.59
Episode length: 754.20 +/- 129.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6531768  |
---------------------------------
Eval num_timesteps=6533760, episode_reward=1656.65 +/- 489.71
Episode length: 881.00 +/- 45.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6533760  |
---------------------------------
Eval num_timesteps=6535752, episode_reward=1789.21 +/- 433.43
Episode length: 855.60 +/- 131.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 856      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 6535752  |
---------------------------------
Eval num_timesteps=6537744, episode_reward=1072.76 +/- 292.94
Episode length: 901.80 +/- 211.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 902         |
|    mean_reward          | 1.07e+03    |
| time/                   |             |
|    total_timesteps      | 6537744     |
| train/                  |             |
|    approx_kl            | 0.004635685 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.26       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0564     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 1.92        |
|    value_loss           | 0.0615      |
-----------------------------------------
Eval num_timesteps=6539736, episode_reward=1485.40 +/- 413.53
Episode length: 903.20 +/- 260.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 6539736  |
---------------------------------
Eval num_timesteps=6541728, episode_reward=1664.74 +/- 567.30
Episode length: 930.20 +/- 80.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 6541728  |
---------------------------------
Eval num_timesteps=6543720, episode_reward=1296.85 +/- 491.38
Episode length: 895.00 +/- 164.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 6543720  |
---------------------------------
Eval num_timesteps=6545712, episode_reward=1454.96 +/- 574.02
Episode length: 1019.00 +/- 211.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6545712  |
---------------------------------
Eval num_timesteps=6547704, episode_reward=1315.25 +/- 592.75
Episode length: 947.40 +/- 59.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 947      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 6547704  |
---------------------------------
Eval num_timesteps=6549696, episode_reward=1348.51 +/- 437.72
Episode length: 956.40 +/- 118.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 956      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6549696  |
---------------------------------
Eval num_timesteps=6551688, episode_reward=1849.17 +/- 668.56
Episode length: 1112.40 +/- 135.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 6551688  |
---------------------------------
Eval num_timesteps=6553680, episode_reward=1464.80 +/- 379.02
Episode length: 837.20 +/- 117.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 6553680  |
---------------------------------
Eval num_timesteps=6555672, episode_reward=1571.32 +/- 687.53
Episode length: 881.60 +/- 60.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 6555672  |
---------------------------------
Eval num_timesteps=6557664, episode_reward=1296.32 +/- 289.29
Episode length: 798.40 +/- 131.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 6557664  |
---------------------------------
Eval num_timesteps=6559656, episode_reward=1502.09 +/- 295.45
Episode length: 770.60 +/- 104.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 6559656  |
---------------------------------
Eval num_timesteps=6561648, episode_reward=1801.48 +/- 879.51
Episode length: 924.80 +/- 158.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 6561648  |
---------------------------------
Eval num_timesteps=6563640, episode_reward=1917.98 +/- 917.14
Episode length: 775.00 +/- 99.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 6563640  |
---------------------------------
Eval num_timesteps=6565632, episode_reward=2040.69 +/- 1051.41
Episode length: 889.40 +/- 159.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 6565632  |
---------------------------------
Eval num_timesteps=6567624, episode_reward=1737.21 +/- 413.22
Episode length: 993.80 +/- 299.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 994      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 6567624  |
---------------------------------
Eval num_timesteps=6569616, episode_reward=1940.28 +/- 828.05
Episode length: 888.40 +/- 146.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 6569616  |
---------------------------------
Eval num_timesteps=6571608, episode_reward=1296.97 +/- 368.05
Episode length: 836.40 +/- 176.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 6571608  |
---------------------------------
Eval num_timesteps=6573600, episode_reward=1565.91 +/- 622.60
Episode length: 858.00 +/- 32.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 6573600  |
---------------------------------
Eval num_timesteps=6575592, episode_reward=1345.30 +/- 312.98
Episode length: 1002.60 +/- 170.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6575592  |
---------------------------------
Eval num_timesteps=6577584, episode_reward=1864.61 +/- 216.86
Episode length: 991.20 +/- 106.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 991      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 6577584  |
---------------------------------
Eval num_timesteps=6579576, episode_reward=1597.72 +/- 457.98
Episode length: 961.20 +/- 173.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6579576  |
---------------------------------
Eval num_timesteps=6581568, episode_reward=1315.12 +/- 252.35
Episode length: 886.80 +/- 207.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 6581568  |
---------------------------------
Eval num_timesteps=6583560, episode_reward=1571.75 +/- 668.51
Episode length: 1006.00 +/- 189.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 6583560  |
---------------------------------
Eval num_timesteps=6585552, episode_reward=1410.97 +/- 501.94
Episode length: 886.40 +/- 193.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6585552  |
---------------------------------
Eval num_timesteps=6587544, episode_reward=1125.30 +/- 465.65
Episode length: 767.00 +/- 175.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 767         |
|    mean_reward          | 1.13e+03    |
| time/                   |             |
|    total_timesteps      | 6587544     |
| train/                  |             |
|    approx_kl            | 0.004476557 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0538     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 1.92        |
|    value_loss           | 0.0676      |
-----------------------------------------
Eval num_timesteps=6589536, episode_reward=1242.22 +/- 484.55
Episode length: 721.00 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6589536  |
---------------------------------
Eval num_timesteps=6591528, episode_reward=1532.19 +/- 556.07
Episode length: 863.00 +/- 176.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6591528  |
---------------------------------
Eval num_timesteps=6593520, episode_reward=1367.80 +/- 283.37
Episode length: 842.60 +/- 240.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6593520  |
---------------------------------
Eval num_timesteps=6595512, episode_reward=1383.40 +/- 565.34
Episode length: 950.40 +/- 218.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6595512  |
---------------------------------
Eval num_timesteps=6597504, episode_reward=1525.14 +/- 570.95
Episode length: 874.40 +/- 142.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6597504  |
---------------------------------
Eval num_timesteps=6599496, episode_reward=1766.27 +/- 557.68
Episode length: 900.00 +/- 46.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 6599496  |
---------------------------------
Eval num_timesteps=6601488, episode_reward=1193.47 +/- 140.88
Episode length: 904.40 +/- 118.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6601488  |
---------------------------------
Eval num_timesteps=6603480, episode_reward=1232.17 +/- 266.47
Episode length: 996.20 +/- 267.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 996      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 6603480  |
---------------------------------
Eval num_timesteps=6605472, episode_reward=1600.48 +/- 701.11
Episode length: 881.80 +/- 135.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6605472  |
---------------------------------
Eval num_timesteps=6607464, episode_reward=1273.23 +/- 443.01
Episode length: 926.20 +/- 208.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6607464  |
---------------------------------
Eval num_timesteps=6609456, episode_reward=1327.40 +/- 413.80
Episode length: 828.40 +/- 63.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 6609456  |
---------------------------------
Eval num_timesteps=6611448, episode_reward=1689.05 +/- 578.10
Episode length: 953.60 +/- 150.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6611448  |
---------------------------------
Eval num_timesteps=6613440, episode_reward=952.26 +/- 324.99
Episode length: 778.20 +/- 102.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 6613440  |
---------------------------------
Eval num_timesteps=6615432, episode_reward=1091.58 +/- 238.58
Episode length: 855.20 +/- 199.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6615432  |
---------------------------------
Eval num_timesteps=6617424, episode_reward=1309.45 +/- 500.41
Episode length: 853.60 +/- 77.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6617424  |
---------------------------------
Eval num_timesteps=6619416, episode_reward=980.64 +/- 375.00
Episode length: 747.00 +/- 103.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 6619416  |
---------------------------------
Eval num_timesteps=6621408, episode_reward=1374.23 +/- 558.31
Episode length: 916.00 +/- 97.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6621408  |
---------------------------------
Eval num_timesteps=6623400, episode_reward=1278.25 +/- 410.72
Episode length: 930.60 +/- 100.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 6623400  |
---------------------------------
Eval num_timesteps=6625392, episode_reward=1643.90 +/- 864.45
Episode length: 860.00 +/- 93.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 860      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 6625392  |
---------------------------------
Eval num_timesteps=6627384, episode_reward=1416.14 +/- 988.87
Episode length: 797.60 +/- 82.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6627384  |
---------------------------------
Eval num_timesteps=6629376, episode_reward=1600.47 +/- 625.16
Episode length: 954.80 +/- 103.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6629376  |
---------------------------------
Eval num_timesteps=6631368, episode_reward=1735.95 +/- 455.14
Episode length: 923.60 +/- 292.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 6631368  |
---------------------------------
Eval num_timesteps=6633360, episode_reward=1119.88 +/- 368.24
Episode length: 840.60 +/- 119.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 6633360  |
---------------------------------
Eval num_timesteps=6635352, episode_reward=1119.22 +/- 425.25
Episode length: 773.00 +/- 173.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 6635352  |
---------------------------------
Eval num_timesteps=6637344, episode_reward=1317.58 +/- 369.60
Episode length: 704.60 +/- 90.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 705         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 6637344     |
| train/                  |             |
|    approx_kl            | 0.004845202 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0504     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 1.93        |
|    value_loss           | 0.074       |
-----------------------------------------
Eval num_timesteps=6639336, episode_reward=1693.54 +/- 571.81
Episode length: 883.40 +/- 189.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6639336  |
---------------------------------
Eval num_timesteps=6641328, episode_reward=1364.07 +/- 430.69
Episode length: 783.20 +/- 103.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6641328  |
---------------------------------
Eval num_timesteps=6643320, episode_reward=1263.80 +/- 551.68
Episode length: 780.20 +/- 84.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6643320  |
---------------------------------
Eval num_timesteps=6645312, episode_reward=1454.43 +/- 326.68
Episode length: 801.00 +/- 109.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6645312  |
---------------------------------
Eval num_timesteps=6647304, episode_reward=1048.11 +/- 179.64
Episode length: 857.80 +/- 90.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 6647304  |
---------------------------------
Eval num_timesteps=6649296, episode_reward=1243.59 +/- 368.15
Episode length: 826.40 +/- 179.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6649296  |
---------------------------------
Eval num_timesteps=6651288, episode_reward=1604.50 +/- 431.37
Episode length: 798.00 +/- 68.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6651288  |
---------------------------------
Eval num_timesteps=6653280, episode_reward=930.97 +/- 185.11
Episode length: 840.60 +/- 99.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 6653280  |
---------------------------------
Eval num_timesteps=6655272, episode_reward=1015.09 +/- 460.96
Episode length: 734.80 +/- 107.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 6655272  |
---------------------------------
Eval num_timesteps=6657264, episode_reward=1256.36 +/- 256.13
Episode length: 743.40 +/- 109.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6657264  |
---------------------------------
Eval num_timesteps=6659256, episode_reward=942.35 +/- 164.31
Episode length: 842.20 +/- 232.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 6659256  |
---------------------------------
Eval num_timesteps=6661248, episode_reward=1484.40 +/- 432.26
Episode length: 882.60 +/- 100.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 6661248  |
---------------------------------
Eval num_timesteps=6663240, episode_reward=1322.30 +/- 390.19
Episode length: 797.80 +/- 197.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 6663240  |
---------------------------------
Eval num_timesteps=6665232, episode_reward=1399.90 +/- 480.18
Episode length: 818.20 +/- 123.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6665232  |
---------------------------------
Eval num_timesteps=6667224, episode_reward=1548.13 +/- 649.26
Episode length: 807.20 +/- 144.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6667224  |
---------------------------------
Eval num_timesteps=6669216, episode_reward=1607.09 +/- 608.36
Episode length: 775.80 +/- 118.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 6669216  |
---------------------------------
Eval num_timesteps=6671208, episode_reward=1089.67 +/- 366.40
Episode length: 828.40 +/- 107.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6671208  |
---------------------------------
Eval num_timesteps=6673200, episode_reward=1293.20 +/- 517.94
Episode length: 739.20 +/- 71.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6673200  |
---------------------------------
Eval num_timesteps=6675192, episode_reward=1532.35 +/- 598.14
Episode length: 794.80 +/- 185.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6675192  |
---------------------------------
Eval num_timesteps=6677184, episode_reward=1243.07 +/- 412.33
Episode length: 733.80 +/- 77.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6677184  |
---------------------------------
Eval num_timesteps=6679176, episode_reward=1449.10 +/- 297.95
Episode length: 820.20 +/- 75.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6679176  |
---------------------------------
Eval num_timesteps=6681168, episode_reward=1799.79 +/- 357.74
Episode length: 966.60 +/- 214.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 967      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 6681168  |
---------------------------------
Eval num_timesteps=6683160, episode_reward=1496.53 +/- 928.11
Episode length: 802.80 +/- 151.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 6683160  |
---------------------------------
Eval num_timesteps=6685152, episode_reward=1591.73 +/- 365.75
Episode length: 796.00 +/- 145.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 796          |
|    mean_reward          | 1.59e+03     |
| time/                   |              |
|    total_timesteps      | 6685152      |
| train/                  |              |
|    approx_kl            | 0.0066719567 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0554      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.93         |
|    value_loss           | 0.0623       |
------------------------------------------
Eval num_timesteps=6687144, episode_reward=1178.83 +/- 652.94
Episode length: 737.00 +/- 103.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6687144  |
---------------------------------
Eval num_timesteps=6689136, episode_reward=1227.88 +/- 485.34
Episode length: 954.80 +/- 305.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 6689136  |
---------------------------------
Eval num_timesteps=6691128, episode_reward=1135.58 +/- 411.14
Episode length: 823.00 +/- 210.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6691128  |
---------------------------------
Eval num_timesteps=6693120, episode_reward=1149.60 +/- 538.61
Episode length: 679.20 +/- 44.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6693120  |
---------------------------------
Eval num_timesteps=6695112, episode_reward=1093.08 +/- 358.12
Episode length: 729.00 +/- 115.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6695112  |
---------------------------------
Eval num_timesteps=6697104, episode_reward=1177.19 +/- 409.65
Episode length: 680.20 +/- 137.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6697104  |
---------------------------------
Eval num_timesteps=6699096, episode_reward=990.99 +/- 223.77
Episode length: 769.00 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 6699096  |
---------------------------------
Eval num_timesteps=6701088, episode_reward=724.36 +/- 82.89
Episode length: 676.80 +/- 59.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 6701088  |
---------------------------------
Eval num_timesteps=6703080, episode_reward=972.48 +/- 210.17
Episode length: 743.00 +/- 60.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 6703080  |
---------------------------------
Eval num_timesteps=6705072, episode_reward=1146.09 +/- 609.77
Episode length: 693.60 +/- 187.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6705072  |
---------------------------------
Eval num_timesteps=6707064, episode_reward=1700.90 +/- 605.93
Episode length: 813.60 +/- 182.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 6707064  |
---------------------------------
Eval num_timesteps=6709056, episode_reward=1136.00 +/- 578.83
Episode length: 711.60 +/- 147.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6709056  |
---------------------------------
Eval num_timesteps=6711048, episode_reward=1547.77 +/- 558.82
Episode length: 873.60 +/- 71.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6711048  |
---------------------------------
Eval num_timesteps=6713040, episode_reward=982.81 +/- 475.12
Episode length: 708.60 +/- 175.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 6713040  |
---------------------------------
Eval num_timesteps=6715032, episode_reward=1437.57 +/- 834.84
Episode length: 863.60 +/- 150.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 6715032  |
---------------------------------
Eval num_timesteps=6717024, episode_reward=1605.14 +/- 798.58
Episode length: 751.20 +/- 76.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 6717024  |
---------------------------------
Eval num_timesteps=6719016, episode_reward=1182.30 +/- 446.21
Episode length: 733.80 +/- 52.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6719016  |
---------------------------------
Eval num_timesteps=6721008, episode_reward=1531.62 +/- 470.83
Episode length: 636.80 +/- 80.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6721008  |
---------------------------------
Eval num_timesteps=6723000, episode_reward=1109.17 +/- 397.81
Episode length: 709.60 +/- 111.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 6723000  |
---------------------------------
Eval num_timesteps=6724992, episode_reward=1075.10 +/- 421.14
Episode length: 731.40 +/- 74.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 6724992  |
---------------------------------
Eval num_timesteps=6726984, episode_reward=1138.76 +/- 582.20
Episode length: 840.20 +/- 154.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6726984  |
---------------------------------
Eval num_timesteps=6728976, episode_reward=1396.82 +/- 430.26
Episode length: 760.20 +/- 124.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6728976  |
---------------------------------
Eval num_timesteps=6730968, episode_reward=1372.70 +/- 477.34
Episode length: 747.40 +/- 147.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6730968  |
---------------------------------
Eval num_timesteps=6732960, episode_reward=1781.22 +/- 505.12
Episode length: 803.80 +/- 158.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 6732960  |
---------------------------------
Eval num_timesteps=6734952, episode_reward=1387.77 +/- 646.94
Episode length: 815.60 +/- 116.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | 1.39e+03    |
| time/                   |             |
|    total_timesteps      | 6734952     |
| train/                  |             |
|    approx_kl            | 0.006334515 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0571     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 1.94        |
|    value_loss           | 0.0615      |
-----------------------------------------
Eval num_timesteps=6736944, episode_reward=662.37 +/- 66.62
Episode length: 768.40 +/- 166.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 6736944  |
---------------------------------
Eval num_timesteps=6738936, episode_reward=1559.26 +/- 900.22
Episode length: 759.60 +/- 146.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 6738936  |
---------------------------------
Eval num_timesteps=6740928, episode_reward=1037.69 +/- 522.27
Episode length: 728.60 +/- 71.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 6740928  |
---------------------------------
Eval num_timesteps=6742920, episode_reward=837.70 +/- 300.04
Episode length: 761.60 +/- 105.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 6742920  |
---------------------------------
Eval num_timesteps=6744912, episode_reward=1009.33 +/- 322.87
Episode length: 854.20 +/- 131.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 6744912  |
---------------------------------
Eval num_timesteps=6746904, episode_reward=1499.50 +/- 644.42
Episode length: 779.00 +/- 70.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 6746904  |
---------------------------------
Eval num_timesteps=6748896, episode_reward=1064.58 +/- 895.30
Episode length: 697.00 +/- 188.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6748896  |
---------------------------------
Eval num_timesteps=6750888, episode_reward=725.12 +/- 421.16
Episode length: 603.40 +/- 227.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 6750888  |
---------------------------------
Eval num_timesteps=6752880, episode_reward=1242.59 +/- 610.44
Episode length: 768.80 +/- 125.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6752880  |
---------------------------------
Eval num_timesteps=6754872, episode_reward=1488.94 +/- 456.70
Episode length: 705.80 +/- 128.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 6754872  |
---------------------------------
Eval num_timesteps=6756864, episode_reward=985.86 +/- 307.51
Episode length: 766.80 +/- 114.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 6756864  |
---------------------------------
Eval num_timesteps=6758856, episode_reward=1191.23 +/- 640.63
Episode length: 734.60 +/- 124.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6758856  |
---------------------------------
Eval num_timesteps=6760848, episode_reward=867.89 +/- 125.36
Episode length: 700.20 +/- 90.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 6760848  |
---------------------------------
Eval num_timesteps=6762840, episode_reward=687.66 +/- 60.89
Episode length: 654.80 +/- 54.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 6762840  |
---------------------------------
Eval num_timesteps=6764832, episode_reward=881.12 +/- 280.93
Episode length: 725.80 +/- 172.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 6764832  |
---------------------------------
Eval num_timesteps=6766824, episode_reward=1266.56 +/- 544.95
Episode length: 771.40 +/- 127.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6766824  |
---------------------------------
Eval num_timesteps=6768816, episode_reward=1376.08 +/- 398.91
Episode length: 735.00 +/- 140.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 6768816  |
---------------------------------
Eval num_timesteps=6770808, episode_reward=1249.02 +/- 574.74
Episode length: 585.20 +/- 95.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 6770808  |
---------------------------------
Eval num_timesteps=6772800, episode_reward=1055.19 +/- 486.22
Episode length: 716.00 +/- 91.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6772800  |
---------------------------------
Eval num_timesteps=6774792, episode_reward=1133.00 +/- 449.79
Episode length: 864.80 +/- 284.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6774792  |
---------------------------------
Eval num_timesteps=6776784, episode_reward=845.55 +/- 153.76
Episode length: 715.40 +/- 186.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 6776784  |
---------------------------------
Eval num_timesteps=6778776, episode_reward=793.39 +/- 118.99
Episode length: 659.60 +/- 70.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 6778776  |
---------------------------------
Eval num_timesteps=6780768, episode_reward=1408.95 +/- 539.95
Episode length: 822.80 +/- 242.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6780768  |
---------------------------------
Eval num_timesteps=6782760, episode_reward=1149.23 +/- 524.75
Episode length: 874.80 +/- 192.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6782760  |
---------------------------------
Eval num_timesteps=6784752, episode_reward=1224.63 +/- 723.88
Episode length: 602.00 +/- 152.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 602         |
|    mean_reward          | 1.22e+03    |
| time/                   |             |
|    total_timesteps      | 6784752     |
| train/                  |             |
|    approx_kl            | 0.005581204 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.32       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0574     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00154    |
|    std                  | 1.95        |
|    value_loss           | 0.0594      |
-----------------------------------------
Eval num_timesteps=6786744, episode_reward=1181.98 +/- 700.53
Episode length: 817.80 +/- 171.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6786744  |
---------------------------------
Eval num_timesteps=6788736, episode_reward=1357.82 +/- 307.96
Episode length: 747.20 +/- 75.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6788736  |
---------------------------------
Eval num_timesteps=6790728, episode_reward=1346.97 +/- 472.44
Episode length: 766.60 +/- 246.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6790728  |
---------------------------------
Eval num_timesteps=6792720, episode_reward=932.15 +/- 273.26
Episode length: 728.60 +/- 102.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 6792720  |
---------------------------------
Eval num_timesteps=6794712, episode_reward=1138.68 +/- 310.81
Episode length: 692.80 +/- 75.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6794712  |
---------------------------------
Eval num_timesteps=6796704, episode_reward=947.37 +/- 664.06
Episode length: 655.80 +/- 246.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 6796704  |
---------------------------------
Eval num_timesteps=6798696, episode_reward=874.58 +/- 491.00
Episode length: 584.00 +/- 141.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 6798696  |
---------------------------------
Eval num_timesteps=6800688, episode_reward=1192.40 +/- 634.97
Episode length: 614.60 +/- 186.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6800688  |
---------------------------------
Eval num_timesteps=6802680, episode_reward=1205.65 +/- 467.21
Episode length: 827.20 +/- 120.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 6802680  |
---------------------------------
Eval num_timesteps=6804672, episode_reward=1186.82 +/- 590.45
Episode length: 701.80 +/- 219.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6804672  |
---------------------------------
Eval num_timesteps=6806664, episode_reward=1341.45 +/- 635.22
Episode length: 812.00 +/- 189.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 6806664  |
---------------------------------
Eval num_timesteps=6808656, episode_reward=960.75 +/- 563.86
Episode length: 712.80 +/- 188.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 6808656  |
---------------------------------
Eval num_timesteps=6810648, episode_reward=1136.21 +/- 574.14
Episode length: 726.40 +/- 71.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 6810648  |
---------------------------------
Eval num_timesteps=6812640, episode_reward=1294.90 +/- 548.37
Episode length: 667.20 +/- 29.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6812640  |
---------------------------------
Eval num_timesteps=6814632, episode_reward=848.24 +/- 229.00
Episode length: 760.60 +/- 200.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 6814632  |
---------------------------------
Eval num_timesteps=6816624, episode_reward=971.86 +/- 548.19
Episode length: 730.40 +/- 47.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 6816624  |
---------------------------------
Eval num_timesteps=6818616, episode_reward=587.07 +/- 260.70
Episode length: 681.40 +/- 362.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 587      |
| time/              |          |
|    total_timesteps | 6818616  |
---------------------------------
Eval num_timesteps=6820608, episode_reward=1372.28 +/- 570.69
Episode length: 904.20 +/- 129.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 6820608  |
---------------------------------
Eval num_timesteps=6822600, episode_reward=1048.20 +/- 660.48
Episode length: 673.20 +/- 122.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 6822600  |
---------------------------------
Eval num_timesteps=6824592, episode_reward=617.59 +/- 148.89
Episode length: 618.40 +/- 155.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 6824592  |
---------------------------------
Eval num_timesteps=6826584, episode_reward=610.46 +/- 181.80
Episode length: 703.40 +/- 194.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 6826584  |
---------------------------------
Eval num_timesteps=6828576, episode_reward=704.55 +/- 58.90
Episode length: 715.00 +/- 126.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 6828576  |
---------------------------------
Eval num_timesteps=6830568, episode_reward=1352.03 +/- 433.36
Episode length: 759.60 +/- 143.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6830568  |
---------------------------------
Eval num_timesteps=6832560, episode_reward=1265.28 +/- 633.37
Episode length: 835.20 +/- 277.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 835          |
|    mean_reward          | 1.27e+03     |
| time/                   |              |
|    total_timesteps      | 6832560      |
| train/                  |              |
|    approx_kl            | 0.0056084897 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.35        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | -0.0564      |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.96         |
|    value_loss           | 0.063        |
------------------------------------------
Eval num_timesteps=6834552, episode_reward=1134.77 +/- 399.08
Episode length: 746.80 +/- 133.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6834552  |
---------------------------------
Eval num_timesteps=6836544, episode_reward=1501.24 +/- 311.12
Episode length: 643.60 +/- 71.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 6836544  |
---------------------------------
Eval num_timesteps=6838536, episode_reward=1527.58 +/- 443.77
Episode length: 782.40 +/- 111.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 6838536  |
---------------------------------
Eval num_timesteps=6840528, episode_reward=1330.98 +/- 368.81
Episode length: 730.60 +/- 126.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 6840528  |
---------------------------------
Eval num_timesteps=6842520, episode_reward=1518.29 +/- 753.85
Episode length: 887.00 +/- 207.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 6842520  |
---------------------------------
Eval num_timesteps=6844512, episode_reward=1110.66 +/- 376.00
Episode length: 790.00 +/- 71.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 6844512  |
---------------------------------
Eval num_timesteps=6846504, episode_reward=1108.33 +/- 566.51
Episode length: 636.00 +/- 164.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 6846504  |
---------------------------------
Eval num_timesteps=6848496, episode_reward=1183.46 +/- 423.89
Episode length: 726.40 +/- 144.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6848496  |
---------------------------------
Eval num_timesteps=6850488, episode_reward=983.45 +/- 368.52
Episode length: 630.40 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 6850488  |
---------------------------------
Eval num_timesteps=6852480, episode_reward=1359.16 +/- 569.26
Episode length: 784.80 +/- 151.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6852480  |
---------------------------------
Eval num_timesteps=6854472, episode_reward=1090.52 +/- 590.34
Episode length: 696.00 +/- 140.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6854472  |
---------------------------------
Eval num_timesteps=6856464, episode_reward=922.36 +/- 587.02
Episode length: 709.20 +/- 77.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 6856464  |
---------------------------------
Eval num_timesteps=6858456, episode_reward=1307.67 +/- 476.23
Episode length: 789.20 +/- 196.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6858456  |
---------------------------------
Eval num_timesteps=6860448, episode_reward=1311.98 +/- 696.20
Episode length: 818.20 +/- 217.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6860448  |
---------------------------------
Eval num_timesteps=6862440, episode_reward=1154.18 +/- 257.65
Episode length: 702.60 +/- 130.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6862440  |
---------------------------------
Eval num_timesteps=6864432, episode_reward=1688.99 +/- 673.71
Episode length: 823.60 +/- 170.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 6864432  |
---------------------------------
Eval num_timesteps=6866424, episode_reward=1170.91 +/- 332.21
Episode length: 662.00 +/- 75.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 6866424  |
---------------------------------
Eval num_timesteps=6868416, episode_reward=1348.26 +/- 519.72
Episode length: 677.60 +/- 122.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6868416  |
---------------------------------
Eval num_timesteps=6870408, episode_reward=1004.24 +/- 295.92
Episode length: 812.80 +/- 143.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 6870408  |
---------------------------------
Eval num_timesteps=6872400, episode_reward=783.94 +/- 398.03
Episode length: 630.00 +/- 140.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 6872400  |
---------------------------------
Eval num_timesteps=6874392, episode_reward=947.24 +/- 453.28
Episode length: 725.80 +/- 75.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 6874392  |
---------------------------------
Eval num_timesteps=6876384, episode_reward=1414.39 +/- 354.87
Episode length: 701.00 +/- 69.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6876384  |
---------------------------------
Eval num_timesteps=6878376, episode_reward=1548.29 +/- 567.03
Episode length: 841.80 +/- 117.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 842      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 6878376  |
---------------------------------
Eval num_timesteps=6880368, episode_reward=1720.18 +/- 540.74
Episode length: 671.20 +/- 96.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 6880368  |
---------------------------------
Eval num_timesteps=6882360, episode_reward=1228.70 +/- 404.48
Episode length: 686.40 +/- 99.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 686          |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 6882360      |
| train/                  |              |
|    approx_kl            | 0.0071234703 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0548      |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.97         |
|    value_loss           | 0.0664       |
------------------------------------------
Eval num_timesteps=6884352, episode_reward=1361.08 +/- 295.26
Episode length: 883.20 +/- 175.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 6884352  |
---------------------------------
Eval num_timesteps=6886344, episode_reward=968.26 +/- 255.05
Episode length: 766.60 +/- 147.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 6886344  |
---------------------------------
Eval num_timesteps=6888336, episode_reward=1213.51 +/- 520.40
Episode length: 677.00 +/- 96.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 6888336  |
---------------------------------
Eval num_timesteps=6890328, episode_reward=1313.16 +/- 496.55
Episode length: 801.60 +/- 119.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6890328  |
---------------------------------
Eval num_timesteps=6892320, episode_reward=1595.61 +/- 442.69
Episode length: 811.00 +/- 130.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 6892320  |
---------------------------------
Eval num_timesteps=6894312, episode_reward=1097.69 +/- 285.51
Episode length: 711.20 +/- 44.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 6894312  |
---------------------------------
Eval num_timesteps=6896304, episode_reward=1272.97 +/- 559.00
Episode length: 772.60 +/- 47.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6896304  |
---------------------------------
Eval num_timesteps=6898296, episode_reward=1244.32 +/- 617.70
Episode length: 705.60 +/- 83.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6898296  |
---------------------------------
Eval num_timesteps=6900288, episode_reward=1713.27 +/- 274.09
Episode length: 721.20 +/- 142.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 6900288  |
---------------------------------
Eval num_timesteps=6902280, episode_reward=757.88 +/- 106.21
Episode length: 643.80 +/- 71.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 6902280  |
---------------------------------
Eval num_timesteps=6904272, episode_reward=910.90 +/- 268.28
Episode length: 774.60 +/- 148.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 6904272  |
---------------------------------
Eval num_timesteps=6906264, episode_reward=1183.23 +/- 440.39
Episode length: 655.80 +/- 87.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 6906264  |
---------------------------------
Eval num_timesteps=6908256, episode_reward=949.29 +/- 343.08
Episode length: 698.60 +/- 133.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 6908256  |
---------------------------------
Eval num_timesteps=6910248, episode_reward=1020.02 +/- 300.87
Episode length: 696.40 +/- 70.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 6910248  |
---------------------------------
Eval num_timesteps=6912240, episode_reward=1146.60 +/- 192.63
Episode length: 615.80 +/- 41.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6912240  |
---------------------------------
Eval num_timesteps=6914232, episode_reward=1287.40 +/- 520.78
Episode length: 763.80 +/- 176.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6914232  |
---------------------------------
Eval num_timesteps=6916224, episode_reward=1421.17 +/- 376.36
Episode length: 661.80 +/- 81.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 6916224  |
---------------------------------
Eval num_timesteps=6918216, episode_reward=1149.84 +/- 349.29
Episode length: 697.20 +/- 93.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6918216  |
---------------------------------
Eval num_timesteps=6920208, episode_reward=1146.05 +/- 252.87
Episode length: 643.20 +/- 99.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 6920208  |
---------------------------------
Eval num_timesteps=6922200, episode_reward=1092.71 +/- 310.69
Episode length: 642.20 +/- 47.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 6922200  |
---------------------------------
Eval num_timesteps=6924192, episode_reward=1452.44 +/- 680.43
Episode length: 777.00 +/- 107.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 6924192  |
---------------------------------
Eval num_timesteps=6926184, episode_reward=1271.24 +/- 352.86
Episode length: 631.20 +/- 101.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6926184  |
---------------------------------
Eval num_timesteps=6928176, episode_reward=1305.76 +/- 394.50
Episode length: 753.80 +/- 91.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6928176  |
---------------------------------
Eval num_timesteps=6930168, episode_reward=1125.96 +/- 472.28
Episode length: 612.00 +/- 64.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 6930168  |
---------------------------------
Eval num_timesteps=6932160, episode_reward=1400.14 +/- 309.73
Episode length: 757.20 +/- 132.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 757          |
|    mean_reward          | 1.4e+03      |
| time/                   |              |
|    total_timesteps      | 6932160      |
| train/                  |              |
|    approx_kl            | 0.0033462138 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.38        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0525      |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.98         |
|    value_loss           | 0.071        |
------------------------------------------
Eval num_timesteps=6934152, episode_reward=1410.50 +/- 376.96
Episode length: 868.80 +/- 160.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 6934152  |
---------------------------------
Eval num_timesteps=6936144, episode_reward=1251.51 +/- 364.46
Episode length: 779.80 +/- 153.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 6936144  |
---------------------------------
Eval num_timesteps=6938136, episode_reward=1309.32 +/- 465.39
Episode length: 756.80 +/- 91.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6938136  |
---------------------------------
Eval num_timesteps=6940128, episode_reward=833.95 +/- 616.34
Episode length: 644.00 +/- 236.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 6940128  |
---------------------------------
Eval num_timesteps=6942120, episode_reward=1002.38 +/- 273.66
Episode length: 684.20 +/- 87.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 6942120  |
---------------------------------
Eval num_timesteps=6944112, episode_reward=1288.33 +/- 434.99
Episode length: 694.80 +/- 158.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 6944112  |
---------------------------------
Eval num_timesteps=6946104, episode_reward=1262.99 +/- 558.54
Episode length: 746.20 +/- 109.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 6946104  |
---------------------------------
Eval num_timesteps=6948096, episode_reward=1156.04 +/- 609.41
Episode length: 658.20 +/- 70.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 6948096  |
---------------------------------
Eval num_timesteps=6950088, episode_reward=1236.24 +/- 672.43
Episode length: 910.20 +/- 285.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6950088  |
---------------------------------
Eval num_timesteps=6952080, episode_reward=1235.27 +/- 423.04
Episode length: 617.60 +/- 82.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 6952080  |
---------------------------------
Eval num_timesteps=6954072, episode_reward=1200.55 +/- 380.62
Episode length: 764.00 +/- 195.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 6954072  |
---------------------------------
Eval num_timesteps=6956064, episode_reward=1400.23 +/- 751.55
Episode length: 652.80 +/- 179.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6956064  |
---------------------------------
Eval num_timesteps=6958056, episode_reward=1017.93 +/- 368.72
Episode length: 622.60 +/- 73.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 6958056  |
---------------------------------
Eval num_timesteps=6960048, episode_reward=1681.90 +/- 491.49
Episode length: 692.60 +/- 105.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 6960048  |
---------------------------------
Eval num_timesteps=6962040, episode_reward=863.16 +/- 278.22
Episode length: 664.80 +/- 48.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 6962040  |
---------------------------------
Eval num_timesteps=6964032, episode_reward=1353.42 +/- 461.36
Episode length: 761.00 +/- 85.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 6964032  |
---------------------------------
Eval num_timesteps=6966024, episode_reward=1621.96 +/- 655.12
Episode length: 780.00 +/- 179.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 6966024  |
---------------------------------
Eval num_timesteps=6968016, episode_reward=1174.17 +/- 462.31
Episode length: 704.00 +/- 130.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 6968016  |
---------------------------------
Eval num_timesteps=6970008, episode_reward=926.83 +/- 452.10
Episode length: 664.00 +/- 104.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 6970008  |
---------------------------------
Eval num_timesteps=6972000, episode_reward=1055.64 +/- 382.40
Episode length: 909.20 +/- 198.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 6972000  |
---------------------------------
Eval num_timesteps=6973992, episode_reward=1102.12 +/- 429.48
Episode length: 642.40 +/- 58.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 6973992  |
---------------------------------
Eval num_timesteps=6975984, episode_reward=1585.24 +/- 776.95
Episode length: 735.40 +/- 209.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 6975984  |
---------------------------------
Eval num_timesteps=6977976, episode_reward=1119.31 +/- 307.49
Episode length: 683.20 +/- 74.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 6977976  |
---------------------------------
Eval num_timesteps=6979968, episode_reward=1040.39 +/- 351.23
Episode length: 788.20 +/- 148.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 788          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 6979968      |
| train/                  |              |
|    approx_kl            | 0.0041636704 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0564      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.99         |
|    value_loss           | 0.0628       |
------------------------------------------
Eval num_timesteps=6981960, episode_reward=1314.35 +/- 423.32
Episode length: 756.40 +/- 58.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6981960  |
---------------------------------
Eval num_timesteps=6983952, episode_reward=1396.57 +/- 578.21
Episode length: 1005.20 +/- 188.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 6983952  |
---------------------------------
Eval num_timesteps=6985944, episode_reward=837.16 +/- 96.00
Episode length: 703.60 +/- 123.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 6985944  |
---------------------------------
Eval num_timesteps=6987936, episode_reward=1001.30 +/- 213.17
Episode length: 635.80 +/- 103.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 6987936  |
---------------------------------
Eval num_timesteps=6989928, episode_reward=1306.10 +/- 453.52
Episode length: 704.40 +/- 106.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 6989928  |
---------------------------------
Eval num_timesteps=6991920, episode_reward=1191.86 +/- 491.68
Episode length: 816.00 +/- 132.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 6991920  |
---------------------------------
Eval num_timesteps=6993912, episode_reward=1472.80 +/- 732.37
Episode length: 775.20 +/- 60.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 6993912  |
---------------------------------
Eval num_timesteps=6995904, episode_reward=784.32 +/- 117.94
Episode length: 633.80 +/- 130.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 6995904  |
---------------------------------
Eval num_timesteps=6997896, episode_reward=1035.53 +/- 501.29
Episode length: 684.60 +/- 98.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 6997896  |
---------------------------------
Eval num_timesteps=6999888, episode_reward=1271.58 +/- 482.86
Episode length: 809.00 +/- 79.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 6999888  |
---------------------------------
Eval num_timesteps=7001880, episode_reward=1026.05 +/- 466.62
Episode length: 770.20 +/- 153.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7001880  |
---------------------------------
Eval num_timesteps=7003872, episode_reward=1290.60 +/- 576.82
Episode length: 813.60 +/- 254.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 7003872  |
---------------------------------
Eval num_timesteps=7005864, episode_reward=1393.53 +/- 431.48
Episode length: 784.40 +/- 161.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 7005864  |
---------------------------------
Eval num_timesteps=7007856, episode_reward=776.53 +/- 110.76
Episode length: 720.60 +/- 139.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 7007856  |
---------------------------------
Eval num_timesteps=7009848, episode_reward=1266.23 +/- 427.90
Episode length: 739.20 +/- 199.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 7009848  |
---------------------------------
Eval num_timesteps=7011840, episode_reward=1343.31 +/- 406.23
Episode length: 807.00 +/- 135.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 7011840  |
---------------------------------
Eval num_timesteps=7013832, episode_reward=820.59 +/- 123.79
Episode length: 684.80 +/- 75.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 7013832  |
---------------------------------
Eval num_timesteps=7015824, episode_reward=1113.56 +/- 249.09
Episode length: 705.80 +/- 110.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 7015824  |
---------------------------------
Eval num_timesteps=7017816, episode_reward=1045.60 +/- 360.28
Episode length: 672.80 +/- 99.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 7017816  |
---------------------------------
Eval num_timesteps=7019808, episode_reward=969.10 +/- 269.48
Episode length: 744.60 +/- 72.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 7019808  |
---------------------------------
Eval num_timesteps=7021800, episode_reward=1149.04 +/- 376.29
Episode length: 668.20 +/- 44.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 7021800  |
---------------------------------
Eval num_timesteps=7023792, episode_reward=1466.67 +/- 712.71
Episode length: 854.00 +/- 299.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 7023792  |
---------------------------------
Eval num_timesteps=7025784, episode_reward=1310.58 +/- 511.59
Episode length: 683.60 +/- 59.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 7025784  |
---------------------------------
Eval num_timesteps=7027776, episode_reward=763.57 +/- 80.61
Episode length: 699.40 +/- 87.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 7027776  |
---------------------------------
Eval num_timesteps=7029768, episode_reward=1027.65 +/- 650.61
Episode length: 685.40 +/- 142.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 685          |
|    mean_reward          | 1.03e+03     |
| time/                   |              |
|    total_timesteps      | 7029768      |
| train/                  |              |
|    approx_kl            | 0.0056073754 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0593      |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 2            |
|    value_loss           | 0.0589       |
------------------------------------------
Eval num_timesteps=7031760, episode_reward=1389.95 +/- 774.68
Episode length: 727.20 +/- 132.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 7031760  |
---------------------------------
Eval num_timesteps=7033752, episode_reward=1218.25 +/- 505.64
Episode length: 709.00 +/- 60.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 7033752  |
---------------------------------
Eval num_timesteps=7035744, episode_reward=957.65 +/- 493.83
Episode length: 625.00 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 7035744  |
---------------------------------
Eval num_timesteps=7037736, episode_reward=998.27 +/- 525.88
Episode length: 725.40 +/- 76.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 7037736  |
---------------------------------
Eval num_timesteps=7039728, episode_reward=1058.40 +/- 462.47
Episode length: 640.00 +/- 85.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 7039728  |
---------------------------------
Eval num_timesteps=7041720, episode_reward=1188.96 +/- 292.52
Episode length: 725.60 +/- 106.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 7041720  |
---------------------------------
Eval num_timesteps=7043712, episode_reward=1540.00 +/- 449.00
Episode length: 681.20 +/- 54.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 7043712  |
---------------------------------
Eval num_timesteps=7045704, episode_reward=1147.76 +/- 390.24
Episode length: 676.60 +/- 129.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 7045704  |
---------------------------------
Eval num_timesteps=7047696, episode_reward=1220.70 +/- 586.96
Episode length: 758.20 +/- 191.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 7047696  |
---------------------------------
Eval num_timesteps=7049688, episode_reward=1268.42 +/- 465.60
Episode length: 630.00 +/- 83.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 7049688  |
---------------------------------
Eval num_timesteps=7051680, episode_reward=1188.82 +/- 430.90
Episode length: 812.80 +/- 241.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 7051680  |
---------------------------------
Eval num_timesteps=7053672, episode_reward=831.68 +/- 117.12
Episode length: 744.60 +/- 112.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 7053672  |
---------------------------------
Eval num_timesteps=7055664, episode_reward=962.59 +/- 427.97
Episode length: 650.60 +/- 72.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 7055664  |
---------------------------------
Eval num_timesteps=7057656, episode_reward=1190.47 +/- 420.83
Episode length: 708.60 +/- 69.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 7057656  |
---------------------------------
Eval num_timesteps=7059648, episode_reward=798.27 +/- 368.11
Episode length: 679.60 +/- 188.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 7059648  |
---------------------------------
Eval num_timesteps=7061640, episode_reward=742.75 +/- 94.35
Episode length: 623.60 +/- 100.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 7061640  |
---------------------------------
Eval num_timesteps=7063632, episode_reward=1067.67 +/- 413.31
Episode length: 825.60 +/- 315.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 7063632  |
---------------------------------
Eval num_timesteps=7065624, episode_reward=1043.23 +/- 308.48
Episode length: 801.80 +/- 256.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 7065624  |
---------------------------------
Eval num_timesteps=7067616, episode_reward=1562.61 +/- 657.78
Episode length: 802.60 +/- 110.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 7067616  |
---------------------------------
Eval num_timesteps=7069608, episode_reward=1251.43 +/- 440.67
Episode length: 886.60 +/- 184.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 7069608  |
---------------------------------
Eval num_timesteps=7071600, episode_reward=1293.15 +/- 500.56
Episode length: 668.80 +/- 107.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 7071600  |
---------------------------------
Eval num_timesteps=7073592, episode_reward=1071.38 +/- 357.66
Episode length: 587.20 +/- 68.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 7073592  |
---------------------------------
Eval num_timesteps=7075584, episode_reward=1078.58 +/- 547.80
Episode length: 606.80 +/- 173.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 7075584  |
---------------------------------
Eval num_timesteps=7077576, episode_reward=904.15 +/- 455.51
Episode length: 602.80 +/- 43.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 7077576  |
---------------------------------
Eval num_timesteps=7079568, episode_reward=1732.01 +/- 252.27
Episode length: 751.20 +/- 100.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 751          |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 7079568      |
| train/                  |              |
|    approx_kl            | 0.0041848496 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.45        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0635      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 2.02         |
|    value_loss           | 0.0492       |
------------------------------------------
Eval num_timesteps=7081560, episode_reward=953.20 +/- 413.68
Episode length: 630.20 +/- 69.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 7081560  |
---------------------------------
Eval num_timesteps=7083552, episode_reward=1077.10 +/- 469.53
Episode length: 673.40 +/- 61.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 7083552  |
---------------------------------
Eval num_timesteps=7085544, episode_reward=867.44 +/- 158.08
Episode length: 690.80 +/- 106.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 7085544  |
---------------------------------
Eval num_timesteps=7087536, episode_reward=1496.65 +/- 487.89
Episode length: 588.40 +/- 69.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 7087536  |
---------------------------------
Eval num_timesteps=7089528, episode_reward=983.91 +/- 492.73
Episode length: 626.60 +/- 172.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 984      |
| time/              |          |
|    total_timesteps | 7089528  |
---------------------------------
Eval num_timesteps=7091520, episode_reward=930.91 +/- 371.76
Episode length: 711.20 +/- 183.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 7091520  |
---------------------------------
Eval num_timesteps=7093512, episode_reward=1430.65 +/- 580.03
Episode length: 699.40 +/- 82.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 7093512  |
---------------------------------
Eval num_timesteps=7095504, episode_reward=774.51 +/- 28.13
Episode length: 620.40 +/- 82.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 7095504  |
---------------------------------
Eval num_timesteps=7097496, episode_reward=1086.00 +/- 422.67
Episode length: 779.60 +/- 118.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 7097496  |
---------------------------------
Eval num_timesteps=7099488, episode_reward=1140.52 +/- 360.10
Episode length: 627.80 +/- 101.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 7099488  |
---------------------------------
Eval num_timesteps=7101480, episode_reward=1550.84 +/- 478.56
Episode length: 761.60 +/- 153.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 7101480  |
---------------------------------
Eval num_timesteps=7103472, episode_reward=1295.53 +/- 523.57
Episode length: 748.60 +/- 70.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 7103472  |
---------------------------------
Eval num_timesteps=7105464, episode_reward=1315.71 +/- 503.14
Episode length: 863.80 +/- 368.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 864      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 7105464  |
---------------------------------
Eval num_timesteps=7107456, episode_reward=1076.34 +/- 411.75
Episode length: 773.80 +/- 136.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 7107456  |
---------------------------------
Eval num_timesteps=7109448, episode_reward=1065.28 +/- 410.35
Episode length: 695.00 +/- 77.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 7109448  |
---------------------------------
Eval num_timesteps=7111440, episode_reward=1095.30 +/- 288.19
Episode length: 728.80 +/- 102.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 7111440  |
---------------------------------
Eval num_timesteps=7113432, episode_reward=937.02 +/- 357.15
Episode length: 681.80 +/- 103.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 7113432  |
---------------------------------
Eval num_timesteps=7115424, episode_reward=1630.81 +/- 439.27
Episode length: 637.00 +/- 61.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 7115424  |
---------------------------------
Eval num_timesteps=7117416, episode_reward=1225.18 +/- 880.40
Episode length: 655.40 +/- 240.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 7117416  |
---------------------------------
Eval num_timesteps=7119408, episode_reward=1209.76 +/- 544.08
Episode length: 702.00 +/- 116.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 7119408  |
---------------------------------
Eval num_timesteps=7121400, episode_reward=948.55 +/- 358.09
Episode length: 692.40 +/- 128.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 7121400  |
---------------------------------
Eval num_timesteps=7123392, episode_reward=1162.15 +/- 515.65
Episode length: 671.00 +/- 103.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 7123392  |
---------------------------------
Eval num_timesteps=7125384, episode_reward=1190.38 +/- 569.04
Episode length: 673.60 +/- 46.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 7125384  |
---------------------------------
Eval num_timesteps=7127376, episode_reward=1387.13 +/- 391.64
Episode length: 798.60 +/- 262.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 799         |
|    mean_reward          | 1.39e+03    |
| time/                   |             |
|    total_timesteps      | 7127376     |
| train/                  |             |
|    approx_kl            | 0.005024267 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.47       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0613     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 2.03        |
|    value_loss           | 0.0543      |
-----------------------------------------
Eval num_timesteps=7129368, episode_reward=868.19 +/- 290.30
Episode length: 554.00 +/- 68.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 7129368  |
---------------------------------
Eval num_timesteps=7131360, episode_reward=1121.02 +/- 360.97
Episode length: 575.80 +/- 70.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 7131360  |
---------------------------------
Eval num_timesteps=7133352, episode_reward=1097.24 +/- 503.84
Episode length: 650.60 +/- 41.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 7133352  |
---------------------------------
Eval num_timesteps=7135344, episode_reward=1162.83 +/- 442.76
Episode length: 656.20 +/- 90.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 7135344  |
---------------------------------
Eval num_timesteps=7137336, episode_reward=1031.49 +/- 282.15
Episode length: 623.80 +/- 79.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7137336  |
---------------------------------
Eval num_timesteps=7139328, episode_reward=1048.23 +/- 351.15
Episode length: 742.60 +/- 82.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 7139328  |
---------------------------------
Eval num_timesteps=7141320, episode_reward=1042.44 +/- 222.17
Episode length: 618.80 +/- 101.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 7141320  |
---------------------------------
Eval num_timesteps=7143312, episode_reward=1327.41 +/- 314.26
Episode length: 700.00 +/- 93.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 7143312  |
---------------------------------
Eval num_timesteps=7145304, episode_reward=970.18 +/- 317.18
Episode length: 648.80 +/- 88.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 7145304  |
---------------------------------
Eval num_timesteps=7147296, episode_reward=1171.16 +/- 548.70
Episode length: 664.20 +/- 91.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 7147296  |
---------------------------------
Eval num_timesteps=7149288, episode_reward=1564.57 +/- 149.16
Episode length: 714.00 +/- 119.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 7149288  |
---------------------------------
Eval num_timesteps=7151280, episode_reward=1179.69 +/- 297.31
Episode length: 627.20 +/- 107.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 7151280  |
---------------------------------
Eval num_timesteps=7153272, episode_reward=1231.68 +/- 285.60
Episode length: 610.00 +/- 72.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 7153272  |
---------------------------------
Eval num_timesteps=7155264, episode_reward=939.73 +/- 466.20
Episode length: 577.20 +/- 46.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 7155264  |
---------------------------------
Eval num_timesteps=7157256, episode_reward=1183.81 +/- 466.26
Episode length: 636.20 +/- 109.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 7157256  |
---------------------------------
Eval num_timesteps=7159248, episode_reward=1033.26 +/- 423.39
Episode length: 587.20 +/- 50.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7159248  |
---------------------------------
Eval num_timesteps=7161240, episode_reward=1070.30 +/- 466.67
Episode length: 598.00 +/- 42.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 7161240  |
---------------------------------
Eval num_timesteps=7163232, episode_reward=1379.22 +/- 799.69
Episode length: 733.40 +/- 98.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 7163232  |
---------------------------------
Eval num_timesteps=7165224, episode_reward=1068.14 +/- 262.82
Episode length: 742.00 +/- 179.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 7165224  |
---------------------------------
Eval num_timesteps=7167216, episode_reward=1153.57 +/- 419.22
Episode length: 600.80 +/- 52.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 7167216  |
---------------------------------
Eval num_timesteps=7169208, episode_reward=1634.94 +/- 346.69
Episode length: 627.80 +/- 114.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 7169208  |
---------------------------------
Eval num_timesteps=7171200, episode_reward=1308.41 +/- 369.69
Episode length: 798.00 +/- 202.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 7171200  |
---------------------------------
Eval num_timesteps=7173192, episode_reward=1577.07 +/- 442.37
Episode length: 594.20 +/- 56.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 7173192  |
---------------------------------
Eval num_timesteps=7175184, episode_reward=1376.75 +/- 314.19
Episode length: 762.20 +/- 163.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 7175184  |
---------------------------------
Eval num_timesteps=7177176, episode_reward=1190.47 +/- 375.09
Episode length: 621.60 +/- 79.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 622          |
|    mean_reward          | 1.19e+03     |
| time/                   |              |
|    total_timesteps      | 7177176      |
| train/                  |              |
|    approx_kl            | 0.0047197635 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0663      |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 2.04         |
|    value_loss           | 0.0455       |
------------------------------------------
Eval num_timesteps=7179168, episode_reward=1156.13 +/- 400.80
Episode length: 633.60 +/- 83.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 7179168  |
---------------------------------
Eval num_timesteps=7181160, episode_reward=1021.36 +/- 261.31
Episode length: 779.00 +/- 266.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 7181160  |
---------------------------------
Eval num_timesteps=7183152, episode_reward=1238.07 +/- 446.75
Episode length: 775.60 +/- 59.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 7183152  |
---------------------------------
Eval num_timesteps=7185144, episode_reward=844.81 +/- 66.80
Episode length: 715.40 +/- 147.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 7185144  |
---------------------------------
Eval num_timesteps=7187136, episode_reward=1151.02 +/- 449.73
Episode length: 658.00 +/- 51.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 7187136  |
---------------------------------
Eval num_timesteps=7189128, episode_reward=911.52 +/- 180.50
Episode length: 649.00 +/- 48.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 7189128  |
---------------------------------
Eval num_timesteps=7191120, episode_reward=1592.31 +/- 379.46
Episode length: 738.20 +/- 164.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 7191120  |
---------------------------------
Eval num_timesteps=7193112, episode_reward=1480.76 +/- 534.98
Episode length: 832.00 +/- 230.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 7193112  |
---------------------------------
Eval num_timesteps=7195104, episode_reward=1380.68 +/- 268.76
Episode length: 800.60 +/- 153.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 7195104  |
---------------------------------
Eval num_timesteps=7197096, episode_reward=1218.85 +/- 247.17
Episode length: 670.60 +/- 44.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 7197096  |
---------------------------------
Eval num_timesteps=7199088, episode_reward=876.09 +/- 161.10
Episode length: 751.00 +/- 95.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 7199088  |
---------------------------------
Eval num_timesteps=7201080, episode_reward=1303.40 +/- 708.88
Episode length: 812.00 +/- 198.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 7201080  |
---------------------------------
Eval num_timesteps=7203072, episode_reward=1320.78 +/- 330.20
Episode length: 625.20 +/- 158.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 7203072  |
---------------------------------
Eval num_timesteps=7205064, episode_reward=1271.24 +/- 302.87
Episode length: 753.20 +/- 172.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 7205064  |
---------------------------------
Eval num_timesteps=7207056, episode_reward=1530.81 +/- 540.26
Episode length: 795.00 +/- 174.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 7207056  |
---------------------------------
Eval num_timesteps=7209048, episode_reward=1642.04 +/- 247.82
Episode length: 647.40 +/- 187.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 7209048  |
---------------------------------
Eval num_timesteps=7211040, episode_reward=1033.87 +/- 200.39
Episode length: 785.00 +/- 187.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7211040  |
---------------------------------
Eval num_timesteps=7213032, episode_reward=1290.70 +/- 355.37
Episode length: 631.00 +/- 108.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 7213032  |
---------------------------------
Eval num_timesteps=7215024, episode_reward=1131.91 +/- 451.79
Episode length: 729.40 +/- 228.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 7215024  |
---------------------------------
Eval num_timesteps=7217016, episode_reward=1384.76 +/- 361.82
Episode length: 930.00 +/- 107.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 7217016  |
---------------------------------
Eval num_timesteps=7219008, episode_reward=902.92 +/- 229.07
Episode length: 656.60 +/- 132.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 7219008  |
---------------------------------
Eval num_timesteps=7221000, episode_reward=1201.62 +/- 330.92
Episode length: 669.60 +/- 78.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 7221000  |
---------------------------------
Eval num_timesteps=7222992, episode_reward=1485.07 +/- 336.79
Episode length: 911.00 +/- 245.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 7222992  |
---------------------------------
Eval num_timesteps=7224984, episode_reward=952.24 +/- 331.40
Episode length: 651.00 +/- 81.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 7224984  |
---------------------------------
Eval num_timesteps=7226976, episode_reward=982.77 +/- 435.66
Episode length: 621.40 +/- 194.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 621          |
|    mean_reward          | 983          |
| time/                   |              |
|    total_timesteps      | 7226976      |
| train/                  |              |
|    approx_kl            | 0.0043958677 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0649      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 2.05         |
|    value_loss           | 0.0496       |
------------------------------------------
Eval num_timesteps=7228968, episode_reward=1086.48 +/- 384.50
Episode length: 698.40 +/- 64.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 7228968  |
---------------------------------
Eval num_timesteps=7230960, episode_reward=1160.66 +/- 579.84
Episode length: 731.20 +/- 206.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 7230960  |
---------------------------------
Eval num_timesteps=7232952, episode_reward=1030.48 +/- 386.79
Episode length: 630.00 +/- 72.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7232952  |
---------------------------------
Eval num_timesteps=7234944, episode_reward=1227.67 +/- 307.52
Episode length: 806.40 +/- 204.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 7234944  |
---------------------------------
Eval num_timesteps=7236936, episode_reward=1397.36 +/- 593.73
Episode length: 713.20 +/- 87.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 7236936  |
---------------------------------
Eval num_timesteps=7238928, episode_reward=1272.92 +/- 509.87
Episode length: 797.00 +/- 204.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 7238928  |
---------------------------------
Eval num_timesteps=7240920, episode_reward=1207.30 +/- 388.63
Episode length: 765.80 +/- 130.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 7240920  |
---------------------------------
Eval num_timesteps=7242912, episode_reward=1234.18 +/- 528.36
Episode length: 728.20 +/- 112.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 7242912  |
---------------------------------
Eval num_timesteps=7244904, episode_reward=1033.90 +/- 383.75
Episode length: 707.00 +/- 91.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 7244904  |
---------------------------------
Eval num_timesteps=7246896, episode_reward=1188.43 +/- 417.40
Episode length: 737.20 +/- 149.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 7246896  |
---------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 555, in <module>
    sim.run_full(args)
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 349, in run_full
    model.learn(total_timesteps=int(1e7),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 194, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 205, in step
    self.step_async(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 369, in step_async
    self.venv.step_async(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 125, in step_async
    remote.send(("step", action))
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 280, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed