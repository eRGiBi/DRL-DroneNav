AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO 01.07.2024_21.45.53_1
C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py:155: UserWarning: You have specified a mini-batch size of 49152, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=2048 and n_envs=1)
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=2000, episode_reward=-112.84 +/- 56.78
Episode length: 246.60 +/- 70.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-140.09 +/- 51.36
Episode length: 215.40 +/- 43.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 215       |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 4000      |
| train/                  |           |
|    approx_kl            | 0.0085294 |
|    clip_fraction        | 0.0397    |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.68     |
|    explained_variance   | -2.63e-05 |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+03  |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.00697  |
|    std                  | 1         |
|    value_loss           | 3.26e+03  |
---------------------------------------
Eval num_timesteps=6000, episode_reward=-125.34 +/- 43.88
Episode length: 230.40 +/- 41.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 6000         |
| train/                  |              |
|    approx_kl            | 0.0028840117 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0563       |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+03      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00203     |
|    std                  | 1            |
|    value_loss           | 3.44e+03     |
------------------------------------------
Eval num_timesteps=8000, episode_reward=-120.92 +/- 27.06
Episode length: 246.20 +/- 31.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 8000         |
| train/                  |              |
|    approx_kl            | 0.0033121682 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0316       |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1            |
|    value_loss           | 2.79e+03     |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-118.83 +/- 21.22
Episode length: 246.00 +/- 30.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.004625364 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.001       |
|    loss                 | 1.7e+03     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00323    |
|    std                  | 0.999       |
|    value_loss           | 3.42e+03    |
-----------------------------------------
Eval num_timesteps=12000, episode_reward=-106.67 +/- 19.19
Episode length: 305.20 +/- 30.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | -107        |
| time/                   |             |
|    total_timesteps      | 12000       |
| train/                  |             |
|    approx_kl            | 0.002628095 |
|    clip_fraction        | 0.00278     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.0706      |
|    learning_rate        | 0.001       |
|    loss                 | 1.66e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000919   |
|    std                  | 1           |
|    value_loss           | 3.35e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=14000, episode_reward=-119.79 +/- 40.50
Episode length: 269.80 +/- 89.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 270        |
|    mean_reward          | -120       |
| time/                   |            |
|    total_timesteps      | 14000      |
| train/                  |            |
|    approx_kl            | 0.00367097 |
|    clip_fraction        | 0.00571    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.68      |
|    explained_variance   | 0.102      |
|    learning_rate        | 0.001      |
|    loss                 | 1.52e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00285   |
|    std                  | 1          |
|    value_loss           | 3.11e+03   |
----------------------------------------
Eval num_timesteps=16000, episode_reward=-129.81 +/- 55.11
Episode length: 249.40 +/- 79.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 16000        |
| train/                  |              |
|    approx_kl            | 0.0014394587 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000972    |
|    std                  | 1            |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=-106.61 +/- 23.13
Episode length: 281.00 +/- 38.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -107         |
| time/                   |              |
|    total_timesteps      | 18000        |
| train/                  |              |
|    approx_kl            | 0.0008402229 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.01         |
|    value_loss           | 2.45e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-130.84 +/- 53.84
Episode length: 237.80 +/- 56.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0012924211 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000803    |
|    std                  | 1.01         |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=22000, episode_reward=-114.21 +/- 24.13
Episode length: 241.20 +/- 21.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 241           |
|    mean_reward          | -114          |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 0.00078787265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.71         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+03      |
|    n_updates            | 100           |
|    policy_gradient_loss | -0.00115      |
|    std                  | 1.01          |
|    value_loss           | 2.64e+03      |
-------------------------------------------
Eval num_timesteps=24000, episode_reward=-123.88 +/- 13.96
Episode length: 241.20 +/- 27.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 0.0012771292 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.01         |
|    value_loss           | 2.29e+03     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-71.97 +/- 42.83
Episode length: 324.80 +/- 44.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | -72          |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 0.0006382183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000461    |
|    std                  | 1.01         |
|    value_loss           | 2.38e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=28000, episode_reward=-98.71 +/- 48.44
Episode length: 291.20 +/- 55.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | -98.7        |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 0.0009354926 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.001        |
|    loss                 | 951          |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.01         |
|    value_loss           | 1.97e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-80.09 +/- 72.29
Episode length: 298.80 +/- 83.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | -80.1        |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0022884274 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.01         |
|    value_loss           | 2.55e+03     |
------------------------------------------
Eval num_timesteps=32000, episode_reward=-136.84 +/- 39.02
Episode length: 245.60 +/- 66.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 32000       |
| train/                  |             |
|    approx_kl            | 0.002465032 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.73       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00159    |
|    std                  | 1.02        |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=34000, episode_reward=-135.41 +/- 40.29
Episode length: 236.40 +/- 67.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 0.0012685757 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.74        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000543    |
|    std                  | 1.02         |
|    value_loss           | 2.54e+03     |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-158.54 +/- 7.12
Episode length: 206.80 +/- 15.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 207           |
|    mean_reward          | -159          |
| time/                   |               |
|    total_timesteps      | 36000         |
| train/                  |               |
|    approx_kl            | 0.00019058707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.74         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+03      |
|    n_updates            | 170           |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.02          |
|    value_loss           | 2.97e+03      |
-------------------------------------------
Eval num_timesteps=38000, episode_reward=-129.72 +/- 61.64
Episode length: 229.80 +/- 61.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 230           |
|    mean_reward          | -130          |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 0.00072516676 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.75         |
|    explained_variance   | 0.446         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000736     |
|    std                  | 1.02          |
|    value_loss           | 2.33e+03      |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-140.93 +/- 49.46
Episode length: 217.60 +/- 33.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | -141         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0022774832 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.76        |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.02         |
|    value_loss           | 2.61e+03     |
------------------------------------------
Eval num_timesteps=42000, episode_reward=29.62 +/- 120.61
Episode length: 400.20 +/- 128.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 42000        |
| train/                  |              |
|    approx_kl            | 0.0025566095 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.001        |
|    loss                 | 989          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.03         |
|    value_loss           | 2.01e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=44000, episode_reward=-45.50 +/- 128.27
Episode length: 415.80 +/- 191.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | -45.5        |
| time/                   |              |
|    total_timesteps      | 44000        |
| train/                  |              |
|    approx_kl            | 0.0026601045 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.001        |
|    loss                 | 704          |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.03         |
|    value_loss           | 1.42e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-31.43 +/- 145.40
Episode length: 355.40 +/- 130.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | -31.4        |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0033045304 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.03         |
|    value_loss           | 2.17e+03     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-58.25 +/- 55.81
Episode length: 372.60 +/- 86.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | -58.3        |
| time/                   |              |
|    total_timesteps      | 48000        |
| train/                  |              |
|    approx_kl            | 0.0049638026 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.82        |
|    explained_variance   | 0.489        |
|    learning_rate        | 0.001        |
|    loss                 | 883          |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00375     |
|    std                  | 1.04         |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-57.49 +/- 59.58
Episode length: 364.20 +/- 75.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | -57.5         |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00069019385 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 0.53          |
|    learning_rate        | 0.001         |
|    loss                 | 675           |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.04          |
|    value_loss           | 1.38e+03      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=-28.48 +/- 75.87
Episode length: 345.80 +/- 66.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | -28.5        |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0011702944 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.001        |
|    loss                 | 810          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00161     |
|    std                  | 1.05         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-49.72 +/- 37.95
Episode length: 389.40 +/- 87.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | -49.7        |
| time/                   |              |
|    total_timesteps      | 54000        |
| train/                  |              |
|    approx_kl            | 0.0028426987 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.001        |
|    loss                 | 792          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00397     |
|    std                  | 1.05         |
|    value_loss           | 1.6e+03      |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-12.61 +/- 39.88
Episode length: 451.60 +/- 91.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | -12.6        |
| time/                   |              |
|    total_timesteps      | 56000        |
| train/                  |              |
|    approx_kl            | 0.0039555775 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.001        |
|    loss                 | 543          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.06         |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-41.10 +/- 49.23
Episode length: 405.20 +/- 82.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | -41.1        |
| time/                   |              |
|    total_timesteps      | 58000        |
| train/                  |              |
|    approx_kl            | 0.0015487091 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.91        |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.001        |
|    loss                 | 664          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00109     |
|    std                  | 1.06         |
|    value_loss           | 1.36e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-8.38 +/- 41.08
Episode length: 533.80 +/- 57.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 534          |
|    mean_reward          | -8.38        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0023949652 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.635        |
|    learning_rate        | 0.001        |
|    loss                 | 658          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.06         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-27.21 +/- 21.33
Episode length: 451.60 +/- 70.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | -27.2        |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0025894663 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.66         |
|    learning_rate        | 0.001        |
|    loss                 | 528          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.07         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-75.76 +/- 33.81
Episode length: 379.20 +/- 102.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | -75.8        |
| time/                   |              |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 0.0031362975 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.001        |
|    loss                 | 656          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.07         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=66000, episode_reward=-38.74 +/- 56.16
Episode length: 466.20 +/- 127.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | -38.7        |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0107475985 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 390          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00385     |
|    std                  | 1.07         |
|    value_loss           | 789          |
------------------------------------------
Eval num_timesteps=68000, episode_reward=-97.40 +/- 18.33
Episode length: 327.00 +/- 55.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 327          |
|    mean_reward          | -97.4        |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0025528031 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.645        |
|    learning_rate        | 0.001        |
|    loss                 | 749          |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.07         |
|    value_loss           | 1.52e+03     |
------------------------------------------
Eval num_timesteps=70000, episode_reward=-116.28 +/- 14.82
Episode length: 289.80 +/- 26.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 290         |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.004330232 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.001       |
|    loss                 | 835         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00237    |
|    std                  | 1.07        |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=72000, episode_reward=-137.05 +/- 26.53
Episode length: 251.60 +/- 50.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.003921655 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.001       |
|    loss                 | 674         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00149    |
|    std                  | 1.07        |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=74000, episode_reward=-95.26 +/- 47.59
Episode length: 281.40 +/- 61.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | -95.3       |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.005012417 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.001       |
|    loss                 | 608         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00182    |
|    std                  | 1.07        |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=76000, episode_reward=-121.93 +/- 12.90
Episode length: 268.60 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -122          |
| time/                   |               |
|    total_timesteps      | 76000         |
| train/                  |               |
|    approx_kl            | 0.00097033265 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 946           |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.00108      |
|    std                  | 1.07          |
|    value_loss           | 1.94e+03      |
-------------------------------------------
Eval num_timesteps=78000, episode_reward=-120.56 +/- 35.32
Episode length: 253.40 +/- 55.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | -121        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.012942723 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.001       |
|    loss                 | 714         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.07        |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=-142.12 +/- 41.91
Episode length: 233.20 +/- 69.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0065776156 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.693        |
|    learning_rate        | 0.001        |
|    loss                 | 596          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0032      |
|    std                  | 1.07         |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=82000, episode_reward=-110.35 +/- 47.75
Episode length: 298.00 +/- 97.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | -110         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0012628009 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.664        |
|    learning_rate        | 0.001        |
|    loss                 | 826          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000992    |
|    std                  | 1.07         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=84000, episode_reward=-95.43 +/- 50.63
Episode length: 304.40 +/- 83.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | -95.4        |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0016812116 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.678        |
|    learning_rate        | 0.001        |
|    loss                 | 814          |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.07         |
|    value_loss           | 1.67e+03     |
------------------------------------------
Eval num_timesteps=86000, episode_reward=-114.72 +/- 27.16
Episode length: 296.40 +/- 45.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-90.01 +/- 49.57
Episode length: 314.00 +/- 83.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 314           |
|    mean_reward          | -90           |
| time/                   |               |
|    total_timesteps      | 88000         |
| train/                  |               |
|    approx_kl            | 0.00059005385 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.662         |
|    learning_rate        | 0.001         |
|    loss                 | 967           |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000596     |
|    std                  | 1.07          |
|    value_loss           | 1.98e+03      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-117.07 +/- 64.64
Episode length: 277.20 +/- 107.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 277           |
|    mean_reward          | -117          |
| time/                   |               |
|    total_timesteps      | 90000         |
| train/                  |               |
|    approx_kl            | 0.00047045044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.703         |
|    learning_rate        | 0.001         |
|    loss                 | 707           |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.00124      |
|    std                  | 1.07          |
|    value_loss           | 1.44e+03      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=-143.67 +/- 37.96
Episode length: 247.00 +/- 69.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 92000        |
| train/                  |              |
|    approx_kl            | 0.0030211576 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.96        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 817          |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.07         |
|    value_loss           | 1.66e+03     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-110.30 +/- 18.17
Episode length: 288.80 +/- 30.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | -110        |
| time/                   |             |
|    total_timesteps      | 94000       |
| train/                  |             |
|    approx_kl            | 0.006502066 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.001       |
|    loss                 | 704         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00432    |
|    std                  | 1.08        |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=-82.24 +/- 70.45
Episode length: 316.00 +/- 95.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | -82.2         |
| time/                   |               |
|    total_timesteps      | 96000         |
| train/                  |               |
|    approx_kl            | 0.00083652127 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.98         |
|    explained_variance   | 0.656         |
|    learning_rate        | 0.001         |
|    loss                 | 917           |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000803     |
|    std                  | 1.08          |
|    value_loss           | 1.85e+03      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=-120.19 +/- 51.15
Episode length: 259.20 +/- 61.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 259           |
|    mean_reward          | -120          |
| time/                   |               |
|    total_timesteps      | 98000         |
| train/                  |               |
|    approx_kl            | 0.00033029553 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.98         |
|    explained_variance   | 0.682         |
|    learning_rate        | 0.001         |
|    loss                 | 923           |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000321     |
|    std                  | 1.08          |
|    value_loss           | 1.86e+03      |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-146.36 +/- 35.03
Episode length: 227.40 +/- 54.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | -146        |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.001138022 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.001       |
|    loss                 | 915         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00151    |
|    std                  | 1.08        |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=102000, episode_reward=-137.14 +/- 29.11
Episode length: 251.20 +/- 61.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 102000       |
| train/                  |              |
|    approx_kl            | 0.0023559912 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.656        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00223     |
|    std                  | 1.08         |
|    value_loss           | 2.02e+03     |
------------------------------------------
Eval num_timesteps=104000, episode_reward=-89.19 +/- 51.13
Episode length: 307.80 +/- 74.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -89.2        |
| time/                   |              |
|    total_timesteps      | 104000       |
| train/                  |              |
|    approx_kl            | 0.0009777483 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 771          |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000763    |
|    std                  | 1.08         |
|    value_loss           | 1.57e+03     |
------------------------------------------
Eval num_timesteps=106000, episode_reward=-87.59 +/- 86.94
Episode length: 296.00 +/- 111.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | -87.6        |
| time/                   |              |
|    total_timesteps      | 106000       |
| train/                  |              |
|    approx_kl            | 0.0021851258 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.001        |
|    loss                 | 892          |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.08         |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=108000, episode_reward=-71.33 +/- 66.22
Episode length: 341.20 +/- 102.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | -71.3        |
| time/                   |              |
|    total_timesteps      | 108000       |
| train/                  |              |
|    approx_kl            | 0.0016708465 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 779          |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000538    |
|    std                  | 1.08         |
|    value_loss           | 1.6e+03      |
------------------------------------------
Eval num_timesteps=110000, episode_reward=-65.05 +/- 48.81
Episode length: 350.60 +/- 83.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | -65.1        |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0010936917 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6           |
|    explained_variance   | 0.713        |
|    learning_rate        | 0.001        |
|    loss                 | 774          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.08         |
|    value_loss           | 1.56e+03     |
------------------------------------------
Eval num_timesteps=112000, episode_reward=-106.60 +/- 50.39
Episode length: 287.20 +/- 79.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | -107        |
| time/                   |             |
|    total_timesteps      | 112000      |
| train/                  |             |
|    approx_kl            | 0.000921373 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.001       |
|    loss                 | 664         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00131    |
|    std                  | 1.09        |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=114000, episode_reward=-122.81 +/- 66.02
Episode length: 253.20 +/- 85.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | -123         |
| time/                   |              |
|    total_timesteps      | 114000       |
| train/                  |              |
|    approx_kl            | 0.0018262492 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.001        |
|    loss                 | 763          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.09         |
|    value_loss           | 1.55e+03     |
------------------------------------------
Eval num_timesteps=116000, episode_reward=-127.11 +/- 54.70
Episode length: 277.20 +/- 100.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 116000       |
| train/                  |              |
|    approx_kl            | 0.0018787023 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.001        |
|    loss                 | 760          |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.09         |
|    value_loss           | 1.54e+03     |
------------------------------------------
Eval num_timesteps=118000, episode_reward=-171.53 +/- 7.32
Episode length: 186.20 +/- 9.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 186          |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 118000       |
| train/                  |              |
|    approx_kl            | 0.0029605983 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.734        |
|    learning_rate        | 0.001        |
|    loss                 | 657          |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.09         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=120000, episode_reward=-95.70 +/- 61.36
Episode length: 295.00 +/- 87.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | -95.7        |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0011118986 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.001        |
|    loss                 | 856          |
|    n_updates            | 580          |
|    policy_gradient_loss | 0.000197     |
|    std                  | 1.09         |
|    value_loss           | 1.76e+03     |
------------------------------------------
Eval num_timesteps=122000, episode_reward=-97.96 +/- 81.87
Episode length: 306.80 +/- 110.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -98          |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0031865796 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.001        |
|    loss                 | 438          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00304     |
|    std                  | 1.09         |
|    value_loss           | 889          |
------------------------------------------
Eval num_timesteps=124000, episode_reward=-114.90 +/- 45.11
Episode length: 291.60 +/- 85.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 292          |
|    mean_reward          | -115         |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 0.0041866875 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.02        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 689          |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00193     |
|    std                  | 1.09         |
|    value_loss           | 1.46e+03     |
------------------------------------------
Eval num_timesteps=126000, episode_reward=-56.82 +/- 66.79
Episode length: 373.80 +/- 88.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | -56.8       |
| time/                   |             |
|    total_timesteps      | 126000      |
| train/                  |             |
|    approx_kl            | 0.002130798 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.001       |
|    loss                 | 528         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00197    |
|    std                  | 1.09        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=128000, episode_reward=-70.35 +/- 62.05
Episode length: 374.40 +/- 117.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | -70.3        |
| time/                   |              |
|    total_timesteps      | 128000       |
| train/                  |              |
|    approx_kl            | 0.0018474897 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 742          |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000305    |
|    std                  | 1.09         |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-98.06 +/- 53.54
Episode length: 314.00 +/- 86.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | -98.1        |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0023838647 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 748          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.09         |
|    value_loss           | 1.51e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-68.10 +/- 49.38
Episode length: 321.00 +/- 59.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | -68.1       |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.004478093 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.001       |
|    loss                 | 626         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00247    |
|    std                  | 1.1         |
|    value_loss           | 1.27e+03    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-98.43 +/- 51.51
Episode length: 308.00 +/- 67.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -98.4        |
| time/                   |              |
|    total_timesteps      | 134000       |
| train/                  |              |
|    approx_kl            | 0.0017550603 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.776        |
|    learning_rate        | 0.001        |
|    loss                 | 622          |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.000315    |
|    std                  | 1.1          |
|    value_loss           | 1.27e+03     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-113.99 +/- 29.67
Episode length: 281.00 +/- 51.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -114         |
| time/                   |              |
|    total_timesteps      | 136000       |
| train/                  |              |
|    approx_kl            | 0.0021363136 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.704        |
|    learning_rate        | 0.001        |
|    loss                 | 832          |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.1          |
|    value_loss           | 1.73e+03     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-80.42 +/- 79.49
Episode length: 329.20 +/- 133.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | -80.4        |
| time/                   |              |
|    total_timesteps      | 138000       |
| train/                  |              |
|    approx_kl            | 0.0014091064 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.758        |
|    learning_rate        | 0.001        |
|    loss                 | 750          |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.1          |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-86.37 +/- 33.77
Episode length: 307.60 +/- 58.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -86.4        |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0012493368 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.001        |
|    loss                 | 729          |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.1          |
|    value_loss           | 1.47e+03     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-138.76 +/- 39.21
Episode length: 238.40 +/- 61.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 238           |
|    mean_reward          | -139          |
| time/                   |               |
|    total_timesteps      | 142000        |
| train/                  |               |
|    approx_kl            | 0.00065209717 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.06         |
|    explained_variance   | 0.768         |
|    learning_rate        | 0.001         |
|    loss                 | 675           |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000995     |
|    std                  | 1.1           |
|    value_loss           | 1.38e+03      |
-------------------------------------------
Eval num_timesteps=144000, episode_reward=-128.48 +/- 22.70
Episode length: 266.00 +/- 51.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 266           |
|    mean_reward          | -128          |
| time/                   |               |
|    total_timesteps      | 144000        |
| train/                  |               |
|    approx_kl            | 0.00069751195 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.75          |
|    learning_rate        | 0.001         |
|    loss                 | 830           |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000239     |
|    std                  | 1.1           |
|    value_loss           | 1.69e+03      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=-49.33 +/- 47.23
Episode length: 378.60 +/- 80.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | -49.3        |
| time/                   |              |
|    total_timesteps      | 146000       |
| train/                  |              |
|    approx_kl            | 0.0012283282 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 609          |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.1          |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-99.79 +/- 55.78
Episode length: 298.60 +/- 97.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | -99.8         |
| time/                   |               |
|    total_timesteps      | 148000        |
| train/                  |               |
|    approx_kl            | 0.00063365465 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.001         |
|    loss                 | 713           |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000416     |
|    std                  | 1.11          |
|    value_loss           | 1.45e+03      |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=12.09 +/- 86.25
Episode length: 429.80 +/- 99.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0016270678 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.08        |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 634          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00192     |
|    std                  | 1.11         |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=152000, episode_reward=-47.34 +/- 37.12
Episode length: 423.00 +/- 100.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | -47.3       |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.004483111 |
|    clip_fraction        | 0.00903     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.001       |
|    loss                 | 510         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.11        |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=154000, episode_reward=98.54 +/- 205.32
Episode length: 511.00 +/- 182.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | 98.5         |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0032039504 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 526          |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.11         |
|    value_loss           | 1.07e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=156000, episode_reward=48.33 +/- 56.61
Episode length: 607.00 +/- 94.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 48.3         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0063198535 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 416          |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00537     |
|    std                  | 1.11         |
|    value_loss           | 883          |
------------------------------------------
Eval num_timesteps=158000, episode_reward=190.14 +/- 99.82
Episode length: 770.40 +/- 137.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 770          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0029511722 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.001        |
|    loss                 | 395          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0029      |
|    std                  | 1.11         |
|    value_loss           | 827          |
------------------------------------------
New best mean reward!
Eval num_timesteps=160000, episode_reward=360.22 +/- 495.47
Episode length: 677.00 +/- 132.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 677          |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0034135468 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.001        |
|    loss                 | 508          |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00284     |
|    std                  | 1.11         |
|    value_loss           | 1.07e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=162000, episode_reward=182.14 +/- 152.65
Episode length: 727.60 +/- 310.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 728        |
|    mean_reward          | 182        |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.00132553 |
|    clip_fraction        | 0.000928   |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.11      |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.001      |
|    loss                 | 312        |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.00182   |
|    std                  | 1.11       |
|    value_loss           | 808        |
----------------------------------------
Eval num_timesteps=164000, episode_reward=278.38 +/- 207.28
Episode length: 770.00 +/- 143.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 770          |
|    mean_reward          | 278          |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0015673733 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.702        |
|    learning_rate        | 0.001        |
|    loss                 | 555          |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000727    |
|    std                  | 1.12         |
|    value_loss           | 1.15e+03     |
------------------------------------------
Eval num_timesteps=166000, episode_reward=165.77 +/- 98.40
Episode length: 646.60 +/- 93.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 647         |
|    mean_reward          | 166         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.002042999 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.001       |
|    loss                 | 418         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00176    |
|    std                  | 1.12        |
|    value_loss           | 862         |
-----------------------------------------
Eval num_timesteps=168000, episode_reward=155.80 +/- 250.10
Episode length: 668.20 +/- 140.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 668          |
|    mean_reward          | 156          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0043501537 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.12        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 392          |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00296     |
|    std                  | 1.12         |
|    value_loss           | 844          |
------------------------------------------
Eval num_timesteps=170000, episode_reward=125.50 +/- 168.89
Episode length: 722.40 +/- 192.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 722          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0043889424 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 457          |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00271     |
|    std                  | 1.12         |
|    value_loss           | 972          |
------------------------------------------
Eval num_timesteps=172000, episode_reward=95.27 +/- 96.54
Episode length: 676.40 +/- 159.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 95.3     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=174000, episode_reward=608.90 +/- 900.96
Episode length: 846.00 +/- 292.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 846         |
|    mean_reward          | 609         |
| time/                   |             |
|    total_timesteps      | 174000      |
| train/                  |             |
|    approx_kl            | 0.004640513 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.14       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.001       |
|    loss                 | 296         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00279    |
|    std                  | 1.13        |
|    value_loss           | 688         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=176000, episode_reward=58.24 +/- 96.05
Episode length: 624.80 +/- 152.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 625          |
|    mean_reward          | 58.2         |
| time/                   |              |
|    total_timesteps      | 176000       |
| train/                  |              |
|    approx_kl            | 0.0044545773 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.15        |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.001        |
|    loss                 | 561          |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.13         |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=94.09 +/- 66.49
Episode length: 742.40 +/- 108.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 742         |
|    mean_reward          | 94.1        |
| time/                   |             |
|    total_timesteps      | 178000      |
| train/                  |             |
|    approx_kl            | 0.005310307 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.001       |
|    loss                 | 362         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0033     |
|    std                  | 1.13        |
|    value_loss           | 806         |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=58.14 +/- 69.86
Episode length: 702.80 +/- 141.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 703         |
|    mean_reward          | 58.1        |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.009105243 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 310         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00469    |
|    std                  | 1.13        |
|    value_loss           | 635         |
-----------------------------------------
Eval num_timesteps=182000, episode_reward=120.07 +/- 118.21
Episode length: 728.60 +/- 143.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 729          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 182000       |
| train/                  |              |
|    approx_kl            | 0.0056299316 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.19        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 405          |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.14         |
|    value_loss           | 865          |
------------------------------------------
Eval num_timesteps=184000, episode_reward=94.13 +/- 107.05
Episode length: 657.40 +/- 167.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 657         |
|    mean_reward          | 94.1        |
| time/                   |             |
|    total_timesteps      | 184000      |
| train/                  |             |
|    approx_kl            | 0.010272846 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.001       |
|    loss                 | 336         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00377    |
|    std                  | 1.14        |
|    value_loss           | 724         |
-----------------------------------------
Eval num_timesteps=186000, episode_reward=579.42 +/- 980.91
Episode length: 844.40 +/- 316.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 844          |
|    mean_reward          | 579          |
| time/                   |              |
|    total_timesteps      | 186000       |
| train/                  |              |
|    approx_kl            | 0.0012811199 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.738        |
|    learning_rate        | 0.001        |
|    loss                 | 428          |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.000767    |
|    std                  | 1.15         |
|    value_loss           | 890          |
------------------------------------------
Eval num_timesteps=188000, episode_reward=32.47 +/- 60.26
Episode length: 644.60 +/- 157.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 645        |
|    mean_reward          | 32.5       |
| time/                   |            |
|    total_timesteps      | 188000     |
| train/                  |            |
|    approx_kl            | 0.01383828 |
|    clip_fraction        | 0.0529     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.001      |
|    loss                 | 342        |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.00651   |
|    std                  | 1.15       |
|    value_loss           | 725        |
----------------------------------------
Eval num_timesteps=190000, episode_reward=158.25 +/- 94.59
Episode length: 744.80 +/- 109.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 745          |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0033675423 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.23        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 307          |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00231     |
|    std                  | 1.15         |
|    value_loss           | 650          |
------------------------------------------
Eval num_timesteps=192000, episode_reward=164.05 +/- 135.77
Episode length: 625.00 +/- 105.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 192000      |
| train/                  |             |
|    approx_kl            | 0.005965256 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.001       |
|    loss                 | 329         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.16        |
|    value_loss           | 834         |
-----------------------------------------
Eval num_timesteps=194000, episode_reward=203.85 +/- 125.41
Episode length: 724.20 +/- 125.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 724          |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 194000       |
| train/                  |              |
|    approx_kl            | 0.0014380219 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.27        |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 409          |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.16         |
|    value_loss           | 900          |
------------------------------------------
Eval num_timesteps=196000, episode_reward=166.46 +/- 270.38
Episode length: 660.60 +/- 175.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 661          |
|    mean_reward          | 166          |
| time/                   |              |
|    total_timesteps      | 196000       |
| train/                  |              |
|    approx_kl            | 0.0051178937 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.001        |
|    loss                 | 466          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.0032      |
|    std                  | 1.17         |
|    value_loss           | 976          |
------------------------------------------
Eval num_timesteps=198000, episode_reward=162.30 +/- 198.97
Episode length: 657.40 +/- 229.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 657        |
|    mean_reward          | 162        |
| time/                   |            |
|    total_timesteps      | 198000     |
| train/                  |            |
|    approx_kl            | 0.00457043 |
|    clip_fraction        | 0.0109     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.31      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.001      |
|    loss                 | 299        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00242   |
|    std                  | 1.17       |
|    value_loss           | 659        |
----------------------------------------
Eval num_timesteps=200000, episode_reward=152.72 +/- 120.67
Episode length: 807.40 +/- 193.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.004387034 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.33       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00238    |
|    std                  | 1.18        |
|    value_loss           | 576         |
-----------------------------------------
Eval num_timesteps=202000, episode_reward=631.56 +/- 779.86
Episode length: 940.40 +/- 358.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 940         |
|    mean_reward          | 632         |
| time/                   |             |
|    total_timesteps      | 202000      |
| train/                  |             |
|    approx_kl            | 0.010043975 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 280         |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00554    |
|    std                  | 1.19        |
|    value_loss           | 590         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=204000, episode_reward=241.33 +/- 218.86
Episode length: 857.20 +/- 309.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 857          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 204000       |
| train/                  |              |
|    approx_kl            | 0.0040481137 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.36        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.19         |
|    value_loss           | 429          |
------------------------------------------
Eval num_timesteps=206000, episode_reward=164.29 +/- 229.54
Episode length: 801.00 +/- 390.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 801         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 206000      |
| train/                  |             |
|    approx_kl            | 0.002990792 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 222         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 1.2         |
|    value_loss           | 491         |
-----------------------------------------
Eval num_timesteps=208000, episode_reward=279.12 +/- 240.48
Episode length: 949.20 +/- 252.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 949          |
|    mean_reward          | 279          |
| time/                   |              |
|    total_timesteps      | 208000       |
| train/                  |              |
|    approx_kl            | 0.0023562987 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.4         |
|    explained_variance   | 0.614        |
|    learning_rate        | 0.001        |
|    loss                 | 343          |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 1.2          |
|    value_loss           | 828          |
------------------------------------------
Eval num_timesteps=210000, episode_reward=173.27 +/- 116.82
Episode length: 820.00 +/- 229.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 820          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0039888285 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.41        |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.001        |
|    loss                 | 328          |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.2          |
|    value_loss           | 688          |
------------------------------------------
Eval num_timesteps=212000, episode_reward=216.89 +/- 143.20
Episode length: 856.40 +/- 168.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 856         |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 212000      |
| train/                  |             |
|    approx_kl            | 0.008780886 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.001       |
|    loss                 | 415         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00378    |
|    std                  | 1.21        |
|    value_loss           | 855         |
-----------------------------------------
Eval num_timesteps=214000, episode_reward=531.82 +/- 444.78
Episode length: 925.20 +/- 142.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 925         |
|    mean_reward          | 532         |
| time/                   |             |
|    total_timesteps      | 214000      |
| train/                  |             |
|    approx_kl            | 0.027356297 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.001       |
|    loss                 | 311         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0077     |
|    std                  | 1.22        |
|    value_loss           | 736         |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=366.83 +/- 249.32
Episode length: 876.80 +/- 164.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 877         |
|    mean_reward          | 367         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.003736406 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 315         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00118    |
|    std                  | 1.22        |
|    value_loss           | 725         |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=276.67 +/- 265.85
Episode length: 743.80 +/- 158.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 744          |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 218000       |
| train/                  |              |
|    approx_kl            | 0.0012992494 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.001        |
|    loss                 | 3.45e+03     |
|    n_updates            | 1060         |
|    policy_gradient_loss | 9.96e-05     |
|    std                  | 1.22         |
|    value_loss           | 7.63e+03     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=331.89 +/- 183.68
Episode length: 927.60 +/- 247.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 928          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0048916643 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 204          |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 1.22         |
|    value_loss           | 439          |
------------------------------------------
Eval num_timesteps=222000, episode_reward=99.57 +/- 85.67
Episode length: 650.80 +/- 73.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 651         |
|    mean_reward          | 99.6        |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.010192127 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.001       |
|    loss                 | 528         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.000726   |
|    std                  | 1.22        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=77.36 +/- 102.66
Episode length: 630.60 +/- 131.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 631          |
|    mean_reward          | 77.4         |
| time/                   |              |
|    total_timesteps      | 224000       |
| train/                  |              |
|    approx_kl            | 0.0027833816 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 330          |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 1.22         |
|    value_loss           | 706          |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-9.77 +/- 37.86
Episode length: 493.00 +/- 91.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | -9.77       |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.006933964 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.001       |
|    loss                 | 418         |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00254    |
|    std                  | 1.23        |
|    value_loss           | 884         |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=129.84 +/- 137.84
Episode length: 680.40 +/- 180.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 680         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.003958728 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.001       |
|    loss                 | 348         |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 1.23        |
|    value_loss           | 718         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-16.89 +/- 16.35
Episode length: 522.20 +/- 24.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | -16.9        |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0067954143 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 440          |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00471     |
|    std                  | 1.23         |
|    value_loss           | 915          |
------------------------------------------
Eval num_timesteps=232000, episode_reward=4.13 +/- 69.93
Episode length: 542.00 +/- 102.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | 4.13         |
| time/                   |              |
|    total_timesteps      | 232000       |
| train/                  |              |
|    approx_kl            | 0.0060774893 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 460          |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 1.23         |
|    value_loss           | 933          |
------------------------------------------
Eval num_timesteps=234000, episode_reward=45.54 +/- 33.77
Episode length: 605.00 +/- 79.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 605        |
|    mean_reward          | 45.5       |
| time/                   |            |
|    total_timesteps      | 234000     |
| train/                  |            |
|    approx_kl            | 0.00413334 |
|    clip_fraction        | 0.0062     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.51      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.001      |
|    loss                 | 304        |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.00164   |
|    std                  | 1.23       |
|    value_loss           | 646        |
----------------------------------------
Eval num_timesteps=236000, episode_reward=35.65 +/- 165.91
Episode length: 515.20 +/- 263.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 35.6        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.012301223 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 425         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0045     |
|    std                  | 1.24        |
|    value_loss           | 875         |
-----------------------------------------
Eval num_timesteps=238000, episode_reward=242.16 +/- 236.61
Episode length: 625.60 +/- 228.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 626         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.013862609 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.001       |
|    loss                 | 300         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00428    |
|    std                  | 1.25        |
|    value_loss           | 616         |
-----------------------------------------
Eval num_timesteps=240000, episode_reward=272.90 +/- 248.04
Episode length: 553.40 +/- 136.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 553         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.011151483 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.56       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.001       |
|    loss                 | 396         |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00393    |
|    std                  | 1.25        |
|    value_loss           | 817         |
-----------------------------------------
Eval num_timesteps=242000, episode_reward=423.39 +/- 317.62
Episode length: 583.20 +/- 64.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 583         |
|    mean_reward          | 423         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.013611348 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.57       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.001       |
|    loss                 | 500         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 1.26        |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=244000, episode_reward=567.19 +/- 328.86
Episode length: 797.20 +/- 116.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 797         |
|    mean_reward          | 567         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.007647325 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.59       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.001       |
|    loss                 | 2.56e+03    |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 1.26        |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=246000, episode_reward=261.90 +/- 216.17
Episode length: 679.60 +/- 229.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 680          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0020854976 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 441          |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 1.26         |
|    value_loss           | 906          |
------------------------------------------
Eval num_timesteps=248000, episode_reward=212.57 +/- 288.20
Episode length: 642.20 +/- 258.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 642         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.020255642 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 312         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00503    |
|    std                  | 1.26        |
|    value_loss           | 671         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=177.89 +/- 183.57
Episode length: 697.80 +/- 195.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 698         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.008731096 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 227         |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 1.27        |
|    value_loss           | 487         |
-----------------------------------------
Eval num_timesteps=252000, episode_reward=131.64 +/- 147.12
Episode length: 679.00 +/- 234.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 132         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.012941811 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.62       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.001       |
|    loss                 | 306         |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00587    |
|    std                  | 1.27        |
|    value_loss           | 638         |
-----------------------------------------
Eval num_timesteps=254000, episode_reward=240.36 +/- 85.15
Episode length: 838.40 +/- 57.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 838          |
|    mean_reward          | 240          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0065697716 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.61        |
|    explained_variance   | 0.726        |
|    learning_rate        | 0.001        |
|    loss                 | 444          |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00332     |
|    std                  | 1.27         |
|    value_loss           | 951          |
------------------------------------------
Eval num_timesteps=256000, episode_reward=55.90 +/- 84.29
Episode length: 585.80 +/- 179.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 55.9     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=258000, episode_reward=188.75 +/- 141.97
Episode length: 686.20 +/- 130.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 686         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 258000      |
| train/                  |             |
|    approx_kl            | 0.015926696 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.001       |
|    loss                 | 313         |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00588    |
|    std                  | 1.27        |
|    value_loss           | 654         |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=137.47 +/- 157.36
Episode length: 630.20 +/- 145.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 630          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0050754766 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.63        |
|    explained_variance   | 0.67         |
|    learning_rate        | 0.001        |
|    loss                 | 492          |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.28         |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=262000, episode_reward=-40.95 +/- 103.97
Episode length: 381.20 +/- 163.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | -41         |
| time/                   |             |
|    total_timesteps      | 262000      |
| train/                  |             |
|    approx_kl            | 0.009216495 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.001       |
|    loss                 | 486         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00458    |
|    std                  | 1.28        |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=264000, episode_reward=-100.53 +/- 58.17
Episode length: 298.40 +/- 115.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 298         |
|    mean_reward          | -101        |
| time/                   |             |
|    total_timesteps      | 264000      |
| train/                  |             |
|    approx_kl            | 0.012498142 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.67       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 381         |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 1.29        |
|    value_loss           | 784         |
-----------------------------------------
Eval num_timesteps=266000, episode_reward=-142.70 +/- 28.64
Episode length: 222.20 +/- 60.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 266000      |
| train/                  |             |
|    approx_kl            | 0.009212378 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.68       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.001       |
|    loss                 | 673         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00349    |
|    std                  | 1.29        |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=268000, episode_reward=-74.36 +/- 42.47
Episode length: 323.20 +/- 67.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | -74.4       |
| time/                   |             |
|    total_timesteps      | 268000      |
| train/                  |             |
|    approx_kl            | 0.006751897 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 701         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 1.29        |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=0.74 +/- 112.13
Episode length: 492.00 +/- 254.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 0.74         |
| time/                   |              |
|    total_timesteps      | 270000       |
| train/                  |              |
|    approx_kl            | 0.0040245666 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.669        |
|    learning_rate        | 0.001        |
|    loss                 | 548          |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1.3          |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=272000, episode_reward=-22.86 +/- 60.96
Episode length: 438.80 +/- 107.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | -22.9        |
| time/                   |              |
|    total_timesteps      | 272000       |
| train/                  |              |
|    approx_kl            | 0.0006322592 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 702          |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000168    |
|    std                  | 1.3          |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=274000, episode_reward=-76.66 +/- 19.36
Episode length: 330.40 +/- 43.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 330         |
|    mean_reward          | -76.7       |
| time/                   |             |
|    total_timesteps      | 274000      |
| train/                  |             |
|    approx_kl            | 0.004944398 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 598         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.3         |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=-23.75 +/- 25.18
Episode length: 410.80 +/- 34.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | -23.8        |
| time/                   |              |
|    total_timesteps      | 276000       |
| train/                  |              |
|    approx_kl            | 0.0073527824 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 272          |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1.3          |
|    value_loss           | 566          |
------------------------------------------
Eval num_timesteps=278000, episode_reward=69.80 +/- 94.34
Episode length: 601.60 +/- 201.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 602         |
|    mean_reward          | 69.8        |
| time/                   |             |
|    total_timesteps      | 278000      |
| train/                  |             |
|    approx_kl            | 0.007956503 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.74       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.001       |
|    loss                 | 507         |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 1.31        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=159.50 +/- 87.27
Episode length: 766.20 +/- 134.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 766        |
|    mean_reward          | 160        |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.00246539 |
|    clip_fraction        | 0.002      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.75      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.001      |
|    loss                 | 314        |
|    n_updates            | 1360       |
|    policy_gradient_loss | 0.000566   |
|    std                  | 1.31       |
|    value_loss           | 685        |
----------------------------------------
Eval num_timesteps=282000, episode_reward=136.88 +/- 116.86
Episode length: 672.20 +/- 154.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 672         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 282000      |
| train/                  |             |
|    approx_kl            | 0.012567889 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.76       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 320         |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00392    |
|    std                  | 1.32        |
|    value_loss           | 681         |
-----------------------------------------
Eval num_timesteps=284000, episode_reward=316.97 +/- 294.56
Episode length: 767.20 +/- 263.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 767          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 0.0054043597 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.78        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 227          |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00251     |
|    std                  | 1.32         |
|    value_loss           | 539          |
------------------------------------------
Eval num_timesteps=286000, episode_reward=284.07 +/- 299.95
Episode length: 610.00 +/- 201.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 610         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 286000      |
| train/                  |             |
|    approx_kl            | 0.009829946 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 244         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00624    |
|    std                  | 1.33        |
|    value_loss           | 551         |
-----------------------------------------
Eval num_timesteps=288000, episode_reward=306.18 +/- 427.19
Episode length: 649.80 +/- 220.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 650         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 288000      |
| train/                  |             |
|    approx_kl            | 0.007249461 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.81       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.001       |
|    loss                 | 380         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 1.33        |
|    value_loss           | 781         |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=77.84 +/- 116.23
Episode length: 628.00 +/- 183.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 628         |
|    mean_reward          | 77.8        |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.013684451 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.001       |
|    loss                 | 389         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00461    |
|    std                  | 1.34        |
|    value_loss           | 798         |
-----------------------------------------
Eval num_timesteps=292000, episode_reward=131.29 +/- 132.30
Episode length: 633.40 +/- 153.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 633         |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 292000      |
| train/                  |             |
|    approx_kl            | 0.006160868 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 321         |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 1.35        |
|    value_loss           | 701         |
-----------------------------------------
Eval num_timesteps=294000, episode_reward=176.47 +/- 82.18
Episode length: 649.80 +/- 52.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 650         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 294000      |
| train/                  |             |
|    approx_kl            | 0.008899244 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 266         |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 1.35        |
|    value_loss           | 612         |
-----------------------------------------
Eval num_timesteps=296000, episode_reward=59.66 +/- 72.92
Episode length: 590.00 +/- 72.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 590        |
|    mean_reward          | 59.7       |
| time/                   |            |
|    total_timesteps      | 296000     |
| train/                  |            |
|    approx_kl            | 0.01551274 |
|    clip_fraction        | 0.0789     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.89      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.001      |
|    loss                 | 299        |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.00308   |
|    std                  | 1.36       |
|    value_loss           | 587        |
----------------------------------------
Eval num_timesteps=298000, episode_reward=146.80 +/- 77.10
Episode length: 633.40 +/- 46.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 633           |
|    mean_reward          | 147           |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 0.00093005423 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.9          |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 413           |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000193     |
|    std                  | 1.36          |
|    value_loss           | 869           |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=70.15 +/- 63.10
Episode length: 616.80 +/- 26.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 617         |
|    mean_reward          | 70.2        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.002697747 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.001       |
|    loss                 | 291         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.36        |
|    value_loss           | 646         |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=56.77 +/- 88.92
Episode length: 579.80 +/- 60.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 580         |
|    mean_reward          | 56.8        |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.008650675 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 317         |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00421    |
|    std                  | 1.36        |
|    value_loss           | 700         |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=134.79 +/- 136.26
Episode length: 628.80 +/- 120.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 629         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.007556814 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 254         |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 1.36        |
|    value_loss           | 537         |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=96.65 +/- 79.38
Episode length: 639.20 +/- 64.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 639          |
|    mean_reward          | 96.7         |
| time/                   |              |
|    total_timesteps      | 306000       |
| train/                  |              |
|    approx_kl            | 0.0015308829 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.92        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 244          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.000229    |
|    std                  | 1.37         |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=308000, episode_reward=16.20 +/- 29.29
Episode length: 564.20 +/- 46.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 564         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.014878204 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 315         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00408    |
|    std                  | 1.37        |
|    value_loss           | 647         |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=96.60 +/- 151.19
Episode length: 595.20 +/- 85.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 595         |
|    mean_reward          | 96.6        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.006509998 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 309         |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 1.38        |
|    value_loss           | 633         |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=49.50 +/- 75.35
Episode length: 517.60 +/- 89.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 49.5        |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.006070341 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 244         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0033     |
|    std                  | 1.38        |
|    value_loss           | 496         |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=-4.41 +/- 66.98
Episode length: 528.20 +/- 105.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 528          |
|    mean_reward          | -4.41        |
| time/                   |              |
|    total_timesteps      | 314000       |
| train/                  |              |
|    approx_kl            | 0.0045157317 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 311          |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00256     |
|    std                  | 1.38         |
|    value_loss           | 642          |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-20.40 +/- 23.82
Episode length: 507.40 +/- 43.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | -20.4       |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.008154616 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 312         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 1.39        |
|    value_loss           | 640         |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=-5.62 +/- 71.02
Episode length: 491.60 +/- 65.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 492         |
|    mean_reward          | -5.62       |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.002017221 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 295         |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.000693   |
|    std                  | 1.39        |
|    value_loss           | 605         |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-22.68 +/- 65.37
Episode length: 494.40 +/- 75.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -22.7        |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0039674304 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 293          |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00228     |
|    std                  | 1.4          |
|    value_loss           | 599          |
------------------------------------------
Eval num_timesteps=322000, episode_reward=-6.16 +/- 68.80
Episode length: 498.20 +/- 85.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | -6.16        |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0051369355 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.03        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 287          |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 1.41         |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=324000, episode_reward=9.48 +/- 44.15
Episode length: 551.20 +/- 35.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 551         |
|    mean_reward          | 9.48        |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.016081616 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 1.41        |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=326000, episode_reward=-17.08 +/- 63.68
Episode length: 499.80 +/- 82.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -17.1       |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.006167679 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 282         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 1.41        |
|    value_loss           | 585         |
-----------------------------------------
Eval num_timesteps=328000, episode_reward=7.12 +/- 56.61
Episode length: 535.00 +/- 60.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 535          |
|    mean_reward          | 7.12         |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0056390855 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 275          |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 1.42         |
|    value_loss           | 570          |
------------------------------------------
Eval num_timesteps=330000, episode_reward=33.38 +/- 69.16
Episode length: 557.80 +/- 107.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 558          |
|    mean_reward          | 33.4         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0066908547 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.07        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 266          |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.43         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=332000, episode_reward=22.30 +/- 85.15
Episode length: 543.40 +/- 87.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 543         |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.008175997 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00595    |
|    std                  | 1.44        |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=334000, episode_reward=-17.66 +/- 27.91
Episode length: 515.60 +/- 34.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 516        |
|    mean_reward          | -17.7      |
| time/                   |            |
|    total_timesteps      | 334000     |
| train/                  |            |
|    approx_kl            | 0.01731756 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.14      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.001      |
|    loss                 | 257        |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.00582   |
|    std                  | 1.45       |
|    value_loss           | 524        |
----------------------------------------
Eval num_timesteps=336000, episode_reward=-29.29 +/- 49.98
Episode length: 472.00 +/- 79.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | -29.3        |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0067068404 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 328          |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 1.46         |
|    value_loss           | 724          |
------------------------------------------
Eval num_timesteps=338000, episode_reward=-31.25 +/- 23.04
Episode length: 515.40 +/- 30.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -31.2        |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0075062653 |
|    clip_fraction        | 0.0957       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 315          |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.47         |
|    value_loss           | 665          |
------------------------------------------
Eval num_timesteps=340000, episode_reward=-55.70 +/- 35.55
Episode length: 442.00 +/- 58.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 442        |
|    mean_reward          | -55.7      |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.00978043 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.23      |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.001      |
|    loss                 | 334        |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.00221   |
|    std                  | 1.48       |
|    value_loss           | 756        |
----------------------------------------
Eval num_timesteps=342000, episode_reward=-36.97 +/- 46.98
Episode length: 481.40 +/- 82.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-42.80 +/- 26.09
Episode length: 454.20 +/- 66.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | -42.8       |
| time/                   |             |
|    total_timesteps      | 344000      |
| train/                  |             |
|    approx_kl            | 0.008063167 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 311         |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.49        |
|    value_loss           | 642         |
-----------------------------------------
Eval num_timesteps=346000, episode_reward=-8.96 +/- 30.69
Episode length: 499.40 +/- 24.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 499         |
|    mean_reward          | -8.96       |
| time/                   |             |
|    total_timesteps      | 346000      |
| train/                  |             |
|    approx_kl            | 0.005747608 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 247         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 1.49        |
|    value_loss           | 508         |
-----------------------------------------
Eval num_timesteps=348000, episode_reward=-53.92 +/- 76.04
Episode length: 426.60 +/- 129.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 427        |
|    mean_reward          | -53.9      |
| time/                   |            |
|    total_timesteps      | 348000     |
| train/                  |            |
|    approx_kl            | 0.00773879 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.27      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.001      |
|    loss                 | 265        |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0025    |
|    std                  | 1.5        |
|    value_loss           | 540        |
----------------------------------------
Eval num_timesteps=350000, episode_reward=-61.43 +/- 63.99
Episode length: 447.20 +/- 85.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | -61.4        |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0065829596 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 294          |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.0049      |
|    std                  | 1.5          |
|    value_loss           | 602          |
------------------------------------------
Eval num_timesteps=352000, episode_reward=-93.24 +/- 18.03
Episode length: 402.40 +/- 34.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | -93.2       |
| time/                   |             |
|    total_timesteps      | 352000      |
| train/                  |             |
|    approx_kl            | 0.011503174 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 292         |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00352    |
|    std                  | 1.5         |
|    value_loss           | 605         |
-----------------------------------------
Eval num_timesteps=354000, episode_reward=-125.03 +/- 16.44
Episode length: 331.40 +/- 61.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 331         |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 354000      |
| train/                  |             |
|    approx_kl            | 0.014751496 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 280         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00603    |
|    std                  | 1.51        |
|    value_loss           | 572         |
-----------------------------------------
Eval num_timesteps=356000, episode_reward=-113.75 +/- 26.66
Episode length: 347.20 +/- 46.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | -114        |
| time/                   |             |
|    total_timesteps      | 356000      |
| train/                  |             |
|    approx_kl            | 0.004530143 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 274         |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.52        |
|    value_loss           | 561         |
-----------------------------------------
Eval num_timesteps=358000, episode_reward=-92.26 +/- 29.04
Episode length: 391.80 +/- 49.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | -92.3        |
| time/                   |              |
|    total_timesteps      | 358000       |
| train/                  |              |
|    approx_kl            | 0.0042505115 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 267          |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 1.53         |
|    value_loss           | 547          |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-86.30 +/- 24.08
Episode length: 413.20 +/- 31.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | -86.3        |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0051825168 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.38        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 268          |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 1.54         |
|    value_loss           | 540          |
------------------------------------------
Eval num_timesteps=362000, episode_reward=-78.48 +/- 41.90
Episode length: 415.80 +/- 90.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | -78.5       |
| time/                   |             |
|    total_timesteps      | 362000      |
| train/                  |             |
|    approx_kl            | 0.008575588 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 259         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0031     |
|    std                  | 1.56        |
|    value_loss           | 528         |
-----------------------------------------
Eval num_timesteps=364000, episode_reward=-102.62 +/- 21.15
Episode length: 381.20 +/- 43.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | -103        |
| time/                   |             |
|    total_timesteps      | 364000      |
| train/                  |             |
|    approx_kl            | 0.003229017 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.001       |
|    loss                 | 301         |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.56        |
|    value_loss           | 614         |
-----------------------------------------
Eval num_timesteps=366000, episode_reward=-109.64 +/- 10.93
Episode length: 376.00 +/- 28.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | -110         |
| time/                   |              |
|    total_timesteps      | 366000       |
| train/                  |              |
|    approx_kl            | 0.0073598498 |
|    clip_fraction        | 0.0512       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 247          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00332     |
|    std                  | 1.57         |
|    value_loss           | 502          |
------------------------------------------
Eval num_timesteps=368000, episode_reward=-127.38 +/- 47.54
Episode length: 333.20 +/- 94.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | -127        |
| time/                   |             |
|    total_timesteps      | 368000      |
| train/                  |             |
|    approx_kl            | 0.012045311 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 307         |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00252    |
|    std                  | 1.58        |
|    value_loss           | 632         |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=-144.63 +/- 20.87
Episode length: 288.40 +/- 50.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0034281923 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000861    |
|    std                  | 1.59         |
|    value_loss           | 683          |
------------------------------------------
Eval num_timesteps=372000, episode_reward=-148.75 +/- 24.42
Episode length: 270.60 +/- 38.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 372000      |
| train/                  |             |
|    approx_kl            | 0.008826354 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 279         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00367    |
|    std                  | 1.59        |
|    value_loss           | 573         |
-----------------------------------------
Eval num_timesteps=374000, episode_reward=-125.78 +/- 20.84
Episode length: 315.60 +/- 45.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | -126         |
| time/                   |              |
|    total_timesteps      | 374000       |
| train/                  |              |
|    approx_kl            | 0.0062328614 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 322          |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.6          |
|    value_loss           | 652          |
------------------------------------------
Eval num_timesteps=376000, episode_reward=-126.08 +/- 26.15
Episode length: 293.60 +/- 67.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 294         |
|    mean_reward          | -126        |
| time/                   |             |
|    total_timesteps      | 376000      |
| train/                  |             |
|    approx_kl            | 0.005119621 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.55       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 308         |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00256    |
|    std                  | 1.6         |
|    value_loss           | 637         |
-----------------------------------------
Eval num_timesteps=378000, episode_reward=-118.69 +/- 28.33
Episode length: 303.20 +/- 52.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | -119         |
| time/                   |              |
|    total_timesteps      | 378000       |
| train/                  |              |
|    approx_kl            | 0.0033348282 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 303          |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.000724    |
|    std                  | 1.61         |
|    value_loss           | 624          |
------------------------------------------
Eval num_timesteps=380000, episode_reward=-124.46 +/- 34.16
Episode length: 301.40 +/- 66.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0071001956 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00307     |
|    std                  | 1.62         |
|    value_loss           | 623          |
------------------------------------------
Eval num_timesteps=382000, episode_reward=-113.19 +/- 20.66
Episode length: 356.40 +/- 66.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | -113        |
| time/                   |             |
|    total_timesteps      | 382000      |
| train/                  |             |
|    approx_kl            | 0.006069214 |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 295         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00132    |
|    std                  | 1.63        |
|    value_loss           | 604         |
-----------------------------------------
Eval num_timesteps=384000, episode_reward=-77.28 +/- 50.20
Episode length: 409.20 +/- 69.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | -77.3       |
| time/                   |             |
|    total_timesteps      | 384000      |
| train/                  |             |
|    approx_kl            | 0.011762994 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | 212         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00754    |
|    std                  | 1.63        |
|    value_loss           | 433         |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=-84.70 +/- 46.68
Episode length: 361.60 +/- 77.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | -84.7       |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.006814392 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 247         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00554    |
|    std                  | 1.65        |
|    value_loss           | 522         |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=-13.75 +/- 48.30
Episode length: 480.80 +/- 58.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | -13.7       |
| time/                   |             |
|    total_timesteps      | 388000      |
| train/                  |             |
|    approx_kl            | 0.008903231 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 243         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00465    |
|    std                  | 1.66        |
|    value_loss           | 504         |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=101.99 +/- 118.70
Episode length: 563.40 +/- 71.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 563          |
|    mean_reward          | 102          |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0068614893 |
|    clip_fraction        | 0.0829       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 164          |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.000563    |
|    std                  | 1.66         |
|    value_loss           | 343          |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-4.02 +/- 38.27
Episode length: 531.60 +/- 73.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | -4.02       |
| time/                   |             |
|    total_timesteps      | 392000      |
| train/                  |             |
|    approx_kl            | 0.010185816 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.001       |
|    loss                 | 226         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00361    |
|    std                  | 1.67        |
|    value_loss           | 525         |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=-94.33 +/- 25.56
Episode length: 376.40 +/- 62.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | -94.3        |
| time/                   |              |
|    total_timesteps      | 394000       |
| train/                  |              |
|    approx_kl            | 0.0047223438 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 165          |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.67         |
|    value_loss           | 352          |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-13.39 +/- 80.86
Episode length: 446.60 +/- 119.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | -13.4        |
| time/                   |              |
|    total_timesteps      | 396000       |
| train/                  |              |
|    approx_kl            | 0.0054111215 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 1.68         |
|    value_loss           | 406          |
------------------------------------------
Eval num_timesteps=398000, episode_reward=18.55 +/- 131.31
Episode length: 469.40 +/- 134.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 398000       |
| train/                  |              |
|    approx_kl            | 0.0061368793 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 206          |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 1.68         |
|    value_loss           | 427          |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-37.09 +/- 73.12
Episode length: 432.80 +/- 105.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 433         |
|    mean_reward          | -37.1       |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.004939537 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.001       |
|    loss                 | 350         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00413    |
|    std                  | 1.68        |
|    value_loss           | 944         |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-10.53 +/- 41.66
Episode length: 537.00 +/- 87.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 537         |
|    mean_reward          | -10.5       |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.010836059 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 253         |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00444    |
|    std                  | 1.69        |
|    value_loss           | 593         |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=-53.88 +/- 81.87
Episode length: 397.60 +/- 98.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | -53.9       |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.008758506 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.001       |
|    loss                 | 152         |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.69        |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=406000, episode_reward=149.13 +/- 401.88
Episode length: 494.60 +/- 152.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.003654842 |
|    clip_fraction        | 0.00522     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.001       |
|    loss                 | 629         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 1.7         |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=408000, episode_reward=-63.74 +/- 40.80
Episode length: 409.60 +/- 71.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | -63.7        |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0024116517 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 256          |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.7          |
|    value_loss           | 748          |
------------------------------------------
Eval num_timesteps=410000, episode_reward=-120.36 +/- 22.89
Episode length: 361.80 +/- 79.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | -120         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0027117375 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 257          |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.7          |
|    value_loss           | 654          |
------------------------------------------
Eval num_timesteps=412000, episode_reward=-103.69 +/- 31.30
Episode length: 352.80 +/- 88.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 353          |
|    mean_reward          | -104         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0022200185 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 284          |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 1.7          |
|    value_loss           | 664          |
------------------------------------------
Eval num_timesteps=414000, episode_reward=-148.23 +/- 22.78
Episode length: 309.00 +/- 78.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 309          |
|    mean_reward          | -148         |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0039787646 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 361          |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000616    |
|    std                  | 1.7          |
|    value_loss           | 836          |
------------------------------------------
Eval num_timesteps=416000, episode_reward=-150.27 +/- 31.51
Episode length: 263.80 +/- 80.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0014207696 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 309          |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.71         |
|    value_loss           | 735          |
------------------------------------------
Eval num_timesteps=418000, episode_reward=-118.30 +/- 12.06
Episode length: 346.40 +/- 16.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0013034423 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.81        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 311          |
|    n_updates            | 2040         |
|    policy_gradient_loss | 0.000462     |
|    std                  | 1.71         |
|    value_loss           | 679          |
------------------------------------------
Eval num_timesteps=420000, episode_reward=-62.38 +/- 62.45
Episode length: 423.20 +/- 98.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | -62.4        |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0018985379 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.81        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 382          |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.71         |
|    value_loss           | 824          |
------------------------------------------
Eval num_timesteps=422000, episode_reward=-80.66 +/- 66.09
Episode length: 356.60 +/- 90.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | -80.7        |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0038311894 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.82        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 326          |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.71         |
|    value_loss           | 673          |
------------------------------------------
Eval num_timesteps=424000, episode_reward=-70.32 +/- 70.65
Episode length: 392.60 +/- 79.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | -70.3       |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.010093602 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 272         |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 1.72        |
|    value_loss           | 584         |
-----------------------------------------
Eval num_timesteps=426000, episode_reward=-41.51 +/- 82.31
Episode length: 460.40 +/- 104.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | -41.5       |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.007333617 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.83       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 150         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0029     |
|    std                  | 1.72        |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=428000, episode_reward=-31.03 +/- 55.65
Episode length: 475.60 +/- 41.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -31      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=32.21 +/- 53.41
Episode length: 568.80 +/- 88.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 32.2         |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0041282573 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.85        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | 189          |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 1.73         |
|    value_loss           | 454          |
------------------------------------------
Eval num_timesteps=432000, episode_reward=-7.33 +/- 25.30
Episode length: 495.20 +/- 29.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -7.33       |
| time/                   |             |
|    total_timesteps      | 432000      |
| train/                  |             |
|    approx_kl            | 0.011948531 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 162         |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 1.74        |
|    value_loss           | 373         |
-----------------------------------------
Eval num_timesteps=434000, episode_reward=5.19 +/- 97.00
Episode length: 498.20 +/- 117.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 498         |
|    mean_reward          | 5.19        |
| time/                   |             |
|    total_timesteps      | 434000      |
| train/                  |             |
|    approx_kl            | 0.010297652 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 167         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.75        |
|    value_loss           | 360         |
-----------------------------------------
Eval num_timesteps=436000, episode_reward=-99.24 +/- 20.10
Episode length: 352.00 +/- 59.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | -99.2       |
| time/                   |             |
|    total_timesteps      | 436000      |
| train/                  |             |
|    approx_kl            | 0.005807123 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.91       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 183         |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00414    |
|    std                  | 1.75        |
|    value_loss           | 390         |
-----------------------------------------
Eval num_timesteps=438000, episode_reward=-23.65 +/- 49.38
Episode length: 455.20 +/- 78.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | -23.6        |
| time/                   |              |
|    total_timesteps      | 438000       |
| train/                  |              |
|    approx_kl            | 0.0042253425 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 1.76         |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-89.63 +/- 30.66
Episode length: 364.60 +/- 38.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | -89.6        |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0068626455 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 215          |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.76         |
|    value_loss           | 469          |
------------------------------------------
Eval num_timesteps=442000, episode_reward=-44.80 +/- 53.62
Episode length: 419.00 +/- 80.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | -44.8        |
| time/                   |              |
|    total_timesteps      | 442000       |
| train/                  |              |
|    approx_kl            | 0.0063398574 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 188          |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.0033      |
|    std                  | 1.76         |
|    value_loss           | 393          |
------------------------------------------
Eval num_timesteps=444000, episode_reward=-17.86 +/- 51.01
Episode length: 421.20 +/- 57.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | -17.9        |
| time/                   |              |
|    total_timesteps      | 444000       |
| train/                  |              |
|    approx_kl            | 0.0032777386 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.94        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 185          |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.000622    |
|    std                  | 1.76         |
|    value_loss           | 383          |
------------------------------------------
Eval num_timesteps=446000, episode_reward=49.04 +/- 86.87
Episode length: 463.80 +/- 66.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 49          |
| time/                   |             |
|    total_timesteps      | 446000      |
| train/                  |             |
|    approx_kl            | 0.011109788 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.94       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 246         |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 1.76        |
|    value_loss           | 599         |
-----------------------------------------
Eval num_timesteps=448000, episode_reward=66.34 +/- 102.87
Episode length: 545.80 +/- 160.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 66.3         |
| time/                   |              |
|    total_timesteps      | 448000       |
| train/                  |              |
|    approx_kl            | 0.0024839311 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.94        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.77         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=450000, episode_reward=172.26 +/- 112.03
Episode length: 632.60 +/- 112.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 633          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 0.0034914156 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.95        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 184          |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.77         |
|    value_loss           | 482          |
------------------------------------------
Eval num_timesteps=452000, episode_reward=222.38 +/- 170.91
Episode length: 574.60 +/- 78.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 575         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 452000      |
| train/                  |             |
|    approx_kl            | 0.002911511 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.95       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 1.77        |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=454000, episode_reward=48.60 +/- 126.49
Episode length: 553.20 +/- 88.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 48.6         |
| time/                   |              |
|    total_timesteps      | 454000       |
| train/                  |              |
|    approx_kl            | 0.0068612727 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.96        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 485          |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.77         |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=456000, episode_reward=127.15 +/- 138.40
Episode length: 628.00 +/- 75.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 628          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 456000       |
| train/                  |              |
|    approx_kl            | 0.0077281776 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.96        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.77         |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=458000, episode_reward=271.63 +/- 489.21
Episode length: 553.20 +/- 144.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 458000       |
| train/                  |              |
|    approx_kl            | 0.0026793564 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.95        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 246          |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000855    |
|    std                  | 1.77         |
|    value_loss           | 640          |
------------------------------------------
Eval num_timesteps=460000, episode_reward=126.21 +/- 83.71
Episode length: 564.60 +/- 106.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 565        |
|    mean_reward          | 126        |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.00799432 |
|    clip_fraction        | 0.0761     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.96      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.001      |
|    loss                 | 172        |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.00474   |
|    std                  | 1.78       |
|    value_loss           | 395        |
----------------------------------------
Eval num_timesteps=462000, episode_reward=32.06 +/- 128.40
Episode length: 401.80 +/- 95.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 32.1        |
| time/                   |             |
|    total_timesteps      | 462000      |
| train/                  |             |
|    approx_kl            | 0.012618063 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.000921   |
|    std                  | 1.78        |
|    value_loss           | 439         |
-----------------------------------------
Eval num_timesteps=464000, episode_reward=200.99 +/- 43.14
Episode length: 610.20 +/- 59.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 610         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 464000      |
| train/                  |             |
|    approx_kl            | 0.012088574 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 323         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 1.79        |
|    value_loss           | 794         |
-----------------------------------------
Eval num_timesteps=466000, episode_reward=297.21 +/- 178.78
Episode length: 693.80 +/- 178.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 694          |
|    mean_reward          | 297          |
| time/                   |              |
|    total_timesteps      | 466000       |
| train/                  |              |
|    approx_kl            | 0.0014879412 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 201          |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.000895    |
|    std                  | 1.79         |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=468000, episode_reward=268.50 +/- 169.12
Episode length: 654.80 +/- 101.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 655          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 468000       |
| train/                  |              |
|    approx_kl            | 0.0039197905 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.98        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 219          |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.79         |
|    value_loss           | 558          |
------------------------------------------
Eval num_timesteps=470000, episode_reward=478.66 +/- 478.68
Episode length: 646.80 +/- 43.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 647         |
|    mean_reward          | 479         |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.005278821 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 246         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 1.79        |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=133.42 +/- 176.66
Episode length: 539.00 +/- 151.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 539         |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 472000      |
| train/                  |             |
|    approx_kl            | 0.003681664 |
|    clip_fraction        | 0.00405     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.001       |
|    loss                 | 634         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 1.79        |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=342.59 +/- 180.78
Episode length: 720.00 +/- 79.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 720          |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 474000       |
| train/                  |              |
|    approx_kl            | 0.0019524075 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.0594       |
|    learning_rate        | 0.001        |
|    loss                 | 8.19e+03     |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.000775    |
|    std                  | 1.79         |
|    value_loss           | 1.79e+04     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=675.75 +/- 698.35
Episode length: 643.80 +/- 128.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 644          |
|    mean_reward          | 676          |
| time/                   |              |
|    total_timesteps      | 476000       |
| train/                  |              |
|    approx_kl            | 0.0019150861 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 364          |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.79         |
|    value_loss           | 929          |
------------------------------------------
New best mean reward!
Eval num_timesteps=478000, episode_reward=474.45 +/- 503.57
Episode length: 671.00 +/- 70.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 671          |
|    mean_reward          | 474          |
| time/                   |              |
|    total_timesteps      | 478000       |
| train/                  |              |
|    approx_kl            | 0.0014382673 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0.468        |
|    learning_rate        | 0.001        |
|    loss                 | 830          |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.79         |
|    value_loss           | 2.12e+03     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=2.81 +/- 202.62
Episode length: 615.00 +/- 96.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 615         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.006783168 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 143         |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00248    |
|    std                  | 1.79        |
|    value_loss           | 395         |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=316.91 +/- 332.57
Episode length: 575.20 +/- 99.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 575         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 482000      |
| train/                  |             |
|    approx_kl            | 0.004942823 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 209         |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00241    |
|    std                  | 1.79        |
|    value_loss           | 521         |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=96.60 +/- 166.20
Episode length: 552.20 +/- 116.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 552         |
|    mean_reward          | 96.6        |
| time/                   |             |
|    total_timesteps      | 484000      |
| train/                  |             |
|    approx_kl            | 0.003326456 |
|    clip_fraction        | 0.00449     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 192         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.000551   |
|    std                  | 1.79        |
|    value_loss           | 432         |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=436.32 +/- 411.49
Episode length: 637.00 +/- 57.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 436          |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0043016486 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 481          |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.79         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=576.57 +/- 237.19
Episode length: 702.20 +/- 88.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 702         |
|    mean_reward          | 577         |
| time/                   |             |
|    total_timesteps      | 488000      |
| train/                  |             |
|    approx_kl            | 0.004211604 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.001       |
|    loss                 | 253         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00173    |
|    std                  | 1.79        |
|    value_loss           | 580         |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=240.48 +/- 232.39
Episode length: 602.80 +/- 171.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 603         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.010642988 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 191         |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 1.8         |
|    value_loss           | 417         |
-----------------------------------------
Eval num_timesteps=492000, episode_reward=734.92 +/- 376.57
Episode length: 678.20 +/- 61.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 678          |
|    mean_reward          | 735          |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0028179777 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 428          |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.8          |
|    value_loss           | 965          |
------------------------------------------
New best mean reward!
Eval num_timesteps=494000, episode_reward=915.60 +/- 459.97
Episode length: 695.60 +/- 28.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 696          |
|    mean_reward          | 916          |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0017777025 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 378          |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.8          |
|    value_loss           | 921          |
------------------------------------------
New best mean reward!
Eval num_timesteps=496000, episode_reward=269.35 +/- 236.73
Episode length: 637.80 +/- 82.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 638         |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.008063516 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.001       |
|    loss                 | 582         |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00303    |
|    std                  | 1.81        |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=498000, episode_reward=286.30 +/- 262.71
Episode length: 680.00 +/- 114.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 680         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.001007284 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.001       |
|    loss                 | 3.92e+03    |
|    n_updates            | 2430        |
|    policy_gradient_loss | 0.00016     |
|    std                  | 1.81        |
|    value_loss           | 8.36e+03    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=476.27 +/- 170.89
Episode length: 719.40 +/- 97.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 719          |
|    mean_reward          | 476          |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0004943004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.001        |
|    loss                 | 3.64e+03     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.000316    |
|    std                  | 1.81         |
|    value_loss           | 7.6e+03      |
------------------------------------------
Eval num_timesteps=502000, episode_reward=596.68 +/- 499.62
Episode length: 631.40 +/- 113.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 631           |
|    mean_reward          | 597           |
| time/                   |               |
|    total_timesteps      | 502000        |
| train/                  |               |
|    approx_kl            | 0.00068079575 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.562         |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+03      |
|    n_updates            | 2450          |
|    policy_gradient_loss | -0.00058      |
|    std                  | 1.81          |
|    value_loss           | 5e+03         |
-------------------------------------------
Eval num_timesteps=504000, episode_reward=183.30 +/- 111.83
Episode length: 597.80 +/- 105.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 598           |
|    mean_reward          | 183           |
| time/                   |               |
|    total_timesteps      | 504000        |
| train/                  |               |
|    approx_kl            | 0.00042923383 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.708         |
|    learning_rate        | 0.001         |
|    loss                 | 897           |
|    n_updates            | 2460          |
|    policy_gradient_loss | -0.000467     |
|    std                  | 1.81          |
|    value_loss           | 2.56e+03      |
-------------------------------------------
Eval num_timesteps=506000, episode_reward=458.88 +/- 378.37
Episode length: 647.00 +/- 87.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 647           |
|    mean_reward          | 459           |
| time/                   |               |
|    total_timesteps      | 506000        |
| train/                  |               |
|    approx_kl            | 0.00034074805 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.518         |
|    learning_rate        | 0.001         |
|    loss                 | 5.29e+03      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -0.000648     |
|    std                  | 1.81          |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=508000, episode_reward=100.26 +/- 350.73
Episode length: 619.60 +/- 100.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 620           |
|    mean_reward          | 100           |
| time/                   |               |
|    total_timesteps      | 508000        |
| train/                  |               |
|    approx_kl            | 4.3430045e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.741         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+03      |
|    n_updates            | 2480          |
|    policy_gradient_loss | -1.16e-05     |
|    std                  | 1.81          |
|    value_loss           | 2.74e+03      |
-------------------------------------------
Eval num_timesteps=510000, episode_reward=269.34 +/- 417.01
Episode length: 681.20 +/- 93.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 681         |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.000446417 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 158         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 1.81        |
|    value_loss           | 480         |
-----------------------------------------
Eval num_timesteps=512000, episode_reward=535.89 +/- 263.73
Episode length: 673.60 +/- 113.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=514000, episode_reward=344.68 +/- 228.37
Episode length: 633.80 +/- 149.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 634          |
|    mean_reward          | 345          |
| time/                   |              |
|    total_timesteps      | 514000       |
| train/                  |              |
|    approx_kl            | 0.0031397324 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 177          |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.81         |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=516000, episode_reward=-54.00 +/- 74.10
Episode length: 343.80 +/- 114.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | -54          |
| time/                   |              |
|    total_timesteps      | 516000       |
| train/                  |              |
|    approx_kl            | 0.0068606194 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 155          |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.8          |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=518000, episode_reward=409.49 +/- 573.72
Episode length: 637.20 +/- 144.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 409          |
| time/                   |              |
|    total_timesteps      | 518000       |
| train/                  |              |
|    approx_kl            | 0.0024371075 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 618          |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.8          |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=520000, episode_reward=0.67 +/- 101.15
Episode length: 399.20 +/- 107.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 0.674        |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0013983226 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 401          |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.8          |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=522000, episode_reward=-44.84 +/- 58.29
Episode length: 356.40 +/- 76.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | -44.8       |
| time/                   |             |
|    total_timesteps      | 522000      |
| train/                  |             |
|    approx_kl            | 0.003550226 |
|    clip_fraction        | 0.00366     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.001       |
|    loss                 | 427         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00186    |
|    std                  | 1.8         |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=524000, episode_reward=106.03 +/- 258.15
Episode length: 479.80 +/- 224.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 524000       |
| train/                  |              |
|    approx_kl            | 0.0040931683 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 468          |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 1.8          |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=526000, episode_reward=-65.33 +/- 56.58
Episode length: 338.60 +/- 89.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | -65.3        |
| time/                   |              |
|    total_timesteps      | 526000       |
| train/                  |              |
|    approx_kl            | 0.0033338347 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 443          |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.8          |
|    value_loss           | 1.12e+03     |
------------------------------------------
Eval num_timesteps=528000, episode_reward=-33.66 +/- 53.38
Episode length: 385.00 +/- 82.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 385         |
|    mean_reward          | -33.7       |
| time/                   |             |
|    total_timesteps      | 528000      |
| train/                  |             |
|    approx_kl            | 0.001829276 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.001       |
|    loss                 | 296         |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.81        |
|    value_loss           | 721         |
-----------------------------------------
Eval num_timesteps=530000, episode_reward=-103.86 +/- 70.56
Episode length: 270.60 +/- 95.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | -104         |
| time/                   |              |
|    total_timesteps      | 530000       |
| train/                  |              |
|    approx_kl            | 0.0012130304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 396          |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000642    |
|    std                  | 1.81         |
|    value_loss           | 882          |
------------------------------------------
Eval num_timesteps=532000, episode_reward=42.51 +/- 161.16
Episode length: 419.80 +/- 94.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | 42.5        |
| time/                   |             |
|    total_timesteps      | 532000      |
| train/                  |             |
|    approx_kl            | 0.001040162 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 278         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00087    |
|    std                  | 1.81        |
|    value_loss           | 641         |
-----------------------------------------
Eval num_timesteps=534000, episode_reward=-36.25 +/- 43.45
Episode length: 380.00 +/- 41.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | -36.2        |
| time/                   |              |
|    total_timesteps      | 534000       |
| train/                  |              |
|    approx_kl            | 0.0019743743 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 251          |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.81         |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=536000, episode_reward=12.31 +/- 60.75
Episode length: 415.60 +/- 53.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 536000      |
| train/                  |             |
|    approx_kl            | 0.006758389 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 204         |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00215    |
|    std                  | 1.81        |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=538000, episode_reward=-13.76 +/- 45.64
Episode length: 408.40 +/- 43.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | -13.8        |
| time/                   |              |
|    total_timesteps      | 538000       |
| train/                  |              |
|    approx_kl            | 0.0050049657 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.05        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1.81         |
|    value_loss           | 295          |
------------------------------------------
Eval num_timesteps=540000, episode_reward=-78.95 +/- 69.55
Episode length: 313.40 +/- 97.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 313        |
|    mean_reward          | -78.9      |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.01016189 |
|    clip_fraction        | 0.0586     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.04      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 160        |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.00444   |
|    std                  | 1.81       |
|    value_loss           | 349        |
----------------------------------------
Eval num_timesteps=542000, episode_reward=-39.35 +/- 64.52
Episode length: 358.00 +/- 72.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | -39.4       |
| time/                   |             |
|    total_timesteps      | 542000      |
| train/                  |             |
|    approx_kl            | 0.007578903 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000503   |
|    std                  | 1.82        |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=544000, episode_reward=142.53 +/- 101.81
Episode length: 531.60 +/- 87.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 544000      |
| train/                  |             |
|    approx_kl            | 0.008684956 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 162         |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 1.82        |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=546000, episode_reward=153.92 +/- 210.17
Episode length: 487.20 +/- 112.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 154         |
| time/                   |             |
|    total_timesteps      | 546000      |
| train/                  |             |
|    approx_kl            | 0.009711161 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 1.82        |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=548000, episode_reward=331.57 +/- 466.64
Episode length: 522.20 +/- 131.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 548000       |
| train/                  |              |
|    approx_kl            | 0.0076920814 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 1.82         |
|    value_loss           | 247          |
------------------------------------------
Eval num_timesteps=550000, episode_reward=61.85 +/- 94.76
Episode length: 508.60 +/- 161.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 61.8        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.009668846 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 133         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00373    |
|    std                  | 1.82        |
|    value_loss           | 286         |
-----------------------------------------
Eval num_timesteps=552000, episode_reward=204.27 +/- 131.10
Episode length: 648.00 +/- 34.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 648         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 552000      |
| train/                  |             |
|    approx_kl            | 0.016063731 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00597    |
|    std                  | 1.84        |
|    value_loss           | 277         |
-----------------------------------------
Eval num_timesteps=554000, episode_reward=268.88 +/- 133.18
Episode length: 679.60 +/- 7.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 680          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 554000       |
| train/                  |              |
|    approx_kl            | 0.0059873397 |
|    clip_fraction        | 0.0936       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 183          |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.86         |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=556000, episode_reward=103.53 +/- 57.69
Episode length: 660.20 +/- 47.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 660          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 556000       |
| train/                  |              |
|    approx_kl            | 0.0053086258 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.87         |
|    value_loss           | 417          |
------------------------------------------
Eval num_timesteps=558000, episode_reward=220.19 +/- 144.51
Episode length: 652.40 +/- 111.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 652          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 558000       |
| train/                  |              |
|    approx_kl            | 0.0013382568 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 156          |
|    n_updates            | 2720         |
|    policy_gradient_loss | 2.72e-07     |
|    std                  | 1.87         |
|    value_loss           | 428          |
------------------------------------------
Eval num_timesteps=560000, episode_reward=70.20 +/- 39.33
Episode length: 623.20 +/- 38.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 623         |
|    mean_reward          | 70.2        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.006309867 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 1.87        |
|    value_loss           | 288         |
-----------------------------------------
Eval num_timesteps=562000, episode_reward=75.80 +/- 31.60
Episode length: 628.80 +/- 32.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 629          |
|    mean_reward          | 75.8         |
| time/                   |              |
|    total_timesteps      | 562000       |
| train/                  |              |
|    approx_kl            | 0.0058764336 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.16        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 1.87         |
|    value_loss           | 282          |
------------------------------------------
Eval num_timesteps=564000, episode_reward=97.05 +/- 77.04
Episode length: 623.20 +/- 64.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 623          |
|    mean_reward          | 97.1         |
| time/                   |              |
|    total_timesteps      | 564000       |
| train/                  |              |
|    approx_kl            | 0.0050144424 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.16        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 1.87         |
|    value_loss           | 320          |
------------------------------------------
Eval num_timesteps=566000, episode_reward=62.48 +/- 48.19
Episode length: 588.40 +/- 37.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 588          |
|    mean_reward          | 62.5         |
| time/                   |              |
|    total_timesteps      | 566000       |
| train/                  |              |
|    approx_kl            | 0.0056087547 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.87         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=568000, episode_reward=25.13 +/- 22.00
Episode length: 555.60 +/- 30.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 556         |
|    mean_reward          | 25.1        |
| time/                   |             |
|    total_timesteps      | 568000      |
| train/                  |             |
|    approx_kl            | 0.005612639 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00227    |
|    std                  | 1.88        |
|    value_loss           | 322         |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=26.80 +/- 48.52
Episode length: 572.40 +/- 30.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 572          |
|    mean_reward          | 26.8         |
| time/                   |              |
|    total_timesteps      | 570000       |
| train/                  |              |
|    approx_kl            | 0.0013839116 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.000737    |
|    std                  | 1.88         |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=572000, episode_reward=30.35 +/- 62.39
Episode length: 539.80 +/- 63.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | 30.4         |
| time/                   |              |
|    total_timesteps      | 572000       |
| train/                  |              |
|    approx_kl            | 0.0050883293 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.19        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 96.6         |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.88         |
|    value_loss           | 224          |
------------------------------------------
Eval num_timesteps=574000, episode_reward=21.46 +/- 45.70
Episode length: 533.00 +/- 51.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 533         |
|    mean_reward          | 21.5        |
| time/                   |             |
|    total_timesteps      | 574000      |
| train/                  |             |
|    approx_kl            | 0.013718279 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 160         |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0038     |
|    std                  | 1.89        |
|    value_loss           | 386         |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=-10.99 +/- 18.66
Episode length: 489.20 +/- 36.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 489         |
|    mean_reward          | -11         |
| time/                   |             |
|    total_timesteps      | 576000      |
| train/                  |             |
|    approx_kl            | 0.006125408 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.0021     |
|    std                  | 1.89        |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=578000, episode_reward=9.58 +/- 44.04
Episode length: 519.80 +/- 51.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | 9.58         |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0050737173 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 89.2         |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.9          |
|    value_loss           | 207          |
------------------------------------------
Eval num_timesteps=580000, episode_reward=-28.23 +/- 47.81
Episode length: 502.20 +/- 50.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -28.2       |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.007932762 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 1.9         |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=582000, episode_reward=-0.18 +/- 30.19
Episode length: 516.40 +/- 22.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.177      |
| time/                   |             |
|    total_timesteps      | 582000      |
| train/                  |             |
|    approx_kl            | 0.004435688 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00412    |
|    std                  | 1.91        |
|    value_loss           | 256         |
-----------------------------------------
Eval num_timesteps=584000, episode_reward=20.70 +/- 34.57
Episode length: 542.00 +/- 35.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 542        |
|    mean_reward          | 20.7       |
| time/                   |            |
|    total_timesteps      | 584000     |
| train/                  |            |
|    approx_kl            | 0.00567424 |
|    clip_fraction        | 0.0386     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.25      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.001      |
|    loss                 | 123        |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.000935  |
|    std                  | 1.92       |
|    value_loss           | 273        |
----------------------------------------
Eval num_timesteps=586000, episode_reward=-1.05 +/- 23.43
Episode length: 527.80 +/- 23.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 528         |
|    mean_reward          | -1.05       |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.002761866 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.93        |
|    value_loss           | 239         |
-----------------------------------------
Eval num_timesteps=588000, episode_reward=51.57 +/- 32.18
Episode length: 571.20 +/- 32.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 571         |
|    mean_reward          | 51.6        |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.005299105 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.31       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 118         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00404    |
|    std                  | 1.95        |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=71.39 +/- 111.51
Episode length: 572.60 +/- 76.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 573         |
|    mean_reward          | 71.4        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.004448466 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 86.2        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.95        |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=592000, episode_reward=17.53 +/- 55.91
Episode length: 530.80 +/- 60.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 531         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.008793309 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.34       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 97.2        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 1.96        |
|    value_loss           | 208         |
-----------------------------------------
Eval num_timesteps=594000, episode_reward=-48.68 +/- 18.39
Episode length: 461.00 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | -48.7       |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.014465181 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.005      |
|    std                  | 1.97        |
|    value_loss           | 249         |
-----------------------------------------
Eval num_timesteps=596000, episode_reward=-45.81 +/- 20.54
Episode length: 457.80 +/- 18.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | -45.8       |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.008725813 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 131         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 1.97        |
|    value_loss           | 280         |
-----------------------------------------
Eval num_timesteps=598000, episode_reward=-44.11 +/- 45.00
Episode length: 453.80 +/- 50.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | -44.1    |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-52.52 +/- 32.12
Episode length: 454.00 +/- 39.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | -52.5       |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.008337881 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 95.7        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00502    |
|    std                  | 1.97        |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=602000, episode_reward=-2.15 +/- 24.44
Episode length: 494.80 +/- 19.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -2.15       |
| time/                   |             |
|    total_timesteps      | 602000      |
| train/                  |             |
|    approx_kl            | 0.005660245 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.38       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00114    |
|    std                  | 1.98        |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=604000, episode_reward=-31.68 +/- 34.35
Episode length: 483.80 +/- 33.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | -31.7       |
| time/                   |             |
|    total_timesteps      | 604000      |
| train/                  |             |
|    approx_kl            | 0.012361802 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.4        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 119         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00577    |
|    std                  | 1.99        |
|    value_loss           | 244         |
-----------------------------------------
Eval num_timesteps=606000, episode_reward=-21.93 +/- 18.89
Episode length: 483.80 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | -21.9        |
| time/                   |              |
|    total_timesteps      | 606000       |
| train/                  |              |
|    approx_kl            | 0.0038514803 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 94.5         |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2            |
|    value_loss           | 194          |
------------------------------------------
Eval num_timesteps=608000, episode_reward=-29.84 +/- 41.90
Episode length: 471.20 +/- 40.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | -29.8       |
| time/                   |             |
|    total_timesteps      | 608000      |
| train/                  |             |
|    approx_kl            | 0.011687456 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.44       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 87.6        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00409    |
|    std                  | 2.01        |
|    value_loss           | 187         |
-----------------------------------------
Eval num_timesteps=610000, episode_reward=4.75 +/- 32.68
Episode length: 506.00 +/- 35.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 4.75        |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.008176929 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.46       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00259    |
|    std                  | 2.02        |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=-9.12 +/- 20.48
Episode length: 494.80 +/- 21.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -9.12       |
| time/                   |             |
|    total_timesteps      | 612000      |
| train/                  |             |
|    approx_kl            | 0.019158553 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.48       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 83.8        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0039     |
|    std                  | 2.04        |
|    value_loss           | 174         |
-----------------------------------------
Eval num_timesteps=614000, episode_reward=-20.45 +/- 37.19
Episode length: 478.80 +/- 40.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | -20.5       |
| time/                   |             |
|    total_timesteps      | 614000      |
| train/                  |             |
|    approx_kl            | 0.009659045 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 2990        |
|    policy_gradient_loss | 0.000558    |
|    std                  | 2.05        |
|    value_loss           | 227         |
-----------------------------------------
Eval num_timesteps=616000, episode_reward=-6.42 +/- 15.84
Episode length: 502.00 +/- 21.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -6.42       |
| time/                   |             |
|    total_timesteps      | 616000      |
| train/                  |             |
|    approx_kl            | 0.008341516 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 81          |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00643    |
|    std                  | 2.06        |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=618000, episode_reward=-28.43 +/- 31.00
Episode length: 479.80 +/- 30.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | -28.4       |
| time/                   |             |
|    total_timesteps      | 618000      |
| train/                  |             |
|    approx_kl            | 0.015128855 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 77.6        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.000271   |
|    std                  | 2.06        |
|    value_loss           | 162         |
-----------------------------------------
Eval num_timesteps=620000, episode_reward=-24.37 +/- 28.86
Episode length: 478.20 +/- 37.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | -24.4        |
| time/                   |              |
|    total_timesteps      | 620000       |
| train/                  |              |
|    approx_kl            | 0.0061393916 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.00291     |
|    std                  | 2.06         |
|    value_loss           | 166          |
------------------------------------------
Eval num_timesteps=622000, episode_reward=30.00 +/- 63.65
Episode length: 527.80 +/- 52.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 528          |
|    mean_reward          | 30           |
| time/                   |              |
|    total_timesteps      | 622000       |
| train/                  |              |
|    approx_kl            | 0.0039992775 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 91.1         |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 2.07         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=624000, episode_reward=-8.29 +/- 15.04
Episode length: 517.80 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | -8.29       |
| time/                   |             |
|    total_timesteps      | 624000      |
| train/                  |             |
|    approx_kl            | 0.010744985 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.55       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 71.4        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 2.08        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=626000, episode_reward=47.09 +/- 43.56
Episode length: 555.00 +/- 38.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 555         |
|    mean_reward          | 47.1        |
| time/                   |             |
|    total_timesteps      | 626000      |
| train/                  |             |
|    approx_kl            | 0.007962052 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 70.1        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00249    |
|    std                  | 2.08        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=628000, episode_reward=17.18 +/- 17.60
Episode length: 524.20 +/- 35.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 524          |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 628000       |
| train/                  |              |
|    approx_kl            | 0.0043002972 |
|    clip_fraction        | 0.0643       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 61.3         |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 2.09         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=630000, episode_reward=22.78 +/- 30.90
Episode length: 535.00 +/- 34.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 535         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.008759819 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 66          |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00379    |
|    std                  | 2.09        |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=632000, episode_reward=-13.42 +/- 22.12
Episode length: 489.40 +/- 33.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | -13.4        |
| time/                   |              |
|    total_timesteps      | 632000       |
| train/                  |              |
|    approx_kl            | 0.0077921716 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 63.6         |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00315     |
|    std                  | 2.09         |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=634000, episode_reward=18.06 +/- 20.20
Episode length: 538.40 +/- 26.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 634000       |
| train/                  |              |
|    approx_kl            | 0.0044492166 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 63.9         |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 2.1          |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=636000, episode_reward=37.53 +/- 17.41
Episode length: 548.40 +/- 20.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 548         |
|    mean_reward          | 37.5        |
| time/                   |             |
|    total_timesteps      | 636000      |
| train/                  |             |
|    approx_kl            | 0.006193174 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.61       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 58.6        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 2.11        |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=638000, episode_reward=-0.21 +/- 50.19
Episode length: 505.00 +/- 51.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | -0.207      |
| time/                   |             |
|    total_timesteps      | 638000      |
| train/                  |             |
|    approx_kl            | 0.015062636 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.62       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 68.3        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 2.12        |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=54.96 +/- 17.51
Episode length: 557.00 +/- 23.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 557         |
|    mean_reward          | 55          |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.010308532 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.65       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 90.3        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.000984   |
|    std                  | 2.13        |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=-21.62 +/- 48.48
Episode length: 478.60 +/- 67.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | -21.6        |
| time/                   |              |
|    total_timesteps      | 642000       |
| train/                  |              |
|    approx_kl            | 0.0075516463 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 77.3         |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 2.14         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=644000, episode_reward=-1.01 +/- 33.57
Episode length: 506.20 +/- 48.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | -1.01       |
| time/                   |             |
|    total_timesteps      | 644000      |
| train/                  |             |
|    approx_kl            | 0.007958618 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.68       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 96.8        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 2.15        |
|    value_loss           | 231         |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=6.97 +/- 68.61
Episode length: 505.60 +/- 73.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 6.97         |
| time/                   |              |
|    total_timesteps      | 646000       |
| train/                  |              |
|    approx_kl            | 0.0063837976 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.69        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 75.7         |
|    n_updates            | 3150         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.16         |
|    value_loss           | 160          |
------------------------------------------
Eval num_timesteps=648000, episode_reward=43.08 +/- 41.31
Episode length: 558.00 +/- 34.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 558         |
|    mean_reward          | 43.1        |
| time/                   |             |
|    total_timesteps      | 648000      |
| train/                  |             |
|    approx_kl            | 0.005689059 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.71       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 66          |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 2.17        |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=56.08 +/- 21.14
Episode length: 569.40 +/- 19.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 56.1         |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0040121065 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 69.2         |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 2.17         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=652000, episode_reward=19.57 +/- 32.34
Episode length: 529.20 +/- 61.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 529         |
|    mean_reward          | 19.6        |
| time/                   |             |
|    total_timesteps      | 652000      |
| train/                  |             |
|    approx_kl            | 0.009924701 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.74       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 77.4        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00458    |
|    std                  | 2.18        |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=59.22 +/- 50.14
Episode length: 551.20 +/- 40.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 551          |
|    mean_reward          | 59.2         |
| time/                   |              |
|    total_timesteps      | 654000       |
| train/                  |              |
|    approx_kl            | 0.0074616065 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.74        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 39.8         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00365     |
|    std                  | 2.18         |
|    value_loss           | 83.1         |
------------------------------------------
Eval num_timesteps=656000, episode_reward=87.65 +/- 32.46
Episode length: 569.40 +/- 34.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 87.6         |
| time/                   |              |
|    total_timesteps      | 656000       |
| train/                  |              |
|    approx_kl            | 0.0040004095 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 53           |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00396     |
|    std                  | 2.18         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=658000, episode_reward=79.68 +/- 22.04
Episode length: 588.20 +/- 22.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 588         |
|    mean_reward          | 79.7        |
| time/                   |             |
|    total_timesteps      | 658000      |
| train/                  |             |
|    approx_kl            | 0.005291974 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.73       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 60.9        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 2.18        |
|    value_loss           | 139         |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=127.40 +/- 65.63
Episode length: 605.00 +/- 27.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 605          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0053078635 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.74        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 50           |
|    n_updates            | 3220         |
|    policy_gradient_loss | -0.000514    |
|    std                  | 2.19         |
|    value_loss           | 99.6         |
------------------------------------------
Eval num_timesteps=662000, episode_reward=159.43 +/- 30.17
Episode length: 607.40 +/- 42.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0043517193 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.75        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 38.3         |
|    n_updates            | 3230         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 2.2          |
|    value_loss           | 80.3         |
------------------------------------------
Eval num_timesteps=664000, episode_reward=219.28 +/- 102.28
Episode length: 600.80 +/- 24.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 601          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0030413703 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.76        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 48.8         |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 2.2          |
|    value_loss           | 117          |
------------------------------------------
Eval num_timesteps=666000, episode_reward=439.11 +/- 301.25
Episode length: 595.20 +/- 14.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 595          |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0032561761 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.77        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 52.4         |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.0041      |
|    std                  | 2.2          |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=668000, episode_reward=245.37 +/- 116.74
Episode length: 596.40 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 596          |
|    mean_reward          | 245          |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0053978525 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 79.9         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 2.21         |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=670000, episode_reward=156.64 +/- 60.38
Episode length: 619.00 +/- 19.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 619         |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.003921192 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.8        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 67.6        |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00107    |
|    std                  | 2.23        |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=672000, episode_reward=298.62 +/- 174.24
Episode length: 607.00 +/- 22.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 607         |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.008457333 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.83       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00075    |
|    std                  | 2.24        |
|    value_loss           | 365         |
-----------------------------------------
Eval num_timesteps=674000, episode_reward=373.72 +/- 216.23
Episode length: 621.40 +/- 35.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 621          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0034413806 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.85        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 64.4         |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.000851    |
|    std                  | 2.25         |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=676000, episode_reward=458.76 +/- 200.11
Episode length: 579.60 +/- 35.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 580          |
|    mean_reward          | 459          |
| time/                   |              |
|    total_timesteps      | 676000       |
| train/                  |              |
|    approx_kl            | 0.0018181759 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.86        |
|    explained_variance   | 0.631        |
|    learning_rate        | 0.001        |
|    loss                 | 632          |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.000323    |
|    std                  | 2.25         |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=678000, episode_reward=468.11 +/- 200.77
Episode length: 618.40 +/- 33.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | 468          |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0007017135 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.86        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 77.1         |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.000582    |
|    std                  | 2.25         |
|    value_loss           | 194          |
------------------------------------------
Eval num_timesteps=680000, episode_reward=482.27 +/- 282.67
Episode length: 563.60 +/- 44.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 564          |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0014020847 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 395          |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00068     |
|    std                  | 2.26         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=682000, episode_reward=761.43 +/- 349.99
Episode length: 579.00 +/- 16.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 579          |
|    mean_reward          | 761          |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0015538123 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 436          |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.000121    |
|    std                  | 2.26         |
|    value_loss           | 1.46e+03     |
------------------------------------------
Eval num_timesteps=684000, episode_reward=550.19 +/- 283.35
Episode length: 556.20 +/- 72.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=686000, episode_reward=387.25 +/- 251.95
Episode length: 576.40 +/- 60.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 576           |
|    mean_reward          | 387           |
| time/                   |               |
|    total_timesteps      | 686000        |
| train/                  |               |
|    approx_kl            | 0.00016879622 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.88         |
|    explained_variance   | 0.749         |
|    learning_rate        | 0.001         |
|    loss                 | 2.08e+03      |
|    n_updates            | 3340          |
|    policy_gradient_loss | -3.06e-05     |
|    std                  | 2.26          |
|    value_loss           | 4.76e+03      |
-------------------------------------------
Eval num_timesteps=688000, episode_reward=550.20 +/- 419.19
Episode length: 568.00 +/- 59.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 568           |
|    mean_reward          | 550           |
| time/                   |               |
|    total_timesteps      | 688000        |
| train/                  |               |
|    approx_kl            | 0.00071145303 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.88         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 296           |
|    n_updates            | 3350          |
|    policy_gradient_loss | -0.00133      |
|    std                  | 2.26          |
|    value_loss           | 926           |
-------------------------------------------
Eval num_timesteps=690000, episode_reward=407.22 +/- 375.97
Episode length: 516.60 +/- 66.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 690000       |
| train/                  |              |
|    approx_kl            | 0.0047485786 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 160          |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 2.26         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=692000, episode_reward=369.11 +/- 386.99
Episode length: 443.20 +/- 83.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 369          |
| time/                   |              |
|    total_timesteps      | 692000       |
| train/                  |              |
|    approx_kl            | 0.0043530897 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 96.8         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 2.26         |
|    value_loss           | 239          |
------------------------------------------
Eval num_timesteps=694000, episode_reward=181.15 +/- 85.26
Episode length: 466.00 +/- 51.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 694000       |
| train/                  |              |
|    approx_kl            | 0.0030846104 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.000109    |
|    std                  | 2.26         |
|    value_loss           | 284          |
------------------------------------------
Eval num_timesteps=696000, episode_reward=285.81 +/- 96.25
Episode length: 516.60 +/- 53.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 696000       |
| train/                  |              |
|    approx_kl            | 0.0005008285 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 85.6         |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000288    |
|    std                  | 2.27         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=698000, episode_reward=226.17 +/- 89.03
Episode length: 518.60 +/- 82.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 519         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 698000      |
| train/                  |             |
|    approx_kl            | 0.002805062 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 98.2        |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 2.27        |
|    value_loss           | 296         |
-----------------------------------------
Eval num_timesteps=700000, episode_reward=261.29 +/- 119.08
Episode length: 573.40 +/- 41.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 573         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.007265646 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 137         |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00176    |
|    std                  | 2.27        |
|    value_loss           | 388         |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=267.48 +/- 167.26
Episode length: 582.00 +/- 66.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 582          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 702000       |
| train/                  |              |
|    approx_kl            | 0.0016255572 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 142          |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.000801    |
|    std                  | 2.27         |
|    value_loss           | 417          |
------------------------------------------
Eval num_timesteps=704000, episode_reward=409.24 +/- 293.85
Episode length: 620.20 +/- 27.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 620         |
|    mean_reward          | 409         |
| time/                   |             |
|    total_timesteps      | 704000      |
| train/                  |             |
|    approx_kl            | 0.004316341 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.9        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 53.1        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 2.27        |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=706000, episode_reward=244.11 +/- 270.91
Episode length: 599.20 +/- 88.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 599          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 706000       |
| train/                  |              |
|    approx_kl            | 0.0033670417 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+03     |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 2.28         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=708000, episode_reward=379.06 +/- 187.57
Episode length: 557.60 +/- 58.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 558           |
|    mean_reward          | 379           |
| time/                   |               |
|    total_timesteps      | 708000        |
| train/                  |               |
|    approx_kl            | 0.00048362525 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.91         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 167           |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.00135      |
|    std                  | 2.28          |
|    value_loss           | 529           |
-------------------------------------------
Eval num_timesteps=710000, episode_reward=650.02 +/- 434.05
Episode length: 627.60 +/- 88.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 628          |
|    mean_reward          | 650          |
| time/                   |              |
|    total_timesteps      | 710000       |
| train/                  |              |
|    approx_kl            | 0.0015447731 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.543        |
|    learning_rate        | 0.001        |
|    loss                 | 2.89e+03     |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 2.28         |
|    value_loss           | 7.23e+03     |
------------------------------------------
Eval num_timesteps=712000, episode_reward=257.91 +/- 124.06
Episode length: 571.80 +/- 55.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 572           |
|    mean_reward          | 258           |
| time/                   |               |
|    total_timesteps      | 712000        |
| train/                  |               |
|    approx_kl            | 0.00036868855 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.91         |
|    explained_variance   | 0.617         |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+03      |
|    n_updates            | 3470          |
|    policy_gradient_loss | -0.000423     |
|    std                  | 2.28          |
|    value_loss           | 9.27e+03      |
-------------------------------------------
Eval num_timesteps=714000, episode_reward=789.72 +/- 830.01
Episode length: 566.20 +/- 35.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 566          |
|    mean_reward          | 790          |
| time/                   |              |
|    total_timesteps      | 714000       |
| train/                  |              |
|    approx_kl            | 0.0037324373 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 2.28         |
|    value_loss           | 577          |
------------------------------------------
Eval num_timesteps=716000, episode_reward=1093.51 +/- 547.93
Episode length: 537.20 +/- 52.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 1.09e+03     |
| time/                   |              |
|    total_timesteps      | 716000       |
| train/                  |              |
|    approx_kl            | 0.0068007438 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 302          |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 2.28         |
|    value_loss           | 767          |
------------------------------------------
New best mean reward!
Eval num_timesteps=718000, episode_reward=360.83 +/- 168.19
Episode length: 560.20 +/- 80.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 560          |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 718000       |
| train/                  |              |
|    approx_kl            | 0.0010463747 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+03     |
|    n_updates            | 3500         |
|    policy_gradient_loss | 0.000699     |
|    std                  | 2.28         |
|    value_loss           | 4.02e+03     |
------------------------------------------
Eval num_timesteps=720000, episode_reward=581.22 +/- 367.16
Episode length: 541.60 +/- 76.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 542         |
|    mean_reward          | 581         |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.002492595 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.91       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 186         |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 2.28        |
|    value_loss           | 548         |
-----------------------------------------
Eval num_timesteps=722000, episode_reward=533.65 +/- 431.62
Episode length: 479.00 +/- 86.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 534         |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.007115058 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.92       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 99.6        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 2.28        |
|    value_loss           | 250         |
-----------------------------------------
Eval num_timesteps=724000, episode_reward=590.28 +/- 619.95
Episode length: 512.00 +/- 91.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 512          |
|    mean_reward          | 590          |
| time/                   |              |
|    total_timesteps      | 724000       |
| train/                  |              |
|    approx_kl            | 0.0047853626 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 2.28         |
|    value_loss           | 4.08e+03     |
------------------------------------------
Eval num_timesteps=726000, episode_reward=326.58 +/- 297.67
Episode length: 538.20 +/- 103.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 327          |
| time/                   |              |
|    total_timesteps      | 726000       |
| train/                  |              |
|    approx_kl            | 0.0016919188 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+03     |
|    n_updates            | 3540         |
|    policy_gradient_loss | -0.000456    |
|    std                  | 2.29         |
|    value_loss           | 7.04e+03     |
------------------------------------------
Eval num_timesteps=728000, episode_reward=462.04 +/- 483.51
Episode length: 493.20 +/- 40.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 493           |
|    mean_reward          | 462           |
| time/                   |               |
|    total_timesteps      | 728000        |
| train/                  |               |
|    approx_kl            | 0.00027815378 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.92         |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.001         |
|    loss                 | 334           |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.000328     |
|    std                  | 2.29          |
|    value_loss           | 941           |
-------------------------------------------
Eval num_timesteps=730000, episode_reward=381.61 +/- 304.72
Episode length: 457.80 +/- 73.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 730000       |
| train/                  |              |
|    approx_kl            | 0.0008532781 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 467          |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000918    |
|    std                  | 2.29         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=732000, episode_reward=445.99 +/- 127.00
Episode length: 442.00 +/- 27.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 732000        |
| train/                  |               |
|    approx_kl            | 0.00044800676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0.66          |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+03      |
|    n_updates            | 3570          |
|    policy_gradient_loss | -2.28e-05     |
|    std                  | 2.29          |
|    value_loss           | 3.93e+03      |
-------------------------------------------
Eval num_timesteps=734000, episode_reward=465.01 +/- 346.14
Episode length: 470.20 +/- 28.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 465           |
| time/                   |               |
|    total_timesteps      | 734000        |
| train/                  |               |
|    approx_kl            | 0.00078343914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | -0.325        |
|    learning_rate        | 0.001         |
|    loss                 | 8.88e+03      |
|    n_updates            | 3580          |
|    policy_gradient_loss | -0.00106      |
|    std                  | 2.29          |
|    value_loss           | 1.99e+04      |
-------------------------------------------
Eval num_timesteps=736000, episode_reward=707.21 +/- 1076.52
Episode length: 497.80 +/- 56.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | 707           |
| time/                   |               |
|    total_timesteps      | 736000        |
| train/                  |               |
|    approx_kl            | 0.00034871104 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0.516         |
|    learning_rate        | 0.001         |
|    loss                 | 3.41e+03      |
|    n_updates            | 3590          |
|    policy_gradient_loss | -2.56e-05     |
|    std                  | 2.29          |
|    value_loss           | 8.18e+03      |
-------------------------------------------
Eval num_timesteps=738000, episode_reward=320.60 +/- 192.53
Episode length: 501.00 +/- 55.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 738000      |
| train/                  |             |
|    approx_kl            | 0.000493317 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.93       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.001       |
|    loss                 | 436         |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.000647   |
|    std                  | 2.29        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=414.50 +/- 234.82
Episode length: 508.60 +/- 57.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 415         |
| time/                   |             |
|    total_timesteps      | 740000      |
| train/                  |             |
|    approx_kl            | 0.002392271 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.93       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.001       |
|    loss                 | 502         |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.000994   |
|    std                  | 2.29        |
|    value_loss           | 1.38e+03    |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=416.41 +/- 379.07
Episode length: 405.20 +/- 54.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 742000       |
| train/                  |              |
|    approx_kl            | 0.0016918562 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.001        |
|    loss                 | 3.58e+03     |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.29         |
|    value_loss           | 8.36e+03     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=416.51 +/- 390.18
Episode length: 457.80 +/- 50.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 744000       |
| train/                  |              |
|    approx_kl            | 0.0011185729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.94        |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.000328    |
|    std                  | 2.3          |
|    value_loss           | 5.98e+03     |
------------------------------------------
Eval num_timesteps=746000, episode_reward=441.62 +/- 239.04
Episode length: 506.40 +/- 103.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 442          |
| time/                   |              |
|    total_timesteps      | 746000       |
| train/                  |              |
|    approx_kl            | 0.0014282303 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.94        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.001        |
|    loss                 | 494          |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.000618    |
|    std                  | 2.3          |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=748000, episode_reward=707.00 +/- 868.27
Episode length: 424.40 +/- 50.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 707          |
| time/                   |              |
|    total_timesteps      | 748000       |
| train/                  |              |
|    approx_kl            | 0.0016717697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.0814       |
|    learning_rate        | 0.001        |
|    loss                 | 6.61e+03     |
|    n_updates            | 3650         |
|    policy_gradient_loss | 0.000482     |
|    std                  | 2.3          |
|    value_loss           | 1.49e+04     |
------------------------------------------
Eval num_timesteps=750000, episode_reward=401.44 +/- 381.61
Episode length: 455.00 +/- 51.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0007440458 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.622        |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+03     |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000247    |
|    std                  | 2.3          |
|    value_loss           | 5.77e+03     |
------------------------------------------
Eval num_timesteps=752000, episode_reward=351.91 +/- 211.58
Episode length: 399.00 +/- 47.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0035941657 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 2.31         |
|    value_loss           | 4.4e+03      |
------------------------------------------
Eval num_timesteps=754000, episode_reward=834.81 +/- 842.81
Episode length: 517.20 +/- 97.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 835          |
| time/                   |              |
|    total_timesteps      | 754000       |
| train/                  |              |
|    approx_kl            | 0.0023085373 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 570          |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.000632    |
|    std                  | 2.31         |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=756000, episode_reward=495.53 +/- 208.66
Episode length: 506.00 +/- 40.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 496          |
| time/                   |              |
|    total_timesteps      | 756000       |
| train/                  |              |
|    approx_kl            | 0.0012330093 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.000386    |
|    std                  | 2.31         |
|    value_loss           | 4.35e+03     |
------------------------------------------
Eval num_timesteps=758000, episode_reward=304.23 +/- 198.54
Episode length: 467.00 +/- 70.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 304           |
| time/                   |               |
|    total_timesteps      | 758000        |
| train/                  |               |
|    approx_kl            | 0.00026889145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.97         |
|    explained_variance   | 0.464         |
|    learning_rate        | 0.001         |
|    loss                 | 8.3e+03       |
|    n_updates            | 3700          |
|    policy_gradient_loss | 0.000133      |
|    std                  | 2.31          |
|    value_loss           | 1.69e+04      |
-------------------------------------------
Eval num_timesteps=760000, episode_reward=440.73 +/- 293.32
Episode length: 508.40 +/- 107.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 441          |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0001099357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.001        |
|    loss                 | 4.01e+03     |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.000382    |
|    std                  | 2.31         |
|    value_loss           | 8.81e+03     |
------------------------------------------
Eval num_timesteps=762000, episode_reward=1035.47 +/- 1110.81
Episode length: 541.00 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 541          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0010551186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 719          |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 2.32         |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=764000, episode_reward=268.54 +/- 155.96
Episode length: 425.60 +/- 53.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0026793564 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.455        |
|    learning_rate        | 0.001        |
|    loss                 | 2.94e+03     |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 2.32         |
|    value_loss           | 7.19e+03     |
------------------------------------------
Eval num_timesteps=766000, episode_reward=335.11 +/- 189.87
Episode length: 413.80 +/- 23.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 335          |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0015691428 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.001        |
|    loss                 | 5.86e+03     |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000529    |
|    std                  | 2.32         |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=768000, episode_reward=491.67 +/- 688.30
Episode length: 420.00 +/- 95.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=55.69 +/- 55.20
Episode length: 353.80 +/- 25.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 55.7         |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 0.0024014905 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.001        |
|    loss                 | 632          |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 2.32         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=772000, episode_reward=168.52 +/- 283.98
Episode length: 346.60 +/- 62.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 772000      |
| train/                  |             |
|    approx_kl            | 0.007167035 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 316         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 2.32        |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=774000, episode_reward=474.99 +/- 563.67
Episode length: 414.40 +/- 69.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 475         |
| time/                   |             |
|    total_timesteps      | 774000      |
| train/                  |             |
|    approx_kl            | 0.011215764 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 234         |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.32        |
|    value_loss           | 549         |
-----------------------------------------
Eval num_timesteps=776000, episode_reward=58.84 +/- 21.67
Episode length: 379.20 +/- 24.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 58.8         |
| time/                   |              |
|    total_timesteps      | 776000       |
| train/                  |              |
|    approx_kl            | 0.0014034373 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 3780         |
|    policy_gradient_loss | 3.9e-05      |
|    std                  | 2.32         |
|    value_loss           | 3.92e+03     |
------------------------------------------
Eval num_timesteps=778000, episode_reward=93.34 +/- 76.56
Episode length: 370.60 +/- 44.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 93.3        |
| time/                   |             |
|    total_timesteps      | 778000      |
| train/                  |             |
|    approx_kl            | 0.002943565 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.98       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 258         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00098    |
|    std                  | 2.32        |
|    value_loss           | 595         |
-----------------------------------------
Eval num_timesteps=780000, episode_reward=38.73 +/- 61.19
Episode length: 359.00 +/- 26.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 38.7         |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0019932627 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+03     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000112    |
|    std                  | 2.31         |
|    value_loss           | 4.86e+03     |
------------------------------------------
Eval num_timesteps=782000, episode_reward=226.97 +/- 236.55
Episode length: 428.60 +/- 81.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 782000       |
| train/                  |              |
|    approx_kl            | 0.0006862045 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 221          |
|    n_updates            | 3810         |
|    policy_gradient_loss | -4.51e-05    |
|    std                  | 2.32         |
|    value_loss           | 576          |
------------------------------------------
Eval num_timesteps=784000, episode_reward=227.13 +/- 148.54
Episode length: 421.20 +/- 42.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 784000       |
| train/                  |              |
|    approx_kl            | 0.0045359535 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 195          |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.32         |
|    value_loss           | 444          |
------------------------------------------
Eval num_timesteps=786000, episode_reward=195.39 +/- 192.50
Episode length: 391.20 +/- 44.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 195           |
| time/                   |               |
|    total_timesteps      | 786000        |
| train/                  |               |
|    approx_kl            | 0.00087308115 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.98         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 144           |
|    n_updates            | 3830          |
|    policy_gradient_loss | -0.000506     |
|    std                  | 2.32          |
|    value_loss           | 332           |
-------------------------------------------
Eval num_timesteps=788000, episode_reward=135.05 +/- 135.19
Episode length: 398.80 +/- 31.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 788000       |
| train/                  |              |
|    approx_kl            | 0.0043467805 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 146          |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 2.33         |
|    value_loss           | 318          |
------------------------------------------
Eval num_timesteps=790000, episode_reward=303.89 +/- 158.44
Episode length: 422.80 +/- 25.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 790000      |
| train/                  |             |
|    approx_kl            | 0.006500548 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.01       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 146         |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 2.35        |
|    value_loss           | 347         |
-----------------------------------------
Eval num_timesteps=792000, episode_reward=387.19 +/- 403.34
Episode length: 457.00 +/- 60.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | 387          |
| time/                   |              |
|    total_timesteps      | 792000       |
| train/                  |              |
|    approx_kl            | 0.0014232666 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.04        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.0001      |
|    std                  | 2.36         |
|    value_loss           | 314          |
------------------------------------------
Eval num_timesteps=794000, episode_reward=2390.91 +/- 1761.64
Episode length: 575.60 +/- 72.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 576         |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 794000      |
| train/                  |             |
|    approx_kl            | 0.008816303 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.06       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.001       |
|    loss                 | 6.59e+03    |
|    n_updates            | 3870        |
|    policy_gradient_loss | 0.000285    |
|    std                  | 2.37        |
|    value_loss           | 1.38e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=796000, episode_reward=674.92 +/- 377.33
Episode length: 548.80 +/- 82.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 549          |
|    mean_reward          | 675          |
| time/                   |              |
|    total_timesteps      | 796000       |
| train/                  |              |
|    approx_kl            | 0.0069330917 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.06        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 422          |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 2.37         |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=798000, episode_reward=598.70 +/- 188.54
Episode length: 544.60 +/- 143.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 545         |
|    mean_reward          | 599         |
| time/                   |             |
|    total_timesteps      | 798000      |
| train/                  |             |
|    approx_kl            | 0.003682834 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 391         |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.000614   |
|    std                  | 2.37        |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=108.87 +/- 133.97
Episode length: 392.40 +/- 38.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.010118268 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 390         |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 2.37        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=802000, episode_reward=418.72 +/- 348.51
Episode length: 490.20 +/- 22.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 802000       |
| train/                  |              |
|    approx_kl            | 0.0014638766 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.609        |
|    learning_rate        | 0.001        |
|    loss                 | 2.36e+03     |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.000595    |
|    std                  | 2.37         |
|    value_loss           | 5.51e+03     |
------------------------------------------
Eval num_timesteps=804000, episode_reward=449.26 +/- 374.88
Episode length: 579.20 +/- 127.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 579           |
|    mean_reward          | 449           |
| time/                   |               |
|    total_timesteps      | 804000        |
| train/                  |               |
|    approx_kl            | 0.00076768774 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.07         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.001         |
|    loss                 | 2.26e+03      |
|    n_updates            | 3920          |
|    policy_gradient_loss | -0.000637     |
|    std                  | 2.38          |
|    value_loss           | 7.13e+03      |
-------------------------------------------
Eval num_timesteps=806000, episode_reward=418.51 +/- 333.86
Episode length: 487.60 +/- 100.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 419         |
| time/                   |             |
|    total_timesteps      | 806000      |
| train/                  |             |
|    approx_kl            | 0.002863845 |
|    clip_fraction        | 0.0021      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 383         |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 2.37        |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=808000, episode_reward=557.81 +/- 524.58
Episode length: 523.80 +/- 94.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 524         |
|    mean_reward          | 558         |
| time/                   |             |
|    total_timesteps      | 808000      |
| train/                  |             |
|    approx_kl            | 0.004392659 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 261         |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 2.37        |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=810000, episode_reward=99.53 +/- 154.05
Episode length: 471.20 +/- 22.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 99.5         |
| time/                   |              |
|    total_timesteps      | 810000       |
| train/                  |              |
|    approx_kl            | 0.0030059966 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.001        |
|    loss                 | 212          |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.37         |
|    value_loss           | 689          |
------------------------------------------
Eval num_timesteps=812000, episode_reward=140.58 +/- 174.16
Episode length: 519.20 +/- 61.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 519          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 812000       |
| train/                  |              |
|    approx_kl            | 0.0024145725 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 166          |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.37         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=814000, episode_reward=96.50 +/- 266.92
Episode length: 457.40 +/- 78.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 96.5          |
| time/                   |               |
|    total_timesteps      | 814000        |
| train/                  |               |
|    approx_kl            | 0.00090654363 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.07         |
|    explained_variance   | 0.798         |
|    learning_rate        | 0.001         |
|    loss                 | 330           |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.000787     |
|    std                  | 2.38          |
|    value_loss           | 1.06e+03      |
-------------------------------------------
Eval num_timesteps=816000, episode_reward=223.77 +/- 228.34
Episode length: 510.80 +/- 41.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | 224          |
| time/                   |              |
|    total_timesteps      | 816000       |
| train/                  |              |
|    approx_kl            | 0.0031898736 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 635          |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 2.38         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=818000, episode_reward=267.37 +/- 325.28
Episode length: 543.60 +/- 117.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 818000       |
| train/                  |              |
|    approx_kl            | 0.0012067112 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.691        |
|    learning_rate        | 0.001        |
|    loss                 | 720          |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.000469    |
|    std                  | 2.38         |
|    value_loss           | 2.04e+03     |
------------------------------------------
Eval num_timesteps=820000, episode_reward=-110.67 +/- 32.71
Episode length: 396.60 +/- 37.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | -111        |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.002342917 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 217         |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 2.38        |
|    value_loss           | 620         |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=-17.79 +/- 145.64
Episode length: 416.60 +/- 70.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | -17.8        |
| time/                   |              |
|    total_timesteps      | 822000       |
| train/                  |              |
|    approx_kl            | 0.0050715236 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 2.38         |
|    value_loss           | 561          |
------------------------------------------
Eval num_timesteps=824000, episode_reward=-85.20 +/- 45.18
Episode length: 382.60 +/- 34.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | -85.2       |
| time/                   |             |
|    total_timesteps      | 824000      |
| train/                  |             |
|    approx_kl            | 0.009297089 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 144         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 2.38        |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=826000, episode_reward=-89.29 +/- 88.66
Episode length: 344.60 +/- 32.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 345         |
|    mean_reward          | -89.3       |
| time/                   |             |
|    total_timesteps      | 826000      |
| train/                  |             |
|    approx_kl            | 0.001788704 |
|    clip_fraction        | 0.00303     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 132         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 2.38        |
|    value_loss           | 371         |
-----------------------------------------
Eval num_timesteps=828000, episode_reward=-86.98 +/- 51.80
Episode length: 376.40 +/- 31.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | -87         |
| time/                   |             |
|    total_timesteps      | 828000      |
| train/                  |             |
|    approx_kl            | 0.005920974 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 101         |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 2.39        |
|    value_loss           | 266         |
-----------------------------------------
Eval num_timesteps=830000, episode_reward=-112.16 +/- 22.49
Episode length: 368.80 +/- 15.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0101801595 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.09        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.00329     |
|    std                  | 2.39         |
|    value_loss           | 292          |
------------------------------------------
Eval num_timesteps=832000, episode_reward=-112.01 +/- 30.30
Episode length: 354.80 +/- 40.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 832000      |
| train/                  |             |
|    approx_kl            | 0.004418454 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 86.3        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00136    |
|    std                  | 2.4         |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=834000, episode_reward=-124.87 +/- 13.68
Episode length: 326.00 +/- 13.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0068337824 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 88.7         |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00362     |
|    std                  | 2.41         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=836000, episode_reward=-57.19 +/- 46.76
Episode length: 388.20 +/- 33.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | -57.2       |
| time/                   |             |
|    total_timesteps      | 836000      |
| train/                  |             |
|    approx_kl            | 0.004293788 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 86.2        |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.000774   |
|    std                  | 2.42        |
|    value_loss           | 212         |
-----------------------------------------
Eval num_timesteps=838000, episode_reward=-70.57 +/- 63.00
Episode length: 384.60 +/- 65.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 385        |
|    mean_reward          | -70.6      |
| time/                   |            |
|    total_timesteps      | 838000     |
| train/                  |            |
|    approx_kl            | 0.00686601 |
|    clip_fraction        | 0.0314     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.14      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.001      |
|    loss                 | 108        |
|    n_updates            | 4090       |
|    policy_gradient_loss | -0.000789  |
|    std                  | 2.43       |
|    value_loss           | 277        |
----------------------------------------
Eval num_timesteps=840000, episode_reward=-14.93 +/- 68.64
Episode length: 422.60 +/- 31.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | -14.9        |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0066224886 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.16        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 68.6         |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00323     |
|    std                  | 2.44         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=842000, episode_reward=-102.38 +/- 43.53
Episode length: 324.60 +/- 61.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | -102        |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.008087521 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.18       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 161         |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00205    |
|    std                  | 2.45        |
|    value_loss           | 420         |
-----------------------------------------
Eval num_timesteps=844000, episode_reward=133.16 +/- 476.63
Episode length: 389.60 +/- 73.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0056716637 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.2         |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 89.5         |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.46         |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=846000, episode_reward=-50.08 +/- 18.56
Episode length: 408.00 +/- 36.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | -50.1        |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 0.0071701095 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.23        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 2.48         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=848000, episode_reward=769.51 +/- 1140.82
Episode length: 493.80 +/- 195.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | 770         |
| time/                   |             |
|    total_timesteps      | 848000      |
| train/                  |             |
|    approx_kl            | 0.008069694 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.24       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.001       |
|    loss                 | 353         |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 2.48        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=850000, episode_reward=99.21 +/- 220.86
Episode length: 471.20 +/- 159.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 99.2        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.001147801 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.25       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.001       |
|    loss                 | 628         |
|    n_updates            | 4150        |
|    policy_gradient_loss | 0.000123    |
|    std                  | 2.49        |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=852000, episode_reward=145.82 +/- 229.57
Episode length: 428.40 +/- 62.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 0.0052534644 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.26        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 331          |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 2.49         |
|    value_loss           | 897          |
------------------------------------------
Eval num_timesteps=854000, episode_reward=596.54 +/- 912.78
Episode length: 464.60 +/- 113.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 465      |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
Eval num_timesteps=856000, episode_reward=228.10 +/- 89.77
Episode length: 394.80 +/- 44.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 856000      |
| train/                  |             |
|    approx_kl            | 0.011616828 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.27       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.001       |
|    loss                 | 464         |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 2.5         |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=858000, episode_reward=85.96 +/- 64.34
Episode length: 337.20 +/- 25.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | 86           |
| time/                   |              |
|    total_timesteps      | 858000       |
| train/                  |              |
|    approx_kl            | 0.0034951584 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 2.5          |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=860000, episode_reward=93.32 +/- 64.48
Episode length: 352.00 +/- 35.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 93.3         |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 0.0060339607 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 239          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 2.5          |
|    value_loss           | 704          |
------------------------------------------
Eval num_timesteps=862000, episode_reward=108.20 +/- 91.07
Episode length: 337.60 +/- 46.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 108          |
| time/                   |              |
|    total_timesteps      | 862000       |
| train/                  |              |
|    approx_kl            | 0.0070371768 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 190          |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 2.5          |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=864000, episode_reward=58.91 +/- 51.72
Episode length: 315.60 +/- 34.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | 58.9         |
| time/                   |              |
|    total_timesteps      | 864000       |
| train/                  |              |
|    approx_kl            | 0.0026367393 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 220          |
|    n_updates            | 4210         |
|    policy_gradient_loss | 0.000121     |
|    std                  | 2.5          |
|    value_loss           | 544          |
------------------------------------------
Eval num_timesteps=866000, episode_reward=143.59 +/- 77.94
Episode length: 363.40 +/- 43.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 363         |
|    mean_reward          | 144         |
| time/                   |             |
|    total_timesteps      | 866000      |
| train/                  |             |
|    approx_kl            | 0.002627975 |
|    clip_fraction        | 0.000635    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.28       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 151         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 2.51        |
|    value_loss           | 423         |
-----------------------------------------
Eval num_timesteps=868000, episode_reward=86.47 +/- 73.48
Episode length: 326.60 +/- 59.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 327         |
|    mean_reward          | 86.5        |
| time/                   |             |
|    total_timesteps      | 868000      |
| train/                  |             |
|    approx_kl            | 0.005863006 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.29       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 2.51        |
|    value_loss           | 428         |
-----------------------------------------
Eval num_timesteps=870000, episode_reward=13.31 +/- 31.83
Episode length: 287.40 +/- 23.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 870000       |
| train/                  |              |
|    approx_kl            | 0.0059602903 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.3         |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 140          |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.000375    |
|    std                  | 2.52         |
|    value_loss           | 367          |
------------------------------------------
Eval num_timesteps=872000, episode_reward=22.65 +/- 77.78
Episode length: 292.40 +/- 49.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 872000      |
| train/                  |             |
|    approx_kl            | 0.007470903 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.32       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 97.7        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00366    |
|    std                  | 2.53        |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=874000, episode_reward=20.63 +/- 77.47
Episode length: 284.80 +/- 40.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 285        |
|    mean_reward          | 20.6       |
| time/                   |            |
|    total_timesteps      | 874000     |
| train/                  |            |
|    approx_kl            | 0.00458988 |
|    clip_fraction        | 0.00962    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.001      |
|    loss                 | 85.1       |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.000348  |
|    std                  | 2.54       |
|    value_loss           | 201        |
----------------------------------------
Eval num_timesteps=876000, episode_reward=168.17 +/- 52.04
Episode length: 359.00 +/- 15.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 876000      |
| train/                  |             |
|    approx_kl            | 0.004763577 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.34       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 100         |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 2.55        |
|    value_loss           | 210         |
-----------------------------------------
Eval num_timesteps=878000, episode_reward=205.61 +/- 171.63
Episode length: 368.20 +/- 53.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 878000      |
| train/                  |             |
|    approx_kl            | 0.002889333 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.35       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.001       |
|    loss                 | 286         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00204    |
|    std                  | 2.55        |
|    value_loss           | 1.34e+03    |
-----------------------------------------
Eval num_timesteps=880000, episode_reward=1.73 +/- 46.55
Episode length: 301.40 +/- 43.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.003400403 |
|    clip_fraction        | 0.00347     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.36       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 87.1        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 2.56        |
|    value_loss           | 260         |
-----------------------------------------
Eval num_timesteps=882000, episode_reward=100.54 +/- 175.37
Episode length: 321.80 +/- 67.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 101          |
| time/                   |              |
|    total_timesteps      | 882000       |
| train/                  |              |
|    approx_kl            | 0.0069014933 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.36        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 96.6         |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 2.56         |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=884000, episode_reward=99.29 +/- 57.52
Episode length: 329.20 +/- 23.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 329        |
|    mean_reward          | 99.3       |
| time/                   |            |
|    total_timesteps      | 884000     |
| train/                  |            |
|    approx_kl            | 0.00825037 |
|    clip_fraction        | 0.0222     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.36      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.001      |
|    loss                 | 70.8       |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.00271   |
|    std                  | 2.56       |
|    value_loss           | 165        |
----------------------------------------
Eval num_timesteps=886000, episode_reward=235.84 +/- 159.82
Episode length: 391.80 +/- 40.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 886000      |
| train/                  |             |
|    approx_kl            | 0.009806545 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.36       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 70.5        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 2.56        |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=888000, episode_reward=138.89 +/- 102.13
Episode length: 362.60 +/- 32.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 363          |
|    mean_reward          | 139          |
| time/                   |              |
|    total_timesteps      | 888000       |
| train/                  |              |
|    approx_kl            | 0.0053100754 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.37        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 51.4         |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.000926    |
|    std                  | 2.56         |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=890000, episode_reward=110.02 +/- 96.77
Episode length: 350.60 +/- 57.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 110         |
| time/                   |             |
|    total_timesteps      | 890000      |
| train/                  |             |
|    approx_kl            | 0.004986956 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.38       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 63.3        |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00131    |
|    std                  | 2.57        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=892000, episode_reward=237.38 +/- 268.69
Episode length: 370.40 +/- 40.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 892000      |
| train/                  |             |
|    approx_kl            | 0.006931443 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.38       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 57.1        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0043     |
|    std                  | 2.57        |
|    value_loss           | 128         |
-----------------------------------------
Eval num_timesteps=894000, episode_reward=188.34 +/- 176.06
Episode length: 376.60 +/- 62.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 188          |
| time/                   |              |
|    total_timesteps      | 894000       |
| train/                  |              |
|    approx_kl            | 0.0071611586 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.4         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 49           |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 2.58         |
|    value_loss           | 116          |
------------------------------------------
Eval num_timesteps=896000, episode_reward=119.05 +/- 240.87
Episode length: 343.60 +/- 66.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 896000       |
| train/                  |              |
|    approx_kl            | 0.0055734445 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 80.9         |
|    n_updates            | 4370         |
|    policy_gradient_loss | 0.000104     |
|    std                  | 2.59         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=898000, episode_reward=173.62 +/- 252.07
Episode length: 389.20 +/- 95.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 174          |
| time/                   |              |
|    total_timesteps      | 898000       |
| train/                  |              |
|    approx_kl            | 0.0077408706 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 64           |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 2.58         |
|    value_loss           | 171          |
------------------------------------------
Eval num_timesteps=900000, episode_reward=83.75 +/- 139.21
Episode length: 351.80 +/- 46.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 83.7         |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0063280985 |
|    clip_fraction        | 0.07         |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 66.7         |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 2.58         |
|    value_loss           | 174          |
------------------------------------------
Eval num_timesteps=902000, episode_reward=8.67 +/- 45.21
Episode length: 367.60 +/- 43.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 8.67         |
| time/                   |              |
|    total_timesteps      | 902000       |
| train/                  |              |
|    approx_kl            | 0.0049991403 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 93.9         |
|    n_updates            | 4400         |
|    policy_gradient_loss | 0.000835     |
|    std                  | 2.58         |
|    value_loss           | 208          |
------------------------------------------
Eval num_timesteps=904000, episode_reward=34.81 +/- 138.32
Episode length: 359.80 +/- 61.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 360          |
|    mean_reward          | 34.8         |
| time/                   |              |
|    total_timesteps      | 904000       |
| train/                  |              |
|    approx_kl            | 0.0033306691 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 73.4         |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.000595    |
|    std                  | 2.58         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=906000, episode_reward=311.40 +/- 351.37
Episode length: 423.60 +/- 63.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 906000       |
| train/                  |              |
|    approx_kl            | 0.0075006364 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00298     |
|    std                  | 2.59         |
|    value_loss           | 359          |
------------------------------------------
Eval num_timesteps=908000, episode_reward=1281.76 +/- 2173.11
Episode length: 480.00 +/- 115.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 908000      |
| train/                  |             |
|    approx_kl            | 0.012449624 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 140         |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 2.59        |
|    value_loss           | 370         |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=227.51 +/- 341.14
Episode length: 400.00 +/- 61.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0018356111 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 313          |
|    n_updates            | 4440         |
|    policy_gradient_loss | 4.42e-05     |
|    std                  | 2.59         |
|    value_loss           | 730          |
------------------------------------------
Eval num_timesteps=912000, episode_reward=96.84 +/- 61.90
Episode length: 404.60 +/- 31.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 96.8        |
| time/                   |             |
|    total_timesteps      | 912000      |
| train/                  |             |
|    approx_kl            | 0.007874457 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 403         |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 2.59        |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=914000, episode_reward=13.38 +/- 31.72
Episode length: 377.00 +/- 23.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 914000       |
| train/                  |              |
|    approx_kl            | 0.0012498097 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 438          |
|    n_updates            | 4460         |
|    policy_gradient_loss | 3.35e-06     |
|    std                  | 2.59         |
|    value_loss           | 1.02e+03     |
------------------------------------------
Eval num_timesteps=916000, episode_reward=-16.42 +/- 28.69
Episode length: 389.80 +/- 34.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | -16.4        |
| time/                   |              |
|    total_timesteps      | 916000       |
| train/                  |              |
|    approx_kl            | 0.0049859416 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 52.9         |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.000779    |
|    std                  | 2.59         |
|    value_loss           | 191          |
------------------------------------------
Eval num_timesteps=918000, episode_reward=14.59 +/- 55.46
Episode length: 418.60 +/- 66.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 918000      |
| train/                  |             |
|    approx_kl            | 0.005149808 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 54.2        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00058    |
|    std                  | 2.59        |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=920000, episode_reward=9.55 +/- 88.27
Episode length: 476.60 +/- 132.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 9.55         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0050894003 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 2.59         |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=922000, episode_reward=36.75 +/- 32.67
Episode length: 532.20 +/- 110.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 532          |
|    mean_reward          | 36.7         |
| time/                   |              |
|    total_timesteps      | 922000       |
| train/                  |              |
|    approx_kl            | 0.0034370702 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.43        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 2.6          |
|    value_loss           | 484          |
------------------------------------------
Eval num_timesteps=924000, episode_reward=-27.45 +/- 29.00
Episode length: 435.40 +/- 47.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | -27.5        |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0010916447 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.43        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000107    |
|    std                  | 2.6          |
|    value_loss           | 377          |
------------------------------------------
Eval num_timesteps=926000, episode_reward=21.59 +/- 55.30
Episode length: 470.80 +/- 55.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 21.6         |
| time/                   |              |
|    total_timesteps      | 926000       |
| train/                  |              |
|    approx_kl            | 0.0049911314 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.44        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 87.4         |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 2.6          |
|    value_loss           | 326          |
------------------------------------------
Eval num_timesteps=928000, episode_reward=6.34 +/- 50.04
Episode length: 522.80 +/- 79.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 523         |
|    mean_reward          | 6.34        |
| time/                   |             |
|    total_timesteps      | 928000      |
| train/                  |             |
|    approx_kl            | 0.010316268 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.44       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 71.4        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 2.6         |
|    value_loss           | 290         |
-----------------------------------------
Eval num_timesteps=930000, episode_reward=-35.44 +/- 28.76
Episode length: 500.60 +/- 42.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | -35.4       |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.002653364 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.44       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 92.6        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00112    |
|    std                  | 2.61        |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=932000, episode_reward=-73.85 +/- 42.80
Episode length: 448.80 +/- 91.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | -73.8        |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 0.0051375655 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 2.61         |
|    value_loss           | 288          |
------------------------------------------
Eval num_timesteps=934000, episode_reward=-79.03 +/- 19.32
Episode length: 386.40 +/- 30.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | -79          |
| time/                   |              |
|    total_timesteps      | 934000       |
| train/                  |              |
|    approx_kl            | 0.0026362329 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 70           |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.62         |
|    value_loss           | 190          |
------------------------------------------
Eval num_timesteps=936000, episode_reward=-68.78 +/- 8.31
Episode length: 441.20 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | -68.8        |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0051661115 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 74.5         |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 2.61         |
|    value_loss           | 198          |
------------------------------------------
Eval num_timesteps=938000, episode_reward=-112.23 +/- 9.19
Episode length: 413.60 +/- 40.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 938000       |
| train/                  |              |
|    approx_kl            | 0.0049475683 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 67.1         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 2.61         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=940000, episode_reward=-92.80 +/- 32.07
Episode length: 407.80 +/- 42.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -92.8    |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=942000, episode_reward=-87.03 +/- 25.64
Episode length: 394.00 +/- 32.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 394        |
|    mean_reward          | -87        |
| time/                   |            |
|    total_timesteps      | 942000     |
| train/                  |            |
|    approx_kl            | 0.00219108 |
|    clip_fraction        | 0.00234    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.001      |
|    loss                 | 85.7       |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.000538  |
|    std                  | 2.61       |
|    value_loss           | 198        |
----------------------------------------
Eval num_timesteps=944000, episode_reward=-85.49 +/- 33.40
Episode length: 385.40 +/- 74.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | -85.5        |
| time/                   |              |
|    total_timesteps      | 944000       |
| train/                  |              |
|    approx_kl            | 0.0052355225 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 67.5         |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 2.62         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=946000, episode_reward=-59.01 +/- 18.99
Episode length: 411.00 +/- 38.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | -59          |
| time/                   |              |
|    total_timesteps      | 946000       |
| train/                  |              |
|    approx_kl            | 0.0041834963 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 54.7         |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 2.62         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=948000, episode_reward=-22.88 +/- 38.17
Episode length: 409.20 +/- 58.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | -22.9        |
| time/                   |              |
|    total_timesteps      | 948000       |
| train/                  |              |
|    approx_kl            | 0.0029817384 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.46        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 69.6         |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.000474    |
|    std                  | 2.62         |
|    value_loss           | 182          |
------------------------------------------
Eval num_timesteps=950000, episode_reward=-92.17 +/- 10.94
Episode length: 401.60 +/- 44.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | -92.2        |
| time/                   |              |
|    total_timesteps      | 950000       |
| train/                  |              |
|    approx_kl            | 0.0051535703 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.46        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 2.62         |
|    value_loss           | 308          |
------------------------------------------
Eval num_timesteps=952000, episode_reward=-70.24 +/- 24.02
Episode length: 427.20 +/- 33.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | -70.2       |
| time/                   |             |
|    total_timesteps      | 952000      |
| train/                  |             |
|    approx_kl            | 0.004368231 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.47       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 55.1        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 2.63        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=954000, episode_reward=-75.50 +/- 7.06
Episode length: 415.80 +/- 19.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | -75.5       |
| time/                   |             |
|    total_timesteps      | 954000      |
| train/                  |             |
|    approx_kl            | 0.009254493 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.48       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 76.5        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00498    |
|    std                  | 2.64        |
|    value_loss           | 202         |
-----------------------------------------
Eval num_timesteps=956000, episode_reward=-15.98 +/- 72.52
Episode length: 496.20 +/- 57.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -16          |
| time/                   |              |
|    total_timesteps      | 956000       |
| train/                  |              |
|    approx_kl            | 0.0071984483 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.48        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 56.6         |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 2.64         |
|    value_loss           | 196          |
------------------------------------------
Eval num_timesteps=958000, episode_reward=-63.80 +/- 40.11
Episode length: 423.80 +/- 32.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | -63.8       |
| time/                   |             |
|    total_timesteps      | 958000      |
| train/                  |             |
|    approx_kl            | 0.008205177 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.49       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 37.7        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00307    |
|    std                  | 2.65        |
|    value_loss           | 99.8        |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=-112.19 +/- 44.94
Episode length: 428.00 +/- 71.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.010703154 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.49       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 75.8        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00345    |
|    std                  | 2.65        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=962000, episode_reward=-65.86 +/- 35.41
Episode length: 448.20 +/- 67.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | -65.9        |
| time/                   |              |
|    total_timesteps      | 962000       |
| train/                  |              |
|    approx_kl            | 0.0063028196 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 52.2         |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 2.66         |
|    value_loss           | 127          |
------------------------------------------
Eval num_timesteps=964000, episode_reward=-42.99 +/- 60.86
Episode length: 475.60 +/- 57.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | -43         |
| time/                   |             |
|    total_timesteps      | 964000      |
| train/                  |             |
|    approx_kl            | 0.004717756 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.52       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 46          |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00148    |
|    std                  | 2.67        |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=966000, episode_reward=-64.14 +/- 33.69
Episode length: 457.20 +/- 39.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | -64.1        |
| time/                   |              |
|    total_timesteps      | 966000       |
| train/                  |              |
|    approx_kl            | 0.0050114766 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 63.7         |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 2.67         |
|    value_loss           | 185          |
------------------------------------------
Eval num_timesteps=968000, episode_reward=42.35 +/- 139.12
Episode length: 481.00 +/- 103.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 42.4         |
| time/                   |              |
|    total_timesteps      | 968000       |
| train/                  |              |
|    approx_kl            | 0.0077830874 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 38.9         |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00419     |
|    std                  | 2.67         |
|    value_loss           | 84.8         |
------------------------------------------
Eval num_timesteps=970000, episode_reward=-43.84 +/- 70.45
Episode length: 416.60 +/- 63.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 417        |
|    mean_reward          | -43.8      |
| time/                   |            |
|    total_timesteps      | 970000     |
| train/                  |            |
|    approx_kl            | 0.00967334 |
|    clip_fraction        | 0.0796     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.53      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.001      |
|    loss                 | 35         |
|    n_updates            | 4730       |
|    policy_gradient_loss | -0.00491   |
|    std                  | 2.66       |
|    value_loss           | 97.9       |
----------------------------------------
Eval num_timesteps=972000, episode_reward=-35.97 +/- 33.13
Episode length: 442.00 +/- 32.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | -36         |
| time/                   |             |
|    total_timesteps      | 972000      |
| train/                  |             |
|    approx_kl            | 0.013831923 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.53       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 44.7        |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.67        |
|    value_loss           | 113         |
-----------------------------------------
Eval num_timesteps=974000, episode_reward=218.11 +/- 240.61
Episode length: 510.20 +/- 16.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 974000      |
| train/                  |             |
|    approx_kl            | 0.008835798 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.55       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 47.9        |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 2.67        |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=976000, episode_reward=146.19 +/- 123.32
Episode length: 525.60 +/- 16.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 976000       |
| train/                  |              |
|    approx_kl            | 0.0075565996 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.55        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 66.8         |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 2.68         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=978000, episode_reward=124.81 +/- 115.64
Episode length: 504.00 +/- 15.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 978000       |
| train/                  |              |
|    approx_kl            | 0.0054656356 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.56        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 61.4         |
|    n_updates            | 4770         |
|    policy_gradient_loss | -3.79e-05    |
|    std                  | 2.68         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=980000, episode_reward=212.99 +/- 182.36
Episode length: 486.60 +/- 62.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 213           |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 0.00076838856 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.56         |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.001         |
|    loss                 | 39.9          |
|    n_updates            | 4780          |
|    policy_gradient_loss | -0.00061      |
|    std                  | 2.68          |
|    value_loss           | 102           |
-------------------------------------------
Eval num_timesteps=982000, episode_reward=72.05 +/- 94.62
Episode length: 476.40 +/- 53.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 72           |
| time/                   |              |
|    total_timesteps      | 982000       |
| train/                  |              |
|    approx_kl            | 0.0020367599 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.57        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 79.3         |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.68         |
|    value_loss           | 277          |
------------------------------------------
Eval num_timesteps=984000, episode_reward=19.82 +/- 50.63
Episode length: 510.00 +/- 24.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 19.8        |
| time/                   |             |
|    total_timesteps      | 984000      |
| train/                  |             |
|    approx_kl            | 0.004063602 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.57       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 94.1        |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.000969   |
|    std                  | 2.69        |
|    value_loss           | 327         |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=62.43 +/- 158.17
Episode length: 462.80 +/- 74.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 62.4         |
| time/                   |              |
|    total_timesteps      | 986000       |
| train/                  |              |
|    approx_kl            | 0.0020295247 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 46           |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.69         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=988000, episode_reward=116.41 +/- 109.79
Episode length: 490.80 +/- 60.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 116         |
| time/                   |             |
|    total_timesteps      | 988000      |
| train/                  |             |
|    approx_kl            | 0.008343512 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 62.5        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 2.7         |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=373.69 +/- 544.73
Episode length: 488.80 +/- 74.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 0.0005477442 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.59        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 90           |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000151    |
|    std                  | 2.7          |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=992000, episode_reward=192.67 +/- 85.82
Episode length: 528.60 +/- 17.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 529         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 992000      |
| train/                  |             |
|    approx_kl            | 0.008195194 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 35.5        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00282    |
|    std                  | 2.7         |
|    value_loss           | 93.8        |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=568.96 +/- 429.62
Episode length: 530.40 +/- 18.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 530         |
|    mean_reward          | 569         |
| time/                   |             |
|    total_timesteps      | 994000      |
| train/                  |             |
|    approx_kl            | 0.002276261 |
|    clip_fraction        | 0.004       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 37          |
|    n_updates            | 4850        |
|    policy_gradient_loss | -2.4e-05    |
|    std                  | 2.7         |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=315.77 +/- 145.93
Episode length: 514.80 +/- 37.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 996000      |
| train/                  |             |
|    approx_kl            | 0.010445779 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 58.8        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 2.71        |
|    value_loss           | 196         |
-----------------------------------------
Eval num_timesteps=998000, episode_reward=254.40 +/- 225.35
Episode length: 437.20 +/- 80.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 998000      |
| train/                  |             |
|    approx_kl            | 0.004462145 |
|    clip_fraction        | 0.00659     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.6        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 88.7        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 2.71        |
|    value_loss           | 328         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=285.88 +/- 175.74
Episode length: 417.00 +/- 33.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0023715359 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+03      |
|    n_updates            | 4880         |
|    policy_gradient_loss | 0.000203     |
|    std                  | 2.71         |
|    value_loss           | 8.24e+03     |
------------------------------------------
Eval num_timesteps=1002000, episode_reward=30.28 +/- 174.44
Episode length: 440.40 +/- 54.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 30.3         |
| time/                   |              |
|    total_timesteps      | 1002000      |
| train/                  |              |
|    approx_kl            | 0.0019197343 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 84.8         |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 2.71         |
|    value_loss           | 293          |
------------------------------------------
Eval num_timesteps=1004000, episode_reward=466.09 +/- 373.06
Episode length: 423.80 +/- 62.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 1004000      |
| train/                  |              |
|    approx_kl            | 0.0062180934 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 64.8         |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 2.72         |
|    value_loss           | 189          |
------------------------------------------
Eval num_timesteps=1006000, episode_reward=356.96 +/- 260.54
Episode length: 421.60 +/- 28.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 357         |
| time/                   |             |
|    total_timesteps      | 1006000     |
| train/                  |             |
|    approx_kl            | 0.009460546 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.62       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 430         |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 2.72        |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=1008000, episode_reward=194.33 +/- 112.72
Episode length: 409.00 +/- 52.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 1008000      |
| train/                  |              |
|    approx_kl            | 0.0053957067 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.001        |
|    loss                 | 5.33e+03     |
|    n_updates            | 4920         |
|    policy_gradient_loss | 0.00158      |
|    std                  | 2.72         |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=1010000, episode_reward=216.22 +/- 147.48
Episode length: 410.20 +/- 27.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 0.0006057468 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+03     |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.000232    |
|    std                  | 2.72         |
|    value_loss           | 5.42e+03     |
------------------------------------------
Eval num_timesteps=1012000, episode_reward=367.28 +/- 203.21
Episode length: 391.40 +/- 34.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 1012000      |
| train/                  |              |
|    approx_kl            | 6.491263e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 3.37e+03     |
|    n_updates            | 4940         |
|    policy_gradient_loss | -1.73e-05    |
|    std                  | 2.72         |
|    value_loss           | 7.74e+03     |
------------------------------------------
Eval num_timesteps=1014000, episode_reward=118.57 +/- 110.24
Episode length: 365.20 +/- 24.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 1014000      |
| train/                  |              |
|    approx_kl            | 0.0044367155 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 142          |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 2.72         |
|    value_loss           | 663          |
------------------------------------------
Eval num_timesteps=1016000, episode_reward=393.60 +/- 284.85
Episode length: 413.80 +/- 25.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 394         |
| time/                   |             |
|    total_timesteps      | 1016000     |
| train/                  |             |
|    approx_kl            | 0.010004212 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 63.5        |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00366    |
|    std                  | 2.72        |
|    value_loss           | 241         |
-----------------------------------------
Eval num_timesteps=1018000, episode_reward=396.11 +/- 214.96
Episode length: 414.00 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 0.0026761927 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 1.76e+03     |
|    n_updates            | 4970         |
|    policy_gradient_loss | -7.8e-05     |
|    std                  | 2.71         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=586.17 +/- 234.31
Episode length: 419.80 +/- 72.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 586          |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0007461169 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.736        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+03     |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 2.71         |
|    value_loss           | 7.49e+03     |
------------------------------------------
Eval num_timesteps=1022000, episode_reward=265.50 +/- 318.83
Episode length: 411.60 +/- 43.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 1022000      |
| train/                  |              |
|    approx_kl            | 0.0001196003 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 2.54e+03     |
|    n_updates            | 4990         |
|    policy_gradient_loss | 0.000293     |
|    std                  | 2.71         |
|    value_loss           | 5.24e+03     |
------------------------------------------
Eval num_timesteps=1024000, episode_reward=229.72 +/- 317.14
Episode length: 408.80 +/- 83.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 409      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1026000, episode_reward=386.45 +/- 94.02
Episode length: 386.80 +/- 17.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 1026000     |
| train/                  |             |
|    approx_kl            | 3.62813e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.6        |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.001       |
|    loss                 | 5.09e+03    |
|    n_updates            | 5000        |
|    policy_gradient_loss | -7.6e-05    |
|    std                  | 2.71        |
|    value_loss           | 1.05e+04    |
-----------------------------------------
Eval num_timesteps=1028000, episode_reward=571.86 +/- 328.49
Episode length: 418.00 +/- 26.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 572          |
| time/                   |              |
|    total_timesteps      | 1028000      |
| train/                  |              |
|    approx_kl            | 7.111393e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.000401    |
|    std                  | 2.72         |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=1030000, episode_reward=534.60 +/- 450.54
Episode length: 454.40 +/- 52.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 535           |
| time/                   |               |
|    total_timesteps      | 1030000       |
| train/                  |               |
|    approx_kl            | 0.00031462757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.6          |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 136           |
|    n_updates            | 5020          |
|    policy_gradient_loss | -0.000103     |
|    std                  | 2.72          |
|    value_loss           | 610           |
-------------------------------------------
Eval num_timesteps=1032000, episode_reward=170.36 +/- 254.18
Episode length: 404.80 +/- 23.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 1032000      |
| train/                  |              |
|    approx_kl            | 0.0001792731 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 5.79e+03     |
|    n_updates            | 5030         |
|    policy_gradient_loss | 0.000314     |
|    std                  | 2.71         |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=1034000, episode_reward=591.69 +/- 441.40
Episode length: 439.40 +/- 59.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 592          |
| time/                   |              |
|    total_timesteps      | 1034000      |
| train/                  |              |
|    approx_kl            | 0.0002455939 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 249          |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.000271    |
|    std                  | 2.72         |
|    value_loss           | 768          |
------------------------------------------
Eval num_timesteps=1036000, episode_reward=481.57 +/- 299.38
Episode length: 416.00 +/- 44.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 1036000      |
| train/                  |              |
|    approx_kl            | 0.0005557415 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.001        |
|    loss                 | 2.26e+03     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 2.72         |
|    value_loss           | 4.9e+03      |
------------------------------------------
Eval num_timesteps=1038000, episode_reward=511.89 +/- 248.16
Episode length: 393.20 +/- 30.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | 512         |
| time/                   |             |
|    total_timesteps      | 1038000     |
| train/                  |             |
|    approx_kl            | 0.000365106 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.001       |
|    loss                 | 3.75e+03    |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.000417   |
|    std                  | 2.72        |
|    value_loss           | 7.74e+03    |
-----------------------------------------
Eval num_timesteps=1040000, episode_reward=430.48 +/- 309.02
Episode length: 415.60 +/- 29.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 1040000       |
| train/                  |               |
|    approx_kl            | 0.00081597304 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.61         |
|    explained_variance   | 0.709         |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+03      |
|    n_updates            | 5070          |
|    policy_gradient_loss | -0.00118      |
|    std                  | 2.72          |
|    value_loss           | 9.77e+03      |
-------------------------------------------
Eval num_timesteps=1042000, episode_reward=646.47 +/- 211.96
Episode length: 397.20 +/- 39.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 646          |
| time/                   |              |
|    total_timesteps      | 1042000      |
| train/                  |              |
|    approx_kl            | 0.0005203659 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+03     |
|    n_updates            | 5080         |
|    policy_gradient_loss | -8.83e-05    |
|    std                  | 2.72         |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=1044000, episode_reward=497.64 +/- 373.01
Episode length: 416.60 +/- 50.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 498           |
| time/                   |               |
|    total_timesteps      | 1044000       |
| train/                  |               |
|    approx_kl            | 0.00010497696 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.61         |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 111           |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000116     |
|    std                  | 2.72          |
|    value_loss           | 326           |
-------------------------------------------
Eval num_timesteps=1046000, episode_reward=479.97 +/- 282.88
Episode length: 392.00 +/- 29.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 480         |
| time/                   |             |
|    total_timesteps      | 1046000     |
| train/                  |             |
|    approx_kl            | 0.007131762 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.001       |
|    loss                 | 220         |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00204    |
|    std                  | 2.72        |
|    value_loss           | 881         |
-----------------------------------------
Eval num_timesteps=1048000, episode_reward=180.78 +/- 279.30
Episode length: 354.60 +/- 45.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 1048000     |
| train/                  |             |
|    approx_kl            | 0.005217372 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 164         |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00124    |
|    std                  | 2.72        |
|    value_loss           | 527         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=522.87 +/- 168.06
Episode length: 373.20 +/- 28.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 523          |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0007183843 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.62        |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 5120         |
|    policy_gradient_loss | 0.000261     |
|    std                  | 2.73         |
|    value_loss           | 5.63e+03     |
------------------------------------------
Eval num_timesteps=1052000, episode_reward=279.34 +/- 193.24
Episode length: 338.60 +/- 40.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 339           |
|    mean_reward          | 279           |
| time/                   |               |
|    total_timesteps      | 1052000       |
| train/                  |               |
|    approx_kl            | 0.00071063853 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.62         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 104           |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 2.73          |
|    value_loss           | 319           |
-------------------------------------------
Eval num_timesteps=1054000, episode_reward=555.67 +/- 204.61
Episode length: 377.60 +/- 10.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 1054000      |
| train/                  |              |
|    approx_kl            | 0.0046258816 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.63        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2.74         |
|    value_loss           | 233          |
------------------------------------------
Eval num_timesteps=1056000, episode_reward=398.39 +/- 173.66
Episode length: 381.60 +/- 28.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 398          |
| time/                   |              |
|    total_timesteps      | 1056000      |
| train/                  |              |
|    approx_kl            | 0.0065069287 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 2.73e+03     |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.000333    |
|    std                  | 2.75         |
|    value_loss           | 6.45e+03     |
------------------------------------------
Eval num_timesteps=1058000, episode_reward=399.29 +/- 332.48
Episode length: 384.80 +/- 27.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 385           |
|    mean_reward          | 399           |
| time/                   |               |
|    total_timesteps      | 1058000       |
| train/                  |               |
|    approx_kl            | 0.00074662676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.683         |
|    learning_rate        | 0.001         |
|    loss                 | 5.54e+03      |
|    n_updates            | 5160          |
|    policy_gradient_loss | 0.000121      |
|    std                  | 2.75          |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=1060000, episode_reward=467.99 +/- 417.43
Episode length: 436.00 +/- 58.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 468           |
| time/                   |               |
|    total_timesteps      | 1060000       |
| train/                  |               |
|    approx_kl            | 6.3854124e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.762         |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+03      |
|    n_updates            | 5170          |
|    policy_gradient_loss | -3.48e-05     |
|    std                  | 2.76          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1062000, episode_reward=612.89 +/- 472.73
Episode length: 454.60 +/- 40.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 613          |
| time/                   |              |
|    total_timesteps      | 1062000      |
| train/                  |              |
|    approx_kl            | 8.246969e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.66        |
|    explained_variance   | 0.695        |
|    learning_rate        | 0.001        |
|    loss                 | 5.41e+03     |
|    n_updates            | 5180         |
|    policy_gradient_loss | -4.97e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=1064000, episode_reward=137.32 +/- 248.48
Episode length: 440.60 +/- 28.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 1064000      |
| train/                  |              |
|    approx_kl            | 4.991045e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.66        |
|    explained_variance   | 0.717        |
|    learning_rate        | 0.001        |
|    loss                 | 5.63e+03     |
|    n_updates            | 5190         |
|    policy_gradient_loss | -8.29e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=1066000, episode_reward=342.28 +/- 303.73
Episode length: 407.20 +/- 38.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 342           |
| time/                   |               |
|    total_timesteps      | 1066000       |
| train/                  |               |
|    approx_kl            | 0.00017830715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.736         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 5200          |
|    policy_gradient_loss | -0.000269     |
|    std                  | 2.76          |
|    value_loss           | 8.42e+03      |
-------------------------------------------
Eval num_timesteps=1068000, episode_reward=502.75 +/- 413.70
Episode length: 449.80 +/- 45.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 1068000      |
| train/                  |              |
|    approx_kl            | 7.995157e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 5.34e+03     |
|    n_updates            | 5210         |
|    policy_gradient_loss | -5.52e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=1070000, episode_reward=-176.45 +/- 213.88
Episode length: 469.60 +/- 19.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | -176          |
| time/                   |               |
|    total_timesteps      | 1070000       |
| train/                  |               |
|    approx_kl            | 0.00012068558 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.773         |
|    learning_rate        | 0.001         |
|    loss                 | 5.98e+03      |
|    n_updates            | 5220          |
|    policy_gradient_loss | -0.000276     |
|    std                  | 2.76          |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=1072000, episode_reward=218.71 +/- 304.61
Episode length: 428.80 +/- 45.94
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 429            |
|    mean_reward          | 219            |
| time/                   |                |
|    total_timesteps      | 1072000        |
| train/                  |                |
|    approx_kl            | 0.000110044755 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.67          |
|    explained_variance   | 0.743          |
|    learning_rate        | 0.001          |
|    loss                 | 6.29e+03       |
|    n_updates            | 5230           |
|    policy_gradient_loss | -2.36e-05      |
|    std                  | 2.76           |
|    value_loss           | 1.33e+04       |
--------------------------------------------
Eval num_timesteps=1074000, episode_reward=455.71 +/- 342.75
Episode length: 466.00 +/- 41.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 456           |
| time/                   |               |
|    total_timesteps      | 1074000       |
| train/                  |               |
|    approx_kl            | 0.00010889501 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 204           |
|    n_updates            | 5240          |
|    policy_gradient_loss | -0.000221     |
|    std                  | 2.76          |
|    value_loss           | 584           |
-------------------------------------------
Eval num_timesteps=1076000, episode_reward=45.01 +/- 236.16
Episode length: 429.00 +/- 80.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 45            |
| time/                   |               |
|    total_timesteps      | 1076000       |
| train/                  |               |
|    approx_kl            | 0.00075032533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.783         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 5250          |
|    policy_gradient_loss | -0.00105      |
|    std                  | 2.76          |
|    value_loss           | 6.29e+03      |
-------------------------------------------
Eval num_timesteps=1078000, episode_reward=17.89 +/- 266.54
Episode length: 406.40 +/- 40.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 1078000      |
| train/                  |              |
|    approx_kl            | 0.0015863214 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 173          |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000336    |
|    std                  | 2.76         |
|    value_loss           | 781          |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=235.17 +/- 410.96
Episode length: 406.00 +/- 31.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 0.0002659746 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 5270         |
|    policy_gradient_loss | 6.75e-05     |
|    std                  | 2.76         |
|    value_loss           | 508          |
------------------------------------------
Eval num_timesteps=1082000, episode_reward=178.22 +/- 270.82
Episode length: 402.60 +/- 61.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 403           |
|    mean_reward          | 178           |
| time/                   |               |
|    total_timesteps      | 1082000       |
| train/                  |               |
|    approx_kl            | 0.00035865913 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.822         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+03      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000597     |
|    std                  | 2.76          |
|    value_loss           | 3.72e+03      |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=227.90 +/- 354.56
Episode length: 391.80 +/- 36.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 1084000      |
| train/                  |              |
|    approx_kl            | 0.0004800302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+03     |
|    n_updates            | 5290         |
|    policy_gradient_loss | -6.09e-05    |
|    std                  | 2.76         |
|    value_loss           | 5.2e+03      |
------------------------------------------
Eval num_timesteps=1086000, episode_reward=53.41 +/- 366.31
Episode length: 431.20 +/- 23.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 53.4          |
| time/                   |               |
|    total_timesteps      | 1086000       |
| train/                  |               |
|    approx_kl            | 0.00018707328 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.792         |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+03      |
|    n_updates            | 5300          |
|    policy_gradient_loss | 4.55e-05      |
|    std                  | 2.76          |
|    value_loss           | 6.31e+03      |
-------------------------------------------
Eval num_timesteps=1088000, episode_reward=376.07 +/- 388.60
Episode length: 419.60 +/- 38.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 1088000      |
| train/                  |              |
|    approx_kl            | 0.0014932523 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 533          |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.76         |
|    value_loss           | 1.61e+03     |
------------------------------------------
Eval num_timesteps=1090000, episode_reward=323.82 +/- 340.07
Episode length: 496.20 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 324          |
| time/                   |              |
|    total_timesteps      | 1090000      |
| train/                  |              |
|    approx_kl            | 0.0023664564 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.000747    |
|    std                  | 2.76         |
|    value_loss           | 4.57e+03     |
------------------------------------------
Eval num_timesteps=1092000, episode_reward=358.67 +/- 285.66
Episode length: 480.60 +/- 45.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 1092000       |
| train/                  |               |
|    approx_kl            | 0.00074998365 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.607         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+04      |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.00126      |
|    std                  | 2.76          |
|    value_loss           | 2.49e+04      |
-------------------------------------------
Eval num_timesteps=1094000, episode_reward=61.90 +/- 506.62
Episode length: 456.60 +/- 22.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 61.9          |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 0.00019555789 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.779         |
|    learning_rate        | 0.001         |
|    loss                 | 4.74e+03      |
|    n_updates            | 5340          |
|    policy_gradient_loss | 2.58e-05      |
|    std                  | 2.76          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1096000, episode_reward=431.41 +/- 280.88
Episode length: 494.40 +/- 49.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 494           |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 1096000       |
| train/                  |               |
|    approx_kl            | 2.7150178e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+03      |
|    n_updates            | 5350          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.76          |
|    value_loss           | 6.64e+03      |
-------------------------------------------
Eval num_timesteps=1098000, episode_reward=580.09 +/- 298.40
Episode length: 496.40 +/- 31.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 580          |
| time/                   |              |
|    total_timesteps      | 1098000      |
| train/                  |              |
|    approx_kl            | 0.0001429372 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+03     |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.000446    |
|    std                  | 2.77         |
|    value_loss           | 6.37e+03     |
------------------------------------------
Eval num_timesteps=1100000, episode_reward=250.32 +/- 361.19
Episode length: 444.40 +/- 57.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 250           |
| time/                   |               |
|    total_timesteps      | 1100000       |
| train/                  |               |
|    approx_kl            | 0.00023322768 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.81          |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+03      |
|    n_updates            | 5370          |
|    policy_gradient_loss | -0.000443     |
|    std                  | 2.77          |
|    value_loss           | 4.37e+03      |
-------------------------------------------
Eval num_timesteps=1102000, episode_reward=437.11 +/- 256.20
Episode length: 458.60 +/- 39.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 437           |
| time/                   |               |
|    total_timesteps      | 1102000       |
| train/                  |               |
|    approx_kl            | 0.00074634934 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.001         |
|    loss                 | 2.6e+03       |
|    n_updates            | 5380          |
|    policy_gradient_loss | -0.000946     |
|    std                  | 2.77          |
|    value_loss           | 6.01e+03      |
-------------------------------------------
Eval num_timesteps=1104000, episode_reward=-85.59 +/- 224.86
Episode length: 459.00 +/- 63.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | -85.6         |
| time/                   |               |
|    total_timesteps      | 1104000       |
| train/                  |               |
|    approx_kl            | 0.00072457804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.752         |
|    learning_rate        | 0.001         |
|    loss                 | 2.97e+03      |
|    n_updates            | 5390          |
|    policy_gradient_loss | -1.93e-05     |
|    std                  | 2.77          |
|    value_loss           | 6.96e+03      |
-------------------------------------------
Eval num_timesteps=1106000, episode_reward=351.74 +/- 200.60
Episode length: 482.60 +/- 51.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 352           |
| time/                   |               |
|    total_timesteps      | 1106000       |
| train/                  |               |
|    approx_kl            | 0.00022668517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 5400          |
|    policy_gradient_loss | 0.00011       |
|    std                  | 2.77          |
|    value_loss           | 4.5e+03       |
-------------------------------------------
Eval num_timesteps=1108000, episode_reward=81.45 +/- 398.05
Episode length: 482.20 +/- 48.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 81.5         |
| time/                   |              |
|    total_timesteps      | 1108000      |
| train/                  |              |
|    approx_kl            | 2.121643e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.745        |
|    learning_rate        | 0.001        |
|    loss                 | 3.14e+03     |
|    n_updates            | 5410         |
|    policy_gradient_loss | -1.64e-05    |
|    std                  | 2.77         |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=1110000, episode_reward=228.15 +/- 353.92
Episode length: 458.80 +/- 32.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
Eval num_timesteps=1112000, episode_reward=140.71 +/- 276.34
Episode length: 489.60 +/- 26.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | 141           |
| time/                   |               |
|    total_timesteps      | 1112000       |
| train/                  |               |
|    approx_kl            | 0.00010469614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.742         |
|    learning_rate        | 0.001         |
|    loss                 | 6.66e+03      |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000295     |
|    std                  | 2.77          |
|    value_loss           | 1.47e+04      |
-------------------------------------------
Eval num_timesteps=1114000, episode_reward=283.74 +/- 388.57
Episode length: 496.00 +/- 36.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 1114000      |
| train/                  |              |
|    approx_kl            | 0.0001864188 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+03     |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.000348    |
|    std                  | 2.77         |
|    value_loss           | 4.29e+03     |
------------------------------------------
Eval num_timesteps=1116000, episode_reward=472.11 +/- 237.07
Episode length: 440.40 +/- 65.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 472          |
| time/                   |              |
|    total_timesteps      | 1116000      |
| train/                  |              |
|    approx_kl            | 0.0007480604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.000493    |
|    std                  | 2.77         |
|    value_loss           | 666          |
------------------------------------------
Eval num_timesteps=1118000, episode_reward=271.27 +/- 116.44
Episode length: 438.60 +/- 30.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 271           |
| time/                   |               |
|    total_timesteps      | 1118000       |
| train/                  |               |
|    approx_kl            | 0.00042650214 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 266           |
|    n_updates            | 5450          |
|    policy_gradient_loss | -8.1e-05      |
|    std                  | 2.77          |
|    value_loss           | 857           |
-------------------------------------------
Eval num_timesteps=1120000, episode_reward=36.43 +/- 98.49
Episode length: 458.40 +/- 58.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 36.4          |
| time/                   |               |
|    total_timesteps      | 1120000       |
| train/                  |               |
|    approx_kl            | 0.00064291456 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.791         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 5460          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 2.77          |
|    value_loss           | 5.97e+03      |
-------------------------------------------
Eval num_timesteps=1122000, episode_reward=299.00 +/- 230.80
Episode length: 505.80 +/- 46.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 299          |
| time/                   |              |
|    total_timesteps      | 1122000      |
| train/                  |              |
|    approx_kl            | 0.0014300132 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 309          |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.001       |
|    std                  | 2.77         |
|    value_loss           | 905          |
------------------------------------------
Eval num_timesteps=1124000, episode_reward=678.54 +/- 497.30
Episode length: 518.00 +/- 20.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | 679          |
| time/                   |              |
|    total_timesteps      | 1124000      |
| train/                  |              |
|    approx_kl            | 0.0026846044 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.69        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 62           |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.000948    |
|    std                  | 2.78         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=1126000, episode_reward=181.02 +/- 133.03
Episode length: 473.80 +/- 68.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 1126000      |
| train/                  |              |
|    approx_kl            | 0.0038319496 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.7         |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 52.3         |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.000817    |
|    std                  | 2.79         |
|    value_loss           | 163          |
------------------------------------------
Eval num_timesteps=1128000, episode_reward=107.74 +/- 104.94
Episode length: 446.80 +/- 45.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 108          |
| time/                   |              |
|    total_timesteps      | 1128000      |
| train/                  |              |
|    approx_kl            | 0.0059802076 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 2.79         |
|    value_loss           | 402          |
------------------------------------------
Eval num_timesteps=1130000, episode_reward=308.40 +/- 217.68
Episode length: 419.60 +/- 41.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 1130000      |
| train/                  |              |
|    approx_kl            | 0.0027752027 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 61.8         |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00068     |
|    std                  | 2.79         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=1132000, episode_reward=216.72 +/- 321.12
Episode length: 466.60 +/- 47.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | 217          |
| time/                   |              |
|    total_timesteps      | 1132000      |
| train/                  |              |
|    approx_kl            | 0.0067759603 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 41           |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.79         |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=1134000, episode_reward=179.50 +/- 122.50
Episode length: 462.40 +/- 59.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 1134000      |
| train/                  |              |
|    approx_kl            | 0.0020540007 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 46.2         |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.79         |
|    value_loss           | 152          |
------------------------------------------
Eval num_timesteps=1136000, episode_reward=297.17 +/- 255.83
Episode length: 475.40 +/- 29.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 1136000     |
| train/                  |             |
|    approx_kl            | 0.007625806 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.71       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 54.9        |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.79        |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=1138000, episode_reward=700.49 +/- 314.62
Episode length: 495.20 +/- 26.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 700          |
| time/                   |              |
|    total_timesteps      | 1138000      |
| train/                  |              |
|    approx_kl            | 0.0027589605 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.72        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 28.8         |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000789    |
|    std                  | 2.8          |
|    value_loss           | 99.3         |
------------------------------------------
Eval num_timesteps=1140000, episode_reward=314.55 +/- 174.37
Episode length: 473.40 +/- 48.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0016390863 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.73        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+03     |
|    n_updates            | 5560         |
|    policy_gradient_loss | -9.11e-06    |
|    std                  | 2.8          |
|    value_loss           | 4.47e+03     |
------------------------------------------
Eval num_timesteps=1142000, episode_reward=74.99 +/- 213.53
Episode length: 490.60 +/- 33.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 75          |
| time/                   |             |
|    total_timesteps      | 1142000     |
| train/                  |             |
|    approx_kl            | 0.000376558 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.001       |
|    loss                 | 2.31e+03    |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.000386   |
|    std                  | 2.8         |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=1144000, episode_reward=233.18 +/- 185.53
Episode length: 516.60 +/- 20.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 1144000     |
| train/                  |             |
|    approx_kl            | 0.011782505 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 30.2        |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 2.8         |
|    value_loss           | 86.8        |
-----------------------------------------
Eval num_timesteps=1146000, episode_reward=278.35 +/- 282.76
Episode length: 480.60 +/- 22.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 481        |
|    mean_reward          | 278        |
| time/                   |            |
|    total_timesteps      | 1146000    |
| train/                  |            |
|    approx_kl            | 0.00396178 |
|    clip_fraction        | 0.00371    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.72      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.001      |
|    loss                 | 119        |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.00146   |
|    std                  | 2.79       |
|    value_loss           | 334        |
----------------------------------------
Eval num_timesteps=1148000, episode_reward=234.72 +/- 152.42
Episode length: 495.80 +/- 29.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1148000      |
| train/                  |              |
|    approx_kl            | 0.0069391737 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.72        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 53.6         |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 2.8          |
|    value_loss           | 188          |
------------------------------------------
Eval num_timesteps=1150000, episode_reward=169.52 +/- 114.84
Episode length: 468.60 +/- 63.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.007075223 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 38.7        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 2.8         |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=1152000, episode_reward=152.50 +/- 78.33
Episode length: 468.40 +/- 52.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 152          |
| time/                   |              |
|    total_timesteps      | 1152000      |
| train/                  |              |
|    approx_kl            | 0.0028471705 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 44.2         |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 2.8          |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=1154000, episode_reward=284.39 +/- 185.30
Episode length: 496.20 +/- 28.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 1154000      |
| train/                  |              |
|    approx_kl            | 0.0039370353 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 65.9         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.81         |
|    value_loss           | 203          |
------------------------------------------
Eval num_timesteps=1156000, episode_reward=429.50 +/- 203.78
Episode length: 502.80 +/- 36.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 429          |
| time/                   |              |
|    total_timesteps      | 1156000      |
| train/                  |              |
|    approx_kl            | 0.0042452635 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.75        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 93.1         |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.82         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=1158000, episode_reward=396.16 +/- 213.99
Episode length: 504.00 +/- 27.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 1158000     |
| train/                  |             |
|    approx_kl            | 0.004601136 |
|    clip_fraction        | 0.00908     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.76       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 76          |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 2.82        |
|    value_loss           | 220         |
-----------------------------------------
Eval num_timesteps=1160000, episode_reward=708.70 +/- 239.95
Episode length: 485.60 +/- 25.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 709         |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.004411298 |
|    clip_fraction        | 0.00513     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.77       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 2.83        |
|    value_loss           | 308         |
-----------------------------------------
Eval num_timesteps=1162000, episode_reward=782.54 +/- 134.34
Episode length: 478.20 +/- 33.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 1162000     |
| train/                  |             |
|    approx_kl            | 0.003906991 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.77       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 172         |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.000273   |
|    std                  | 2.83        |
|    value_loss           | 573         |
-----------------------------------------
Eval num_timesteps=1164000, episode_reward=467.13 +/- 431.48
Episode length: 470.40 +/- 46.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 467           |
| time/                   |               |
|    total_timesteps      | 1164000       |
| train/                  |               |
|    approx_kl            | 0.00046858503 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.77         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 196           |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.000292     |
|    std                  | 2.83          |
|    value_loss           | 619           |
-------------------------------------------
Eval num_timesteps=1166000, episode_reward=572.52 +/- 469.43
Episode length: 468.00 +/- 35.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 573          |
| time/                   |              |
|    total_timesteps      | 1166000      |
| train/                  |              |
|    approx_kl            | 0.0006105879 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.77        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.001        |
|    loss                 | 3.51e+03     |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.000134    |
|    std                  | 2.83         |
|    value_loss           | 8.35e+03     |
------------------------------------------
Eval num_timesteps=1168000, episode_reward=267.70 +/- 473.99
Episode length: 449.40 +/- 19.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 1168000      |
| train/                  |              |
|    approx_kl            | 0.0010448743 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 81.8         |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.000579    |
|    std                  | 2.84         |
|    value_loss           | 299          |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=393.35 +/- 462.19
Episode length: 466.40 +/- 42.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 393          |
| time/                   |              |
|    total_timesteps      | 1170000      |
| train/                  |              |
|    approx_kl            | 0.0005818042 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 243          |
|    n_updates            | 5710         |
|    policy_gradient_loss | 0.000201     |
|    std                  | 2.84         |
|    value_loss           | 839          |
------------------------------------------
Eval num_timesteps=1172000, episode_reward=142.02 +/- 410.53
Episode length: 495.00 +/- 32.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 142          |
| time/                   |              |
|    total_timesteps      | 1172000      |
| train/                  |              |
|    approx_kl            | 0.0020818184 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.723        |
|    learning_rate        | 0.001        |
|    loss                 | 3.4e+03      |
|    n_updates            | 5720         |
|    policy_gradient_loss | 0.000453     |
|    std                  | 2.84         |
|    value_loss           | 8.83e+03     |
------------------------------------------
Eval num_timesteps=1174000, episode_reward=107.82 +/- 422.63
Episode length: 484.60 +/- 49.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 108           |
| time/                   |               |
|    total_timesteps      | 1174000       |
| train/                  |               |
|    approx_kl            | 0.00063969404 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.704         |
|    learning_rate        | 0.001         |
|    loss                 | 5.45e+03      |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000696     |
|    std                  | 2.84          |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=1176000, episode_reward=229.22 +/- 312.32
Episode length: 486.00 +/- 25.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 229           |
| time/                   |               |
|    total_timesteps      | 1176000       |
| train/                  |               |
|    approx_kl            | 0.00015915645 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.791         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 5740          |
|    policy_gradient_loss | 0.000203      |
|    std                  | 2.84          |
|    value_loss           | 8.83e+03      |
-------------------------------------------
Eval num_timesteps=1178000, episode_reward=568.24 +/- 509.05
Episode length: 489.60 +/- 22.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | 568           |
| time/                   |               |
|    total_timesteps      | 1178000       |
| train/                  |               |
|    approx_kl            | 0.00040179677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 106           |
|    n_updates            | 5750          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 2.84          |
|    value_loss           | 352           |
-------------------------------------------
Eval num_timesteps=1180000, episode_reward=602.64 +/- 339.55
Episode length: 470.40 +/- 63.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 470         |
|    mean_reward          | 603         |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.004323437 |
|    clip_fraction        | 0.00513     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.79       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 128         |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 2.84        |
|    value_loss           | 531         |
-----------------------------------------
Eval num_timesteps=1182000, episode_reward=168.38 +/- 476.56
Episode length: 430.00 +/- 35.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 1182000      |
| train/                  |              |
|    approx_kl            | 0.0060846154 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.79        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.001        |
|    loss                 | 5.05e+03     |
|    n_updates            | 5770         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.85         |
|    value_loss           | 1.13e+04     |
------------------------------------------
Eval num_timesteps=1184000, episode_reward=115.91 +/- 346.42
Episode length: 438.20 +/- 48.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 116           |
| time/                   |               |
|    total_timesteps      | 1184000       |
| train/                  |               |
|    approx_kl            | 0.00095577084 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 335           |
|    n_updates            | 5780          |
|    policy_gradient_loss | -0.000143     |
|    std                  | 2.85          |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=1186000, episode_reward=200.97 +/- 320.53
Episode length: 474.00 +/- 35.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 1186000       |
| train/                  |               |
|    approx_kl            | 0.00014671634 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.8          |
|    explained_variance   | 0.691         |
|    learning_rate        | 0.001         |
|    loss                 | 8.11e+03      |
|    n_updates            | 5790          |
|    policy_gradient_loss | 0.000125      |
|    std                  | 2.85          |
|    value_loss           | 1.78e+04      |
-------------------------------------------
Eval num_timesteps=1188000, episode_reward=354.44 +/- 419.16
Episode length: 459.00 +/- 40.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 1188000     |
| train/                  |             |
|    approx_kl            | 0.000251792 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.8        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 216         |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.000172   |
|    std                  | 2.85        |
|    value_loss           | 802         |
-----------------------------------------
Eval num_timesteps=1190000, episode_reward=598.86 +/- 515.32
Episode length: 444.20 +/- 25.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 599           |
| time/                   |               |
|    total_timesteps      | 1190000       |
| train/                  |               |
|    approx_kl            | 0.00040137424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.8          |
|    explained_variance   | 0.748         |
|    learning_rate        | 0.001         |
|    loss                 | 3.12e+03      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.000115     |
|    std                  | 2.85          |
|    value_loss           | 7.45e+03      |
-------------------------------------------
Eval num_timesteps=1192000, episode_reward=267.60 +/- 397.41
Episode length: 464.20 +/- 15.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 1192000      |
| train/                  |              |
|    approx_kl            | 0.0001900449 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.001        |
|    loss                 | 2.59e+03     |
|    n_updates            | 5820         |
|    policy_gradient_loss | -0.000314    |
|    std                  | 2.85         |
|    value_loss           | 6.34e+03     |
------------------------------------------
Eval num_timesteps=1194000, episode_reward=202.86 +/- 520.63
Episode length: 456.40 +/- 35.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 1194000      |
| train/                  |              |
|    approx_kl            | 0.0014410439 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.000758    |
|    std                  | 2.86         |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=1196000, episode_reward=290.56 +/- 425.58
Episode length: 421.00 +/- 69.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 1196000  |
---------------------------------
Eval num_timesteps=1198000, episode_reward=132.22 +/- 364.70
Episode length: 400.00 +/- 24.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 132          |
| time/                   |              |
|    total_timesteps      | 1198000      |
| train/                  |              |
|    approx_kl            | 0.0032739239 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 39           |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.000728    |
|    std                  | 2.85         |
|    value_loss           | 131          |
------------------------------------------
Eval num_timesteps=1200000, episode_reward=89.06 +/- 367.29
Episode length: 409.20 +/- 12.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 89.1         |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0064218896 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 39.2         |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 2.85         |
|    value_loss           | 172          |
------------------------------------------
Eval num_timesteps=1202000, episode_reward=397.39 +/- 344.90
Episode length: 413.20 +/- 20.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 1202000     |
| train/                  |             |
|    approx_kl            | 0.009645417 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.81       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0021     |
|    std                  | 2.86        |
|    value_loss           | 478         |
-----------------------------------------
Eval num_timesteps=1204000, episode_reward=267.49 +/- 232.91
Episode length: 389.80 +/- 33.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 1204000      |
| train/                  |              |
|    approx_kl            | 0.0049791476 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 2.11e+03     |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.000484    |
|    std                  | 2.87         |
|    value_loss           | 4.76e+03     |
------------------------------------------
Eval num_timesteps=1206000, episode_reward=99.86 +/- 372.25
Episode length: 371.40 +/- 23.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 99.9        |
| time/                   |             |
|    total_timesteps      | 1206000     |
| train/                  |             |
|    approx_kl            | 0.010188799 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.82       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 38.6        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 2.88        |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=1208000, episode_reward=255.24 +/- 92.65
Episode length: 358.80 +/- 20.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 1208000     |
| train/                  |             |
|    approx_kl            | 0.005035518 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 49          |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.000701   |
|    std                  | 2.88        |
|    value_loss           | 204         |
-----------------------------------------
Eval num_timesteps=1210000, episode_reward=270.54 +/- 266.87
Episode length: 368.60 +/- 20.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 369         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 1210000     |
| train/                  |             |
|    approx_kl            | 0.008887691 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.84       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 31.3        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 2.88        |
|    value_loss           | 144         |
-----------------------------------------
Eval num_timesteps=1212000, episode_reward=337.96 +/- 321.53
Episode length: 369.20 +/- 17.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | 338          |
| time/                   |              |
|    total_timesteps      | 1212000      |
| train/                  |              |
|    approx_kl            | 0.0016845313 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.84        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 28.6         |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.000401    |
|    std                  | 2.88         |
|    value_loss           | 81.9         |
------------------------------------------
Eval num_timesteps=1214000, episode_reward=162.31 +/- 318.05
Episode length: 355.20 +/- 33.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 1214000     |
| train/                  |             |
|    approx_kl            | 0.004704209 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.001       |
|    loss                 | 3.37e+03    |
|    n_updates            | 5920        |
|    policy_gradient_loss | 0.000912    |
|    std                  | 2.88        |
|    value_loss           | 7.96e+03    |
-----------------------------------------
Eval num_timesteps=1216000, episode_reward=169.55 +/- 287.49
Episode length: 354.40 +/- 44.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 1216000      |
| train/                  |              |
|    approx_kl            | 0.0031114686 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 640          |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 2.88         |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=1218000, episode_reward=66.40 +/- 307.21
Episode length: 360.80 +/- 58.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 361          |
|    mean_reward          | 66.4         |
| time/                   |              |
|    total_timesteps      | 1218000      |
| train/                  |              |
|    approx_kl            | 0.0016789439 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 2.32e+03     |
|    n_updates            | 5940         |
|    policy_gradient_loss | 0.00113      |
|    std                  | 2.88         |
|    value_loss           | 5.37e+03     |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=309.41 +/- 84.20
Episode length: 340.40 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 1220000      |
| train/                  |              |
|    approx_kl            | 0.0025107893 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 88.5         |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000941    |
|    std                  | 2.87         |
|    value_loss           | 344          |
------------------------------------------
Eval num_timesteps=1222000, episode_reward=300.87 +/- 201.08
Episode length: 328.40 +/- 36.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 328          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 1222000      |
| train/                  |              |
|    approx_kl            | 0.0040261056 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00066     |
|    std                  | 2.87         |
|    value_loss           | 5.47e+03     |
------------------------------------------
Eval num_timesteps=1224000, episode_reward=200.67 +/- 119.09
Episode length: 349.60 +/- 21.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 1224000      |
| train/                  |              |
|    approx_kl            | 0.0028146124 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 81.2         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.87         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=1226000, episode_reward=211.95 +/- 232.73
Episode length: 359.20 +/- 20.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 1226000      |
| train/                  |              |
|    approx_kl            | 0.0012608338 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 51.8         |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.0005      |
|    std                  | 2.87         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=1228000, episode_reward=281.52 +/- 199.86
Episode length: 334.60 +/- 22.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 1228000     |
| train/                  |             |
|    approx_kl            | 0.002593807 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.001       |
|    loss                 | 1.48e+03    |
|    n_updates            | 5990        |
|    policy_gradient_loss | 0.000311    |
|    std                  | 2.88        |
|    value_loss           | 3.7e+03     |
-----------------------------------------
Eval num_timesteps=1230000, episode_reward=212.36 +/- 238.47
Episode length: 343.00 +/- 30.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 1230000      |
| train/                  |              |
|    approx_kl            | 0.0009122004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 104          |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.000598    |
|    std                  | 2.88         |
|    value_loss           | 383          |
------------------------------------------
Eval num_timesteps=1232000, episode_reward=177.74 +/- 110.12
Episode length: 319.80 +/- 20.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 178           |
| time/                   |               |
|    total_timesteps      | 1232000       |
| train/                  |               |
|    approx_kl            | 0.00072673906 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.84         |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.001         |
|    loss                 | 26.5          |
|    n_updates            | 6010          |
|    policy_gradient_loss | -0.000203     |
|    std                  | 2.89          |
|    value_loss           | 98.4          |
-------------------------------------------
Eval num_timesteps=1234000, episode_reward=267.43 +/- 233.69
Episode length: 347.20 +/- 21.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 1234000      |
| train/                  |              |
|    approx_kl            | 0.0066038594 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.85        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 27           |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.9          |
|    value_loss           | 137          |
------------------------------------------
Eval num_timesteps=1236000, episode_reward=188.10 +/- 65.98
Episode length: 335.40 +/- 6.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 335        |
|    mean_reward          | 188        |
| time/                   |            |
|    total_timesteps      | 1236000    |
| train/                  |            |
|    approx_kl            | 0.01004136 |
|    clip_fraction        | 0.0471     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.87      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.001      |
|    loss                 | 25.7       |
|    n_updates            | 6030       |
|    policy_gradient_loss | -0.00249   |
|    std                  | 2.91       |
|    value_loss           | 114        |
----------------------------------------
Eval num_timesteps=1238000, episode_reward=348.38 +/- 65.45
Episode length: 358.00 +/- 21.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | 348         |
| time/                   |             |
|    total_timesteps      | 1238000     |
| train/                  |             |
|    approx_kl            | 0.008648017 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.87       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 32.8        |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.00316    |
|    std                  | 2.91        |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=1240000, episode_reward=275.10 +/- 345.92
Episode length: 379.00 +/- 36.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 275          |
| time/                   |              |
|    total_timesteps      | 1240000      |
| train/                  |              |
|    approx_kl            | 0.0012206855 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.87        |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+03      |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 2.91         |
|    value_loss           | 5.74e+03     |
------------------------------------------
Eval num_timesteps=1242000, episode_reward=119.29 +/- 391.76
Episode length: 386.20 +/- 30.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 1242000       |
| train/                  |               |
|    approx_kl            | 0.00032932888 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.87         |
|    explained_variance   | 0.838         |
|    learning_rate        | 0.001         |
|    loss                 | 2.35e+03      |
|    n_updates            | 6060          |
|    policy_gradient_loss | 3.89e-05      |
|    std                  | 2.91          |
|    value_loss           | 5.04e+03      |
-------------------------------------------
Eval num_timesteps=1244000, episode_reward=296.36 +/- 329.68
Episode length: 393.60 +/- 12.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 296           |
| time/                   |               |
|    total_timesteps      | 1244000       |
| train/                  |               |
|    approx_kl            | 0.00011167521 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.87         |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+03      |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.91          |
|    value_loss           | 4.84e+03      |
-------------------------------------------
Eval num_timesteps=1246000, episode_reward=394.84 +/- 239.20
Episode length: 363.40 +/- 18.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 363          |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 1246000      |
| train/                  |              |
|    approx_kl            | 0.0002844049 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.87        |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.001        |
|    loss                 | 3.31e+03     |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.000471    |
|    std                  | 2.91         |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=1248000, episode_reward=474.09 +/- 111.91
Episode length: 381.20 +/- 13.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 474           |
| time/                   |               |
|    total_timesteps      | 1248000       |
| train/                  |               |
|    approx_kl            | 0.00030121216 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.88         |
|    explained_variance   | 0.788         |
|    learning_rate        | 0.001         |
|    loss                 | 3.53e+03      |
|    n_updates            | 6090          |
|    policy_gradient_loss | -1.05e-06     |
|    std                  | 2.91          |
|    value_loss           | 7.88e+03      |
-------------------------------------------
Eval num_timesteps=1250000, episode_reward=97.04 +/- 268.30
Episode length: 383.80 +/- 29.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | 97            |
| time/                   |               |
|    total_timesteps      | 1250000       |
| train/                  |               |
|    approx_kl            | 0.00017625684 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.88         |
|    explained_variance   | 0.862         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+03      |
|    n_updates            | 6100          |
|    policy_gradient_loss | -0.000509     |
|    std                  | 2.92          |
|    value_loss           | 4.79e+03      |
-------------------------------------------
Eval num_timesteps=1252000, episode_reward=238.54 +/- 89.02
Episode length: 357.00 +/- 20.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 1252000      |
| train/                  |              |
|    approx_kl            | 9.499525e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 3.33e+03     |
|    n_updates            | 6110         |
|    policy_gradient_loss | 1.67e-06     |
|    std                  | 2.92         |
|    value_loss           | 7.81e+03     |
------------------------------------------
Eval num_timesteps=1254000, episode_reward=77.65 +/- 266.53
Episode length: 380.40 +/- 43.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 77.7         |
| time/                   |              |
|    total_timesteps      | 1254000      |
| train/                  |              |
|    approx_kl            | 0.0034023037 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 48.9         |
|    n_updates            | 6120         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.92         |
|    value_loss           | 306          |
------------------------------------------
Eval num_timesteps=1256000, episode_reward=452.16 +/- 403.71
Episode length: 383.80 +/- 23.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 1256000     |
| train/                  |             |
|    approx_kl            | 0.004355624 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 1.58e+03    |
|    n_updates            | 6130        |
|    policy_gradient_loss | 0.0021      |
|    std                  | 2.92        |
|    value_loss           | 3.38e+03    |
-----------------------------------------
Eval num_timesteps=1258000, episode_reward=312.54 +/- 183.27
Episode length: 339.20 +/- 27.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 339         |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 1258000     |
| train/                  |             |
|    approx_kl            | 0.009572455 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 24.2        |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 2.92        |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=1260000, episode_reward=267.19 +/- 271.66
Episode length: 323.20 +/- 31.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.005161074 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 52.8        |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.000478   |
|    std                  | 2.92        |
|    value_loss           | 223         |
-----------------------------------------
Eval num_timesteps=1262000, episode_reward=348.92 +/- 224.98
Episode length: 343.80 +/- 12.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 349          |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0056347875 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 18           |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 2.92         |
|    value_loss           | 74.5         |
------------------------------------------
Eval num_timesteps=1264000, episode_reward=416.90 +/- 200.61
Episode length: 373.00 +/- 6.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 0.0031175036 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.9         |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 54.5         |
|    n_updates            | 6170         |
|    policy_gradient_loss | 0.000863     |
|    std                  | 2.93         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=1266000, episode_reward=403.82 +/- 313.70
Episode length: 381.40 +/- 28.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 404          |
| time/                   |              |
|    total_timesteps      | 1266000      |
| train/                  |              |
|    approx_kl            | 0.0032712636 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.00075     |
|    std                  | 2.94         |
|    value_loss           | 5.64e+03     |
------------------------------------------
Eval num_timesteps=1268000, episode_reward=353.51 +/- 178.69
Episode length: 393.20 +/- 29.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 1268000      |
| train/                  |              |
|    approx_kl            | 0.0006754121 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 3.73e+03     |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.00085     |
|    std                  | 2.94         |
|    value_loss           | 8.22e+03     |
------------------------------------------
Eval num_timesteps=1270000, episode_reward=316.52 +/- 102.04
Episode length: 408.00 +/- 45.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0048284065 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 57.6         |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.94         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=1272000, episode_reward=250.48 +/- 318.11
Episode length: 422.00 +/- 39.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 1272000     |
| train/                  |             |
|    approx_kl            | 0.002751325 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.91       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 2.33e+03    |
|    n_updates            | 6210        |
|    policy_gradient_loss | 0.000911    |
|    std                  | 2.93        |
|    value_loss           | 5.11e+03    |
-----------------------------------------
Eval num_timesteps=1274000, episode_reward=718.81 +/- 278.58
Episode length: 444.60 +/- 41.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 719          |
| time/                   |              |
|    total_timesteps      | 1274000      |
| train/                  |              |
|    approx_kl            | 0.0038598336 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.93         |
|    value_loss           | 1.02e+03     |
------------------------------------------
Eval num_timesteps=1276000, episode_reward=354.57 +/- 282.08
Episode length: 451.60 +/- 33.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 1276000      |
| train/                  |              |
|    approx_kl            | 0.0032524471 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 3.95e+03     |
|    n_updates            | 6230         |
|    policy_gradient_loss | -0.000639    |
|    std                  | 2.94         |
|    value_loss           | 9.31e+03     |
------------------------------------------
Eval num_timesteps=1278000, episode_reward=552.52 +/- 239.86
Episode length: 427.40 +/- 69.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 553          |
| time/                   |              |
|    total_timesteps      | 1278000      |
| train/                  |              |
|    approx_kl            | 0.0006188059 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 4.25e+03     |
|    n_updates            | 6240         |
|    policy_gradient_loss | -2.92e-05    |
|    std                  | 2.94         |
|    value_loss           | 9.3e+03      |
------------------------------------------
Eval num_timesteps=1280000, episode_reward=58.80 +/- 389.96
Episode length: 426.60 +/- 24.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 58.8     |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1282000, episode_reward=678.60 +/- 288.64
Episode length: 452.40 +/- 49.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 679           |
| time/                   |               |
|    total_timesteps      | 1282000       |
| train/                  |               |
|    approx_kl            | 0.00066540076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 110           |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 2.94          |
|    value_loss           | 548           |
-------------------------------------------
Eval num_timesteps=1284000, episode_reward=180.74 +/- 311.58
Episode length: 458.00 +/- 35.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 181           |
| time/                   |               |
|    total_timesteps      | 1284000       |
| train/                  |               |
|    approx_kl            | 0.00079743005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 6260          |
|    policy_gradient_loss | 3.08e-05      |
|    std                  | 2.94          |
|    value_loss           | 4.26e+03      |
-------------------------------------------
Eval num_timesteps=1286000, episode_reward=115.01 +/- 389.64
Episode length: 475.20 +/- 25.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 115           |
| time/                   |               |
|    total_timesteps      | 1286000       |
| train/                  |               |
|    approx_kl            | 0.00014405232 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.000106     |
|    std                  | 2.94          |
|    value_loss           | 3.71e+03      |
-------------------------------------------
Eval num_timesteps=1288000, episode_reward=738.23 +/- 394.28
Episode length: 502.40 +/- 16.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 502           |
|    mean_reward          | 738           |
| time/                   |               |
|    total_timesteps      | 1288000       |
| train/                  |               |
|    approx_kl            | 0.00011758492 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+03      |
|    n_updates            | 6280          |
|    policy_gradient_loss | -5.23e-05     |
|    std                  | 2.94          |
|    value_loss           | 8.27e+03      |
-------------------------------------------
Eval num_timesteps=1290000, episode_reward=18.65 +/- 317.48
Episode length: 447.00 +/- 19.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 1290000       |
| train/                  |               |
|    approx_kl            | 0.00030888632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 2.94          |
|    value_loss           | 3.03e+03      |
-------------------------------------------
Eval num_timesteps=1292000, episode_reward=292.13 +/- 575.86
Episode length: 490.40 +/- 28.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 1292000      |
| train/                  |              |
|    approx_kl            | 0.0045175934 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 231          |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 2.94         |
|    value_loss           | 584          |
------------------------------------------
Eval num_timesteps=1294000, episode_reward=280.73 +/- 267.57
Episode length: 464.40 +/- 28.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 281          |
| time/                   |              |
|    total_timesteps      | 1294000      |
| train/                  |              |
|    approx_kl            | 0.0023155375 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 1.86e+03     |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.000435    |
|    std                  | 2.94         |
|    value_loss           | 4.59e+03     |
------------------------------------------
Eval num_timesteps=1296000, episode_reward=378.59 +/- 138.66
Episode length: 438.40 +/- 27.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 379          |
| time/                   |              |
|    total_timesteps      | 1296000      |
| train/                  |              |
|    approx_kl            | 0.0006983265 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 3.06e+03     |
|    n_updates            | 6320         |
|    policy_gradient_loss | 1.52e-05     |
|    std                  | 2.94         |
|    value_loss           | 6.95e+03     |
------------------------------------------
Eval num_timesteps=1298000, episode_reward=310.41 +/- 199.46
Episode length: 444.80 +/- 61.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 445         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 1298000     |
| train/                  |             |
|    approx_kl            | 0.000713235 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.92       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 3.09e+03    |
|    n_updates            | 6330        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 2.94        |
|    value_loss           | 6.7e+03     |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=478.11 +/- 339.52
Episode length: 465.80 +/- 61.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0038021589 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 80.2         |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 2.95         |
|    value_loss           | 390          |
------------------------------------------
Eval num_timesteps=1302000, episode_reward=525.69 +/- 200.07
Episode length: 427.80 +/- 60.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 1302000      |
| train/                  |              |
|    approx_kl            | 0.0016892175 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 3.23e+03     |
|    n_updates            | 6350         |
|    policy_gradient_loss | 0.00104      |
|    std                  | 2.95         |
|    value_loss           | 7.77e+03     |
------------------------------------------
Eval num_timesteps=1304000, episode_reward=397.32 +/- 251.48
Episode length: 394.20 +/- 36.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 397           |
| time/                   |               |
|    total_timesteps      | 1304000       |
| train/                  |               |
|    approx_kl            | 0.00011641989 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 4.76e+03      |
|    n_updates            | 6360          |
|    policy_gradient_loss | -0.000138     |
|    std                  | 2.95          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1306000, episode_reward=405.90 +/- 585.16
Episode length: 421.00 +/- 49.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 1306000      |
| train/                  |              |
|    approx_kl            | 0.0001342746 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.93        |
|    explained_variance   | 0.781        |
|    learning_rate        | 0.001        |
|    loss                 | 4.77e+03     |
|    n_updates            | 6370         |
|    policy_gradient_loss | -0.000289    |
|    std                  | 2.95         |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=1308000, episode_reward=598.26 +/- 460.50
Episode length: 419.40 +/- 33.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 598           |
| time/                   |               |
|    total_timesteps      | 1308000       |
| train/                  |               |
|    approx_kl            | 0.00012002335 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.816         |
|    learning_rate        | 0.001         |
|    loss                 | 3.51e+03      |
|    n_updates            | 6380          |
|    policy_gradient_loss | 0.000218      |
|    std                  | 2.95          |
|    value_loss           | 8.57e+03      |
-------------------------------------------
Eval num_timesteps=1310000, episode_reward=139.05 +/- 462.93
Episode length: 428.00 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 428           |
|    mean_reward          | 139           |
| time/                   |               |
|    total_timesteps      | 1310000       |
| train/                  |               |
|    approx_kl            | 0.00024271192 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.817         |
|    learning_rate        | 0.001         |
|    loss                 | 5.91e+03      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000675     |
|    std                  | 2.95          |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 678, in <module>
    sim.run_full()
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 416, in run_full
    model.learn(total_timesteps=int(args.max_steps),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 178, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\policies.py", line 652, in forward
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\policies.py", line 692, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\distributions.py", line 164, in proba_distribution
    self.distribution = Normal(mean_actions, action_std)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\distributions\normal.py", line 56, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\distributions\distribution.py", line 67, in __init__
    if not valid.all():
KeyboardInterrupt