diff --git a/README.md b/README.md
index 6844189..63de2e7 100644
--- a/README.md
+++ b/README.md
@@ -13,8 +13,7 @@ $ pip install -r requirements.txt
 
 Example args:
 
-```azure
-your/python \RL\Sol\Model\pybullet_drone_simulator.py --agent PPO --run_type full --wandb
- False --savemodel False --max_steps 10000000
-
+```
+$ python RL\Sol\Model\pybullet_drone_simulator.py --agent PPO --run_type full --wandb
+ False --savemodel False --max_steps 10e6
 ```
diff --git a/Sol/Model/PBDroneEnv.py b/Sol/Model/PBDroneEnv.py
index 082d4f9..0315da7 100644
--- a/Sol/Model/PBDroneEnv.py
+++ b/Sol/Model/PBDroneEnv.py
@@ -71,6 +71,7 @@ class PBDroneEnv(
         self._steps = 0
         self.steps_since_last_target = 0
         self._last_action = np.zeros(4, dtype=np.float32)
+        self.eps = np.finfo(self._last_action.dtype).eps
         self._prev_distance_to_target = np.linalg.norm(self._current_position - target_points[0])
         self._current_target_index = 0
         self._is_done = False
@@ -96,7 +97,7 @@ class PBDroneEnv(
         self._steps += 1
         self._last_action = action
         self._last_position = copy.deepcopy(self._current_position)
-        self._current_position = np.array(self.pos[0], dtype=np.float32)
+        self._current_position = np.array(self.pos[0] + self.eps, dtype=np.float32) + self.eps
 
         return np.array(obs, dtype=np.float32), np.array(reward, dtype=np.float32), terminated, truncated, info
 
@@ -124,8 +125,8 @@ class PBDroneEnv(
         Kinematic observation of size 12.
 
         """
-
         obs = self._clipAndNormalizeState(self._getDroneStateVector(0))
+        obs = obs + self.eps
         ret = np.hstack([obs[0:3], obs[7:10], obs[10:13], obs[13:16]]).reshape(12, )
         try:
             return ret.astype('float32')
@@ -505,3 +506,4 @@ class PBDroneEnv(
         if len(self.target_visual) > 0:
             p.removeBody(self.target_visual[0])
             self.target_visual = self.target_visual[1:]
+
diff --git a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc
index a670134..e0269da 100644
Binary files a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc and b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc differ
diff --git a/Sol/Model/model_chkpts/save-12.28.2023_19.01.09/model.py b/Sol/Model/model_chkpts/save-12.28.2023_19.01.09/model.py
deleted file mode 100644
index 1de4ab2..0000000
--- a/Sol/Model/model_chkpts/save-12.28.2023_19.01.09/model.py
+++ /dev/null
@@ -1,549 +0,0 @@
-class PBDroneEnv(
-    # BaseAviary,
-    # FlyThruGateAviary,
-    BaseSingleAgentAviary,
-):
-
-    def __init__(self,
-                 target_points, threshold, discount, max_steps, aviary_dim,
-                 save_model=False, save_folder=None,
-                 drone_model: DroneModel = DroneModel.CF2X,
-                 initial_xyzs=None,
-                 initial_rpys=None,
-                 physics: Physics = Physics.PYB,
-                 pyb_freq: int = 240,
-                 ctrl_freq: int = 240,
-                 gui=False,
-                 record=False,
-                 obs: ObservationType = ObservationType.KIN,
-                 act: ActionType = ActionType.RPM,
-                 vision_attributes=False,
-                 obstacles=False,
-                 ):
-
-        self.ACT_TYPE = act
-        self.EPISODE_LEN_SEC = 5
-        self.OBS_TYPE = obs
-        self._OBS_TYPE = obs
-
-        self._target_points = np.array(target_points)
-        self._reached_targets = np.zeros(len(self._target_points), dtype=bool)
-        self._threshold = threshold
-        self._discount = discount
-        self._max_steps = max_steps
-        self._aviary_dim = aviary_dim
-        self._x_low, self._y_low, self._y_low, self._x_high, self._y_high, self._z_high = aviary_dim
-        print("AVIARY DIM", self._aviary_dim)
-
-        super().__init__(drone_model=drone_model,
-                         initial_xyzs=initial_xyzs,
-                         initial_rpys=initial_rpys,
-                         physics=physics,
-                         pyb_freq=pyb_freq,
-                         ctrl_freq=ctrl_freq,
-                         gui=gui,
-                         record=record,
-                         obstacles=obstacles,
-                         # user_debug_gui=False,
-                         # vision_attributes=vision_attributes,
-                         )
-
-        self._current_position = self.INIT_XYZS[0]
-        self._steps = 0
-        self._last_action = np.zeros(4, dtype=np.float64)
-        self._prev_distance_to_target = np.linalg.norm(self._current_position - target_points[0])
-        self._current_target_index = 0
-        self._is_done = False
-
-        self.CLIENT = self.CLIENT
-        self.target_visual = []
-
-        if save_model:
-            assert save_folder is not None
-            self.save_model(save_folder)
-
-        if gui:
-            self.show_targets()
-
-        # self._addObstacles()
-
-    def step(self, action):
-        """Applies the given action to the environment."""
-
-        # print(action)
-        obs, reward, terminated, truncated, info = (
-            super().step(action))
-
-        self._steps += 1
-        self._last_action = action
-        self._current_position = self.pos[0]
-
-        return obs, reward, terminated, truncated, info
-
-    def _actionSpace(self):
-        """Returns the action space of the environment."""
-
-        return spaces.Box(low=-1 * np.ones(4, dtype=np.float64),
-                          high=np.ones(4, dtype=np.float64),
-                          shape=(4,), dtype=np.float64)
-
-    def _observationSpace(self):
-        """Returns the observation space of the environment."""
-
-        return spaces.Box(low=np.array([self._x_low, self._y_low, 0,
-                                        -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=np.float64),
-                          high=np.array([self._x_high, self._y_high, self._z_high,
-                                         1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float64),
-                          dtype=np.float64
-                          )
-
-    def _computeObs(self):
-        """
-        Returns the current observation of the environment.
-
-        Kinematic observation of size 12.
-
-        """
-
-        obs = self._clipAndNormalizeState(self._getDroneStateVector(0))
-        ret = np.hstack([obs[0:3], obs[7:10], obs[10:13], obs[13:16]]).reshape(12, )
-        try:
-            return ret.astype('float64')
-        except FloatingPointError as e:
-            print("Error in _computeObs():", ret)
-            print(f"Underflow error: {e}")
-            # raise FloatingPointError
-            # return np.zeros_like(ret).astype('float32')
-            return np.clip(ret, np.finfo(np.float32).min, np.finfo(np.float32).max).astype('float32')
-
-    def _clipAndNormalizeState(self, state):
-        """Normalizes a drone's state to the [-1,1] range.
-
-        Parameters
-        ----------
-        state : ndarray
-            (20,)-shaped array of floats containing the non-normalized state of a single drone.
-
-        Returns
-        -------
-        ndarray
-            (20,)-shaped array of floats containing the normalized state of a single drone.
-
-        """
-        MAX_LIN_VEL_XY = 3
-        MAX_LIN_VEL_Z = 1
-        self.EPISODE_LEN_SEC = 1
-        MAX_XY = MAX_LIN_VEL_XY * self.EPISODE_LEN_SEC
-        MAX_Z = MAX_LIN_VEL_Z * self.EPISODE_LEN_SEC
-
-        MAX_PITCH_ROLL = np.pi  # Full range
-
-        # clipped_pos_xy = np.clip(state[0:2], -MAX_XY, MAX_XY)
-        clipped_pos_xy = np.clip(state[0:2], self._aviary_dim[0], self._aviary_dim[3])
-        # clipped_pos_z = np.clip(state[2], 0, MAX_Z)
-        clipped_pos_z = np.clip(state[2], 0, self._aviary_dim[5])
-        clipped_rp = np.clip(state[7:9], -MAX_PITCH_ROLL, MAX_PITCH_ROLL)
-        clipped_vel_xy = np.clip(state[10:12], -MAX_LIN_VEL_XY, MAX_LIN_VEL_XY)
-        clipped_vel_z = np.clip(state[12], -MAX_LIN_VEL_Z, MAX_LIN_VEL_Z)
-
-        if self.GUI:
-            self._clipAndNormalizeStateWarning(state,
-                                               clipped_pos_xy,
-                                               clipped_pos_z,
-                                               clipped_rp,
-                                               clipped_vel_xy,
-                                               clipped_vel_z
-                                               )
-
-        normalized_pos_xy = clipped_pos_xy / MAX_XY
-        normalized_pos_z = clipped_pos_z / MAX_Z
-        normalized_rp = clipped_rp / MAX_PITCH_ROLL
-        normalized_y = state[9] / np.pi  # No reason to clip
-        normalized_vel_xy = clipped_vel_xy / MAX_LIN_VEL_XY
-        normalized_vel_z = clipped_vel_z / MAX_LIN_VEL_XY
-        normalized_ang_vel = state[13:16] / np.linalg.norm(state[13:16]) if np.linalg.norm(
-            state[13:16]) != 0 else state[13:16]
-
-        norm_and_clipped = np.hstack([normalized_pos_xy,
-                                      normalized_pos_z,
-                                      state[3:7],
-                                      normalized_rp,
-                                      normalized_y,
-                                      normalized_vel_xy,
-                                      normalized_vel_z,
-                                      normalized_ang_vel,
-                                      state[16:20]
-                                      ]).reshape(20, )
-
-        return norm_and_clipped
-
-    ################################################################################
-
-    def _clipAndNormalizeStateWarning(self,
-                                      state,
-                                      clipped_pos_xy,
-                                      clipped_pos_z,
-                                      clipped_rp,
-                                      clipped_vel_xy,
-                                      clipped_vel_z,
-                                      ):
-        """Debugging printouts associated to `_clipAndNormalizeState`.
-
-        Print a warning if values in a state vector is out of the clipping range.
-
-        """
-        if not (clipped_pos_xy == np.array(state[0:2])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped xy position [{:.2f} {:.2f}]".format(state[0],
-                                                                                                              state[1]))
-        if not (clipped_pos_z == np.array(state[2])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped z position [{:.2f}]".format(state[2]))
-        if not (clipped_rp == np.array(state[7:9])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped roll/pitch [{:.2f} {:.2f}]".format(state[7],
-                                                                                                             state[8]))
-        if not (clipped_vel_xy == np.array(state[10:12])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped xy velocity [{:.2f} {:.2f}]".format(state[10],
-                                                                                                              state[
-                                                                                                                  11]))
-        if not (clipped_vel_z == np.array(state[12])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped z velocity [{:.2f}]".format(state[12]))
-
-    def _computeInfo(self):
-        """Computes the current info dict(s).
-
-        Returns
-        -------
-        dict[str, int]
-            A dict containing the current
-            info values.
-            The number of found targets is stored under the key "found_targets".
-
-        """
-        return {"found_targets": self._current_target_index}
-
-    def _computeTruncated(self):
-        """Computes the current truncated value(s).
-
-        Returns
-        -------
-        bool
-            Whether the agent has reached the time/step limit.
-
-        """
-        if self._max_steps <= self._steps:
-            return True
-        return False
-
-    def _computeTerminated(self):
-        """Computes the current done value.
-
-        Returns
-        -------
-        bool
-            Whether the current episode is done.
-
-        """
-        # print(self._current_target_index, len(self._target_points))
-
-        # print(self.step_counter / self.PYB_FREQ > self.EPISODE_LEN_SEC)
-        # print(self._getDroneStateVector(0)[2] < self.COLLISION_H and self._steps > 100)
-        # print(self._steps)
-        #  or \self.step_counter / self.PYB_FREQ > self.EPISODE_LEN_SEC or
-
-        if self._has_collision_occurred() or self._current_target_index == len(self._target_points):
-            return True
-        else:
-            return False
-
-    def _computeReward(self):
-        """Computes the current reward value.
-
-        Returns
-        -------
-        float
-            The reward value.
-
-        """
-        if self._computeTerminated() and not self._is_done:
-            # print("term and NOT DONE")
-            return -3000
-            # -10 * (len(self._target_points) - self._current_target_index)) #  * np.linalg.norm(velocity)
-
-        reward = 0.0
-
-        distance_to_target = abs(np.linalg.norm(
-            self._target_points[self._current_target_index] - self._current_position))
-
-        # print("tar", self._target_points[self._current_target_index])
-        #
-        # print("dis", distance_to_target)
-        # distance_to_target = self.distance_between_points(self._computeObs()[:3],
-        #                                                   self._target_points[self._current_target_index])
-
-        try:
-            # reward -= distance_to_target ** 2
-            # Reward based on distance to target
-
-            # reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
-            reward += np.exp(-distance_to_target * 5) * 50
-            # Additional reward for progressing towards the target
-            reward += (self._prev_distance_to_target - distance_to_target) * 300
-
-            # # Penalize large actions to avoid erratic behavior
-            # reward -= 0.01 * np.linalg.norm(self._last_action)
-
-        except ZeroDivisionError:
-            # Give a high reward if the drone is at the target (avoiding division by zero)
-            reward += 100
-
-        # Check if the drone has reached a target
-        if distance_to_target <= self._threshold:
-            # print("IN")
-            self._current_target_index += 1
-
-            if self._current_target_index == len(self._target_points):
-                # Reward for reaching all targets
-                reward += 1000000.0  # * self._discount ** self._steps/10  # Reward for reaching all targets
-                self._is_done = True
-            else:
-                # Reward for reaching a target
-                # reward += 1000  # * (self._discount ** (self._steps / 5))
-                reward += 1000 * (self._discount ** (self._steps / 10))
-
-                if self.GUI:
-                    self.remove_target()
-
-        # #####################################
-        #
-        #         # Calculate the Euclidean distance between the drone and each target
-        #         distances = np.linalg.norm(self._target_points - self._current_position, axis=1)
-        #
-        #         # Check if the minimum distance is within the threshold
-        #         near_targets = [distance < self._threshold for distance in distances]
-        #         print(self._reached_targets)
-        #         print(near_targets)
-        #         for i, target in enumerate(near_targets):
-        #             if target and not self._reached_targets[i]:
-        #                 reward += 10
-        #                 self._reached_targets[i] = True
-        #
-        #         #####################################
-
-        self._prev_distance_to_target = distance_to_target
-        return reward
-
-    # quad_pt = np.array(
-    #     list((self.state["position"].x_val, self.state["position"].y_val, self.state["position"].z_val,)))
-    #
-    # if self.state["collision"]:
-    #     reward = -100
-    # else:
-    #     dist = 10000000
-    #     for i in range(0, len(pts) - 1):
-    #         dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i + 1]))) / np.linalg.norm(
-    #             pts[i] - pts[i + 1]))
-    #
-    #     if dist > thresh_dist:
-    #         reward = -10
-    #     else:
-    #         reward_dist = math.exp(-beta * dist) - 0.5
-    #         reward_speed = (np.linalg.norm(
-    #             [self.state["velocity"].x_val, self.state["velocity"].y_val, self.state["velocity"].z_val, ]) - 0.5)
-    #         reward = reward_dist + reward_speed
-    #
-    # def interpret_action(self, action):
-    #     if action == 0:
-    #         quad_offset = (self.step_length, 0, 0)
-    #     elif action == 1:
-    #         quad_offset = (0, self.step_length, 0)
-    #     elif action == 2:
-    #         quad_offset = (0, 0, self.step_length)
-    #     elif action == 3:
-    #         quad_offset = (-self.step_length, 0, 0)
-    #     elif action == 4:
-    #         quad_offset = (0, -self.step_length, 0)
-    #     elif action == 5:
-    #         quad_offset = (0, 0, -self.step_length)
-    #     else:
-    #         quad_offset = (0, 0, 0)
-
-    def reset(self,
-              seed: int = None,
-              options: dict = None):
-        """Resets the environment."""
-
-        self._is_done = False
-        self._current_target_index = 0
-        self._current_position = self.INIT_XYZS[0]
-        self._steps = 0
-        self._prev_distance_to_target = np.linalg.norm(self.INIT_XYZS - self._target_points[0])
-        self._last_action = np.zeros(4, dtype=np.float64)
-        # self._reached_targets = np.zeros(len(self._target_points), dtype=bool)
-
-        ret = super().reset(seed, options)
-        if self.GUI:
-            self.show_targets()
-
-        return ret
-
-    def distance_between_points(self, point1, point2):
-        x1, y1, z1 = point1
-        x2, y2, z2 = point2
-
-        distance = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2)
-        return distance
-
-    def _has_collision_occurred(self) -> bool:
-        """
-        Checks if the drone has collided with the ground or an obstacle.
-
-        Returns
-        -------
-        bool
-            True if the drone has collided, False otherwise.
-
-        """
-        # COLLISION_H = 0.15  # Height at which the drone is considered to have collided with the ground.
-        # Three times the collision height because the drone tend act like a "wheel"
-        # and circle around a lower target point.
-
-        state = self._current_position
-
-        if (state[0] > self._x_high or state[0] < self._x_low or
-                state[1] > self._y_high or state[1] < -self._y_low or
-                (state[2] < self.COLLISION_H * 3 and self._steps > 100) or state[2] > self._z_high):
-            return True
-        else:
-            return False
-
-    def save_model(self, save_folder):
-        """
-        Saves the model to a folder.
-
-        Parameters
-        ----------
-        save_folder : str
-            The folder to save the model to.
-
-        """
-        # Get the source code of the object's class
-        source_code = inspect.getsource(self.__class__)
-
-        # Construct the file path for saving the source code
-        file_path = os.path.join(save_folder, "model.py")
-
-        # Save the source code as text
-        with open(file_path, "w") as file:
-            file.write(source_code)
-        print(f"Object source code saved to: {file_path}")
-
-    def distance_to_line(self, point, line_start, line_end):
-        # Calculate the vector from line_start to line_end
-        line_vector = line_end - line_start
-
-        # Calculate the vector from line_start to the point
-        point_vector = point - line_start
-
-        # Calculate the perpendicular distance
-        distance = np.linalg.norm(np.cross(line_vector, point_vector)) / np.linalg.norm(line_vector)
-
-        return distance
-
-    def show_targets(self):
-
-        self.target_visual = []
-
-        for target in self._target_points:
-            self.target_visual.append(
-                p.loadURDF(
-                    fileName="/resources/target.urdf",
-                    basePosition=target,
-                    useFixedBase=True,
-                    globalScaling=self._threshold / 4.0,
-                    physicsClientId=self.CLIENT,
-                )
-            )
-        for i, visual in enumerate(self.target_visual):
-            p.changeVisualShape(
-                visual,
-                linkIndex=-1,
-                rgbaColor=(0, 1 - (i / len(self.target_visual)), 0, 1),
-                physicsClientId=self.CLIENT
-            )
-
-    def remove_target(self):
-        # delete the reached target and recolour the others
-        if len(self.target_visual) > 0:
-            p.removeBody(self.target_visual[0])
-            self.target_visual = self.target_visual[1:]
-
-
-    # def _computeReward(self):
-    #
-    #     if self._computeTerminated() and not self._is_done:
-    #         # print("term and NOT DONE")
-    #         return -10#  * np.linalg.norm(velocity)
-    #
-    #     reward = 0.0
-    #     # Get the current drone position
-    #     # current_position = self._computeObs()[:3]
-    #
-    #     distance_to_target = abs(np.linalg.norm(
-    #         self._current_position - self._target_points[self._current_target_index]
-    #     ))
-    #
-    #     # print("tar", self._target_points[self._current_target_index])
-    #
-    #     # print("dis", distance_to_target)
-    #     # distance_to_target = self.distance_between_points(self._computeObs()[:3],
-    #     #                                                   self._target_points[self._current_target_index])
-    #
-    #     try:
-    #         # reward -= distance_to_target ** 2
-    #         # Reward based on distance to target
-    #         # print("dis", distance_to_target)
-    #
-    ##        reward += (1 / distance_to_target) * self._discount ** self._steps/10
-    #
-    #         # Additional reward for progressing towards the target
-    #         reward += (self._prev_distance_to_target - distance_to_target) * 1.5
-    #
-    #         # # Penalize large actions to avoid erratic behavior
-    #         reward -= 0.01 * np.linalg.norm(self._last_action)
-    #
-    #     except ZeroDivisionError:
-    #         # Give a high reward if the drone is at the target (avoiding division by zero)
-    #         reward += 10
-    #
-    #     # Check if the drone has reached a target
-    #     if distance_to_target <= self._threshold:
-    #         # print("IN")
-    #         self._current_target_index += 1
-    #
-    #         if self._current_target_index == len(self._target_points):
-    #             # Reward for reaching all targets
-    #             reward += 100000.0  * self._discount ** self._steps/10  # Reward for reaching all targets
-    #             self._is_done = True
-    #         else:
-    #             # Reward for reaching a target
-    #             reward += 1000 * self._discount ** self._steps / 10
-    #
-    #         # If the drone is outside the threshold, give a reward based on distance
-    #     #            reward = max(0.0, 1.0 - distance_to_target / self._threshold)
-    #
-    #     # if self._computeTerminated() and not self._is_done:
-    #     #     reward -= 1
-    #     #
-    #     # if (np.linalg.norm(self._computeObs()[:3] - self._target_points[self._current_target_index])) < self._threshold:
-    #     #     self._current_target_index += 1
-    #     #     if self._current_target_index == len(self._target_points):
-    #     #         self._is_done = True
-    #     #         reward += 10
-    #     #     else:
-    #     #         reward += 1
-    #     self._prev_distance_to_target = distance_to_target
-    #     return reward
diff --git a/Sol/Model/pybullet_drone_simulator.py b/Sol/Model/pybullet_drone_simulator.py
index e1adf56..a235cfb 100644
--- a/Sol/Model/pybullet_drone_simulator.py
+++ b/Sol/Model/pybullet_drone_simulator.py
@@ -269,8 +269,8 @@ class PBDroneSimulator:
         model = PPO("MlpPolicy",
                     train_env,
                     verbose=1,
-                    n_steps=2048,
-                    batch_size=49,
+                    n_steps=args.nums_steps,
+                    batch_size=args.batch_size,
                     device="auto",
                     policy_kwargs=onpolicy_kwargs
                     # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
@@ -292,7 +292,7 @@ class PBDroneSimulator:
     def run_full(self):
         start = time.perf_counter()
 
-        filename = os.path.join("./model_chkpts", 'save-' + datetime.now().strftime("%m.%d.%Y_%H.%M.%S"))
+        filename = os.path.join("./Sol/model_chkpts", 'save-' + datetime.now().strftime("%m.%d.%Y_%H.%M.%S"))
         if not os.path.exists(filename):
             os.makedirs(filename + '/')
 
@@ -342,13 +342,13 @@ class PBDroneSimulator:
             model = PPO(ActorCriticPolicy,
                         train_env,
                         verbose=1,
-                        n_steps=2048 * self.num_envs,
-                        batch_size=49152 * self.num_envs,
+                        n_steps=args.num_steps * self.num_envs,
+                        batch_size=args.batch_size * self.num_envs,
                         ent_coef=0.01,
                         # use_sde=True,
                         # sde_sample_freq=4,
                         clip_range=0.2,
-                        learning_rate=args.learning_rate,
+                        learning_rate=int(args.learning_rate),
                         tensorboard_log="./logs/ppo_tensorboard/" if args.savemodel else None,
                         device="auto",
                         policy_kwargs=onpolicy_kwargs
@@ -374,7 +374,7 @@ class PBDroneSimulator:
                 buffer_size=int(1e6),
                 learning_rate=args.learning_rate,
                 # gamma=0.95,
-                batch_size=49152 // self.num_envs,
+                batch_size=args.batch_size,
                 policy_kwargs=offpolicy_kwargs,  # dict(net_arch=[256, 256, 256]),
                 device="auto",
             )
@@ -532,11 +532,11 @@ def parse_args():
 
     parser.add_argument("--num-envs", type=int, default=1,
                         help="the number of parallel game environments")
-    parser.add_argument('--max_steps', type=int, default=5e6,
+    parser.add_argument('--max_steps', type=str, default=5e6,
                         help="total timesteps of the experiments")
     parser.add_argument('--max_env_steps', type=int, default=5000,
                         help="total timesteps of one episode")
-    parser.add_argument("--learning_rate", type=float, default=1e-3,
+    parser.add_argument("--learning_rate", type=str, default=1e-3,
                         help="the learning rate of the optimizer")
 
     # RL Algorithm specific arguments
@@ -544,8 +544,8 @@ def parse_args():
     parser.add_argument('--agent-config', type=str, default='default')
     parser.add_argument('--discount', type=int, default=0.999)
     parser.add_argument('--threshold', type=int, default=0.3)
-    parser.add_argument('--batch-size', type=int, default=2048)
-    parser.add_argument('--num-steps', type=int, default=2048)
+    parser.add_argument('--batch_size', type=int, default=40960)
+    parser.add_argument('--num_steps', type=int, default=2048)
 
     # PPO specific
     parser.add_argument('--clip_range', type=int, default=0.2)
diff --git a/logs/ppo_tensorboard/PPO 01.06.2024_11.35.12_1/events.out.tfevents.1704537312.Ozymandias-II.16444.1 b/logs/ppo_tensorboard/PPO 01.06.2024_11.35.12_1/events.out.tfevents.1704537312.Ozymandias-II.16444.1
deleted file mode 100644
index fecc467..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.06.2024_11.35.12_1/events.out.tfevents.1704537312.Ozymandias-II.16444.1 and /dev/null differ
diff --git a/logs/ppo_tensorboard/PPO 01.06.2024_18.02.45_1/events.out.tfevents.1704560565.Ozymandias-II.29152.1 b/logs/ppo_tensorboard/PPO 01.06.2024_18.02.45_1/events.out.tfevents.1704560565.Ozymandias-II.29152.1
deleted file mode 100644
index 099b7d0..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.06.2024_18.02.45_1/events.out.tfevents.1704560565.Ozymandias-II.29152.1 and /dev/null differ
diff --git a/logs/ppo_tensorboard/PPO 01.06.2024_22.21.57_1/events.out.tfevents.1704576117.Ozymandias-II.38168.1 b/logs/ppo_tensorboard/PPO 01.06.2024_22.21.57_1/events.out.tfevents.1704576117.Ozymandias-II.38168.1
deleted file mode 100644
index cba6bd5..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.06.2024_22.21.57_1/events.out.tfevents.1704576117.Ozymandias-II.38168.1 and /dev/null differ
diff --git a/logs/ppo_tensorboard/PPO 01.07.2024_20.14.01_1/events.out.tfevents.1704654841.Ozymandias-II.29648.1 b/logs/ppo_tensorboard/PPO 01.07.2024_20.14.01_1/events.out.tfevents.1704654841.Ozymandias-II.29648.1
deleted file mode 100644
index 541b2df..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.07.2024_20.14.01_1/events.out.tfevents.1704654841.Ozymandias-II.29648.1 and /dev/null differ
diff --git a/logs/ppo_tensorboard/PPO 01.07.2024_21.45.53_1/events.out.tfevents.1704660353.Ozymandias-II.31124.1 b/logs/ppo_tensorboard/PPO 01.07.2024_21.45.53_1/events.out.tfevents.1704660353.Ozymandias-II.31124.1
deleted file mode 100644
index e7989d0..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.07.2024_21.45.53_1/events.out.tfevents.1704660353.Ozymandias-II.31124.1 and /dev/null differ
diff --git a/logs/ppo_tensorboard/PPO 01.08.2024_11.47.55_1/events.out.tfevents.1704710875.Ozymandias-II.16288.1 b/logs/ppo_tensorboard/PPO 01.08.2024_11.47.55_1/events.out.tfevents.1704710875.Ozymandias-II.16288.1
deleted file mode 100644
index 5feec34..0000000
Binary files a/logs/ppo_tensorboard/PPO 01.08.2024_11.47.55_1/events.out.tfevents.1704710875.Ozymandias-II.16288.1 and /dev/null differ
diff --git a/model_chkpts/save-12.17.2023_00.46.13/model.py b/model_chkpts/save-12.17.2023_00.46.13/model.py
deleted file mode 100644
index af1bdfb..0000000
--- a/model_chkpts/save-12.17.2023_00.46.13/model.py
+++ /dev/null
@@ -1,402 +0,0 @@
-class PBDroneEnv(
-    # BaseAviary,
-    BaseSingleAgentAviary
-):
-
-    def __init__(self,
-                 target_points, threshold, discount, max_steps,
-                 save_model=False, save_folder=None,
-                 drone_model: DroneModel = DroneModel.CF2X,
-                 initial_xyzs=None,
-                 initial_rpys=None,
-                 physics: Physics = Physics.PYB,
-                 pyb_freq: int = 240,
-                 ctrl_freq: int = 240,
-                 gui=False,
-                 record=False,
-                 obs: ObservationType = ObservationType.KIN,
-                 act: ActionType = ActionType.RPM,
-                 vision_attributes=False,
-                 ):
-
-        super().__init__(drone_model=drone_model,
-                         # num_drones=1,
-                         initial_xyzs=initial_xyzs,
-                         initial_rpys=initial_rpys,
-                         physics=physics,
-                         pyb_freq=pyb_freq,
-                         ctrl_freq=ctrl_freq,
-                         gui=gui,
-                         vision_attributes=vision_attributes,
-                         record=record,
-
-                         # obstacles=False,
-                         # user_debug_gui=False,
-                         # vision_attributes=vision_attributes,
-                         )
-
-        self.ACT_TYPE = act
-        self.EPISODE_LEN_SEC = 5
-        self.OBS_TYPE = obs
-
-        self._target_points = np.array(target_points)
-        self._threshold = threshold
-        self._discount = discount
-        self._max_steps = max_steps
-
-        self._steps = 0
-        self._current_position = np.array([0.0, 0.0, 0.0])
-        self._last_action = np.array([0.0, 0.0, 0.0])
-        self._prev_distance_to_target = np.linalg.norm(initial_xyzs - target_points[0])
-        self._current_target_index = 0
-        self._is_done = False
-
-        if save_model:
-            assert save_folder is not None
-
-            self.save_model(save_folder)
-
-
-    def step(self, action):
-
-        # The last action ended the episode. Ignore the current action and start
-        # a new episode.
-        # print(self._steps)
-
-        self._steps += 1
-
-        obs, reward, terminated, truncated, info = (
-            super().step(action))
-
-        # # # Update position based on action
-        # # self._current_position += action
-        #
-        # # Update position based on action
-        # self._update_position(self._preprocessAction(action))
-
-        # Create a time step
-        # time_step = ts.transition(
-        #     tf.convert_to_tensor(obs),
-        #     tf.convert_to_tensor(reward),
-        #     # discount=tf.constant(self._discount),
-        # )
-        self._last_action = action
-        self._current_position = self._computeObs()[:3]
-
-        return obs, reward, terminated, truncated, info
-
-    def _actionSpace(self):
-        """Returns the action space of the environment."""
-
-        return spaces.Box(low=-1 * np.ones(4), high=np.ones(4),
-                          shape=(4,), dtype=np.float32)
-
-    def _observationSpace(self):
-        """Returns the observation space of the environment."""
-
-        return spaces.Box(low=np.array([-1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]),
-                          high=np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
-                          dtype=np.float32
-                          )
-
-    def _computeObs(self):
-        """
-        Returns the current observation of the environment.
-
-        Kinematic observation of size 12.
-
-        """
-        assert self.OBS_TYPE == ObservationType.KIN
-
-        obs = self._clipAndNormalizeState(self._getDroneStateVector(0))
-        ret = np.hstack([obs[0:3], obs[7:10], obs[10:13], obs[13:16]]).reshape(12, )
-        return ret.astype('float32')
-
-    def _clipAndNormalizeState(self,
-                               state
-                               ):
-        """Normalizes a drone's state to the [-1,1] range.
-
-        Parameters
-        ----------
-        state : ndarray
-            (20,)-shaped array of floats containing the non-normalized state of a single drone.
-
-        Returns
-        -------
-        ndarray
-            (20,)-shaped array of floats containing the normalized state of a single drone.
-
-        """
-        MAX_LIN_VEL_XY = 3
-        MAX_LIN_VEL_Z = 1
-        self.EPISODE_LEN_SEC = 1
-        MAX_XY = MAX_LIN_VEL_XY * self.EPISODE_LEN_SEC
-        MAX_Z = MAX_LIN_VEL_Z * self.EPISODE_LEN_SEC
-
-        MAX_PITCH_ROLL = np.pi  # Full range
-
-        clipped_pos_xy = np.clip(state[0:2], -MAX_XY, MAX_XY)
-        clipped_pos_z = np.clip(state[2], 0, MAX_Z)
-        clipped_rp = np.clip(state[7:9], -MAX_PITCH_ROLL, MAX_PITCH_ROLL)
-        clipped_vel_xy = np.clip(state[10:12], -MAX_LIN_VEL_XY, MAX_LIN_VEL_XY)
-        clipped_vel_z = np.clip(state[12], -MAX_LIN_VEL_Z, MAX_LIN_VEL_Z)
-
-        # if self.GUI:
-        #     self._clipAndNormalizeStateWarning(state,
-        #                                        clipped_pos_xy,
-        #                                        clipped_pos_z,
-        #                                        clipped_rp,
-        #                                        clipped_vel_xy,
-        #                                        clipped_vel_z
-        #                                        )
-
-        normalized_pos_xy = clipped_pos_xy / MAX_XY
-        normalized_pos_z = clipped_pos_z / MAX_Z
-        normalized_rp = clipped_rp / MAX_PITCH_ROLL
-        normalized_y = state[9] / np.pi  # No reason to clip
-        normalized_vel_xy = clipped_vel_xy / MAX_LIN_VEL_XY
-        normalized_vel_z = clipped_vel_z / MAX_LIN_VEL_XY
-        normalized_ang_vel = state[13:16] / np.linalg.norm(state[13:16]) if np.linalg.norm(
-            state[13:16]) != 0 else state[13:16]
-
-        norm_and_clipped = np.hstack([normalized_pos_xy,
-                                      normalized_pos_z,
-                                      state[3:7],
-                                      normalized_rp,
-                                      normalized_y,
-                                      normalized_vel_xy,
-                                      normalized_vel_z,
-                                      normalized_ang_vel,
-                                      state[16:20]
-                                      ]).reshape(20, )
-
-        return norm_and_clipped
-
-    ################################################################################
-
-    def _clipAndNormalizeStateWarning(self,
-                                      state,
-                                      clipped_pos_xy,
-                                      clipped_pos_z,
-                                      clipped_rp,
-                                      clipped_vel_xy,
-                                      clipped_vel_z,
-                                      ):
-        """Debugging printouts associated to `_clipAndNormalizeState`.
-
-        Print a warning if values in a state vector is out of the clipping range.
-
-        """
-        if not (clipped_pos_xy == np.array(state[0:2])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped xy position [{:.2f} {:.2f}]".format(state[0],
-                                                                                                              state[1]))
-        if not (clipped_pos_z == np.array(state[2])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped z position [{:.2f}]".format(state[2]))
-        if not (clipped_rp == np.array(state[7:9])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped roll/pitch [{:.2f} {:.2f}]".format(state[7],
-                                                                                                             state[8]))
-        if not (clipped_vel_xy == np.array(state[10:12])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped xy velocity [{:.2f} {:.2f}]".format(state[10],
-                                                                                                              state[
-                                                                                                                  11]))
-        if not (clipped_vel_z == np.array(state[12])).all():
-            print("[WARNING] it", self.step_counter,
-                  "in FlyThruGateAviary._clipAndNormalizeState(), clipped z velocity [{:.2f}]".format(state[12]))
-
-    def _computeInfo(self):
-        """Computes the current info dict(s).
-
-        Unused.
-
-        Returns
-        -------
-        dict[str, int]
-            Dummy value.
-
-        """
-        return {"answer": 42}
-
-    def _computeTruncated(self):
-        """Computes the current truncated value(s).
-
-        Returns
-        -------
-        bool
-            Whether the agent has reached the time/step limit.
-
-        """
-        if self._max_steps <= self._steps:
-            return True
-        return False
-
-    def _computeTerminated(self):
-        """Computes the current done value.
-
-        Returns
-        -------
-        bool
-            Whether the current episode is done.
-
-        """
-        # print(self._current_target_index, len(self._target_points))
-
-        # print(self.step_counter / self.PYB_FREQ > self.EPISODE_LEN_SEC)
-        # print(self._getDroneStateVector(0)[2] < self.COLLISION_H and self._steps > 100)
-        # print(self._steps)
-        #  or \self.step_counter / self.PYB_FREQ > self.EPISODE_LEN_SEC or
-
-        if self._has_collision_occurred() or self._current_target_index == len(self._target_points):
-            return True
-        else:
-            return False
-
-    def _computeReward(self):
-
-        if self._computeTerminated() and not self._is_done:
-            # print("term and NOT DONE")
-            return -3000 #  * np.linalg.norm(velocity)
-
-        reward = 0.0
-        # Get the current drone position
-        # current_position = self._computeObs()[:3]
-
-        distance_to_target = abs(np.linalg.norm(
-            self._current_position - self._target_points[self._current_target_index]
-        ))
-
-        # print("tar", self._target_points[self._current_target_index])
-
-        # print("dis", distance_to_target)
-        # distance_to_target = self.distance_between_points(self._computeObs()[:3],
-        #                                                   self._target_points[self._current_target_index])
-
-        try:
-            # reward -= distance_to_target ** 2
-            # Reward based on distance to target
-            # print("dis", distance_to_target)
-
-            reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
-
-            # Additional reward for progressing towards the target
-            reward += (self._prev_distance_to_target - distance_to_target)  # * 0.5
-
-            # # Penalize large actions to avoid erratic behavior
-            # reward -= 0.01 * np.linalg.norm(self._last_action)
-
-        except ZeroDivisionError:
-            # Give a high reward if the drone is at the target (avoiding division by zero)
-            reward += 100
-
-        # Check if the drone has reached a target
-        if distance_to_target <= self._threshold:
-            # print("IN")
-            self._current_target_index += 1
-
-            if self._current_target_index == len(self._target_points):
-                # Reward for reaching all targets
-                reward += 100000.0  # * self._discount ** self._steps/10  # Reward for reaching all targets
-                self._is_done = True
-            else:
-                # Reward for reaching a target
-                reward += 700 * self._discount ** self._steps / 10
-
-            # If the drone is outside the threshold, give a reward based on distance
-        #            reward = max(0.0, 1.0 - distance_to_target / self._threshold)
-
-        # if self._computeTerminated() and not self._is_done:
-        #     reward -= 1
-        #
-        # if (np.linalg.norm(self._computeObs()[:3] - self._target_points[self._current_target_index])) < self._threshold:
-        #     self._current_target_index += 1
-        #     if self._current_target_index == len(self._target_points):
-        #         self._is_done = True
-        #         reward += 10
-        #     else:
-        #         reward += 1
-        self._prev_distance_to_target = distance_to_target
-        return reward
-
-    # quad_pt = np.array(
-    #     list((self.state["position"].x_val, self.state["position"].y_val, self.state["position"].z_val,)))
-    #
-    # if self.state["collision"]:
-    #     reward = -100
-    # else:
-    #     dist = 10000000
-    #     for i in range(0, len(pts) - 1):
-    #         dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i + 1]))) / np.linalg.norm(
-    #             pts[i] - pts[i + 1]))
-    #
-    #     if dist > thresh_dist:
-    #         reward = -10
-    #     else:
-    #         reward_dist = math.exp(-beta * dist) - 0.5
-    #         reward_speed = (np.linalg.norm(
-    #             [self.state["velocity"].x_val, self.state["velocity"].y_val, self.state["velocity"].z_val, ]) - 0.5)
-    #         reward = reward_dist + reward_speed
-    #
-    # def interpret_action(self, action):
-    #     if action == 0:
-    #         quad_offset = (self.step_length, 0, 0)
-    #     elif action == 1:
-    #         quad_offset = (0, self.step_length, 0)
-    #     elif action == 2:
-    #         quad_offset = (0, 0, self.step_length)
-    #     elif action == 3:
-    #         quad_offset = (-self.step_length, 0, 0)
-    #     elif action == 4:
-    #         quad_offset = (0, -self.step_length, 0)
-    #     elif action == 5:
-    #         quad_offset = (0, 0, -self.step_length)
-    #     else:
-    #         quad_offset = (0, 0, 0)
-
-    def reset(self,
-              seed: int = None,
-              options: dict = None):
-        """Resets the environment."""
-
-        self._is_done = False
-        self._current_target_index = 0
-        self._current_position = np.array([0, 0, 0])
-        self._steps = 0
-
-        return super().reset(seed, options)
-
-    def distance_between_points(self, point1, point2):
-        x1, y1, z1 = point1
-        x2, y2, z2 = point2
-
-        distance = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2)
-        return distance
-
-    def _has_collision_occurred(self) -> bool:
-        # Three times the collision height because the drone tend act like a "wheel"
-        # and circle around a lower target point.
-
-        state = self._computeObs()[:3]
-
-        if (state[0] >= 1. or state[0] <= -1. or
-                state[1] >= 1. or state[1] <= -1. or
-                (state[2] <= self.COLLISION_H * 3 and self._steps > 100) or state[2] >= 1):
-            return True
-        else:
-            return False
-
-    def save_model(self, save_folder):
-
-        # Get the source code of the object's class
-        source_code = inspect.getsource(self.__class__)
-
-        # Construct the file path for saving the source code
-        file_path = os.path.join(save_folder, "model.py")
-
-        # Save the source code as text
-        with open(file_path, "w") as file:
-            file.write(source_code)
-        print(f"Object source code saved to: {file_path}")
diff --git a/requirements.txt b/requirements.txt
index 0982235..25c6c2e 100644
Binary files a/requirements.txt and b/requirements.txt differ
