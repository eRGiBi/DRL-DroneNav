AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO_113
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-190.08 +/- 54.63
Episode length: 221.80 +/- 71.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-220.42 +/- 23.28
Episode length: 190.60 +/- 28.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-203.66 +/- 23.20
Episode length: 217.00 +/- 39.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
Eval num_timesteps=7968, episode_reward=-210.34 +/- 48.33
Episode length: 214.20 +/- 60.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-193.98 +/- 25.53
Episode length: 229.80 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-204.85 +/- 37.95
Episode length: 202.40 +/- 40.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-198.32 +/- 34.32
Episode length: 221.60 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Eval num_timesteps=15936, episode_reward=-218.77 +/- 28.32
Episode length: 210.40 +/- 31.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 15936    |
---------------------------------
Eval num_timesteps=17928, episode_reward=-228.13 +/- 21.56
Episode length: 196.20 +/- 32.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 17928    |
---------------------------------
Eval num_timesteps=19920, episode_reward=-200.24 +/- 36.23
Episode length: 209.60 +/- 56.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 19920    |
---------------------------------
Eval num_timesteps=21912, episode_reward=-167.81 +/- 69.39
Episode length: 246.80 +/- 74.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 21912    |
---------------------------------
New best mean reward!
Eval num_timesteps=23904, episode_reward=-220.56 +/- 16.43
Episode length: 207.40 +/- 61.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 23904    |
---------------------------------
Eval num_timesteps=25896, episode_reward=-203.70 +/- 22.66
Episode length: 229.60 +/- 44.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 25896    |
---------------------------------
Eval num_timesteps=27888, episode_reward=-174.36 +/- 71.30
Episode length: 238.40 +/- 62.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 27888    |
---------------------------------
Eval num_timesteps=29880, episode_reward=-207.50 +/- 72.39
Episode length: 209.20 +/- 80.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 29880    |
---------------------------------
Eval num_timesteps=31872, episode_reward=-207.76 +/- 41.60
Episode length: 235.20 +/- 47.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 31872    |
---------------------------------
Eval num_timesteps=33864, episode_reward=-243.82 +/- 20.96
Episode length: 182.80 +/- 43.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 33864    |
---------------------------------
Eval num_timesteps=35856, episode_reward=-199.04 +/- 37.89
Episode length: 243.20 +/- 56.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 35856    |
---------------------------------
Eval num_timesteps=37848, episode_reward=-203.05 +/- 70.82
Episode length: 216.80 +/- 64.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 37848    |
---------------------------------
Eval num_timesteps=39840, episode_reward=-184.88 +/- 42.23
Episode length: 248.80 +/- 85.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 39840    |
---------------------------------
Eval num_timesteps=41832, episode_reward=-209.22 +/- 17.95
Episode length: 220.80 +/- 30.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 41832    |
---------------------------------
Eval num_timesteps=43824, episode_reward=-192.37 +/- 40.45
Episode length: 215.00 +/- 52.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 43824    |
---------------------------------
Eval num_timesteps=45816, episode_reward=-233.50 +/- 23.22
Episode length: 193.80 +/- 37.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 45816    |
---------------------------------
Eval num_timesteps=47808, episode_reward=-229.55 +/- 22.95
Episode length: 206.40 +/- 37.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 47808    |
---------------------------------
Eval num_timesteps=49800, episode_reward=-186.38 +/- 42.15
Episode length: 219.80 +/- 41.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 49800       |
| train/                  |             |
|    approx_kl            | 0.003881611 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.72       |
|    explained_variance   | -7.84e-05   |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00229    |
|    std                  | 1.02        |
|    value_loss           | 2.61e+03    |
-----------------------------------------
Eval num_timesteps=51792, episode_reward=-215.10 +/- 36.15
Episode length: 230.60 +/- 38.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 51792    |
---------------------------------
Eval num_timesteps=53784, episode_reward=-199.24 +/- 38.41
Episode length: 213.00 +/- 43.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 53784    |
---------------------------------
Eval num_timesteps=55776, episode_reward=-198.04 +/- 60.17
Episode length: 211.60 +/- 57.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 55776    |
---------------------------------
Eval num_timesteps=57768, episode_reward=-189.66 +/- 40.51
Episode length: 227.00 +/- 38.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 57768    |
---------------------------------
Eval num_timesteps=59760, episode_reward=-215.11 +/- 15.32
Episode length: 211.60 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 59760    |
---------------------------------
Eval num_timesteps=61752, episode_reward=-204.03 +/- 40.86
Episode length: 214.20 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 61752    |
---------------------------------
Eval num_timesteps=63744, episode_reward=-222.51 +/- 12.43
Episode length: 197.60 +/- 22.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 63744    |
---------------------------------
Eval num_timesteps=65736, episode_reward=-186.24 +/- 38.08
Episode length: 231.40 +/- 41.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 65736    |
---------------------------------
Eval num_timesteps=67728, episode_reward=-195.21 +/- 52.02
Episode length: 196.60 +/- 38.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 67728    |
---------------------------------
Eval num_timesteps=69720, episode_reward=-206.16 +/- 49.50
Episode length: 217.20 +/- 75.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 69720    |
---------------------------------
Eval num_timesteps=71712, episode_reward=-199.14 +/- 30.43
Episode length: 221.60 +/- 38.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 71712    |
---------------------------------
Eval num_timesteps=73704, episode_reward=-218.33 +/- 30.35
Episode length: 203.40 +/- 29.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 73704    |
---------------------------------
Eval num_timesteps=75696, episode_reward=-197.31 +/- 40.43
Episode length: 233.00 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 75696    |
---------------------------------
Eval num_timesteps=77688, episode_reward=-216.02 +/- 39.81
Episode length: 219.80 +/- 44.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 77688    |
---------------------------------
Eval num_timesteps=79680, episode_reward=-199.60 +/- 47.52
Episode length: 216.80 +/- 53.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 79680    |
---------------------------------
Eval num_timesteps=81672, episode_reward=-194.45 +/- 42.31
Episode length: 209.40 +/- 47.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 81672    |
---------------------------------
Eval num_timesteps=83664, episode_reward=-191.27 +/- 32.05
Episode length: 225.40 +/- 54.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 83664    |
---------------------------------
Eval num_timesteps=85656, episode_reward=-206.45 +/- 20.37
Episode length: 210.20 +/- 37.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 85656    |
---------------------------------
Eval num_timesteps=87648, episode_reward=-202.96 +/- 29.41
Episode length: 203.40 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 87648    |
---------------------------------
Eval num_timesteps=89640, episode_reward=-159.13 +/- 74.90
Episode length: 268.80 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 89640    |
---------------------------------
New best mean reward!
Eval num_timesteps=91632, episode_reward=-207.09 +/- 26.66
Episode length: 208.80 +/- 30.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 91632    |
---------------------------------
Eval num_timesteps=93624, episode_reward=-220.35 +/- 46.03
Episode length: 218.00 +/- 68.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 93624    |
---------------------------------
Eval num_timesteps=95616, episode_reward=-208.07 +/- 25.86
Episode length: 184.40 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 95616    |
---------------------------------
Eval num_timesteps=97608, episode_reward=-208.36 +/- 23.95
Episode length: 212.20 +/- 49.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 97608    |
---------------------------------
Eval num_timesteps=99600, episode_reward=-198.54 +/- 47.49
Episode length: 227.80 +/- 81.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | -199        |
| time/                   |             |
|    total_timesteps      | 99600       |
| train/                  |             |
|    approx_kl            | 0.003278856 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.78       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.001       |
|    loss                 | 852         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00127    |
|    std                  | 1.03        |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=101592, episode_reward=-175.90 +/- 32.54
Episode length: 248.00 +/- 74.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 101592   |
---------------------------------
Eval num_timesteps=103584, episode_reward=-144.82 +/- 22.10
Episode length: 316.80 +/- 65.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 103584   |
---------------------------------
New best mean reward!
Eval num_timesteps=105576, episode_reward=-175.98 +/- 64.46
Episode length: 263.20 +/- 97.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 105576   |
---------------------------------
Eval num_timesteps=107568, episode_reward=-188.96 +/- 44.01
Episode length: 272.40 +/- 105.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 107568   |
---------------------------------
Eval num_timesteps=109560, episode_reward=-215.65 +/- 9.99
Episode length: 203.60 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 109560   |
---------------------------------
Eval num_timesteps=111552, episode_reward=-204.43 +/- 34.87
Episode length: 249.40 +/- 63.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 111552   |
---------------------------------
Eval num_timesteps=113544, episode_reward=-153.00 +/- 35.33
Episode length: 264.20 +/- 48.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 113544   |
---------------------------------
Eval num_timesteps=115536, episode_reward=-178.95 +/- 62.42
Episode length: 254.80 +/- 71.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 115536   |
---------------------------------
Eval num_timesteps=117528, episode_reward=-186.49 +/- 85.35
Episode length: 233.80 +/- 80.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 117528   |
---------------------------------
Eval num_timesteps=119520, episode_reward=-155.59 +/- 74.58
Episode length: 307.80 +/- 128.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 119520   |
---------------------------------
Eval num_timesteps=121512, episode_reward=-164.38 +/- 55.74
Episode length: 264.20 +/- 56.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 121512   |
---------------------------------
Eval num_timesteps=123504, episode_reward=-182.44 +/- 38.31
Episode length: 253.00 +/- 45.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 123504   |
---------------------------------
Eval num_timesteps=125496, episode_reward=-204.08 +/- 36.97
Episode length: 252.40 +/- 70.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 125496   |
---------------------------------
Eval num_timesteps=127488, episode_reward=-214.39 +/- 14.91
Episode length: 210.40 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 127488   |
---------------------------------
Eval num_timesteps=129480, episode_reward=-221.94 +/- 14.98
Episode length: 216.20 +/- 32.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 129480   |
---------------------------------
Eval num_timesteps=131472, episode_reward=-186.75 +/- 44.55
Episode length: 239.00 +/- 70.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 131472   |
---------------------------------
Eval num_timesteps=133464, episode_reward=-168.54 +/- 33.74
Episode length: 280.00 +/- 44.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 133464   |
---------------------------------
Eval num_timesteps=135456, episode_reward=-211.06 +/- 27.73
Episode length: 225.00 +/- 42.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 135456   |
---------------------------------
Eval num_timesteps=137448, episode_reward=-190.22 +/- 63.96
Episode length: 217.00 +/- 61.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 137448   |
---------------------------------
Eval num_timesteps=139440, episode_reward=-184.30 +/- 68.43
Episode length: 264.00 +/- 89.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 139440   |
---------------------------------
Eval num_timesteps=141432, episode_reward=-208.91 +/- 29.86
Episode length: 216.20 +/- 33.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 141432   |
---------------------------------
Eval num_timesteps=143424, episode_reward=-165.03 +/- 52.00
Episode length: 265.80 +/- 51.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 143424   |
---------------------------------
Eval num_timesteps=145416, episode_reward=-171.95 +/- 42.56
Episode length: 251.00 +/- 67.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 145416   |
---------------------------------
Eval num_timesteps=147408, episode_reward=-218.88 +/- 40.35
Episode length: 203.00 +/- 30.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 147408   |
---------------------------------
Eval num_timesteps=149400, episode_reward=-178.42 +/- 37.20
Episode length: 286.00 +/- 76.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | -178         |
| time/                   |              |
|    total_timesteps      | 149400       |
| train/                  |              |
|    approx_kl            | 0.0033820758 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.615        |
|    learning_rate        | 0.001        |
|    loss                 | 666          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.05         |
|    value_loss           | 1.62e+03     |
------------------------------------------
Eval num_timesteps=151392, episode_reward=-220.20 +/- 18.62
Episode length: 201.60 +/- 32.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 151392   |
---------------------------------
Eval num_timesteps=153384, episode_reward=-178.21 +/- 42.66
Episode length: 260.60 +/- 30.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 153384   |
---------------------------------
Eval num_timesteps=155376, episode_reward=-179.54 +/- 30.17
Episode length: 233.20 +/- 79.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 155376   |
---------------------------------
Eval num_timesteps=157368, episode_reward=-212.46 +/- 28.73
Episode length: 254.00 +/- 109.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 157368   |
---------------------------------
Eval num_timesteps=159360, episode_reward=-177.16 +/- 25.59
Episode length: 299.40 +/- 29.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 159360   |
---------------------------------
Eval num_timesteps=161352, episode_reward=-179.94 +/- 40.82
Episode length: 251.80 +/- 47.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 161352   |
---------------------------------
Eval num_timesteps=163344, episode_reward=-167.47 +/- 70.05
Episode length: 266.80 +/- 67.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 163344   |
---------------------------------
Eval num_timesteps=165336, episode_reward=-192.55 +/- 22.38
Episode length: 211.20 +/- 22.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 165336   |
---------------------------------
Eval num_timesteps=167328, episode_reward=-188.38 +/- 27.12
Episode length: 249.20 +/- 38.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 167328   |
---------------------------------
Eval num_timesteps=169320, episode_reward=-164.30 +/- 32.64
Episode length: 287.40 +/- 65.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 169320   |
---------------------------------
Eval num_timesteps=171312, episode_reward=-201.17 +/- 15.96
Episode length: 240.60 +/- 62.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 171312   |
---------------------------------
Eval num_timesteps=173304, episode_reward=-137.76 +/- 34.66
Episode length: 309.20 +/- 68.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 173304   |
---------------------------------
New best mean reward!
Eval num_timesteps=175296, episode_reward=-182.96 +/- 46.47
Episode length: 247.60 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 175296   |
---------------------------------
Eval num_timesteps=177288, episode_reward=-177.33 +/- 44.30
Episode length: 259.60 +/- 89.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 177288   |
---------------------------------
Eval num_timesteps=179280, episode_reward=-186.82 +/- 83.25
Episode length: 240.60 +/- 97.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 179280   |
---------------------------------
Eval num_timesteps=181272, episode_reward=-214.70 +/- 19.00
Episode length: 213.80 +/- 36.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 181272   |
---------------------------------
Eval num_timesteps=183264, episode_reward=-180.57 +/- 54.31
Episode length: 225.40 +/- 60.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 183264   |
---------------------------------
Eval num_timesteps=185256, episode_reward=-218.37 +/- 21.15
Episode length: 209.00 +/- 44.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 185256   |
---------------------------------
Eval num_timesteps=187248, episode_reward=-140.54 +/- 84.10
Episode length: 263.20 +/- 79.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 187248   |
---------------------------------
Eval num_timesteps=189240, episode_reward=-177.33 +/- 47.80
Episode length: 291.80 +/- 71.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 189240   |
---------------------------------
Eval num_timesteps=191232, episode_reward=-172.21 +/- 31.76
Episode length: 245.60 +/- 44.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 191232   |
---------------------------------
Eval num_timesteps=193224, episode_reward=-166.17 +/- 37.10
Episode length: 257.80 +/- 63.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 193224   |
---------------------------------
Eval num_timesteps=195216, episode_reward=-191.39 +/- 36.73
Episode length: 259.60 +/- 81.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 195216   |
---------------------------------
Eval num_timesteps=197208, episode_reward=-148.96 +/- 36.72
Episode length: 330.00 +/- 87.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | -149         |
| time/                   |              |
|    total_timesteps      | 197208       |
| train/                  |              |
|    approx_kl            | 0.0041104667 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 499          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.05         |
|    value_loss           | 1.26e+03     |
------------------------------------------
Eval num_timesteps=199200, episode_reward=-126.32 +/- 44.96
Episode length: 319.20 +/- 82.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
New best mean reward!
Eval num_timesteps=201192, episode_reward=-135.33 +/- 76.39
Episode length: 287.80 +/- 77.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 201192   |
---------------------------------
Eval num_timesteps=203184, episode_reward=-80.30 +/- 120.60
Episode length: 411.40 +/- 156.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -80.3    |
| time/              |          |
|    total_timesteps | 203184   |
---------------------------------
New best mean reward!
Eval num_timesteps=205176, episode_reward=-79.16 +/- 161.33
Episode length: 357.80 +/- 143.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -79.2    |
| time/              |          |
|    total_timesteps | 205176   |
---------------------------------
New best mean reward!
Eval num_timesteps=207168, episode_reward=-80.34 +/- 135.39
Episode length: 367.60 +/- 205.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -80.3    |
| time/              |          |
|    total_timesteps | 207168   |
---------------------------------
Eval num_timesteps=209160, episode_reward=-116.44 +/- 43.03
Episode length: 316.20 +/- 42.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 209160   |
---------------------------------
Eval num_timesteps=211152, episode_reward=-151.32 +/- 45.91
Episode length: 262.80 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 211152   |
---------------------------------
Eval num_timesteps=213144, episode_reward=-141.69 +/- 28.77
Episode length: 375.20 +/- 35.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 213144   |
---------------------------------
Eval num_timesteps=215136, episode_reward=-126.31 +/- 69.16
Episode length: 336.60 +/- 50.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 215136   |
---------------------------------
Eval num_timesteps=217128, episode_reward=-144.64 +/- 58.49
Episode length: 316.60 +/- 78.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 217128   |
---------------------------------
Eval num_timesteps=219120, episode_reward=-105.34 +/- 73.30
Episode length: 405.40 +/- 95.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 219120   |
---------------------------------
Eval num_timesteps=221112, episode_reward=-130.62 +/- 103.23
Episode length: 295.80 +/- 117.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 221112   |
---------------------------------
Eval num_timesteps=223104, episode_reward=-126.92 +/- 79.13
Episode length: 306.40 +/- 67.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 223104   |
---------------------------------
Eval num_timesteps=225096, episode_reward=-93.49 +/- 56.39
Episode length: 336.80 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -93.5    |
| time/              |          |
|    total_timesteps | 225096   |
---------------------------------
Eval num_timesteps=227088, episode_reward=-167.22 +/- 19.22
Episode length: 318.20 +/- 51.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 227088   |
---------------------------------
Eval num_timesteps=229080, episode_reward=-125.51 +/- 51.55
Episode length: 434.60 +/- 161.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 229080   |
---------------------------------
Eval num_timesteps=231072, episode_reward=-140.09 +/- 49.75
Episode length: 335.40 +/- 68.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 231072   |
---------------------------------
Eval num_timesteps=233064, episode_reward=-141.93 +/- 90.88
Episode length: 341.60 +/- 118.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 233064   |
---------------------------------
Eval num_timesteps=235056, episode_reward=-112.79 +/- 25.26
Episode length: 349.40 +/- 90.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 235056   |
---------------------------------
Eval num_timesteps=237048, episode_reward=-149.44 +/- 38.42
Episode length: 331.60 +/- 116.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 332      |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 237048   |
---------------------------------
Eval num_timesteps=239040, episode_reward=-141.76 +/- 78.19
Episode length: 368.40 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 239040   |
---------------------------------
Eval num_timesteps=241032, episode_reward=-92.77 +/- 41.57
Episode length: 424.60 +/- 45.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | -92.8    |
| time/              |          |
|    total_timesteps | 241032   |
---------------------------------
Eval num_timesteps=243024, episode_reward=-124.44 +/- 66.37
Episode length: 349.40 +/- 94.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 349      |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 243024   |
---------------------------------
Eval num_timesteps=245016, episode_reward=-169.04 +/- 29.91
Episode length: 317.40 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 245016   |
---------------------------------
Eval num_timesteps=247008, episode_reward=-68.88 +/- 75.42
Episode length: 454.80 +/- 119.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | -68.9        |
| time/                   |              |
|    total_timesteps      | 247008       |
| train/                  |              |
|    approx_kl            | 0.0045296527 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00269     |
|    std                  | 1.06         |
|    value_loss           | 865          |
------------------------------------------
New best mean reward!
Eval num_timesteps=249000, episode_reward=-47.22 +/- 89.22
Episode length: 431.60 +/- 93.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | -47.2    |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
New best mean reward!
Eval num_timesteps=250992, episode_reward=-125.88 +/- 76.39
Episode length: 378.20 +/- 161.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 250992   |
---------------------------------
Eval num_timesteps=252984, episode_reward=3.69 +/- 224.84
Episode length: 519.00 +/- 194.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 3.69     |
| time/              |          |
|    total_timesteps | 252984   |
---------------------------------
New best mean reward!
Eval num_timesteps=254976, episode_reward=-96.08 +/- 84.66
Episode length: 406.00 +/- 180.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | -96.1    |
| time/              |          |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=256968, episode_reward=-67.56 +/- 104.49
Episode length: 465.80 +/- 154.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | -67.6    |
| time/              |          |
|    total_timesteps | 256968   |
---------------------------------
Eval num_timesteps=258960, episode_reward=-29.04 +/- 71.24
Episode length: 466.60 +/- 70.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 258960   |
---------------------------------
Eval num_timesteps=260952, episode_reward=-85.54 +/- 67.13
Episode length: 432.80 +/- 128.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | -85.5    |
| time/              |          |
|    total_timesteps | 260952   |
---------------------------------
Eval num_timesteps=262944, episode_reward=-100.48 +/- 51.84
Episode length: 426.60 +/- 96.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 262944   |
---------------------------------
Eval num_timesteps=264936, episode_reward=-76.14 +/- 124.72
Episode length: 421.60 +/- 91.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -76.1    |
| time/              |          |
|    total_timesteps | 264936   |
---------------------------------
Eval num_timesteps=266928, episode_reward=-97.01 +/- 57.42
Episode length: 445.80 +/- 72.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | -97      |
| time/              |          |
|    total_timesteps | 266928   |
---------------------------------
Eval num_timesteps=268920, episode_reward=-80.56 +/- 101.82
Episode length: 453.80 +/- 104.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | -80.6    |
| time/              |          |
|    total_timesteps | 268920   |
---------------------------------
Eval num_timesteps=270912, episode_reward=-94.55 +/- 63.26
Episode length: 383.00 +/- 67.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | -94.6    |
| time/              |          |
|    total_timesteps | 270912   |
---------------------------------
Eval num_timesteps=272904, episode_reward=-65.41 +/- 88.27
Episode length: 443.20 +/- 148.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 443      |
|    mean_reward     | -65.4    |
| time/              |          |
|    total_timesteps | 272904   |
---------------------------------
Eval num_timesteps=274896, episode_reward=-92.86 +/- 72.38
Episode length: 410.80 +/- 134.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -92.9    |
| time/              |          |
|    total_timesteps | 274896   |
---------------------------------
Eval num_timesteps=276888, episode_reward=-67.32 +/- 44.53
Episode length: 489.20 +/- 92.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -67.3    |
| time/              |          |
|    total_timesteps | 276888   |
---------------------------------
Eval num_timesteps=278880, episode_reward=-112.94 +/- 43.36
Episode length: 357.20 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 357      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 278880   |
---------------------------------
Eval num_timesteps=280872, episode_reward=-78.58 +/- 112.71
Episode length: 441.00 +/- 187.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | -78.6    |
| time/              |          |
|    total_timesteps | 280872   |
---------------------------------
Eval num_timesteps=282864, episode_reward=-71.00 +/- 107.43
Episode length: 440.40 +/- 160.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | -71      |
| time/              |          |
|    total_timesteps | 282864   |
---------------------------------
Eval num_timesteps=284856, episode_reward=-63.20 +/- 41.10
Episode length: 494.20 +/- 54.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -63.2    |
| time/              |          |
|    total_timesteps | 284856   |
---------------------------------
Eval num_timesteps=286848, episode_reward=-59.72 +/- 48.93
Episode length: 471.80 +/- 53.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | -59.7    |
| time/              |          |
|    total_timesteps | 286848   |
---------------------------------
Eval num_timesteps=288840, episode_reward=-61.26 +/- 94.42
Episode length: 395.00 +/- 108.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | -61.3    |
| time/              |          |
|    total_timesteps | 288840   |
---------------------------------
Eval num_timesteps=290832, episode_reward=-143.56 +/- 48.22
Episode length: 306.20 +/- 44.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 290832   |
---------------------------------
Eval num_timesteps=292824, episode_reward=-84.54 +/- 55.41
Episode length: 400.40 +/- 66.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -84.5    |
| time/              |          |
|    total_timesteps | 292824   |
---------------------------------
Eval num_timesteps=294816, episode_reward=-3.27 +/- 145.18
Episode length: 480.20 +/- 115.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | -3.27    |
| time/              |          |
|    total_timesteps | 294816   |
---------------------------------
Eval num_timesteps=296808, episode_reward=13.30 +/- 138.98
Episode length: 552.80 +/- 122.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 296808       |
| train/                  |              |
|    approx_kl            | 0.0043285317 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 360          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.07         |
|    value_loss           | 641          |
------------------------------------------
New best mean reward!
Eval num_timesteps=298800, episode_reward=-57.26 +/- 73.46
Episode length: 516.60 +/- 150.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -57.3    |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
Eval num_timesteps=300792, episode_reward=-1.20 +/- 100.05
Episode length: 566.40 +/- 190.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 300792   |
---------------------------------
Eval num_timesteps=302784, episode_reward=7.23 +/- 124.15
Episode length: 644.00 +/- 139.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 7.23     |
| time/              |          |
|    total_timesteps | 302784   |
---------------------------------
Eval num_timesteps=304776, episode_reward=-60.19 +/- 109.63
Episode length: 542.60 +/- 89.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | -60.2    |
| time/              |          |
|    total_timesteps | 304776   |
---------------------------------
Eval num_timesteps=306768, episode_reward=-55.05 +/- 163.22
Episode length: 503.20 +/- 262.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 503      |
|    mean_reward     | -55      |
| time/              |          |
|    total_timesteps | 306768   |
---------------------------------
Eval num_timesteps=308760, episode_reward=-131.01 +/- 108.72
Episode length: 419.40 +/- 137.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 308760   |
---------------------------------
Eval num_timesteps=310752, episode_reward=-24.90 +/- 74.26
Episode length: 551.20 +/- 68.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | -24.9    |
| time/              |          |
|    total_timesteps | 310752   |
---------------------------------
Eval num_timesteps=312744, episode_reward=-22.93 +/- 121.80
Episode length: 479.40 +/- 118.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 479      |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 312744   |
---------------------------------
Eval num_timesteps=314736, episode_reward=-76.14 +/- 63.76
Episode length: 568.00 +/- 135.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | -76.1    |
| time/              |          |
|    total_timesteps | 314736   |
---------------------------------
Eval num_timesteps=316728, episode_reward=-106.53 +/- 35.85
Episode length: 419.40 +/- 138.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 316728   |
---------------------------------
Eval num_timesteps=318720, episode_reward=-77.68 +/- 68.43
Episode length: 428.80 +/- 155.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | -77.7    |
| time/              |          |
|    total_timesteps | 318720   |
---------------------------------
Eval num_timesteps=320712, episode_reward=125.24 +/- 241.93
Episode length: 687.60 +/- 186.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 320712   |
---------------------------------
New best mean reward!
Eval num_timesteps=322704, episode_reward=-135.47 +/- 38.93
Episode length: 457.00 +/- 161.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 322704   |
---------------------------------
Eval num_timesteps=324696, episode_reward=96.35 +/- 208.51
Episode length: 619.40 +/- 141.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 96.3     |
| time/              |          |
|    total_timesteps | 324696   |
---------------------------------
Eval num_timesteps=326688, episode_reward=60.51 +/- 178.13
Episode length: 534.40 +/- 181.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 60.5     |
| time/              |          |
|    total_timesteps | 326688   |
---------------------------------
Eval num_timesteps=328680, episode_reward=-75.73 +/- 118.02
Episode length: 422.20 +/- 124.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -75.7    |
| time/              |          |
|    total_timesteps | 328680   |
---------------------------------
Eval num_timesteps=330672, episode_reward=53.00 +/- 181.11
Episode length: 555.80 +/- 149.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 53       |
| time/              |          |
|    total_timesteps | 330672   |
---------------------------------
Eval num_timesteps=332664, episode_reward=9.30 +/- 216.11
Episode length: 596.40 +/- 191.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 9.3      |
| time/              |          |
|    total_timesteps | 332664   |
---------------------------------
Eval num_timesteps=334656, episode_reward=-13.77 +/- 90.60
Episode length: 580.00 +/- 172.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | -13.8    |
| time/              |          |
|    total_timesteps | 334656   |
---------------------------------
Eval num_timesteps=336648, episode_reward=-29.95 +/- 88.41
Episode length: 488.60 +/- 159.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | -30      |
| time/              |          |
|    total_timesteps | 336648   |
---------------------------------
Eval num_timesteps=338640, episode_reward=-1.31 +/- 159.04
Episode length: 475.80 +/- 180.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -1.31    |
| time/              |          |
|    total_timesteps | 338640   |
---------------------------------
Eval num_timesteps=340632, episode_reward=-37.02 +/- 59.98
Episode length: 466.60 +/- 137.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 340632   |
---------------------------------
Eval num_timesteps=342624, episode_reward=67.04 +/- 191.76
Episode length: 639.20 +/- 300.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 67       |
| time/              |          |
|    total_timesteps | 342624   |
---------------------------------
Eval num_timesteps=344616, episode_reward=30.44 +/- 146.47
Episode length: 686.40 +/- 387.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 686         |
|    mean_reward          | 30.4        |
| time/                   |             |
|    total_timesteps      | 344616      |
| train/                  |             |
|    approx_kl            | 0.004280772 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.001       |
|    loss                 | 213         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00176    |
|    std                  | 1.08        |
|    value_loss           | 492         |
-----------------------------------------
Eval num_timesteps=346608, episode_reward=-47.58 +/- 70.98
Episode length: 626.20 +/- 143.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | -47.6    |
| time/              |          |
|    total_timesteps | 346608   |
---------------------------------
Eval num_timesteps=348600, episode_reward=28.65 +/- 124.59
Episode length: 596.40 +/- 157.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 348600   |
---------------------------------
Eval num_timesteps=350592, episode_reward=-15.79 +/- 176.48
Episode length: 616.60 +/- 154.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | -15.8    |
| time/              |          |
|    total_timesteps | 350592   |
---------------------------------
Eval num_timesteps=352584, episode_reward=69.83 +/- 87.74
Episode length: 774.80 +/- 294.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 69.8     |
| time/              |          |
|    total_timesteps | 352584   |
---------------------------------
Eval num_timesteps=354576, episode_reward=-68.65 +/- 86.75
Episode length: 542.60 +/- 95.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | -68.6    |
| time/              |          |
|    total_timesteps | 354576   |
---------------------------------
Eval num_timesteps=356568, episode_reward=27.60 +/- 129.22
Episode length: 561.00 +/- 185.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 27.6     |
| time/              |          |
|    total_timesteps | 356568   |
---------------------------------
Eval num_timesteps=358560, episode_reward=-20.80 +/- 157.24
Episode length: 481.80 +/- 103.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | -20.8    |
| time/              |          |
|    total_timesteps | 358560   |
---------------------------------
Eval num_timesteps=360552, episode_reward=-77.20 +/- 95.10
Episode length: 433.20 +/- 125.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | -77.2    |
| time/              |          |
|    total_timesteps | 360552   |
---------------------------------
Eval num_timesteps=362544, episode_reward=37.18 +/- 151.42
Episode length: 638.60 +/- 93.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 37.2     |
| time/              |          |
|    total_timesteps | 362544   |
---------------------------------
Eval num_timesteps=364536, episode_reward=-28.38 +/- 194.95
Episode length: 569.00 +/- 204.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 364536   |
---------------------------------
Eval num_timesteps=366528, episode_reward=53.83 +/- 121.10
Episode length: 578.20 +/- 219.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 53.8     |
| time/              |          |
|    total_timesteps | 366528   |
---------------------------------
Eval num_timesteps=368520, episode_reward=-53.86 +/- 105.69
Episode length: 489.60 +/- 133.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | -53.9    |
| time/              |          |
|    total_timesteps | 368520   |
---------------------------------
Eval num_timesteps=370512, episode_reward=-92.16 +/- 43.30
Episode length: 521.60 +/- 159.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | -92.2    |
| time/              |          |
|    total_timesteps | 370512   |
---------------------------------
Eval num_timesteps=372504, episode_reward=-91.93 +/- 49.75
Episode length: 535.20 +/- 152.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | -91.9    |
| time/              |          |
|    total_timesteps | 372504   |
---------------------------------
Eval num_timesteps=374496, episode_reward=-9.69 +/- 105.97
Episode length: 646.60 +/- 134.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | -9.69    |
| time/              |          |
|    total_timesteps | 374496   |
---------------------------------
Eval num_timesteps=376488, episode_reward=412.31 +/- 415.77
Episode length: 817.40 +/- 191.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 376488   |
---------------------------------
New best mean reward!
Eval num_timesteps=378480, episode_reward=55.38 +/- 116.79
Episode length: 539.80 +/- 121.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 55.4     |
| time/              |          |
|    total_timesteps | 378480   |
---------------------------------
Eval num_timesteps=380472, episode_reward=150.20 +/- 194.89
Episode length: 785.80 +/- 177.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 380472   |
---------------------------------
Eval num_timesteps=382464, episode_reward=5.29 +/- 202.73
Episode length: 542.40 +/- 232.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | 5.29     |
| time/              |          |
|    total_timesteps | 382464   |
---------------------------------
Eval num_timesteps=384456, episode_reward=139.13 +/- 239.91
Episode length: 850.20 +/- 258.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 384456   |
---------------------------------
Eval num_timesteps=386448, episode_reward=99.70 +/- 199.07
Episode length: 654.60 +/- 153.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 99.7     |
| time/              |          |
|    total_timesteps | 386448   |
---------------------------------
Eval num_timesteps=388440, episode_reward=-19.97 +/- 54.49
Episode length: 582.00 +/- 143.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 388440   |
---------------------------------
Eval num_timesteps=390432, episode_reward=-6.35 +/- 83.72
Episode length: 495.40 +/- 170.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -6.35    |
| time/              |          |
|    total_timesteps | 390432   |
---------------------------------
Eval num_timesteps=392424, episode_reward=78.91 +/- 149.58
Episode length: 748.00 +/- 302.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 78.9     |
| time/              |          |
|    total_timesteps | 392424   |
---------------------------------
Eval num_timesteps=394416, episode_reward=209.59 +/- 142.35
Episode length: 663.00 +/- 206.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 663          |
|    mean_reward          | 210          |
| time/                   |              |
|    total_timesteps      | 394416       |
| train/                  |              |
|    approx_kl            | 0.0051307133 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 184          |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.1          |
|    value_loss           | 397          |
------------------------------------------
Eval num_timesteps=396408, episode_reward=115.58 +/- 75.00
Episode length: 691.20 +/- 135.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 116      |
| time/              |          |
|    total_timesteps | 396408   |
---------------------------------
Eval num_timesteps=398400, episode_reward=330.66 +/- 331.37
Episode length: 750.00 +/- 188.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=400392, episode_reward=123.03 +/- 174.90
Episode length: 674.80 +/- 126.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 123      |
| time/              |          |
|    total_timesteps | 400392   |
---------------------------------
Eval num_timesteps=402384, episode_reward=205.01 +/- 511.25
Episode length: 693.80 +/- 391.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 402384   |
---------------------------------
Eval num_timesteps=404376, episode_reward=-4.80 +/- 126.87
Episode length: 606.40 +/- 136.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 404376   |
---------------------------------
Eval num_timesteps=406368, episode_reward=66.82 +/- 133.09
Episode length: 779.60 +/- 286.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 66.8     |
| time/              |          |
|    total_timesteps | 406368   |
---------------------------------
Eval num_timesteps=408360, episode_reward=44.37 +/- 100.73
Episode length: 590.00 +/- 79.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 44.4     |
| time/              |          |
|    total_timesteps | 408360   |
---------------------------------
Eval num_timesteps=410352, episode_reward=-33.96 +/- 106.09
Episode length: 584.00 +/- 235.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | -34      |
| time/              |          |
|    total_timesteps | 410352   |
---------------------------------
Eval num_timesteps=412344, episode_reward=62.47 +/- 163.82
Episode length: 725.00 +/- 179.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 412344   |
---------------------------------
Eval num_timesteps=414336, episode_reward=136.09 +/- 62.39
Episode length: 778.00 +/- 136.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 414336   |
---------------------------------
Eval num_timesteps=416328, episode_reward=161.14 +/- 128.14
Episode length: 784.40 +/- 290.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 416328   |
---------------------------------
Eval num_timesteps=418320, episode_reward=79.97 +/- 103.28
Episode length: 696.80 +/- 263.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 80       |
| time/              |          |
|    total_timesteps | 418320   |
---------------------------------
Eval num_timesteps=420312, episode_reward=45.37 +/- 118.86
Episode length: 629.60 +/- 72.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 45.4     |
| time/              |          |
|    total_timesteps | 420312   |
---------------------------------
Eval num_timesteps=422304, episode_reward=0.33 +/- 176.38
Episode length: 569.20 +/- 146.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 0.335    |
| time/              |          |
|    total_timesteps | 422304   |
---------------------------------
Eval num_timesteps=424296, episode_reward=-89.73 +/- 54.12
Episode length: 450.20 +/- 182.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -89.7    |
| time/              |          |
|    total_timesteps | 424296   |
---------------------------------
Eval num_timesteps=426288, episode_reward=14.50 +/- 124.03
Episode length: 508.60 +/- 98.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 426288   |
---------------------------------
Eval num_timesteps=428280, episode_reward=67.42 +/- 128.92
Episode length: 629.20 +/- 74.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 67.4     |
| time/              |          |
|    total_timesteps | 428280   |
---------------------------------
Eval num_timesteps=430272, episode_reward=41.44 +/- 130.23
Episode length: 676.60 +/- 243.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 41.4     |
| time/              |          |
|    total_timesteps | 430272   |
---------------------------------
Eval num_timesteps=432264, episode_reward=273.00 +/- 273.69
Episode length: 895.60 +/- 170.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 432264   |
---------------------------------
Eval num_timesteps=434256, episode_reward=-77.74 +/- 66.91
Episode length: 621.40 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -77.7    |
| time/              |          |
|    total_timesteps | 434256   |
---------------------------------
Eval num_timesteps=436248, episode_reward=215.65 +/- 261.44
Episode length: 742.20 +/- 265.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 436248   |
---------------------------------
Eval num_timesteps=438240, episode_reward=270.35 +/- 216.26
Episode length: 910.80 +/- 158.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 438240   |
---------------------------------
Eval num_timesteps=440232, episode_reward=106.92 +/- 227.93
Episode length: 662.40 +/- 115.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 440232   |
---------------------------------
Eval num_timesteps=442224, episode_reward=75.85 +/- 106.44
Episode length: 740.80 +/- 66.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 75.9     |
| time/              |          |
|    total_timesteps | 442224   |
---------------------------------
Eval num_timesteps=444216, episode_reward=324.60 +/- 245.59
Episode length: 823.60 +/- 218.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 824          |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 444216       |
| train/                  |              |
|    approx_kl            | 0.0043234522 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.11         |
|    value_loss           | 318          |
------------------------------------------
Eval num_timesteps=446208, episode_reward=169.10 +/- 252.68
Episode length: 625.40 +/- 138.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 446208   |
---------------------------------
Eval num_timesteps=448200, episode_reward=106.61 +/- 382.78
Episode length: 576.80 +/- 208.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 448200   |
---------------------------------
Eval num_timesteps=450192, episode_reward=99.68 +/- 93.23
Episode length: 748.60 +/- 243.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 99.7     |
| time/              |          |
|    total_timesteps | 450192   |
---------------------------------
Eval num_timesteps=452184, episode_reward=188.14 +/- 235.23
Episode length: 704.60 +/- 199.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 452184   |
---------------------------------
Eval num_timesteps=454176, episode_reward=111.39 +/- 70.70
Episode length: 695.00 +/- 46.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 454176   |
---------------------------------
Eval num_timesteps=456168, episode_reward=268.89 +/- 314.16
Episode length: 898.00 +/- 268.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 456168   |
---------------------------------
Eval num_timesteps=458160, episode_reward=42.12 +/- 114.36
Episode length: 568.20 +/- 126.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 42.1     |
| time/              |          |
|    total_timesteps | 458160   |
---------------------------------
Eval num_timesteps=460152, episode_reward=231.14 +/- 318.76
Episode length: 1052.00 +/- 384.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 460152   |
---------------------------------
Eval num_timesteps=462144, episode_reward=-22.19 +/- 111.33
Episode length: 576.00 +/- 148.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 462144   |
---------------------------------
Eval num_timesteps=464136, episode_reward=159.58 +/- 220.20
Episode length: 953.20 +/- 388.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 464136   |
---------------------------------
Eval num_timesteps=466128, episode_reward=152.28 +/- 242.43
Episode length: 656.40 +/- 202.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 466128   |
---------------------------------
Eval num_timesteps=468120, episode_reward=154.06 +/- 338.31
Episode length: 705.20 +/- 231.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 154      |
| time/              |          |
|    total_timesteps | 468120   |
---------------------------------
Eval num_timesteps=470112, episode_reward=32.14 +/- 142.34
Episode length: 746.20 +/- 175.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 32.1     |
| time/              |          |
|    total_timesteps | 470112   |
---------------------------------
Eval num_timesteps=472104, episode_reward=276.01 +/- 457.67
Episode length: 921.00 +/- 242.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 472104   |
---------------------------------
Eval num_timesteps=474096, episode_reward=71.09 +/- 129.74
Episode length: 707.60 +/- 96.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 71.1     |
| time/              |          |
|    total_timesteps | 474096   |
---------------------------------
Eval num_timesteps=476088, episode_reward=329.87 +/- 279.57
Episode length: 822.60 +/- 71.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 476088   |
---------------------------------
Eval num_timesteps=478080, episode_reward=378.87 +/- 565.23
Episode length: 875.60 +/- 230.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 478080   |
---------------------------------
Eval num_timesteps=480072, episode_reward=227.98 +/- 191.95
Episode length: 874.80 +/- 350.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 480072   |
---------------------------------
Eval num_timesteps=482064, episode_reward=317.22 +/- 420.85
Episode length: 921.00 +/- 59.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 482064   |
---------------------------------
Eval num_timesteps=484056, episode_reward=438.79 +/- 396.07
Episode length: 773.20 +/- 161.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 484056   |
---------------------------------
New best mean reward!
Eval num_timesteps=486048, episode_reward=339.91 +/- 493.82
Episode length: 873.40 +/- 292.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 486048   |
---------------------------------
Eval num_timesteps=488040, episode_reward=438.24 +/- 361.44
Episode length: 884.20 +/- 164.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 884      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 488040   |
---------------------------------
Eval num_timesteps=490032, episode_reward=276.46 +/- 185.98
Episode length: 802.20 +/- 111.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 490032   |
---------------------------------
Eval num_timesteps=492024, episode_reward=30.30 +/- 263.73
Episode length: 675.40 +/- 143.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 30.3         |
| time/                   |              |
|    total_timesteps      | 492024       |
| train/                  |              |
|    approx_kl            | 0.0038974397 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.12         |
|    value_loss           | 279          |
------------------------------------------
Eval num_timesteps=494016, episode_reward=98.11 +/- 198.96
Episode length: 557.60 +/- 141.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 98.1     |
| time/              |          |
|    total_timesteps | 494016   |
---------------------------------
Eval num_timesteps=496008, episode_reward=227.96 +/- 118.44
Episode length: 931.40 +/- 93.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 496008   |
---------------------------------
Eval num_timesteps=498000, episode_reward=128.13 +/- 251.68
Episode length: 845.40 +/- 194.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 128      |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=499992, episode_reward=565.99 +/- 430.38
Episode length: 947.60 +/- 230.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 499992   |
---------------------------------
New best mean reward!
Eval num_timesteps=501984, episode_reward=363.45 +/- 539.02
Episode length: 1002.00 +/- 191.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 501984   |
---------------------------------
Eval num_timesteps=503976, episode_reward=229.75 +/- 351.47
Episode length: 700.80 +/- 280.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 503976   |
---------------------------------
Eval num_timesteps=505968, episode_reward=517.85 +/- 1117.33
Episode length: 788.60 +/- 315.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 505968   |
---------------------------------
Eval num_timesteps=507960, episode_reward=142.52 +/- 168.17
Episode length: 704.40 +/- 83.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 507960   |
---------------------------------
Eval num_timesteps=509952, episode_reward=600.95 +/- 525.88
Episode length: 1001.60 +/- 245.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 509952   |
---------------------------------
New best mean reward!
Eval num_timesteps=511944, episode_reward=135.54 +/- 72.83
Episode length: 728.60 +/- 107.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 511944   |
---------------------------------
Eval num_timesteps=513936, episode_reward=91.86 +/- 139.87
Episode length: 968.40 +/- 347.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 91.9     |
| time/              |          |
|    total_timesteps | 513936   |
---------------------------------
Eval num_timesteps=515928, episode_reward=558.01 +/- 813.58
Episode length: 968.20 +/- 236.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 515928   |
---------------------------------
Eval num_timesteps=517920, episode_reward=9.70 +/- 126.45
Episode length: 641.80 +/- 184.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 9.7      |
| time/              |          |
|    total_timesteps | 517920   |
---------------------------------
Eval num_timesteps=519912, episode_reward=357.08 +/- 329.19
Episode length: 874.00 +/- 126.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 519912   |
---------------------------------
Eval num_timesteps=521904, episode_reward=386.18 +/- 261.53
Episode length: 1141.80 +/- 343.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 521904   |
---------------------------------
Eval num_timesteps=523896, episode_reward=155.03 +/- 220.80
Episode length: 790.60 +/- 375.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 523896   |
---------------------------------
Eval num_timesteps=525888, episode_reward=22.82 +/- 59.10
Episode length: 719.60 +/- 182.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 525888   |
---------------------------------
Eval num_timesteps=527880, episode_reward=31.32 +/- 182.09
Episode length: 777.80 +/- 249.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 31.3     |
| time/              |          |
|    total_timesteps | 527880   |
---------------------------------
Eval num_timesteps=529872, episode_reward=519.34 +/- 629.27
Episode length: 885.20 +/- 123.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 529872   |
---------------------------------
Eval num_timesteps=531864, episode_reward=306.63 +/- 255.50
Episode length: 973.20 +/- 281.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 531864   |
---------------------------------
Eval num_timesteps=533856, episode_reward=432.07 +/- 421.17
Episode length: 787.40 +/- 186.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 533856   |
---------------------------------
Eval num_timesteps=535848, episode_reward=155.99 +/- 236.02
Episode length: 791.80 +/- 349.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 535848   |
---------------------------------
Eval num_timesteps=537840, episode_reward=525.84 +/- 572.80
Episode length: 1068.20 +/- 326.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 537840   |
---------------------------------
Eval num_timesteps=539832, episode_reward=54.79 +/- 126.25
Episode length: 627.00 +/- 231.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 54.8     |
| time/              |          |
|    total_timesteps | 539832   |
---------------------------------
Eval num_timesteps=541824, episode_reward=521.15 +/- 435.96
Episode length: 1131.60 +/- 351.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 521         |
| time/                   |             |
|    total_timesteps      | 541824      |
| train/                  |             |
|    approx_kl            | 0.004145916 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.001       |
|    loss                 | 99.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.13        |
|    value_loss           | 271         |
-----------------------------------------
Eval num_timesteps=543816, episode_reward=417.52 +/- 368.65
Episode length: 812.40 +/- 112.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 543816   |
---------------------------------
Eval num_timesteps=545808, episode_reward=353.17 +/- 411.41
Episode length: 1086.20 +/- 210.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 545808   |
---------------------------------
Eval num_timesteps=547800, episode_reward=105.82 +/- 92.81
Episode length: 781.60 +/- 142.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 547800   |
---------------------------------
Eval num_timesteps=549792, episode_reward=260.98 +/- 230.81
Episode length: 810.00 +/- 134.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 549792   |
---------------------------------
Eval num_timesteps=551784, episode_reward=396.97 +/- 281.52
Episode length: 970.80 +/- 261.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 551784   |
---------------------------------
Eval num_timesteps=553776, episode_reward=122.09 +/- 172.09
Episode length: 827.20 +/- 109.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 553776   |
---------------------------------
Eval num_timesteps=555768, episode_reward=353.61 +/- 354.04
Episode length: 809.20 +/- 81.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 555768   |
---------------------------------
Eval num_timesteps=557760, episode_reward=476.15 +/- 875.67
Episode length: 738.40 +/- 155.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 557760   |
---------------------------------
Eval num_timesteps=559752, episode_reward=211.88 +/- 147.31
Episode length: 935.60 +/- 119.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 559752   |
---------------------------------
Eval num_timesteps=561744, episode_reward=143.46 +/- 194.51
Episode length: 823.60 +/- 124.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 561744   |
---------------------------------
Eval num_timesteps=563736, episode_reward=279.76 +/- 324.06
Episode length: 939.80 +/- 189.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 563736   |
---------------------------------
Eval num_timesteps=565728, episode_reward=-96.30 +/- 87.76
Episode length: 560.60 +/- 191.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | -96.3    |
| time/              |          |
|    total_timesteps | 565728   |
---------------------------------
Eval num_timesteps=567720, episode_reward=265.05 +/- 135.03
Episode length: 1003.80 +/- 158.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 567720   |
---------------------------------
Eval num_timesteps=569712, episode_reward=557.52 +/- 761.57
Episode length: 876.00 +/- 366.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 569712   |
---------------------------------
Eval num_timesteps=571704, episode_reward=329.98 +/- 186.98
Episode length: 1004.60 +/- 114.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 571704   |
---------------------------------
Eval num_timesteps=573696, episode_reward=146.77 +/- 154.38
Episode length: 924.60 +/- 206.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 573696   |
---------------------------------
Eval num_timesteps=575688, episode_reward=298.28 +/- 380.20
Episode length: 952.60 +/- 202.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 575688   |
---------------------------------
Eval num_timesteps=577680, episode_reward=759.20 +/- 605.48
Episode length: 910.80 +/- 160.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 577680   |
---------------------------------
New best mean reward!
Eval num_timesteps=579672, episode_reward=11.90 +/- 138.86
Episode length: 724.40 +/- 108.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 579672   |
---------------------------------
Eval num_timesteps=581664, episode_reward=210.54 +/- 181.70
Episode length: 927.20 +/- 234.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 581664   |
---------------------------------
Eval num_timesteps=583656, episode_reward=71.17 +/- 205.64
Episode length: 674.80 +/- 218.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 71.2     |
| time/              |          |
|    total_timesteps | 583656   |
---------------------------------
Eval num_timesteps=585648, episode_reward=191.08 +/- 182.71
Episode length: 915.20 +/- 209.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 915      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 585648   |
---------------------------------
Eval num_timesteps=587640, episode_reward=225.00 +/- 174.16
Episode length: 900.40 +/- 184.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 587640   |
---------------------------------
Eval num_timesteps=589632, episode_reward=39.77 +/- 61.49
Episode length: 697.80 +/- 185.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 39.8     |
| time/              |          |
|    total_timesteps | 589632   |
---------------------------------
Eval num_timesteps=591624, episode_reward=807.49 +/- 819.31
Episode length: 984.80 +/- 314.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 985          |
|    mean_reward          | 807          |
| time/                   |              |
|    total_timesteps      | 591624       |
| train/                  |              |
|    approx_kl            | 0.0047262926 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 125          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.15         |
|    value_loss           | 333          |
------------------------------------------
New best mean reward!
Eval num_timesteps=593616, episode_reward=341.81 +/- 291.06
Episode length: 964.00 +/- 239.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 593616   |
---------------------------------
Eval num_timesteps=595608, episode_reward=184.38 +/- 175.09
Episode length: 938.60 +/- 365.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 595608   |
---------------------------------
Eval num_timesteps=597600, episode_reward=328.52 +/- 237.63
Episode length: 871.80 +/- 252.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 597600   |
---------------------------------
Eval num_timesteps=599592, episode_reward=456.44 +/- 471.03
Episode length: 949.20 +/- 277.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 599592   |
---------------------------------
Eval num_timesteps=601584, episode_reward=622.27 +/- 809.10
Episode length: 1032.60 +/- 131.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 601584   |
---------------------------------
Eval num_timesteps=603576, episode_reward=161.36 +/- 256.16
Episode length: 756.60 +/- 150.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 603576   |
---------------------------------
Eval num_timesteps=605568, episode_reward=475.15 +/- 352.95
Episode length: 945.60 +/- 161.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 605568   |
---------------------------------
Eval num_timesteps=607560, episode_reward=115.37 +/- 105.51
Episode length: 824.60 +/- 204.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 607560   |
---------------------------------
Eval num_timesteps=609552, episode_reward=431.87 +/- 699.24
Episode length: 783.80 +/- 300.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 609552   |
---------------------------------
Eval num_timesteps=611544, episode_reward=225.89 +/- 103.40
Episode length: 849.40 +/- 231.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 611544   |
---------------------------------
Eval num_timesteps=613536, episode_reward=445.14 +/- 621.16
Episode length: 882.00 +/- 254.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 613536   |
---------------------------------
Eval num_timesteps=615528, episode_reward=135.42 +/- 194.25
Episode length: 691.80 +/- 166.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 135      |
| time/              |          |
|    total_timesteps | 615528   |
---------------------------------
Eval num_timesteps=617520, episode_reward=219.13 +/- 464.24
Episode length: 728.40 +/- 172.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 617520   |
---------------------------------
Eval num_timesteps=619512, episode_reward=457.30 +/- 560.71
Episode length: 1046.00 +/- 238.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 619512   |
---------------------------------
Eval num_timesteps=621504, episode_reward=418.41 +/- 413.65
Episode length: 948.80 +/- 200.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 621504   |
---------------------------------
Eval num_timesteps=623496, episode_reward=317.29 +/- 392.67
Episode length: 978.20 +/- 298.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 623496   |
---------------------------------
Eval num_timesteps=625488, episode_reward=167.39 +/- 187.45
Episode length: 858.40 +/- 173.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 858      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 625488   |
---------------------------------
Eval num_timesteps=627480, episode_reward=60.66 +/- 178.11
Episode length: 637.20 +/- 96.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 60.7     |
| time/              |          |
|    total_timesteps | 627480   |
---------------------------------
Eval num_timesteps=629472, episode_reward=241.60 +/- 127.91
Episode length: 898.80 +/- 114.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 629472   |
---------------------------------
Eval num_timesteps=631464, episode_reward=260.40 +/- 331.32
Episode length: 840.00 +/- 202.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 631464   |
---------------------------------
Eval num_timesteps=633456, episode_reward=231.07 +/- 320.98
Episode length: 710.40 +/- 161.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 633456   |
---------------------------------
Eval num_timesteps=635448, episode_reward=223.28 +/- 182.96
Episode length: 786.40 +/- 187.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 635448   |
---------------------------------
Eval num_timesteps=637440, episode_reward=143.86 +/- 122.93
Episode length: 937.80 +/- 236.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 144      |
| time/              |          |
|    total_timesteps | 637440   |
---------------------------------
Eval num_timesteps=639432, episode_reward=283.69 +/- 430.85
Episode length: 796.20 +/- 173.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 796         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 639432      |
| train/                  |             |
|    approx_kl            | 0.004008304 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.28       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.001       |
|    loss                 | 82.6        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00119    |
|    std                  | 1.17        |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=641424, episode_reward=546.01 +/- 367.27
Episode length: 904.20 +/- 150.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 641424   |
---------------------------------
Eval num_timesteps=643416, episode_reward=697.55 +/- 1088.23
Episode length: 834.20 +/- 109.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 643416   |
---------------------------------
Eval num_timesteps=645408, episode_reward=231.96 +/- 282.32
Episode length: 768.80 +/- 104.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 645408   |
---------------------------------
Eval num_timesteps=647400, episode_reward=260.98 +/- 319.34
Episode length: 808.00 +/- 205.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 647400   |
---------------------------------
Eval num_timesteps=649392, episode_reward=309.06 +/- 291.29
Episode length: 829.60 +/- 135.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 649392   |
---------------------------------
Eval num_timesteps=651384, episode_reward=564.54 +/- 328.93
Episode length: 999.40 +/- 175.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 651384   |
---------------------------------
Eval num_timesteps=653376, episode_reward=30.50 +/- 60.14
Episode length: 616.60 +/- 103.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 30.5     |
| time/              |          |
|    total_timesteps | 653376   |
---------------------------------
Eval num_timesteps=655368, episode_reward=139.63 +/- 146.49
Episode length: 722.20 +/- 185.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 655368   |
---------------------------------
Eval num_timesteps=657360, episode_reward=742.60 +/- 1178.91
Episode length: 795.00 +/- 108.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 657360   |
---------------------------------
Eval num_timesteps=659352, episode_reward=260.52 +/- 405.58
Episode length: 862.80 +/- 252.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 659352   |
---------------------------------
Eval num_timesteps=661344, episode_reward=487.85 +/- 533.79
Episode length: 860.80 +/- 283.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 661344   |
---------------------------------
Eval num_timesteps=663336, episode_reward=329.44 +/- 422.17
Episode length: 819.80 +/- 230.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 663336   |
---------------------------------
Eval num_timesteps=665328, episode_reward=679.26 +/- 716.64
Episode length: 989.00 +/- 197.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 989      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 665328   |
---------------------------------
Eval num_timesteps=667320, episode_reward=904.37 +/- 1057.29
Episode length: 865.40 +/- 139.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 667320   |
---------------------------------
New best mean reward!
Eval num_timesteps=669312, episode_reward=744.92 +/- 720.32
Episode length: 789.80 +/- 172.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 669312   |
---------------------------------
Eval num_timesteps=671304, episode_reward=714.86 +/- 668.56
Episode length: 876.80 +/- 178.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 671304   |
---------------------------------
Eval num_timesteps=673296, episode_reward=812.46 +/- 458.06
Episode length: 913.40 +/- 194.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 673296   |
---------------------------------
Eval num_timesteps=675288, episode_reward=252.20 +/- 315.53
Episode length: 762.80 +/- 147.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 675288   |
---------------------------------
Eval num_timesteps=677280, episode_reward=528.23 +/- 372.32
Episode length: 1080.40 +/- 289.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 677280   |
---------------------------------
Eval num_timesteps=679272, episode_reward=694.34 +/- 572.24
Episode length: 910.20 +/- 247.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 679272   |
---------------------------------
Eval num_timesteps=681264, episode_reward=323.25 +/- 369.43
Episode length: 798.80 +/- 240.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 681264   |
---------------------------------
Eval num_timesteps=683256, episode_reward=518.00 +/- 598.85
Episode length: 830.80 +/- 253.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 683256   |
---------------------------------
Eval num_timesteps=685248, episode_reward=441.53 +/- 354.05
Episode length: 830.60 +/- 109.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 685248   |
---------------------------------
Eval num_timesteps=687240, episode_reward=155.99 +/- 106.48
Episode length: 688.00 +/- 99.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 687240   |
---------------------------------
Eval num_timesteps=689232, episode_reward=453.12 +/- 183.29
Episode length: 897.20 +/- 37.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 897          |
|    mean_reward          | 453          |
| time/                   |              |
|    total_timesteps      | 689232       |
| train/                  |              |
|    approx_kl            | 0.0046611833 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.31        |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.001        |
|    loss                 | 184          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.17         |
|    value_loss           | 429          |
------------------------------------------
Eval num_timesteps=691224, episode_reward=489.31 +/- 245.53
Episode length: 674.60 +/- 91.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 691224   |
---------------------------------
Eval num_timesteps=693216, episode_reward=971.48 +/- 1116.74
Episode length: 888.80 +/- 182.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 693216   |
---------------------------------
New best mean reward!
Eval num_timesteps=695208, episode_reward=772.22 +/- 257.27
Episode length: 941.00 +/- 182.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 695208   |
---------------------------------
Eval num_timesteps=697200, episode_reward=523.11 +/- 355.91
Episode length: 882.20 +/- 167.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 697200   |
---------------------------------
Eval num_timesteps=699192, episode_reward=772.15 +/- 839.45
Episode length: 843.60 +/- 240.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 699192   |
---------------------------------
Eval num_timesteps=701184, episode_reward=640.67 +/- 339.09
Episode length: 857.00 +/- 107.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 701184   |
---------------------------------
Eval num_timesteps=703176, episode_reward=502.09 +/- 511.81
Episode length: 848.60 +/- 254.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 703176   |
---------------------------------
Eval num_timesteps=705168, episode_reward=405.48 +/- 302.56
Episode length: 857.40 +/- 256.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 705168   |
---------------------------------
Eval num_timesteps=707160, episode_reward=838.56 +/- 416.65
Episode length: 886.60 +/- 177.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 707160   |
---------------------------------
Eval num_timesteps=709152, episode_reward=912.16 +/- 1222.58
Episode length: 805.40 +/- 238.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 709152   |
---------------------------------
Eval num_timesteps=711144, episode_reward=635.80 +/- 627.94
Episode length: 960.00 +/- 196.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 711144   |
---------------------------------
Eval num_timesteps=713136, episode_reward=321.38 +/- 241.16
Episode length: 742.40 +/- 111.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 713136   |
---------------------------------
Eval num_timesteps=715128, episode_reward=846.44 +/- 516.45
Episode length: 825.80 +/- 149.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 715128   |
---------------------------------
Eval num_timesteps=717120, episode_reward=198.25 +/- 75.72
Episode length: 813.80 +/- 88.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 717120   |
---------------------------------
Eval num_timesteps=719112, episode_reward=328.74 +/- 442.72
Episode length: 936.20 +/- 287.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 719112   |
---------------------------------
Eval num_timesteps=721104, episode_reward=723.64 +/- 875.17
Episode length: 844.20 +/- 210.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 721104   |
---------------------------------
Eval num_timesteps=723096, episode_reward=1721.98 +/- 2081.00
Episode length: 926.80 +/- 171.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 723096   |
---------------------------------
New best mean reward!
Eval num_timesteps=725088, episode_reward=347.79 +/- 526.53
Episode length: 699.20 +/- 134.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 725088   |
---------------------------------
Eval num_timesteps=727080, episode_reward=302.23 +/- 340.14
Episode length: 720.60 +/- 111.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 727080   |
---------------------------------
Eval num_timesteps=729072, episode_reward=508.60 +/- 481.97
Episode length: 797.20 +/- 139.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 729072   |
---------------------------------
Eval num_timesteps=731064, episode_reward=306.92 +/- 270.31
Episode length: 827.60 +/- 138.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 731064   |
---------------------------------
Eval num_timesteps=733056, episode_reward=756.85 +/- 747.72
Episode length: 960.80 +/- 318.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 733056   |
---------------------------------
Eval num_timesteps=735048, episode_reward=900.93 +/- 651.64
Episode length: 971.40 +/- 233.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 735048   |
---------------------------------
Eval num_timesteps=737040, episode_reward=284.81 +/- 132.44
Episode length: 749.80 +/- 142.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 737040   |
---------------------------------
Eval num_timesteps=739032, episode_reward=1042.75 +/- 881.18
Episode length: 855.80 +/- 184.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 856          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 739032       |
| train/                  |              |
|    approx_kl            | 0.0034270466 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.34        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.001        |
|    loss                 | 252          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.18         |
|    value_loss           | 600          |
------------------------------------------
Eval num_timesteps=741024, episode_reward=342.25 +/- 318.32
Episode length: 673.20 +/- 231.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 741024   |
---------------------------------
Eval num_timesteps=743016, episode_reward=418.51 +/- 396.97
Episode length: 684.00 +/- 136.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 743016   |
---------------------------------
Eval num_timesteps=745008, episode_reward=211.72 +/- 156.96
Episode length: 746.60 +/- 242.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 745008   |
---------------------------------
Eval num_timesteps=747000, episode_reward=339.11 +/- 226.34
Episode length: 795.60 +/- 346.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=748992, episode_reward=378.56 +/- 168.54
Episode length: 717.20 +/- 63.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 748992   |
---------------------------------
Eval num_timesteps=750984, episode_reward=364.70 +/- 255.70
Episode length: 761.80 +/- 92.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 750984   |
---------------------------------
Eval num_timesteps=752976, episode_reward=1193.22 +/- 630.86
Episode length: 950.80 +/- 221.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 752976   |
---------------------------------
Eval num_timesteps=754968, episode_reward=405.54 +/- 253.57
Episode length: 712.20 +/- 164.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 754968   |
---------------------------------
Eval num_timesteps=756960, episode_reward=451.03 +/- 335.75
Episode length: 760.00 +/- 124.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 756960   |
---------------------------------
Eval num_timesteps=758952, episode_reward=237.37 +/- 102.38
Episode length: 706.00 +/- 81.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 758952   |
---------------------------------
Eval num_timesteps=760944, episode_reward=380.99 +/- 265.56
Episode length: 634.40 +/- 118.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 760944   |
---------------------------------
Eval num_timesteps=762936, episode_reward=504.36 +/- 227.67
Episode length: 758.80 +/- 127.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 762936   |
---------------------------------
Eval num_timesteps=764928, episode_reward=702.20 +/- 586.94
Episode length: 919.20 +/- 143.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 764928   |
---------------------------------
Eval num_timesteps=766920, episode_reward=780.45 +/- 754.23
Episode length: 832.20 +/- 126.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 766920   |
---------------------------------
Eval num_timesteps=768912, episode_reward=804.44 +/- 754.40
Episode length: 867.60 +/- 281.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 768912   |
---------------------------------
Eval num_timesteps=770904, episode_reward=905.76 +/- 1076.75
Episode length: 791.60 +/- 188.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 770904   |
---------------------------------
Eval num_timesteps=772896, episode_reward=444.50 +/- 402.35
Episode length: 744.60 +/- 125.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 772896   |
---------------------------------
Eval num_timesteps=774888, episode_reward=454.06 +/- 270.65
Episode length: 839.80 +/- 221.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 774888   |
---------------------------------
Eval num_timesteps=776880, episode_reward=547.65 +/- 379.06
Episode length: 812.80 +/- 211.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 776880   |
---------------------------------
Eval num_timesteps=778872, episode_reward=1077.80 +/- 1018.85
Episode length: 941.00 +/- 274.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 778872   |
---------------------------------
Eval num_timesteps=780864, episode_reward=1167.39 +/- 1014.02
Episode length: 933.20 +/- 212.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 780864   |
---------------------------------
Eval num_timesteps=782856, episode_reward=648.32 +/- 427.80
Episode length: 846.00 +/- 214.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 782856   |
---------------------------------
Eval num_timesteps=784848, episode_reward=528.46 +/- 368.66
Episode length: 758.00 +/- 37.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 784848   |
---------------------------------
Eval num_timesteps=786840, episode_reward=503.05 +/- 329.13
Episode length: 731.80 +/- 132.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 732         |
|    mean_reward          | 503         |
| time/                   |             |
|    total_timesteps      | 786840      |
| train/                  |             |
|    approx_kl            | 0.004720877 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00138    |
|    std                  | 1.2         |
|    value_loss           | 275         |
-----------------------------------------
Eval num_timesteps=788832, episode_reward=157.49 +/- 69.29
Episode length: 622.20 +/- 52.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 788832   |
---------------------------------
Eval num_timesteps=790824, episode_reward=289.02 +/- 297.59
Episode length: 599.00 +/- 60.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 790824   |
---------------------------------
Eval num_timesteps=792816, episode_reward=532.26 +/- 637.09
Episode length: 705.80 +/- 167.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 792816   |
---------------------------------
Eval num_timesteps=794808, episode_reward=426.05 +/- 425.03
Episode length: 692.40 +/- 102.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 794808   |
---------------------------------
Eval num_timesteps=796800, episode_reward=188.83 +/- 250.96
Episode length: 677.20 +/- 246.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 796800   |
---------------------------------
Eval num_timesteps=798792, episode_reward=713.74 +/- 570.58
Episode length: 689.00 +/- 135.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 798792   |
---------------------------------
Eval num_timesteps=800784, episode_reward=206.69 +/- 182.28
Episode length: 620.00 +/- 53.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 800784   |
---------------------------------
Eval num_timesteps=802776, episode_reward=886.84 +/- 490.48
Episode length: 849.20 +/- 274.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 802776   |
---------------------------------
Eval num_timesteps=804768, episode_reward=567.67 +/- 511.19
Episode length: 866.60 +/- 334.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 804768   |
---------------------------------
Eval num_timesteps=806760, episode_reward=85.63 +/- 80.66
Episode length: 578.80 +/- 59.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 85.6     |
| time/              |          |
|    total_timesteps | 806760   |
---------------------------------
Eval num_timesteps=808752, episode_reward=373.26 +/- 303.85
Episode length: 678.40 +/- 54.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 808752   |
---------------------------------
Eval num_timesteps=810744, episode_reward=412.09 +/- 440.73
Episode length: 705.40 +/- 138.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 810744   |
---------------------------------
Eval num_timesteps=812736, episode_reward=359.00 +/- 502.92
Episode length: 672.20 +/- 201.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 812736   |
---------------------------------
Eval num_timesteps=814728, episode_reward=409.35 +/- 264.04
Episode length: 709.20 +/- 146.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 814728   |
---------------------------------
Eval num_timesteps=816720, episode_reward=344.99 +/- 181.24
Episode length: 718.40 +/- 37.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 816720   |
---------------------------------
Eval num_timesteps=818712, episode_reward=643.86 +/- 582.52
Episode length: 721.00 +/- 114.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 818712   |
---------------------------------
Eval num_timesteps=820704, episode_reward=571.81 +/- 428.41
Episode length: 831.80 +/- 229.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 820704   |
---------------------------------
Eval num_timesteps=822696, episode_reward=174.71 +/- 124.15
Episode length: 637.60 +/- 92.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 822696   |
---------------------------------
Eval num_timesteps=824688, episode_reward=578.24 +/- 495.73
Episode length: 724.40 +/- 90.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 824688   |
---------------------------------
Eval num_timesteps=826680, episode_reward=357.68 +/- 174.59
Episode length: 681.00 +/- 95.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 826680   |
---------------------------------
Eval num_timesteps=828672, episode_reward=355.56 +/- 411.54
Episode length: 660.60 +/- 140.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 828672   |
---------------------------------
Eval num_timesteps=830664, episode_reward=596.92 +/- 354.83
Episode length: 730.80 +/- 130.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 830664   |
---------------------------------
Eval num_timesteps=832656, episode_reward=436.73 +/- 178.88
Episode length: 742.60 +/- 105.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 832656   |
---------------------------------
Eval num_timesteps=834648, episode_reward=614.67 +/- 905.00
Episode length: 700.80 +/- 228.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 834648   |
---------------------------------
Eval num_timesteps=836640, episode_reward=400.20 +/- 430.31
Episode length: 674.20 +/- 91.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 674          |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 836640       |
| train/                  |              |
|    approx_kl            | 0.0040676203 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 96.9         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.21         |
|    value_loss           | 273          |
------------------------------------------
Eval num_timesteps=838632, episode_reward=63.09 +/- 142.77
Episode length: 521.00 +/- 88.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | 63.1     |
| time/              |          |
|    total_timesteps | 838632   |
---------------------------------
Eval num_timesteps=840624, episode_reward=552.37 +/- 543.07
Episode length: 687.00 +/- 103.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 840624   |
---------------------------------
Eval num_timesteps=842616, episode_reward=198.22 +/- 352.61
Episode length: 553.80 +/- 165.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 842616   |
---------------------------------
Eval num_timesteps=844608, episode_reward=471.56 +/- 414.18
Episode length: 715.00 +/- 258.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 844608   |
---------------------------------
Eval num_timesteps=846600, episode_reward=566.12 +/- 487.88
Episode length: 660.40 +/- 136.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 846600   |
---------------------------------
Eval num_timesteps=848592, episode_reward=726.87 +/- 561.55
Episode length: 677.80 +/- 79.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 848592   |
---------------------------------
Eval num_timesteps=850584, episode_reward=316.18 +/- 409.73
Episode length: 680.00 +/- 239.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 850584   |
---------------------------------
Eval num_timesteps=852576, episode_reward=329.34 +/- 496.80
Episode length: 622.40 +/- 142.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 852576   |
---------------------------------
Eval num_timesteps=854568, episode_reward=193.26 +/- 83.58
Episode length: 622.80 +/- 27.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 854568   |
---------------------------------
Eval num_timesteps=856560, episode_reward=540.26 +/- 618.51
Episode length: 660.40 +/- 79.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 856560   |
---------------------------------
Eval num_timesteps=858552, episode_reward=250.67 +/- 185.09
Episode length: 701.00 +/- 197.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 858552   |
---------------------------------
Eval num_timesteps=860544, episode_reward=463.65 +/- 585.61
Episode length: 641.40 +/- 124.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 860544   |
---------------------------------
Eval num_timesteps=862536, episode_reward=365.60 +/- 492.01
Episode length: 699.40 +/- 235.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 862536   |
---------------------------------
Eval num_timesteps=864528, episode_reward=358.38 +/- 359.45
Episode length: 668.80 +/- 128.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 864528   |
---------------------------------
Eval num_timesteps=866520, episode_reward=184.20 +/- 39.96
Episode length: 603.20 +/- 30.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 866520   |
---------------------------------
Eval num_timesteps=868512, episode_reward=427.80 +/- 493.23
Episode length: 672.60 +/- 152.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 868512   |
---------------------------------
Eval num_timesteps=870504, episode_reward=332.91 +/- 291.36
Episode length: 650.40 +/- 134.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 870504   |
---------------------------------
Eval num_timesteps=872496, episode_reward=397.05 +/- 135.65
Episode length: 714.00 +/- 29.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 872496   |
---------------------------------
Eval num_timesteps=874488, episode_reward=194.88 +/- 122.66
Episode length: 605.60 +/- 69.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 874488   |
---------------------------------
Eval num_timesteps=876480, episode_reward=485.43 +/- 421.03
Episode length: 681.40 +/- 92.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 876480   |
---------------------------------
Eval num_timesteps=878472, episode_reward=484.99 +/- 795.21
Episode length: 635.20 +/- 144.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 878472   |
---------------------------------
Eval num_timesteps=880464, episode_reward=256.52 +/- 120.62
Episode length: 643.80 +/- 76.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 880464   |
---------------------------------
Eval num_timesteps=882456, episode_reward=501.60 +/- 318.05
Episode length: 678.40 +/- 78.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 882456   |
---------------------------------
Eval num_timesteps=884448, episode_reward=662.90 +/- 491.50
Episode length: 717.60 +/- 134.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 884448   |
---------------------------------
Eval num_timesteps=886440, episode_reward=565.24 +/- 229.84
Episode length: 701.60 +/- 98.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 702          |
|    mean_reward          | 565          |
| time/                   |              |
|    total_timesteps      | 886440       |
| train/                  |              |
|    approx_kl            | 0.0055571995 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 46.2         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.23         |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=888432, episode_reward=635.72 +/- 482.73
Episode length: 658.60 +/- 106.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 888432   |
---------------------------------
Eval num_timesteps=890424, episode_reward=803.15 +/- 328.23
Episode length: 683.60 +/- 73.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 890424   |
---------------------------------
Eval num_timesteps=892416, episode_reward=681.67 +/- 614.30
Episode length: 694.40 +/- 131.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 892416   |
---------------------------------
Eval num_timesteps=894408, episode_reward=331.55 +/- 215.34
Episode length: 603.40 +/- 82.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 894408   |
---------------------------------
Eval num_timesteps=896400, episode_reward=350.85 +/- 27.19
Episode length: 724.40 +/- 152.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 896400   |
---------------------------------
Eval num_timesteps=898392, episode_reward=534.77 +/- 338.19
Episode length: 691.80 +/- 85.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 898392   |
---------------------------------
Eval num_timesteps=900384, episode_reward=573.29 +/- 695.60
Episode length: 796.60 +/- 294.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 900384   |
---------------------------------
Eval num_timesteps=902376, episode_reward=484.23 +/- 349.88
Episode length: 671.60 +/- 101.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 902376   |
---------------------------------
Eval num_timesteps=904368, episode_reward=950.32 +/- 372.63
Episode length: 791.80 +/- 192.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 904368   |
---------------------------------
Eval num_timesteps=906360, episode_reward=425.33 +/- 173.65
Episode length: 648.80 +/- 63.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 906360   |
---------------------------------
Eval num_timesteps=908352, episode_reward=804.00 +/- 910.54
Episode length: 641.00 +/- 135.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 908352   |
---------------------------------
Eval num_timesteps=910344, episode_reward=618.28 +/- 371.03
Episode length: 695.40 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 910344   |
---------------------------------
Eval num_timesteps=912336, episode_reward=702.50 +/- 280.53
Episode length: 879.00 +/- 143.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 912336   |
---------------------------------
Eval num_timesteps=914328, episode_reward=776.88 +/- 568.21
Episode length: 767.60 +/- 159.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 914328   |
---------------------------------
Eval num_timesteps=916320, episode_reward=418.86 +/- 265.58
Episode length: 663.60 +/- 167.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 916320   |
---------------------------------
Eval num_timesteps=918312, episode_reward=1815.63 +/- 446.65
Episode length: 852.40 +/- 56.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 918312   |
---------------------------------
New best mean reward!
Eval num_timesteps=920304, episode_reward=907.17 +/- 722.94
Episode length: 695.20 +/- 132.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 920304   |
---------------------------------
Eval num_timesteps=922296, episode_reward=1005.84 +/- 457.19
Episode length: 804.00 +/- 188.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 922296   |
---------------------------------
Eval num_timesteps=924288, episode_reward=451.83 +/- 166.71
Episode length: 736.60 +/- 81.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 924288   |
---------------------------------
Eval num_timesteps=926280, episode_reward=427.05 +/- 417.62
Episode length: 701.80 +/- 276.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 926280   |
---------------------------------
Eval num_timesteps=928272, episode_reward=650.52 +/- 322.32
Episode length: 679.00 +/- 155.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 928272   |
---------------------------------
Eval num_timesteps=930264, episode_reward=783.34 +/- 528.34
Episode length: 683.80 +/- 129.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 930264   |
---------------------------------
Eval num_timesteps=932256, episode_reward=1188.31 +/- 639.85
Episode length: 804.60 +/- 184.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 932256   |
---------------------------------
Eval num_timesteps=934248, episode_reward=577.09 +/- 225.57
Episode length: 658.60 +/- 71.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 659         |
|    mean_reward          | 577         |
| time/                   |             |
|    total_timesteps      | 934248      |
| train/                  |             |
|    approx_kl            | 0.003948243 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 55.6        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0018     |
|    std                  | 1.23        |
|    value_loss           | 182         |
-----------------------------------------
Eval num_timesteps=936240, episode_reward=693.19 +/- 396.97
Episode length: 673.00 +/- 89.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 936240   |
---------------------------------
Eval num_timesteps=938232, episode_reward=546.88 +/- 313.67
Episode length: 633.80 +/- 42.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 938232   |
---------------------------------
Eval num_timesteps=940224, episode_reward=570.23 +/- 435.19
Episode length: 703.40 +/- 194.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 940224   |
---------------------------------
Eval num_timesteps=942216, episode_reward=733.21 +/- 95.82
Episode length: 770.00 +/- 165.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 942216   |
---------------------------------
Eval num_timesteps=944208, episode_reward=658.95 +/- 379.04
Episode length: 605.00 +/- 49.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 944208   |
---------------------------------
Eval num_timesteps=946200, episode_reward=337.73 +/- 143.22
Episode length: 642.40 +/- 61.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 946200   |
---------------------------------
Eval num_timesteps=948192, episode_reward=622.93 +/- 372.88
Episode length: 664.00 +/- 74.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 948192   |
---------------------------------
Eval num_timesteps=950184, episode_reward=1134.59 +/- 812.12
Episode length: 711.40 +/- 144.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 950184   |
---------------------------------
Eval num_timesteps=952176, episode_reward=806.55 +/- 397.10
Episode length: 822.20 +/- 228.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 952176   |
---------------------------------
Eval num_timesteps=954168, episode_reward=504.15 +/- 136.02
Episode length: 646.60 +/- 69.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 954168   |
---------------------------------
Eval num_timesteps=956160, episode_reward=561.09 +/- 355.44
Episode length: 673.80 +/- 90.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 956160   |
---------------------------------
Eval num_timesteps=958152, episode_reward=893.72 +/- 255.59
Episode length: 745.60 +/- 77.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 958152   |
---------------------------------
Eval num_timesteps=960144, episode_reward=656.52 +/- 224.85
Episode length: 690.00 +/- 120.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 960144   |
---------------------------------
Eval num_timesteps=962136, episode_reward=862.90 +/- 406.13
Episode length: 720.40 +/- 95.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 962136   |
---------------------------------
Eval num_timesteps=964128, episode_reward=552.29 +/- 294.63
Episode length: 673.20 +/- 84.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 964128   |
---------------------------------
Eval num_timesteps=966120, episode_reward=602.91 +/- 270.21
Episode length: 652.40 +/- 89.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 966120   |
---------------------------------
Eval num_timesteps=968112, episode_reward=456.62 +/- 397.81
Episode length: 629.40 +/- 136.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 968112   |
---------------------------------
Eval num_timesteps=970104, episode_reward=479.65 +/- 171.64
Episode length: 667.60 +/- 123.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 970104   |
---------------------------------
Eval num_timesteps=972096, episode_reward=655.30 +/- 240.11
Episode length: 680.20 +/- 93.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 972096   |
---------------------------------
Eval num_timesteps=974088, episode_reward=699.93 +/- 366.75
Episode length: 694.20 +/- 66.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 974088   |
---------------------------------
Eval num_timesteps=976080, episode_reward=592.12 +/- 214.02
Episode length: 647.40 +/- 74.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 976080   |
---------------------------------
Eval num_timesteps=978072, episode_reward=968.49 +/- 758.88
Episode length: 721.80 +/- 164.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 978072   |
---------------------------------
Eval num_timesteps=980064, episode_reward=745.24 +/- 447.30
Episode length: 758.40 +/- 251.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 980064   |
---------------------------------
Eval num_timesteps=982056, episode_reward=780.08 +/- 342.75
Episode length: 705.00 +/- 71.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 982056   |
---------------------------------
Eval num_timesteps=984048, episode_reward=357.91 +/- 217.11
Episode length: 536.60 +/- 87.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 358          |
| time/                   |              |
|    total_timesteps      | 984048       |
| train/                  |              |
|    approx_kl            | 0.0040704478 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.52        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 116          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.23         |
|    value_loss           | 355          |
------------------------------------------
Eval num_timesteps=986040, episode_reward=549.28 +/- 209.91
Episode length: 666.80 +/- 31.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 986040   |
---------------------------------
Eval num_timesteps=988032, episode_reward=375.38 +/- 239.21
Episode length: 529.20 +/- 54.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 988032   |
---------------------------------
Eval num_timesteps=990024, episode_reward=733.82 +/- 432.99
Episode length: 629.20 +/- 91.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 990024   |
---------------------------------
Eval num_timesteps=992016, episode_reward=419.88 +/- 162.48
Episode length: 607.80 +/- 27.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 992016   |
---------------------------------
Eval num_timesteps=994008, episode_reward=398.70 +/- 174.75
Episode length: 580.20 +/- 54.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 994008   |
---------------------------------
Eval num_timesteps=996000, episode_reward=326.99 +/- 155.01
Episode length: 521.60 +/- 50.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=997992, episode_reward=649.58 +/- 217.07
Episode length: 612.60 +/- 34.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 997992   |
---------------------------------
Eval num_timesteps=999984, episode_reward=816.87 +/- 528.14
Episode length: 671.60 +/- 81.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 999984   |
---------------------------------
Eval num_timesteps=1001976, episode_reward=421.23 +/- 195.49
Episode length: 582.20 +/- 116.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 1001976  |
---------------------------------
Eval num_timesteps=1003968, episode_reward=417.15 +/- 308.53
Episode length: 651.20 +/- 61.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 1003968  |
---------------------------------
Eval num_timesteps=1005960, episode_reward=603.95 +/- 376.17
Episode length: 813.40 +/- 264.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 1005960  |
---------------------------------
Eval num_timesteps=1007952, episode_reward=650.88 +/- 394.44
Episode length: 606.00 +/- 87.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 1007952  |
---------------------------------
Eval num_timesteps=1009944, episode_reward=404.85 +/- 267.46
Episode length: 608.20 +/- 92.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 1009944  |
---------------------------------
Eval num_timesteps=1011936, episode_reward=488.21 +/- 112.68
Episode length: 606.80 +/- 20.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 1011936  |
---------------------------------
Eval num_timesteps=1013928, episode_reward=377.79 +/- 224.00
Episode length: 551.00 +/- 35.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 1013928  |
---------------------------------
Eval num_timesteps=1015920, episode_reward=530.10 +/- 321.79
Episode length: 596.00 +/- 85.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 1015920  |
---------------------------------
Eval num_timesteps=1017912, episode_reward=308.91 +/- 162.99
Episode length: 561.40 +/- 49.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 1017912  |
---------------------------------
Eval num_timesteps=1019904, episode_reward=416.27 +/- 186.70
Episode length: 577.40 +/- 66.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 1019904  |
---------------------------------
Eval num_timesteps=1021896, episode_reward=462.75 +/- 240.33
Episode length: 550.00 +/- 62.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 1021896  |
---------------------------------
Eval num_timesteps=1023888, episode_reward=323.45 +/- 313.36
Episode length: 561.80 +/- 66.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 1023888  |
---------------------------------
Eval num_timesteps=1025880, episode_reward=589.99 +/- 306.69
Episode length: 601.00 +/- 46.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1025880  |
---------------------------------
Eval num_timesteps=1027872, episode_reward=400.27 +/- 157.36
Episode length: 551.00 +/- 48.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 1027872  |
---------------------------------
Eval num_timesteps=1029864, episode_reward=537.77 +/- 360.76
Episode length: 598.00 +/- 74.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 1029864  |
---------------------------------
Eval num_timesteps=1031856, episode_reward=503.99 +/- 267.74
Episode length: 700.60 +/- 215.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 1031856  |
---------------------------------
Eval num_timesteps=1033848, episode_reward=664.09 +/- 303.50
Episode length: 603.00 +/- 76.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 603          |
|    mean_reward          | 664          |
| time/                   |              |
|    total_timesteps      | 1033848      |
| train/                  |              |
|    approx_kl            | 0.0030141238 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.53        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 26.8         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0012      |
|    std                  | 1.24         |
|    value_loss           | 68.3         |
------------------------------------------
Eval num_timesteps=1035840, episode_reward=458.58 +/- 194.35
Episode length: 573.40 +/- 33.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 1035840  |
---------------------------------
Eval num_timesteps=1037832, episode_reward=477.68 +/- 416.81
Episode length: 557.00 +/- 111.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 1037832  |
---------------------------------
Eval num_timesteps=1039824, episode_reward=471.93 +/- 393.52
Episode length: 560.80 +/- 111.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 1039824  |
---------------------------------
Eval num_timesteps=1041816, episode_reward=325.45 +/- 106.67
Episode length: 544.40 +/- 54.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 325      |
| time/              |          |
|    total_timesteps | 1041816  |
---------------------------------
Eval num_timesteps=1043808, episode_reward=440.76 +/- 252.40
Episode length: 569.20 +/- 89.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 1043808  |
---------------------------------
Eval num_timesteps=1045800, episode_reward=368.76 +/- 178.02
Episode length: 529.20 +/- 28.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 1045800  |
---------------------------------
Eval num_timesteps=1047792, episode_reward=333.19 +/- 119.11
Episode length: 549.20 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 1047792  |
---------------------------------
Eval num_timesteps=1049784, episode_reward=530.02 +/- 126.96
Episode length: 591.40 +/- 57.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 1049784  |
---------------------------------
Eval num_timesteps=1051776, episode_reward=749.09 +/- 526.88
Episode length: 600.00 +/- 127.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 1051776  |
---------------------------------
Eval num_timesteps=1053768, episode_reward=666.49 +/- 309.83
Episode length: 587.80 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 1053768  |
---------------------------------
Eval num_timesteps=1055760, episode_reward=277.61 +/- 114.78
Episode length: 529.80 +/- 78.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 1055760  |
---------------------------------
Eval num_timesteps=1057752, episode_reward=550.81 +/- 161.39
Episode length: 576.20 +/- 36.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 1057752  |
---------------------------------
Eval num_timesteps=1059744, episode_reward=629.89 +/- 371.71
Episode length: 596.60 +/- 66.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 1059744  |
---------------------------------
Eval num_timesteps=1061736, episode_reward=502.85 +/- 263.17
Episode length: 574.60 +/- 75.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 1061736  |
---------------------------------
Eval num_timesteps=1063728, episode_reward=452.84 +/- 181.59
Episode length: 571.20 +/- 68.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 1063728  |
---------------------------------
Eval num_timesteps=1065720, episode_reward=762.54 +/- 493.11
Episode length: 693.40 +/- 220.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 1065720  |
---------------------------------
Eval num_timesteps=1067712, episode_reward=292.39 +/- 142.91
Episode length: 492.80 +/- 46.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 1067712  |
---------------------------------
Eval num_timesteps=1069704, episode_reward=540.74 +/- 282.57
Episode length: 644.20 +/- 169.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 1069704  |
---------------------------------
Eval num_timesteps=1071696, episode_reward=643.87 +/- 402.88
Episode length: 574.40 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 1071696  |
---------------------------------
Eval num_timesteps=1073688, episode_reward=851.05 +/- 693.02
Episode length: 682.00 +/- 175.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 1073688  |
---------------------------------
Eval num_timesteps=1075680, episode_reward=844.43 +/- 667.51
Episode length: 732.20 +/- 263.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 1075680  |
---------------------------------
Eval num_timesteps=1077672, episode_reward=406.76 +/- 295.33
Episode length: 564.00 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 1077672  |
---------------------------------
Eval num_timesteps=1079664, episode_reward=457.13 +/- 259.74
Episode length: 571.60 +/- 44.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 1079664  |
---------------------------------
Eval num_timesteps=1081656, episode_reward=379.63 +/- 234.95
Episode length: 513.40 +/- 66.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | 380         |
| time/                   |             |
|    total_timesteps      | 1081656     |
| train/                  |             |
|    approx_kl            | 0.004342344 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 37.9        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00165    |
|    std                  | 1.24        |
|    value_loss           | 83.8        |
-----------------------------------------
Eval num_timesteps=1083648, episode_reward=475.05 +/- 166.81
Episode length: 557.40 +/- 54.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 1083648  |
---------------------------------
Eval num_timesteps=1085640, episode_reward=849.93 +/- 661.86
Episode length: 700.60 +/- 249.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 1085640  |
---------------------------------
Eval num_timesteps=1087632, episode_reward=629.34 +/- 362.92
Episode length: 551.80 +/- 81.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 1087632  |
---------------------------------
Eval num_timesteps=1089624, episode_reward=465.57 +/- 268.84
Episode length: 525.40 +/- 58.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 1089624  |
---------------------------------
Eval num_timesteps=1091616, episode_reward=423.24 +/- 56.12
Episode length: 538.40 +/- 27.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 1091616  |
---------------------------------
Eval num_timesteps=1093608, episode_reward=307.53 +/- 188.05
Episode length: 510.40 +/- 46.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 1093608  |
---------------------------------
Eval num_timesteps=1095600, episode_reward=531.80 +/- 186.66
Episode length: 585.20 +/- 64.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 1095600  |
---------------------------------
Eval num_timesteps=1097592, episode_reward=446.46 +/- 58.48
Episode length: 537.80 +/- 23.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 538      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 1097592  |
---------------------------------
Eval num_timesteps=1099584, episode_reward=458.89 +/- 263.75
Episode length: 545.60 +/- 98.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 1099584  |
---------------------------------
Eval num_timesteps=1101576, episode_reward=523.87 +/- 55.59
Episode length: 570.80 +/- 35.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 524      |
| time/              |          |
|    total_timesteps | 1101576  |
---------------------------------
Eval num_timesteps=1103568, episode_reward=378.04 +/- 318.06
Episode length: 494.00 +/- 82.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 1103568  |
---------------------------------
Eval num_timesteps=1105560, episode_reward=813.67 +/- 653.44
Episode length: 797.80 +/- 328.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 1105560  |
---------------------------------
Eval num_timesteps=1107552, episode_reward=615.40 +/- 444.16
Episode length: 596.80 +/- 106.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 1107552  |
---------------------------------
Eval num_timesteps=1109544, episode_reward=637.94 +/- 446.21
Episode length: 646.00 +/- 203.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 1109544  |
---------------------------------
Eval num_timesteps=1111536, episode_reward=519.84 +/- 181.94
Episode length: 564.00 +/- 67.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 1111536  |
---------------------------------
Eval num_timesteps=1113528, episode_reward=682.15 +/- 232.49
Episode length: 626.40 +/- 98.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 1113528  |
---------------------------------
Eval num_timesteps=1115520, episode_reward=861.90 +/- 367.68
Episode length: 691.60 +/- 134.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 1115520  |
---------------------------------
Eval num_timesteps=1117512, episode_reward=441.94 +/- 106.90
Episode length: 525.40 +/- 30.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 1117512  |
---------------------------------
Eval num_timesteps=1119504, episode_reward=680.61 +/- 346.90
Episode length: 691.40 +/- 209.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 1119504  |
---------------------------------
Eval num_timesteps=1121496, episode_reward=703.12 +/- 454.92
Episode length: 684.60 +/- 199.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 1121496  |
---------------------------------
Eval num_timesteps=1123488, episode_reward=463.42 +/- 208.11
Episode length: 560.00 +/- 65.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 1123488  |
---------------------------------
Eval num_timesteps=1125480, episode_reward=439.32 +/- 242.51
Episode length: 516.80 +/- 54.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 1125480  |
---------------------------------
Eval num_timesteps=1127472, episode_reward=694.21 +/- 183.63
Episode length: 596.60 +/- 54.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 1127472  |
---------------------------------
Eval num_timesteps=1129464, episode_reward=677.93 +/- 228.95
Episode length: 591.60 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 1129464  |
---------------------------------
Eval num_timesteps=1131456, episode_reward=452.15 +/- 207.93
Episode length: 544.00 +/- 110.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 452          |
| time/                   |              |
|    total_timesteps      | 1131456      |
| train/                  |              |
|    approx_kl            | 0.0046774084 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.52        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.001        |
|    loss                 | 17.4         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1.23         |
|    value_loss           | 40.9         |
------------------------------------------
Eval num_timesteps=1133448, episode_reward=566.66 +/- 418.36
Episode length: 605.80 +/- 236.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1133448  |
---------------------------------
Eval num_timesteps=1135440, episode_reward=668.86 +/- 492.64
Episode length: 559.60 +/- 95.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 1135440  |
---------------------------------
Eval num_timesteps=1137432, episode_reward=416.26 +/- 274.57
Episode length: 506.80 +/- 122.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 1137432  |
---------------------------------
Eval num_timesteps=1139424, episode_reward=413.84 +/- 307.98
Episode length: 599.80 +/- 168.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 1139424  |
---------------------------------
Eval num_timesteps=1141416, episode_reward=1202.34 +/- 701.01
Episode length: 707.20 +/- 177.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1141416  |
---------------------------------
Eval num_timesteps=1143408, episode_reward=717.91 +/- 266.97
Episode length: 593.60 +/- 74.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 1143408  |
---------------------------------
Eval num_timesteps=1145400, episode_reward=419.37 +/- 203.22
Episode length: 496.40 +/- 41.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 1145400  |
---------------------------------
Eval num_timesteps=1147392, episode_reward=363.43 +/- 276.17
Episode length: 514.40 +/- 50.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 1147392  |
---------------------------------
Eval num_timesteps=1149384, episode_reward=410.69 +/- 244.61
Episode length: 500.20 +/- 51.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 1149384  |
---------------------------------
Eval num_timesteps=1151376, episode_reward=224.67 +/- 140.76
Episode length: 501.40 +/- 64.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 1151376  |
---------------------------------
Eval num_timesteps=1153368, episode_reward=508.92 +/- 308.67
Episode length: 519.20 +/- 87.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 1153368  |
---------------------------------
Eval num_timesteps=1155360, episode_reward=254.11 +/- 69.17
Episode length: 463.60 +/- 23.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 1155360  |
---------------------------------
Eval num_timesteps=1157352, episode_reward=654.44 +/- 628.65
Episode length: 586.00 +/- 200.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 1157352  |
---------------------------------
Eval num_timesteps=1159344, episode_reward=573.22 +/- 190.80
Episode length: 568.80 +/- 75.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 1159344  |
---------------------------------
Eval num_timesteps=1161336, episode_reward=358.38 +/- 202.49
Episode length: 496.60 +/- 68.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 1161336  |
---------------------------------
Eval num_timesteps=1163328, episode_reward=368.24 +/- 145.09
Episode length: 493.40 +/- 36.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 1163328  |
---------------------------------
Eval num_timesteps=1165320, episode_reward=622.02 +/- 346.28
Episode length: 568.00 +/- 145.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 1165320  |
---------------------------------
Eval num_timesteps=1167312, episode_reward=451.61 +/- 410.80
Episode length: 620.80 +/- 225.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 1167312  |
---------------------------------
Eval num_timesteps=1169304, episode_reward=344.55 +/- 431.38
Episode length: 462.80 +/- 93.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 1169304  |
---------------------------------
Eval num_timesteps=1171296, episode_reward=598.72 +/- 196.99
Episode length: 568.80 +/- 37.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 1171296  |
---------------------------------
Eval num_timesteps=1173288, episode_reward=941.67 +/- 373.83
Episode length: 617.20 +/- 74.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 1173288  |
---------------------------------
Eval num_timesteps=1175280, episode_reward=575.06 +/- 304.57
Episode length: 588.20 +/- 43.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 1175280  |
---------------------------------
Eval num_timesteps=1177272, episode_reward=365.02 +/- 167.57
Episode length: 494.60 +/- 28.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 1177272  |
---------------------------------
Eval num_timesteps=1179264, episode_reward=714.93 +/- 351.41
Episode length: 547.00 +/- 52.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 1179264  |
---------------------------------
Eval num_timesteps=1181256, episode_reward=552.64 +/- 255.45
Episode length: 526.20 +/- 77.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 553          |
| time/                   |              |
|    total_timesteps      | 1181256      |
| train/                  |              |
|    approx_kl            | 0.0034506873 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.53        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 37.1         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.24         |
|    value_loss           | 82           |
------------------------------------------
Eval num_timesteps=1183248, episode_reward=900.96 +/- 530.47
Episode length: 646.20 +/- 187.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 1183248  |
---------------------------------
Eval num_timesteps=1185240, episode_reward=840.95 +/- 656.59
Episode length: 563.00 +/- 100.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 1185240  |
---------------------------------
Eval num_timesteps=1187232, episode_reward=769.83 +/- 732.76
Episode length: 578.20 +/- 186.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 1187232  |
---------------------------------
Eval num_timesteps=1189224, episode_reward=689.80 +/- 717.79
Episode length: 551.60 +/- 152.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 1189224  |
---------------------------------
Eval num_timesteps=1191216, episode_reward=689.32 +/- 219.70
Episode length: 672.40 +/- 133.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 1191216  |
---------------------------------
Eval num_timesteps=1193208, episode_reward=541.70 +/- 212.42
Episode length: 637.60 +/- 216.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 542      |
| time/              |          |
|    total_timesteps | 1193208  |
---------------------------------
Eval num_timesteps=1195200, episode_reward=855.65 +/- 262.03
Episode length: 651.60 +/- 124.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 1195200  |
---------------------------------
Eval num_timesteps=1197192, episode_reward=932.99 +/- 300.21
Episode length: 758.80 +/- 211.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 1197192  |
---------------------------------
Eval num_timesteps=1199184, episode_reward=527.10 +/- 239.70
Episode length: 523.60 +/- 47.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 1199184  |
---------------------------------
Eval num_timesteps=1201176, episode_reward=816.20 +/- 431.53
Episode length: 765.80 +/- 194.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 1201176  |
---------------------------------
Eval num_timesteps=1203168, episode_reward=737.61 +/- 409.73
Episode length: 685.20 +/- 155.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 1203168  |
---------------------------------
Eval num_timesteps=1205160, episode_reward=695.38 +/- 268.68
Episode length: 569.40 +/- 35.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 1205160  |
---------------------------------
Eval num_timesteps=1207152, episode_reward=577.80 +/- 112.91
Episode length: 535.40 +/- 52.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 1207152  |
---------------------------------
Eval num_timesteps=1209144, episode_reward=1148.07 +/- 710.18
Episode length: 721.60 +/- 210.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 1209144  |
---------------------------------
Eval num_timesteps=1211136, episode_reward=733.73 +/- 332.24
Episode length: 624.40 +/- 173.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 1211136  |
---------------------------------
Eval num_timesteps=1213128, episode_reward=566.88 +/- 221.28
Episode length: 527.40 +/- 46.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 527      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1213128  |
---------------------------------
Eval num_timesteps=1215120, episode_reward=438.10 +/- 323.41
Episode length: 638.60 +/- 306.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 1215120  |
---------------------------------
Eval num_timesteps=1217112, episode_reward=1219.41 +/- 560.99
Episode length: 805.40 +/- 184.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1217112  |
---------------------------------
Eval num_timesteps=1219104, episode_reward=840.28 +/- 752.66
Episode length: 770.60 +/- 213.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 771      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 1219104  |
---------------------------------
Eval num_timesteps=1221096, episode_reward=930.76 +/- 355.82
Episode length: 766.20 +/- 252.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 1221096  |
---------------------------------
Eval num_timesteps=1223088, episode_reward=1072.29 +/- 630.19
Episode length: 658.40 +/- 174.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 1223088  |
---------------------------------
Eval num_timesteps=1225080, episode_reward=853.55 +/- 660.32
Episode length: 672.20 +/- 171.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 1225080  |
---------------------------------
Eval num_timesteps=1227072, episode_reward=802.41 +/- 560.69
Episode length: 641.80 +/- 225.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 1227072  |
---------------------------------
Eval num_timesteps=1229064, episode_reward=727.25 +/- 121.02
Episode length: 570.60 +/- 25.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 571          |
|    mean_reward          | 727          |
| time/                   |              |
|    total_timesteps      | 1229064      |
| train/                  |              |
|    approx_kl            | 0.0032418023 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.55        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.0012      |
|    std                  | 1.25         |
|    value_loss           | 258          |
------------------------------------------
Eval num_timesteps=1231056, episode_reward=1176.15 +/- 959.60
Episode length: 641.80 +/- 147.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1231056  |
---------------------------------
Eval num_timesteps=1233048, episode_reward=822.36 +/- 543.07
Episode length: 689.60 +/- 170.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 1233048  |
---------------------------------
Eval num_timesteps=1235040, episode_reward=867.47 +/- 334.69
Episode length: 791.40 +/- 168.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 1235040  |
---------------------------------
Eval num_timesteps=1237032, episode_reward=741.28 +/- 452.75
Episode length: 715.20 +/- 194.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 1237032  |
---------------------------------
Eval num_timesteps=1239024, episode_reward=1127.00 +/- 801.44
Episode length: 788.00 +/- 310.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1239024  |
---------------------------------
Eval num_timesteps=1241016, episode_reward=1132.20 +/- 363.79
Episode length: 969.00 +/- 124.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1241016  |
---------------------------------
Eval num_timesteps=1243008, episode_reward=823.30 +/- 670.98
Episode length: 634.60 +/- 155.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 1243008  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=564.17 +/- 477.82
Episode length: 692.40 +/- 173.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
Eval num_timesteps=1246992, episode_reward=778.63 +/- 394.61
Episode length: 719.20 +/- 207.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 1246992  |
---------------------------------
Eval num_timesteps=1248984, episode_reward=693.59 +/- 499.80
Episode length: 647.80 +/- 217.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 1248984  |
---------------------------------
Eval num_timesteps=1250976, episode_reward=783.92 +/- 417.49
Episode length: 650.40 +/- 107.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 1250976  |
---------------------------------
Eval num_timesteps=1252968, episode_reward=925.39 +/- 547.30
Episode length: 603.00 +/- 87.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 1252968  |
---------------------------------
Eval num_timesteps=1254960, episode_reward=1007.72 +/- 568.79
Episode length: 619.00 +/- 131.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 1254960  |
---------------------------------
Eval num_timesteps=1256952, episode_reward=1315.79 +/- 990.74
Episode length: 717.20 +/- 273.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1256952  |
---------------------------------
Eval num_timesteps=1258944, episode_reward=691.09 +/- 306.45
Episode length: 678.00 +/- 155.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 1258944  |
---------------------------------
Eval num_timesteps=1260936, episode_reward=697.97 +/- 558.91
Episode length: 600.00 +/- 214.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 1260936  |
---------------------------------
Eval num_timesteps=1262928, episode_reward=1300.68 +/- 598.64
Episode length: 890.20 +/- 235.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1262928  |
---------------------------------
Eval num_timesteps=1264920, episode_reward=621.96 +/- 258.99
Episode length: 674.80 +/- 164.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 1264920  |
---------------------------------
Eval num_timesteps=1266912, episode_reward=1448.30 +/- 947.70
Episode length: 760.60 +/- 226.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 761      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1266912  |
---------------------------------
Eval num_timesteps=1268904, episode_reward=1206.80 +/- 896.94
Episode length: 801.40 +/- 155.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1268904  |
---------------------------------
Eval num_timesteps=1270896, episode_reward=551.64 +/- 193.32
Episode length: 620.00 +/- 109.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 1270896  |
---------------------------------
Eval num_timesteps=1272888, episode_reward=1265.90 +/- 911.54
Episode length: 756.80 +/- 244.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1272888  |
---------------------------------
Eval num_timesteps=1274880, episode_reward=1023.53 +/- 643.84
Episode length: 748.40 +/- 204.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1274880  |
---------------------------------
Eval num_timesteps=1276872, episode_reward=775.62 +/- 263.21
Episode length: 648.00 +/- 106.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 1276872  |
---------------------------------
Eval num_timesteps=1278864, episode_reward=1259.66 +/- 667.02
Episode length: 790.00 +/- 185.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 790          |
|    mean_reward          | 1.26e+03     |
| time/                   |              |
|    total_timesteps      | 1278864      |
| train/                  |              |
|    approx_kl            | 0.0037286526 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.57        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 298          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.25         |
|    value_loss           | 714          |
------------------------------------------
Eval num_timesteps=1280856, episode_reward=1594.93 +/- 281.96
Episode length: 867.60 +/- 93.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 868      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1280856  |
---------------------------------
Eval num_timesteps=1282848, episode_reward=1561.87 +/- 499.53
Episode length: 1004.40 +/- 69.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1282848  |
---------------------------------
Eval num_timesteps=1284840, episode_reward=1248.06 +/- 777.30
Episode length: 808.80 +/- 207.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1284840  |
---------------------------------
Eval num_timesteps=1286832, episode_reward=1598.40 +/- 438.99
Episode length: 902.20 +/- 172.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1286832  |
---------------------------------
Eval num_timesteps=1288824, episode_reward=1591.29 +/- 648.78
Episode length: 1015.80 +/- 84.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1288824  |
---------------------------------
Eval num_timesteps=1290816, episode_reward=1174.86 +/- 797.90
Episode length: 844.80 +/- 86.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1290816  |
---------------------------------
Eval num_timesteps=1292808, episode_reward=1386.99 +/- 746.04
Episode length: 829.00 +/- 106.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 1292808  |
---------------------------------
Eval num_timesteps=1294800, episode_reward=1581.04 +/- 912.57
Episode length: 847.40 +/- 195.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 847      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1294800  |
---------------------------------
Eval num_timesteps=1296792, episode_reward=1558.23 +/- 976.01
Episode length: 869.00 +/- 172.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1296792  |
---------------------------------
Eval num_timesteps=1298784, episode_reward=1352.37 +/- 364.47
Episode length: 887.00 +/- 214.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 1298784  |
---------------------------------
Eval num_timesteps=1300776, episode_reward=1327.41 +/- 788.90
Episode length: 841.40 +/- 98.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1300776  |
---------------------------------
Eval num_timesteps=1302768, episode_reward=1311.95 +/- 881.57
Episode length: 792.80 +/- 239.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1302768  |
---------------------------------
Eval num_timesteps=1304760, episode_reward=1340.16 +/- 754.89
Episode length: 948.00 +/- 160.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1304760  |
---------------------------------
Eval num_timesteps=1306752, episode_reward=1575.31 +/- 790.26
Episode length: 854.00 +/- 132.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1306752  |
---------------------------------
Eval num_timesteps=1308744, episode_reward=1120.84 +/- 422.50
Episode length: 860.60 +/- 207.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1308744  |
---------------------------------
Eval num_timesteps=1310736, episode_reward=1262.99 +/- 385.24
Episode length: 986.00 +/- 176.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1310736  |
---------------------------------
Eval num_timesteps=1312728, episode_reward=1491.27 +/- 618.24
Episode length: 938.40 +/- 215.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1312728  |
---------------------------------
Eval num_timesteps=1314720, episode_reward=2051.37 +/- 1004.48
Episode length: 882.00 +/- 135.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1314720  |
---------------------------------
New best mean reward!
Eval num_timesteps=1316712, episode_reward=1085.49 +/- 747.15
Episode length: 802.00 +/- 273.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1316712  |
---------------------------------
Eval num_timesteps=1318704, episode_reward=1396.45 +/- 987.66
Episode length: 896.40 +/- 255.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1318704  |
---------------------------------
Eval num_timesteps=1320696, episode_reward=1496.81 +/- 443.59
Episode length: 890.80 +/- 121.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1320696  |
---------------------------------
Eval num_timesteps=1322688, episode_reward=1431.85 +/- 304.82
Episode length: 888.00 +/- 161.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1322688  |
---------------------------------
Eval num_timesteps=1324680, episode_reward=1430.32 +/- 675.15
Episode length: 938.40 +/- 239.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1324680  |
---------------------------------
Eval num_timesteps=1326672, episode_reward=1767.43 +/- 1327.00
Episode length: 775.00 +/- 236.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1326672  |
---------------------------------
Eval num_timesteps=1328664, episode_reward=1436.14 +/- 482.77
Episode length: 943.40 +/- 47.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 943          |
|    mean_reward          | 1.44e+03     |
| time/                   |              |
|    total_timesteps      | 1328664      |
| train/                  |              |
|    approx_kl            | 0.0046337415 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.58        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 483          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.25         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=1330656, episode_reward=1461.28 +/- 737.52
Episode length: 825.00 +/- 108.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 1330656  |
---------------------------------
Eval num_timesteps=1332648, episode_reward=1856.10 +/- 667.14
Episode length: 905.20 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1332648  |
---------------------------------
Eval num_timesteps=1334640, episode_reward=1121.55 +/- 221.84
Episode length: 936.00 +/- 100.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1334640  |
---------------------------------
Eval num_timesteps=1336632, episode_reward=1389.66 +/- 448.54
Episode length: 897.60 +/- 44.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 1336632  |
---------------------------------
Eval num_timesteps=1338624, episode_reward=1750.55 +/- 871.81
Episode length: 997.60 +/- 90.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1338624  |
---------------------------------
Eval num_timesteps=1340616, episode_reward=1547.46 +/- 307.41
Episode length: 948.40 +/- 43.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1340616  |
---------------------------------
Eval num_timesteps=1342608, episode_reward=1858.41 +/- 796.76
Episode length: 925.20 +/- 23.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1342608  |
---------------------------------
Eval num_timesteps=1344600, episode_reward=1744.38 +/- 664.23
Episode length: 917.00 +/- 56.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 917      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 1344600  |
---------------------------------
Eval num_timesteps=1346592, episode_reward=1350.54 +/- 947.06
Episode length: 976.40 +/- 85.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 1346592  |
---------------------------------
Eval num_timesteps=1348584, episode_reward=1726.39 +/- 593.98
Episode length: 921.00 +/- 75.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 1348584  |
---------------------------------
Eval num_timesteps=1350576, episode_reward=1472.47 +/- 1141.49
Episode length: 928.20 +/- 61.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 928      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1350576  |
---------------------------------
Eval num_timesteps=1352568, episode_reward=974.73 +/- 581.74
Episode length: 879.40 +/- 43.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 1352568  |
---------------------------------
Eval num_timesteps=1354560, episode_reward=1241.78 +/- 630.95
Episode length: 921.00 +/- 79.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 921      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 1354560  |
---------------------------------
Eval num_timesteps=1356552, episode_reward=1181.32 +/- 461.47
Episode length: 884.80 +/- 106.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1356552  |
---------------------------------
Eval num_timesteps=1358544, episode_reward=1626.06 +/- 1002.71
Episode length: 986.20 +/- 82.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1358544  |
---------------------------------
Eval num_timesteps=1360536, episode_reward=1695.50 +/- 245.02
Episode length: 820.60 +/- 114.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1360536  |
---------------------------------
Eval num_timesteps=1362528, episode_reward=1591.98 +/- 864.84
Episode length: 949.40 +/- 85.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1362528  |
---------------------------------
Eval num_timesteps=1364520, episode_reward=1152.86 +/- 259.07
Episode length: 901.00 +/- 78.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 1364520  |
---------------------------------
Eval num_timesteps=1366512, episode_reward=1710.28 +/- 694.14
Episode length: 889.40 +/- 114.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 1366512  |
---------------------------------
Eval num_timesteps=1368504, episode_reward=914.72 +/- 531.94
Episode length: 865.20 +/- 177.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 1368504  |
---------------------------------
Eval num_timesteps=1370496, episode_reward=1903.08 +/- 657.01
Episode length: 986.00 +/- 43.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1370496  |
---------------------------------
Eval num_timesteps=1372488, episode_reward=1681.17 +/- 1003.33
Episode length: 905.60 +/- 183.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1372488  |
---------------------------------
Eval num_timesteps=1374480, episode_reward=1853.46 +/- 776.33
Episode length: 911.60 +/- 29.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 1374480  |
---------------------------------
Eval num_timesteps=1376472, episode_reward=1004.57 +/- 536.95
Episode length: 848.40 +/- 59.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 848          |
|    mean_reward          | 1e+03        |
| time/                   |              |
|    total_timesteps      | 1376472      |
| train/                  |              |
|    approx_kl            | 0.0036355592 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 491          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00113     |
|    std                  | 1.26         |
|    value_loss           | 1.29e+03     |
------------------------------------------
Eval num_timesteps=1378464, episode_reward=928.77 +/- 650.44
Episode length: 801.20 +/- 86.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 1378464  |
---------------------------------
Eval num_timesteps=1380456, episode_reward=1410.43 +/- 699.67
Episode length: 862.00 +/- 130.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1380456  |
---------------------------------
Eval num_timesteps=1382448, episode_reward=1230.05 +/- 356.78
Episode length: 912.60 +/- 79.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1382448  |
---------------------------------
Eval num_timesteps=1384440, episode_reward=1163.56 +/- 404.32
Episode length: 813.80 +/- 217.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 1384440  |
---------------------------------
Eval num_timesteps=1386432, episode_reward=1208.62 +/- 782.14
Episode length: 815.20 +/- 124.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1386432  |
---------------------------------
Eval num_timesteps=1388424, episode_reward=1189.47 +/- 348.93
Episode length: 879.00 +/- 170.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1388424  |
---------------------------------
Eval num_timesteps=1390416, episode_reward=935.53 +/- 399.80
Episode length: 791.40 +/- 137.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 1390416  |
---------------------------------
Eval num_timesteps=1392408, episode_reward=1293.34 +/- 695.82
Episode length: 826.60 +/- 130.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 1392408  |
---------------------------------
Eval num_timesteps=1394400, episode_reward=1617.57 +/- 405.78
Episode length: 877.20 +/- 161.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 877      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1394400  |
---------------------------------
Eval num_timesteps=1396392, episode_reward=1008.00 +/- 410.63
Episode length: 811.80 +/- 116.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 1396392  |
---------------------------------
Eval num_timesteps=1398384, episode_reward=1788.75 +/- 776.46
Episode length: 939.20 +/- 55.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1398384  |
---------------------------------
Eval num_timesteps=1400376, episode_reward=1400.92 +/- 700.56
Episode length: 889.20 +/- 147.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1400376  |
---------------------------------
Eval num_timesteps=1402368, episode_reward=1470.86 +/- 628.60
Episode length: 869.20 +/- 140.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1402368  |
---------------------------------
Eval num_timesteps=1404360, episode_reward=1358.47 +/- 377.92
Episode length: 886.80 +/- 84.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1404360  |
---------------------------------
Eval num_timesteps=1406352, episode_reward=1605.66 +/- 831.09
Episode length: 962.80 +/- 74.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 1406352  |
---------------------------------
Eval num_timesteps=1408344, episode_reward=1618.74 +/- 891.66
Episode length: 871.60 +/- 66.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1408344  |
---------------------------------
Eval num_timesteps=1410336, episode_reward=1138.76 +/- 408.16
Episode length: 975.60 +/- 80.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1410336  |
---------------------------------
Eval num_timesteps=1412328, episode_reward=1297.36 +/- 845.90
Episode length: 899.00 +/- 223.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 899      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1412328  |
---------------------------------
Eval num_timesteps=1414320, episode_reward=1354.23 +/- 368.20
Episode length: 910.40 +/- 134.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 1414320  |
---------------------------------
Eval num_timesteps=1416312, episode_reward=1268.95 +/- 358.66
Episode length: 1020.60 +/- 67.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1416312  |
---------------------------------
Eval num_timesteps=1418304, episode_reward=930.51 +/- 302.64
Episode length: 805.60 +/- 152.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 1418304  |
---------------------------------
Eval num_timesteps=1420296, episode_reward=879.98 +/- 608.25
Episode length: 812.80 +/- 205.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 1420296  |
---------------------------------
Eval num_timesteps=1422288, episode_reward=1645.54 +/- 941.18
Episode length: 941.80 +/- 205.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1422288  |
---------------------------------
Eval num_timesteps=1424280, episode_reward=1672.64 +/- 244.97
Episode length: 972.40 +/- 114.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 972      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 1424280  |
---------------------------------
Eval num_timesteps=1426272, episode_reward=1812.85 +/- 1009.47
Episode length: 1034.00 +/- 210.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.03e+03    |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 1426272     |
| train/                  |             |
|    approx_kl            | 0.004095888 |
|    clip_fraction        | 0.0204      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 471         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00167    |
|    std                  | 1.27        |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=1428264, episode_reward=2172.63 +/- 818.04
Episode length: 930.80 +/- 106.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 1428264  |
---------------------------------
New best mean reward!
Eval num_timesteps=1430256, episode_reward=1880.78 +/- 915.06
Episode length: 907.80 +/- 45.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1430256  |
---------------------------------
Eval num_timesteps=1432248, episode_reward=2048.27 +/- 322.54
Episode length: 893.00 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1432248  |
---------------------------------
Eval num_timesteps=1434240, episode_reward=1215.42 +/- 797.15
Episode length: 938.20 +/- 102.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1434240  |
---------------------------------
Eval num_timesteps=1436232, episode_reward=880.43 +/- 362.69
Episode length: 932.80 +/- 280.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 1436232  |
---------------------------------
Eval num_timesteps=1438224, episode_reward=1200.33 +/- 642.33
Episode length: 861.40 +/- 173.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1438224  |
---------------------------------
Eval num_timesteps=1440216, episode_reward=1578.50 +/- 621.41
Episode length: 954.00 +/- 30.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1440216  |
---------------------------------
Eval num_timesteps=1442208, episode_reward=1028.96 +/- 549.28
Episode length: 894.00 +/- 116.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1442208  |
---------------------------------
Eval num_timesteps=1444200, episode_reward=1612.21 +/- 899.35
Episode length: 951.80 +/- 263.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 952      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 1444200  |
---------------------------------
Eval num_timesteps=1446192, episode_reward=1448.90 +/- 591.15
Episode length: 877.60 +/- 189.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1446192  |
---------------------------------
Eval num_timesteps=1448184, episode_reward=1653.21 +/- 517.62
Episode length: 1006.00 +/- 170.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1448184  |
---------------------------------
Eval num_timesteps=1450176, episode_reward=1236.71 +/- 504.65
Episode length: 794.80 +/- 109.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 1450176  |
---------------------------------
Eval num_timesteps=1452168, episode_reward=1542.70 +/- 702.83
Episode length: 1004.20 +/- 279.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1452168  |
---------------------------------
Eval num_timesteps=1454160, episode_reward=1564.48 +/- 598.30
Episode length: 890.80 +/- 127.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 891      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1454160  |
---------------------------------
Eval num_timesteps=1456152, episode_reward=1377.04 +/- 477.82
Episode length: 867.00 +/- 109.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 1456152  |
---------------------------------
Eval num_timesteps=1458144, episode_reward=1157.61 +/- 651.00
Episode length: 978.20 +/- 213.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 1458144  |
---------------------------------
Eval num_timesteps=1460136, episode_reward=1347.00 +/- 390.53
Episode length: 960.20 +/- 201.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 1460136  |
---------------------------------
Eval num_timesteps=1462128, episode_reward=1469.49 +/- 841.23
Episode length: 963.20 +/- 114.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 963      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1462128  |
---------------------------------
Eval num_timesteps=1464120, episode_reward=1138.76 +/- 555.06
Episode length: 920.00 +/- 151.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1464120  |
---------------------------------
Eval num_timesteps=1466112, episode_reward=2103.13 +/- 688.32
Episode length: 934.80 +/- 74.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 935      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1466112  |
---------------------------------
Eval num_timesteps=1468104, episode_reward=1453.30 +/- 340.99
Episode length: 874.60 +/- 87.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1468104  |
---------------------------------
Eval num_timesteps=1470096, episode_reward=2417.80 +/- 1129.03
Episode length: 1120.00 +/- 183.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1470096  |
---------------------------------
New best mean reward!
Eval num_timesteps=1472088, episode_reward=1519.98 +/- 676.11
Episode length: 966.80 +/- 147.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 967      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1472088  |
---------------------------------
Eval num_timesteps=1474080, episode_reward=1523.33 +/- 932.91
Episode length: 871.00 +/- 284.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 871      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1474080  |
---------------------------------
Eval num_timesteps=1476072, episode_reward=1485.50 +/- 1076.27
Episode length: 831.00 +/- 132.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 831         |
|    mean_reward          | 1.49e+03    |
| time/                   |             |
|    total_timesteps      | 1476072     |
| train/                  |             |
|    approx_kl            | 0.003868547 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.63       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.001       |
|    loss                 | 375         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.27        |
|    value_loss           | 959         |
-----------------------------------------
Eval num_timesteps=1478064, episode_reward=1887.70 +/- 1077.55
Episode length: 1058.20 +/- 143.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1478064  |
---------------------------------
Eval num_timesteps=1480056, episode_reward=1661.51 +/- 777.13
Episode length: 992.60 +/- 146.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 993      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1480056  |
---------------------------------
Eval num_timesteps=1482048, episode_reward=1358.82 +/- 616.80
Episode length: 902.80 +/- 117.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1482048  |
---------------------------------
Eval num_timesteps=1484040, episode_reward=2193.53 +/- 836.18
Episode length: 982.20 +/- 186.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 1484040  |
---------------------------------
Eval num_timesteps=1486032, episode_reward=1519.66 +/- 309.54
Episode length: 1026.80 +/- 208.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1486032  |
---------------------------------
Eval num_timesteps=1488024, episode_reward=1926.15 +/- 655.26
Episode length: 1002.20 +/- 188.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1488024  |
---------------------------------
Eval num_timesteps=1490016, episode_reward=1046.29 +/- 783.72
Episode length: 829.00 +/- 190.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 1490016  |
---------------------------------
Eval num_timesteps=1492008, episode_reward=1677.04 +/- 843.57
Episode length: 889.40 +/- 65.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1492008  |
---------------------------------
Eval num_timesteps=1494000, episode_reward=962.50 +/- 881.28
Episode length: 812.20 +/- 197.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1495992, episode_reward=1023.84 +/- 394.08
Episode length: 854.00 +/- 26.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1495992  |
---------------------------------
Eval num_timesteps=1497984, episode_reward=1578.84 +/- 765.44
Episode length: 950.80 +/- 131.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1497984  |
---------------------------------
Eval num_timesteps=1499976, episode_reward=1042.49 +/- 527.92
Episode length: 1009.20 +/- 123.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 1499976  |
---------------------------------
Eval num_timesteps=1501968, episode_reward=2581.33 +/- 954.00
Episode length: 1039.60 +/- 160.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1501968  |
---------------------------------
New best mean reward!
Eval num_timesteps=1503960, episode_reward=1357.22 +/- 782.06
Episode length: 866.60 +/- 163.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1503960  |
---------------------------------
Eval num_timesteps=1505952, episode_reward=1558.16 +/- 580.42
Episode length: 976.20 +/- 143.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1505952  |
---------------------------------
Eval num_timesteps=1507944, episode_reward=1449.58 +/- 737.36
Episode length: 923.80 +/- 103.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1507944  |
---------------------------------
Eval num_timesteps=1509936, episode_reward=1131.95 +/- 776.33
Episode length: 916.40 +/- 138.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1509936  |
---------------------------------
Eval num_timesteps=1511928, episode_reward=1353.23 +/- 306.24
Episode length: 812.20 +/- 90.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 1511928  |
---------------------------------
Eval num_timesteps=1513920, episode_reward=1408.22 +/- 746.78
Episode length: 867.00 +/- 172.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1513920  |
---------------------------------
Eval num_timesteps=1515912, episode_reward=1438.52 +/- 304.27
Episode length: 998.80 +/- 287.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 999      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 1515912  |
---------------------------------
Eval num_timesteps=1517904, episode_reward=2088.56 +/- 758.21
Episode length: 971.40 +/- 140.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 971      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1517904  |
---------------------------------
Eval num_timesteps=1519896, episode_reward=1475.42 +/- 1027.52
Episode length: 1025.60 +/- 140.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 1519896  |
---------------------------------
Eval num_timesteps=1521888, episode_reward=1230.60 +/- 516.40
Episode length: 943.40 +/- 150.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1521888  |
---------------------------------
Eval num_timesteps=1523880, episode_reward=1045.24 +/- 300.25
Episode length: 961.40 +/- 66.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 961         |
|    mean_reward          | 1.05e+03    |
| time/                   |             |
|    total_timesteps      | 1523880     |
| train/                  |             |
|    approx_kl            | 0.003797981 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 370         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00124    |
|    std                  | 1.28        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=1525872, episode_reward=2025.07 +/- 953.97
Episode length: 1074.00 +/- 144.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1525872  |
---------------------------------
Eval num_timesteps=1527864, episode_reward=1289.71 +/- 401.96
Episode length: 942.00 +/- 84.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 1527864  |
---------------------------------
Eval num_timesteps=1529856, episode_reward=1722.71 +/- 645.02
Episode length: 944.80 +/- 98.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 945      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1529856  |
---------------------------------
Eval num_timesteps=1531848, episode_reward=1232.35 +/- 584.87
Episode length: 911.20 +/- 73.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1531848  |
---------------------------------
Eval num_timesteps=1533840, episode_reward=1603.39 +/- 976.19
Episode length: 851.80 +/- 98.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1533840  |
---------------------------------
Eval num_timesteps=1535832, episode_reward=1079.81 +/- 409.30
Episode length: 953.40 +/- 164.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1535832  |
---------------------------------
Eval num_timesteps=1537824, episode_reward=836.01 +/- 576.29
Episode length: 776.20 +/- 147.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 1537824  |
---------------------------------
Eval num_timesteps=1539816, episode_reward=1449.13 +/- 347.80
Episode length: 885.20 +/- 186.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 885      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 1539816  |
---------------------------------
Eval num_timesteps=1541808, episode_reward=1237.97 +/- 806.40
Episode length: 895.00 +/- 252.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 1541808  |
---------------------------------
Eval num_timesteps=1543800, episode_reward=1716.07 +/- 930.02
Episode length: 975.00 +/- 150.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1543800  |
---------------------------------
Eval num_timesteps=1545792, episode_reward=1188.93 +/- 639.62
Episode length: 905.20 +/- 118.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1545792  |
---------------------------------
Eval num_timesteps=1547784, episode_reward=1601.47 +/- 466.28
Episode length: 960.40 +/- 152.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1547784  |
---------------------------------
Eval num_timesteps=1549776, episode_reward=1484.31 +/- 748.69
Episode length: 923.00 +/- 61.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 1549776  |
---------------------------------
Eval num_timesteps=1551768, episode_reward=1388.25 +/- 879.69
Episode length: 840.20 +/- 169.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 1551768  |
---------------------------------
Eval num_timesteps=1553760, episode_reward=1727.21 +/- 890.10
Episode length: 872.80 +/- 79.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 1553760  |
---------------------------------
Eval num_timesteps=1555752, episode_reward=1807.50 +/- 1153.01
Episode length: 924.00 +/- 57.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 1555752  |
---------------------------------
Eval num_timesteps=1557744, episode_reward=839.15 +/- 389.95
Episode length: 889.00 +/- 35.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 889      |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 1557744  |
---------------------------------
Eval num_timesteps=1559736, episode_reward=1938.29 +/- 957.40
Episode length: 933.00 +/- 69.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 1559736  |
---------------------------------
Eval num_timesteps=1561728, episode_reward=1595.99 +/- 704.44
Episode length: 1035.20 +/- 144.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1561728  |
---------------------------------
Eval num_timesteps=1563720, episode_reward=967.38 +/- 647.74
Episode length: 825.80 +/- 221.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 1563720  |
---------------------------------
Eval num_timesteps=1565712, episode_reward=976.02 +/- 884.43
Episode length: 878.40 +/- 115.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 1565712  |
---------------------------------
Eval num_timesteps=1567704, episode_reward=1296.04 +/- 646.87
Episode length: 801.80 +/- 155.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1567704  |
---------------------------------
Eval num_timesteps=1569696, episode_reward=1310.45 +/- 752.18
Episode length: 834.40 +/- 141.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1569696  |
---------------------------------
Eval num_timesteps=1571688, episode_reward=1301.08 +/- 633.09
Episode length: 907.00 +/- 130.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1571688  |
---------------------------------
Eval num_timesteps=1573680, episode_reward=1556.35 +/- 920.59
Episode length: 892.20 +/- 200.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 892         |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 1573680     |
| train/                  |             |
|    approx_kl            | 0.003556499 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 484         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 1.29        |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=1575672, episode_reward=1899.83 +/- 1045.14
Episode length: 926.20 +/- 115.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1575672  |
---------------------------------
Eval num_timesteps=1577664, episode_reward=1137.07 +/- 875.19
Episode length: 953.60 +/- 223.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 954      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1577664  |
---------------------------------
Eval num_timesteps=1579656, episode_reward=1991.92 +/- 799.31
Episode length: 935.60 +/- 156.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1579656  |
---------------------------------
Eval num_timesteps=1581648, episode_reward=1271.88 +/- 587.72
Episode length: 851.60 +/- 74.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1581648  |
---------------------------------
Eval num_timesteps=1583640, episode_reward=1210.93 +/- 724.16
Episode length: 839.60 +/- 158.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1583640  |
---------------------------------
Eval num_timesteps=1585632, episode_reward=1911.48 +/- 1337.15
Episode length: 995.80 +/- 213.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 996      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1585632  |
---------------------------------
Eval num_timesteps=1587624, episode_reward=1848.44 +/- 471.01
Episode length: 1084.60 +/- 164.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 1587624  |
---------------------------------
Eval num_timesteps=1589616, episode_reward=1324.76 +/- 991.32
Episode length: 1077.00 +/- 232.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1589616  |
---------------------------------
Eval num_timesteps=1591608, episode_reward=1361.32 +/- 852.52
Episode length: 693.80 +/- 204.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1591608  |
---------------------------------
Eval num_timesteps=1593600, episode_reward=2370.02 +/- 535.45
Episode length: 969.00 +/- 229.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 969      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 1593600  |
---------------------------------
Eval num_timesteps=1595592, episode_reward=936.26 +/- 332.64
Episode length: 747.00 +/- 90.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 1595592  |
---------------------------------
Eval num_timesteps=1597584, episode_reward=1533.71 +/- 615.27
Episode length: 994.40 +/- 124.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 994      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1597584  |
---------------------------------
Eval num_timesteps=1599576, episode_reward=1541.81 +/- 572.29
Episode length: 838.80 +/- 72.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1599576  |
---------------------------------
Eval num_timesteps=1601568, episode_reward=1308.81 +/- 966.75
Episode length: 772.80 +/- 81.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1601568  |
---------------------------------
Eval num_timesteps=1603560, episode_reward=1783.07 +/- 149.74
Episode length: 1028.00 +/- 151.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 1603560  |
---------------------------------
Eval num_timesteps=1605552, episode_reward=1602.68 +/- 917.82
Episode length: 870.20 +/- 229.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1605552  |
---------------------------------
Eval num_timesteps=1607544, episode_reward=1880.82 +/- 562.38
Episode length: 979.00 +/- 156.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 979      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1607544  |
---------------------------------
Eval num_timesteps=1609536, episode_reward=1341.79 +/- 962.38
Episode length: 720.20 +/- 232.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1609536  |
---------------------------------
Eval num_timesteps=1611528, episode_reward=1077.87 +/- 414.00
Episode length: 1061.20 +/- 143.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1611528  |
---------------------------------
Eval num_timesteps=1613520, episode_reward=1206.55 +/- 928.36
Episode length: 892.20 +/- 190.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1613520  |
---------------------------------
Eval num_timesteps=1615512, episode_reward=1077.75 +/- 390.57
Episode length: 1016.80 +/- 194.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1615512  |
---------------------------------
Eval num_timesteps=1617504, episode_reward=1843.65 +/- 735.45
Episode length: 978.60 +/- 171.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 979      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 1617504  |
---------------------------------
Eval num_timesteps=1619496, episode_reward=1980.19 +/- 1220.31
Episode length: 995.00 +/- 122.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 995      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 1619496  |
---------------------------------
Eval num_timesteps=1621488, episode_reward=591.45 +/- 338.32
Episode length: 718.40 +/- 120.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 1621488  |
---------------------------------
Eval num_timesteps=1623480, episode_reward=1578.57 +/- 1425.31
Episode length: 827.60 +/- 273.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 828          |
|    mean_reward          | 1.58e+03     |
| time/                   |              |
|    total_timesteps      | 1623480      |
| train/                  |              |
|    approx_kl            | 0.0033306933 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 209          |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.3          |
|    value_loss           | 613          |
------------------------------------------
Eval num_timesteps=1625472, episode_reward=1137.57 +/- 877.58
Episode length: 696.20 +/- 169.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1625472  |
---------------------------------
Eval num_timesteps=1627464, episode_reward=1256.59 +/- 1108.98
Episode length: 1015.20 +/- 195.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1627464  |
---------------------------------
Eval num_timesteps=1629456, episode_reward=1285.53 +/- 948.10
Episode length: 756.00 +/- 156.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 1629456  |
---------------------------------
Eval num_timesteps=1631448, episode_reward=2362.04 +/- 562.58
Episode length: 922.20 +/- 141.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1631448  |
---------------------------------
Eval num_timesteps=1633440, episode_reward=1248.81 +/- 993.19
Episode length: 860.20 +/- 315.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 860      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1633440  |
---------------------------------
Eval num_timesteps=1635432, episode_reward=1565.15 +/- 1365.29
Episode length: 727.60 +/- 189.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1635432  |
---------------------------------
Eval num_timesteps=1637424, episode_reward=1423.18 +/- 1030.50
Episode length: 829.80 +/- 200.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 1637424  |
---------------------------------
Eval num_timesteps=1639416, episode_reward=672.22 +/- 522.13
Episode length: 641.20 +/- 188.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 1639416  |
---------------------------------
Eval num_timesteps=1641408, episode_reward=1900.20 +/- 1168.30
Episode length: 1086.60 +/- 89.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1641408  |
---------------------------------
Eval num_timesteps=1643400, episode_reward=1375.86 +/- 640.82
Episode length: 726.80 +/- 158.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 1643400  |
---------------------------------
Eval num_timesteps=1645392, episode_reward=1148.73 +/- 536.46
Episode length: 689.20 +/- 95.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 1645392  |
---------------------------------
Eval num_timesteps=1647384, episode_reward=1142.09 +/- 538.49
Episode length: 789.60 +/- 214.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1647384  |
---------------------------------
Eval num_timesteps=1649376, episode_reward=2497.69 +/- 965.42
Episode length: 981.00 +/- 163.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1649376  |
---------------------------------
Eval num_timesteps=1651368, episode_reward=1648.06 +/- 813.76
Episode length: 852.00 +/- 116.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1651368  |
---------------------------------
Eval num_timesteps=1653360, episode_reward=1410.18 +/- 909.17
Episode length: 795.80 +/- 172.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1653360  |
---------------------------------
Eval num_timesteps=1655352, episode_reward=1115.48 +/- 665.23
Episode length: 825.80 +/- 181.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1655352  |
---------------------------------
Eval num_timesteps=1657344, episode_reward=1817.57 +/- 852.94
Episode length: 1021.20 +/- 271.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1657344  |
---------------------------------
Eval num_timesteps=1659336, episode_reward=1947.30 +/- 695.46
Episode length: 985.20 +/- 145.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 985      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1659336  |
---------------------------------
Eval num_timesteps=1661328, episode_reward=661.00 +/- 294.52
Episode length: 723.80 +/- 123.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 1661328  |
---------------------------------
Eval num_timesteps=1663320, episode_reward=1632.11 +/- 693.87
Episode length: 895.60 +/- 189.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1663320  |
---------------------------------
Eval num_timesteps=1665312, episode_reward=686.59 +/- 536.60
Episode length: 657.20 +/- 240.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 1665312  |
---------------------------------
Eval num_timesteps=1667304, episode_reward=970.59 +/- 664.15
Episode length: 792.40 +/- 170.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 1667304  |
---------------------------------
Eval num_timesteps=1669296, episode_reward=1488.96 +/- 591.31
Episode length: 997.20 +/- 246.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 997      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1669296  |
---------------------------------
Eval num_timesteps=1671288, episode_reward=1364.79 +/- 561.60
Episode length: 825.20 +/- 140.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 825          |
|    mean_reward          | 1.36e+03     |
| time/                   |              |
|    total_timesteps      | 1671288      |
| train/                  |              |
|    approx_kl            | 0.0033644896 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.73        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 330          |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.3          |
|    value_loss           | 858          |
------------------------------------------
Eval num_timesteps=1673280, episode_reward=1251.42 +/- 637.98
Episode length: 895.60 +/- 266.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1673280  |
---------------------------------
Eval num_timesteps=1675272, episode_reward=1528.24 +/- 1354.85
Episode length: 916.60 +/- 253.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 917      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1675272  |
---------------------------------
Eval num_timesteps=1677264, episode_reward=1282.93 +/- 972.43
Episode length: 913.60 +/- 151.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 914      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 1677264  |
---------------------------------
Eval num_timesteps=1679256, episode_reward=1542.35 +/- 777.14
Episode length: 803.80 +/- 43.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1679256  |
---------------------------------
Eval num_timesteps=1681248, episode_reward=1137.89 +/- 734.37
Episode length: 772.20 +/- 238.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1681248  |
---------------------------------
Eval num_timesteps=1683240, episode_reward=852.65 +/- 426.16
Episode length: 854.00 +/- 241.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 1683240  |
---------------------------------
Eval num_timesteps=1685232, episode_reward=1121.51 +/- 655.66
Episode length: 882.60 +/- 160.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1685232  |
---------------------------------
Eval num_timesteps=1687224, episode_reward=1054.25 +/- 774.15
Episode length: 633.40 +/- 171.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 1687224  |
---------------------------------
Eval num_timesteps=1689216, episode_reward=1871.85 +/- 847.33
Episode length: 941.00 +/- 151.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1689216  |
---------------------------------
Eval num_timesteps=1691208, episode_reward=2292.66 +/- 885.08
Episode length: 1109.60 +/- 156.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 1691208  |
---------------------------------
Eval num_timesteps=1693200, episode_reward=2194.53 +/- 1080.01
Episode length: 841.20 +/- 79.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 1693200  |
---------------------------------
Eval num_timesteps=1695192, episode_reward=1595.56 +/- 1218.98
Episode length: 784.60 +/- 228.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1695192  |
---------------------------------
Eval num_timesteps=1697184, episode_reward=1748.50 +/- 974.78
Episode length: 894.00 +/- 152.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1697184  |
---------------------------------
Eval num_timesteps=1699176, episode_reward=703.44 +/- 660.57
Episode length: 648.00 +/- 189.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 1699176  |
---------------------------------
Eval num_timesteps=1701168, episode_reward=1198.60 +/- 585.45
Episode length: 869.80 +/- 247.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1701168  |
---------------------------------
Eval num_timesteps=1703160, episode_reward=2232.00 +/- 906.12
Episode length: 829.00 +/- 75.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 1703160  |
---------------------------------
Eval num_timesteps=1705152, episode_reward=1129.88 +/- 706.25
Episode length: 722.20 +/- 158.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1705152  |
---------------------------------
Eval num_timesteps=1707144, episode_reward=1321.97 +/- 562.42
Episode length: 842.80 +/- 119.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1707144  |
---------------------------------
Eval num_timesteps=1709136, episode_reward=1867.35 +/- 621.89
Episode length: 875.20 +/- 170.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1709136  |
---------------------------------
Eval num_timesteps=1711128, episode_reward=1997.35 +/- 996.12
Episode length: 853.60 +/- 169.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1711128  |
---------------------------------
Eval num_timesteps=1713120, episode_reward=938.52 +/- 485.60
Episode length: 727.60 +/- 181.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 1713120  |
---------------------------------
Eval num_timesteps=1715112, episode_reward=1100.06 +/- 770.16
Episode length: 818.80 +/- 204.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 1715112  |
---------------------------------
Eval num_timesteps=1717104, episode_reward=1685.78 +/- 561.48
Episode length: 922.00 +/- 136.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1717104  |
---------------------------------
Eval num_timesteps=1719096, episode_reward=1822.67 +/- 1321.16
Episode length: 845.60 +/- 191.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1719096  |
---------------------------------
Eval num_timesteps=1721088, episode_reward=630.58 +/- 524.30
Episode length: 984.40 +/- 200.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 984         |
|    mean_reward          | 631         |
| time/                   |             |
|    total_timesteps      | 1721088     |
| train/                  |             |
|    approx_kl            | 0.002858638 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.76       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 337         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.32        |
|    value_loss           | 1.07e+03    |
-----------------------------------------
Eval num_timesteps=1723080, episode_reward=816.63 +/- 700.19
Episode length: 691.80 +/- 180.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 1723080  |
---------------------------------
Eval num_timesteps=1725072, episode_reward=1055.89 +/- 582.93
Episode length: 910.60 +/- 174.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1725072  |
---------------------------------
Eval num_timesteps=1727064, episode_reward=1645.00 +/- 504.81
Episode length: 964.20 +/- 217.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1727064  |
---------------------------------
Eval num_timesteps=1729056, episode_reward=1064.69 +/- 403.25
Episode length: 901.80 +/- 158.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1729056  |
---------------------------------
Eval num_timesteps=1731048, episode_reward=1564.21 +/- 767.84
Episode length: 983.40 +/- 108.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1731048  |
---------------------------------
Eval num_timesteps=1733040, episode_reward=1636.80 +/- 836.17
Episode length: 827.40 +/- 263.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1733040  |
---------------------------------
Eval num_timesteps=1735032, episode_reward=1312.98 +/- 911.08
Episode length: 840.80 +/- 166.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1735032  |
---------------------------------
Eval num_timesteps=1737024, episode_reward=1301.82 +/- 661.29
Episode length: 801.40 +/- 116.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1737024  |
---------------------------------
Eval num_timesteps=1739016, episode_reward=1588.81 +/- 811.19
Episode length: 907.60 +/- 116.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 908      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1739016  |
---------------------------------
Eval num_timesteps=1741008, episode_reward=1661.69 +/- 947.41
Episode length: 953.40 +/- 118.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1741008  |
---------------------------------
Eval num_timesteps=1743000, episode_reward=1327.06 +/- 730.25
Episode length: 862.40 +/- 121.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1743000  |
---------------------------------
Eval num_timesteps=1744992, episode_reward=2036.58 +/- 913.19
Episode length: 912.40 +/- 103.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 912      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1744992  |
---------------------------------
Eval num_timesteps=1746984, episode_reward=2201.11 +/- 731.59
Episode length: 909.20 +/- 83.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 1746984  |
---------------------------------
Eval num_timesteps=1748976, episode_reward=2260.98 +/- 850.30
Episode length: 949.00 +/- 273.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 949      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 1748976  |
---------------------------------
Eval num_timesteps=1750968, episode_reward=1257.65 +/- 280.18
Episode length: 932.00 +/- 153.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1750968  |
---------------------------------
Eval num_timesteps=1752960, episode_reward=942.59 +/- 769.63
Episode length: 770.20 +/- 138.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 1752960  |
---------------------------------
Eval num_timesteps=1754952, episode_reward=1237.17 +/- 547.37
Episode length: 974.80 +/- 96.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 1754952  |
---------------------------------
Eval num_timesteps=1756944, episode_reward=1789.64 +/- 731.02
Episode length: 943.60 +/- 110.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 944      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1756944  |
---------------------------------
Eval num_timesteps=1758936, episode_reward=1662.10 +/- 651.30
Episode length: 942.40 +/- 153.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1758936  |
---------------------------------
Eval num_timesteps=1760928, episode_reward=2140.64 +/- 1245.75
Episode length: 956.80 +/- 141.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 1760928  |
---------------------------------
Eval num_timesteps=1762920, episode_reward=1566.21 +/- 644.39
Episode length: 955.20 +/- 184.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 955      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1762920  |
---------------------------------
Eval num_timesteps=1764912, episode_reward=1581.81 +/- 516.59
Episode length: 869.60 +/- 96.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1764912  |
---------------------------------
Eval num_timesteps=1766904, episode_reward=1679.82 +/- 960.87
Episode length: 933.40 +/- 106.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1766904  |
---------------------------------
Eval num_timesteps=1768896, episode_reward=1429.97 +/- 624.76
Episode length: 963.80 +/- 79.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1768896  |
---------------------------------
Eval num_timesteps=1770888, episode_reward=1554.71 +/- 773.88
Episode length: 896.40 +/- 179.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 896          |
|    mean_reward          | 1.55e+03     |
| time/                   |              |
|    total_timesteps      | 1770888      |
| train/                  |              |
|    approx_kl            | 0.0035607025 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.79        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 512          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000954    |
|    std                  | 1.32         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=1772880, episode_reward=1329.82 +/- 822.37
Episode length: 777.60 +/- 168.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1772880  |
---------------------------------
Eval num_timesteps=1774872, episode_reward=645.65 +/- 260.53
Episode length: 725.40 +/- 108.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 1774872  |
---------------------------------
Eval num_timesteps=1776864, episode_reward=812.36 +/- 291.86
Episode length: 755.60 +/- 115.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 1776864  |
---------------------------------
Eval num_timesteps=1778856, episode_reward=1463.40 +/- 755.28
Episode length: 938.60 +/- 90.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 939      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 1778856  |
---------------------------------
Eval num_timesteps=1780848, episode_reward=1183.53 +/- 730.04
Episode length: 756.00 +/- 193.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1780848  |
---------------------------------
Eval num_timesteps=1782840, episode_reward=1691.60 +/- 941.94
Episode length: 835.60 +/- 124.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1782840  |
---------------------------------
Eval num_timesteps=1784832, episode_reward=1681.17 +/- 887.06
Episode length: 857.20 +/- 113.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1784832  |
---------------------------------
Eval num_timesteps=1786824, episode_reward=1740.59 +/- 1032.34
Episode length: 773.60 +/- 48.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 1786824  |
---------------------------------
Eval num_timesteps=1788816, episode_reward=2064.46 +/- 762.07
Episode length: 931.40 +/- 165.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1788816  |
---------------------------------
Eval num_timesteps=1790808, episode_reward=1408.23 +/- 234.79
Episode length: 853.60 +/- 98.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1790808  |
---------------------------------
Eval num_timesteps=1792800, episode_reward=1344.56 +/- 641.06
Episode length: 707.40 +/- 116.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1792800  |
---------------------------------
Eval num_timesteps=1794792, episode_reward=1432.09 +/- 1051.06
Episode length: 828.00 +/- 138.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 1794792  |
---------------------------------
Eval num_timesteps=1796784, episode_reward=1900.24 +/- 809.51
Episode length: 867.40 +/- 105.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1796784  |
---------------------------------
Eval num_timesteps=1798776, episode_reward=878.97 +/- 780.88
Episode length: 650.80 +/- 189.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 1798776  |
---------------------------------
Eval num_timesteps=1800768, episode_reward=1169.62 +/- 677.34
Episode length: 754.40 +/- 143.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1800768  |
---------------------------------
Eval num_timesteps=1802760, episode_reward=1723.53 +/- 879.77
Episode length: 813.60 +/- 207.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1802760  |
---------------------------------
Eval num_timesteps=1804752, episode_reward=1694.99 +/- 589.78
Episode length: 851.80 +/- 106.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1804752  |
---------------------------------
Eval num_timesteps=1806744, episode_reward=1988.56 +/- 532.07
Episode length: 917.00 +/- 78.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 917      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1806744  |
---------------------------------
Eval num_timesteps=1808736, episode_reward=1531.81 +/- 926.43
Episode length: 833.00 +/- 73.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1808736  |
---------------------------------
Eval num_timesteps=1810728, episode_reward=1091.36 +/- 440.71
Episode length: 728.00 +/- 65.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1810728  |
---------------------------------
Eval num_timesteps=1812720, episode_reward=1902.02 +/- 714.80
Episode length: 923.60 +/- 136.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1812720  |
---------------------------------
Eval num_timesteps=1814712, episode_reward=1003.91 +/- 862.36
Episode length: 778.80 +/- 127.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 1814712  |
---------------------------------
Eval num_timesteps=1816704, episode_reward=1088.13 +/- 826.07
Episode length: 769.60 +/- 254.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1816704  |
---------------------------------
Eval num_timesteps=1818696, episode_reward=1093.89 +/- 1060.30
Episode length: 869.20 +/- 206.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 869          |
|    mean_reward          | 1.09e+03     |
| time/                   |              |
|    total_timesteps      | 1818696      |
| train/                  |              |
|    approx_kl            | 0.0031706113 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.81        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 412          |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.33         |
|    value_loss           | 777          |
------------------------------------------
Eval num_timesteps=1820688, episode_reward=1530.89 +/- 601.20
Episode length: 963.60 +/- 89.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1820688  |
---------------------------------
Eval num_timesteps=1822680, episode_reward=1301.11 +/- 998.88
Episode length: 730.80 +/- 211.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1822680  |
---------------------------------
Eval num_timesteps=1824672, episode_reward=1753.98 +/- 1277.02
Episode length: 801.00 +/- 211.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1824672  |
---------------------------------
Eval num_timesteps=1826664, episode_reward=1175.11 +/- 929.79
Episode length: 776.40 +/- 171.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1826664  |
---------------------------------
Eval num_timesteps=1828656, episode_reward=826.98 +/- 711.70
Episode length: 668.60 +/- 147.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 1828656  |
---------------------------------
Eval num_timesteps=1830648, episode_reward=1646.79 +/- 607.02
Episode length: 907.40 +/- 152.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1830648  |
---------------------------------
Eval num_timesteps=1832640, episode_reward=1042.05 +/- 767.80
Episode length: 726.00 +/- 170.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 1832640  |
---------------------------------
Eval num_timesteps=1834632, episode_reward=1472.77 +/- 1108.03
Episode length: 741.00 +/- 171.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 1834632  |
---------------------------------
Eval num_timesteps=1836624, episode_reward=2093.12 +/- 765.51
Episode length: 976.20 +/- 130.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1836624  |
---------------------------------
Eval num_timesteps=1838616, episode_reward=1067.04 +/- 479.94
Episode length: 846.00 +/- 160.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 1838616  |
---------------------------------
Eval num_timesteps=1840608, episode_reward=1461.90 +/- 969.08
Episode length: 720.20 +/- 150.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 1840608  |
---------------------------------
Eval num_timesteps=1842600, episode_reward=1157.71 +/- 942.57
Episode length: 758.80 +/- 175.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 1842600  |
---------------------------------
Eval num_timesteps=1844592, episode_reward=740.91 +/- 457.71
Episode length: 778.80 +/- 147.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 1844592  |
---------------------------------
Eval num_timesteps=1846584, episode_reward=1636.78 +/- 839.15
Episode length: 906.60 +/- 233.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1846584  |
---------------------------------
Eval num_timesteps=1848576, episode_reward=1219.09 +/- 586.02
Episode length: 790.20 +/- 172.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1848576  |
---------------------------------
Eval num_timesteps=1850568, episode_reward=955.87 +/- 825.61
Episode length: 648.20 +/- 197.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 1850568  |
---------------------------------
Eval num_timesteps=1852560, episode_reward=2040.65 +/- 916.84
Episode length: 899.60 +/- 167.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1852560  |
---------------------------------
Eval num_timesteps=1854552, episode_reward=804.43 +/- 339.58
Episode length: 976.80 +/- 94.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 977      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 1854552  |
---------------------------------
Eval num_timesteps=1856544, episode_reward=1141.44 +/- 378.15
Episode length: 818.00 +/- 92.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 1856544  |
---------------------------------
Eval num_timesteps=1858536, episode_reward=1913.61 +/- 641.45
Episode length: 842.60 +/- 153.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1858536  |
---------------------------------
Eval num_timesteps=1860528, episode_reward=920.58 +/- 431.05
Episode length: 799.40 +/- 197.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 1860528  |
---------------------------------
Eval num_timesteps=1862520, episode_reward=1721.40 +/- 864.92
Episode length: 811.80 +/- 87.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1862520  |
---------------------------------
Eval num_timesteps=1864512, episode_reward=1477.12 +/- 309.08
Episode length: 818.40 +/- 153.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 1864512  |
---------------------------------
Eval num_timesteps=1866504, episode_reward=1128.85 +/- 1169.45
Episode length: 650.60 +/- 153.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 1866504  |
---------------------------------
Eval num_timesteps=1868496, episode_reward=1722.02 +/- 1225.78
Episode length: 789.80 +/- 149.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 790         |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 1868496     |
| train/                  |             |
|    approx_kl            | 0.002957276 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 226         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.000918   |
|    std                  | 1.34        |
|    value_loss           | 546         |
-----------------------------------------
Eval num_timesteps=1870488, episode_reward=1516.28 +/- 817.41
Episode length: 813.20 +/- 104.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1870488  |
---------------------------------
Eval num_timesteps=1872480, episode_reward=2424.78 +/- 1135.99
Episode length: 1016.80 +/- 41.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1872480  |
---------------------------------
Eval num_timesteps=1874472, episode_reward=1292.69 +/- 854.42
Episode length: 853.40 +/- 100.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 1874472  |
---------------------------------
Eval num_timesteps=1876464, episode_reward=2438.76 +/- 865.87
Episode length: 823.80 +/- 49.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1876464  |
---------------------------------
Eval num_timesteps=1878456, episode_reward=1020.90 +/- 346.52
Episode length: 802.00 +/- 157.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1878456  |
---------------------------------
Eval num_timesteps=1880448, episode_reward=2221.01 +/- 1516.91
Episode length: 821.40 +/- 143.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1880448  |
---------------------------------
Eval num_timesteps=1882440, episode_reward=1605.08 +/- 1022.79
Episode length: 815.80 +/- 127.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 1882440  |
---------------------------------
Eval num_timesteps=1884432, episode_reward=902.42 +/- 638.93
Episode length: 694.00 +/- 157.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 1884432  |
---------------------------------
Eval num_timesteps=1886424, episode_reward=1897.79 +/- 758.21
Episode length: 830.00 +/- 126.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1886424  |
---------------------------------
Eval num_timesteps=1888416, episode_reward=1212.97 +/- 992.13
Episode length: 869.80 +/- 222.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1888416  |
---------------------------------
Eval num_timesteps=1890408, episode_reward=991.86 +/- 799.49
Episode length: 928.60 +/- 179.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 929      |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 1890408  |
---------------------------------
Eval num_timesteps=1892400, episode_reward=1776.12 +/- 1063.77
Episode length: 845.40 +/- 221.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 1892400  |
---------------------------------
Eval num_timesteps=1894392, episode_reward=1301.63 +/- 1319.83
Episode length: 779.60 +/- 127.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1894392  |
---------------------------------
Eval num_timesteps=1896384, episode_reward=1496.44 +/- 627.21
Episode length: 897.20 +/- 98.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 897      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1896384  |
---------------------------------
Eval num_timesteps=1898376, episode_reward=2026.68 +/- 1104.17
Episode length: 802.40 +/- 227.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1898376  |
---------------------------------
Eval num_timesteps=1900368, episode_reward=1971.99 +/- 482.09
Episode length: 1000.20 +/- 163.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 1900368  |
---------------------------------
Eval num_timesteps=1902360, episode_reward=1772.65 +/- 1392.73
Episode length: 818.20 +/- 176.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1902360  |
---------------------------------
Eval num_timesteps=1904352, episode_reward=1858.72 +/- 1016.70
Episode length: 784.60 +/- 122.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1904352  |
---------------------------------
Eval num_timesteps=1906344, episode_reward=1190.78 +/- 854.80
Episode length: 733.60 +/- 181.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1906344  |
---------------------------------
Eval num_timesteps=1908336, episode_reward=1615.79 +/- 1065.36
Episode length: 792.00 +/- 172.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1908336  |
---------------------------------
Eval num_timesteps=1910328, episode_reward=2114.55 +/- 1051.58
Episode length: 779.80 +/- 178.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 1910328  |
---------------------------------
Eval num_timesteps=1912320, episode_reward=1552.57 +/- 701.30
Episode length: 892.40 +/- 131.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1912320  |
---------------------------------
Eval num_timesteps=1914312, episode_reward=2013.12 +/- 808.72
Episode length: 923.80 +/- 76.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1914312  |
---------------------------------
Eval num_timesteps=1916304, episode_reward=1550.44 +/- 822.13
Episode length: 917.60 +/- 194.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1916304  |
---------------------------------
Eval num_timesteps=1918296, episode_reward=1040.67 +/- 712.14
Episode length: 857.00 +/- 113.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 857          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 1918296      |
| train/                  |              |
|    approx_kl            | 0.0036749158 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 145          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.35         |
|    value_loss           | 403          |
------------------------------------------
Eval num_timesteps=1920288, episode_reward=1084.01 +/- 607.46
Episode length: 841.40 +/- 223.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1920288  |
---------------------------------
Eval num_timesteps=1922280, episode_reward=1018.05 +/- 822.90
Episode length: 808.40 +/- 180.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1922280  |
---------------------------------
Eval num_timesteps=1924272, episode_reward=481.78 +/- 584.27
Episode length: 591.00 +/- 191.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 1924272  |
---------------------------------
Eval num_timesteps=1926264, episode_reward=1087.27 +/- 1373.83
Episode length: 759.20 +/- 136.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1926264  |
---------------------------------
Eval num_timesteps=1928256, episode_reward=1678.61 +/- 955.08
Episode length: 868.60 +/- 164.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1928256  |
---------------------------------
Eval num_timesteps=1930248, episode_reward=670.26 +/- 1070.77
Episode length: 647.00 +/- 245.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 1930248  |
---------------------------------
Eval num_timesteps=1932240, episode_reward=911.19 +/- 676.15
Episode length: 799.20 +/- 243.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 1932240  |
---------------------------------
Eval num_timesteps=1934232, episode_reward=1251.44 +/- 1148.60
Episode length: 721.00 +/- 172.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1934232  |
---------------------------------
Eval num_timesteps=1936224, episode_reward=482.39 +/- 218.41
Episode length: 706.20 +/- 86.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 1936224  |
---------------------------------
Eval num_timesteps=1938216, episode_reward=636.29 +/- 530.54
Episode length: 751.00 +/- 218.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 1938216  |
---------------------------------
Eval num_timesteps=1940208, episode_reward=1459.70 +/- 1091.14
Episode length: 713.40 +/- 205.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 1940208  |
---------------------------------
Eval num_timesteps=1942200, episode_reward=344.79 +/- 274.20
Episode length: 571.00 +/- 141.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 1942200  |
---------------------------------
Eval num_timesteps=1944192, episode_reward=1493.40 +/- 816.59
Episode length: 861.00 +/- 177.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1944192  |
---------------------------------
Eval num_timesteps=1946184, episode_reward=1540.65 +/- 1470.49
Episode length: 668.00 +/- 210.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 1946184  |
---------------------------------
Eval num_timesteps=1948176, episode_reward=922.77 +/- 537.23
Episode length: 860.80 +/- 217.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 1948176  |
---------------------------------
Eval num_timesteps=1950168, episode_reward=1308.29 +/- 791.07
Episode length: 881.60 +/- 178.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1950168  |
---------------------------------
Eval num_timesteps=1952160, episode_reward=1436.88 +/- 1319.94
Episode length: 742.80 +/- 182.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 1952160  |
---------------------------------
Eval num_timesteps=1954152, episode_reward=1518.54 +/- 1238.45
Episode length: 775.80 +/- 161.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 1954152  |
---------------------------------
Eval num_timesteps=1956144, episode_reward=837.75 +/- 806.58
Episode length: 837.40 +/- 37.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 1956144  |
---------------------------------
Eval num_timesteps=1958136, episode_reward=841.66 +/- 280.48
Episode length: 733.60 +/- 112.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 1958136  |
---------------------------------
Eval num_timesteps=1960128, episode_reward=1257.72 +/- 863.87
Episode length: 745.40 +/- 153.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1960128  |
---------------------------------
Eval num_timesteps=1962120, episode_reward=1407.34 +/- 1170.45
Episode length: 785.80 +/- 220.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 1962120  |
---------------------------------
Eval num_timesteps=1964112, episode_reward=1641.42 +/- 1347.75
Episode length: 840.00 +/- 300.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1964112  |
---------------------------------
Eval num_timesteps=1966104, episode_reward=1300.49 +/- 487.44
Episode length: 930.00 +/- 61.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 930         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 1966104     |
| train/                  |             |
|    approx_kl            | 0.003933231 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00132    |
|    std                  | 1.37        |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=1968096, episode_reward=663.98 +/- 478.14
Episode length: 683.20 +/- 131.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 1968096  |
---------------------------------
Eval num_timesteps=1970088, episode_reward=643.72 +/- 373.84
Episode length: 831.40 +/- 115.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 1970088  |
---------------------------------
Eval num_timesteps=1972080, episode_reward=1082.70 +/- 998.82
Episode length: 782.20 +/- 56.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 1972080  |
---------------------------------
Eval num_timesteps=1974072, episode_reward=733.38 +/- 783.40
Episode length: 815.80 +/- 72.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 1974072  |
---------------------------------
Eval num_timesteps=1976064, episode_reward=1768.98 +/- 1090.07
Episode length: 830.40 +/- 122.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1976064  |
---------------------------------
Eval num_timesteps=1978056, episode_reward=633.46 +/- 711.87
Episode length: 646.60 +/- 179.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 1978056  |
---------------------------------
Eval num_timesteps=1980048, episode_reward=1030.17 +/- 464.32
Episode length: 794.20 +/- 102.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1980048  |
---------------------------------
Eval num_timesteps=1982040, episode_reward=1000.77 +/- 603.00
Episode length: 743.20 +/- 118.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 1982040  |
---------------------------------
Eval num_timesteps=1984032, episode_reward=841.37 +/- 610.86
Episode length: 890.00 +/- 105.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 1984032  |
---------------------------------
Eval num_timesteps=1986024, episode_reward=1908.66 +/- 1721.27
Episode length: 891.60 +/- 103.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1986024  |
---------------------------------
Eval num_timesteps=1988016, episode_reward=1376.19 +/- 502.52
Episode length: 739.40 +/- 121.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 1988016  |
---------------------------------
Eval num_timesteps=1990008, episode_reward=1274.33 +/- 819.42
Episode length: 824.80 +/- 154.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1990008  |
---------------------------------
Eval num_timesteps=1992000, episode_reward=1273.35 +/- 992.65
Episode length: 812.80 +/- 115.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
Eval num_timesteps=1993992, episode_reward=1481.72 +/- 974.25
Episode length: 862.60 +/- 105.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 863      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 1993992  |
---------------------------------
Eval num_timesteps=1995984, episode_reward=1261.19 +/- 949.50
Episode length: 758.20 +/- 53.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1995984  |
---------------------------------
Eval num_timesteps=1997976, episode_reward=464.91 +/- 256.60
Episode length: 846.20 +/- 147.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 1997976  |
---------------------------------
Eval num_timesteps=1999968, episode_reward=761.62 +/- 314.36
Episode length: 753.40 +/- 145.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 1999968  |
---------------------------------
Eval num_timesteps=2001960, episode_reward=1322.95 +/- 779.39
Episode length: 865.00 +/- 84.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 865      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2001960  |
---------------------------------
Eval num_timesteps=2003952, episode_reward=1173.05 +/- 660.98
Episode length: 909.40 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2003952  |
---------------------------------
Eval num_timesteps=2005944, episode_reward=875.93 +/- 639.27
Episode length: 813.40 +/- 105.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 2005944  |
---------------------------------
Eval num_timesteps=2007936, episode_reward=865.68 +/- 635.72
Episode length: 802.20 +/- 115.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 2007936  |
---------------------------------
Eval num_timesteps=2009928, episode_reward=917.00 +/- 732.56
Episode length: 759.20 +/- 148.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 2009928  |
---------------------------------
Eval num_timesteps=2011920, episode_reward=1229.07 +/- 771.76
Episode length: 859.20 +/- 57.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2011920  |
---------------------------------
Eval num_timesteps=2013912, episode_reward=1261.05 +/- 933.79
Episode length: 832.00 +/- 109.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2013912  |
---------------------------------
Eval num_timesteps=2015904, episode_reward=1020.54 +/- 790.18
Episode length: 709.40 +/- 192.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 709          |
|    mean_reward          | 1.02e+03     |
| time/                   |              |
|    total_timesteps      | 2015904      |
| train/                  |              |
|    approx_kl            | 0.0030095212 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.38         |
|    value_loss           | 443          |
------------------------------------------
Eval num_timesteps=2017896, episode_reward=1987.14 +/- 1205.10
Episode length: 763.00 +/- 158.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2017896  |
---------------------------------
Eval num_timesteps=2019888, episode_reward=1686.71 +/- 1058.93
Episode length: 701.60 +/- 118.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2019888  |
---------------------------------
Eval num_timesteps=2021880, episode_reward=574.21 +/- 636.51
Episode length: 661.00 +/- 181.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 2021880  |
---------------------------------
Eval num_timesteps=2023872, episode_reward=1219.51 +/- 559.51
Episode length: 789.00 +/- 151.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2023872  |
---------------------------------
Eval num_timesteps=2025864, episode_reward=1421.39 +/- 1387.50
Episode length: 665.60 +/- 185.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2025864  |
---------------------------------
Eval num_timesteps=2027856, episode_reward=1758.07 +/- 975.13
Episode length: 849.20 +/- 56.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 2027856  |
---------------------------------
Eval num_timesteps=2029848, episode_reward=1606.00 +/- 924.55
Episode length: 787.80 +/- 203.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2029848  |
---------------------------------
Eval num_timesteps=2031840, episode_reward=678.51 +/- 986.34
Episode length: 619.20 +/- 211.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 2031840  |
---------------------------------
Eval num_timesteps=2033832, episode_reward=1443.12 +/- 561.82
Episode length: 820.40 +/- 132.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2033832  |
---------------------------------
Eval num_timesteps=2035824, episode_reward=817.01 +/- 545.40
Episode length: 735.60 +/- 176.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 2035824  |
---------------------------------
Eval num_timesteps=2037816, episode_reward=263.34 +/- 133.65
Episode length: 607.80 +/- 63.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 2037816  |
---------------------------------
Eval num_timesteps=2039808, episode_reward=1009.88 +/- 986.15
Episode length: 717.20 +/- 108.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2039808  |
---------------------------------
Eval num_timesteps=2041800, episode_reward=1070.89 +/- 308.14
Episode length: 721.60 +/- 120.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2041800  |
---------------------------------
Eval num_timesteps=2043792, episode_reward=635.66 +/- 829.55
Episode length: 658.40 +/- 180.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 2043792  |
---------------------------------
Eval num_timesteps=2045784, episode_reward=2280.46 +/- 987.45
Episode length: 851.60 +/- 106.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 2045784  |
---------------------------------
Eval num_timesteps=2047776, episode_reward=1071.49 +/- 925.67
Episode length: 831.80 +/- 186.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2047776  |
---------------------------------
Eval num_timesteps=2049768, episode_reward=846.29 +/- 906.70
Episode length: 793.60 +/- 195.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 2049768  |
---------------------------------
Eval num_timesteps=2051760, episode_reward=1799.37 +/- 1764.30
Episode length: 714.20 +/- 185.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2051760  |
---------------------------------
Eval num_timesteps=2053752, episode_reward=1260.01 +/- 794.42
Episode length: 797.40 +/- 123.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2053752  |
---------------------------------
Eval num_timesteps=2055744, episode_reward=613.57 +/- 129.16
Episode length: 779.00 +/- 221.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 2055744  |
---------------------------------
Eval num_timesteps=2057736, episode_reward=706.32 +/- 419.40
Episode length: 739.80 +/- 149.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 2057736  |
---------------------------------
Eval num_timesteps=2059728, episode_reward=1334.03 +/- 977.82
Episode length: 699.20 +/- 170.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2059728  |
---------------------------------
Eval num_timesteps=2061720, episode_reward=338.19 +/- 264.23
Episode length: 700.00 +/- 237.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 2061720  |
---------------------------------
Eval num_timesteps=2063712, episode_reward=1021.83 +/- 694.67
Episode length: 819.80 +/- 68.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 820      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2063712  |
---------------------------------
Eval num_timesteps=2065704, episode_reward=812.97 +/- 587.29
Episode length: 665.40 +/- 143.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 665          |
|    mean_reward          | 813          |
| time/                   |              |
|    total_timesteps      | 2065704      |
| train/                  |              |
|    approx_kl            | 0.0037709295 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7           |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 191          |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00081     |
|    std                  | 1.39         |
|    value_loss           | 408          |
------------------------------------------
Eval num_timesteps=2067696, episode_reward=1151.01 +/- 318.03
Episode length: 682.20 +/- 90.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2067696  |
---------------------------------
Eval num_timesteps=2069688, episode_reward=1309.01 +/- 926.26
Episode length: 733.20 +/- 42.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2069688  |
---------------------------------
Eval num_timesteps=2071680, episode_reward=373.44 +/- 335.41
Episode length: 682.80 +/- 167.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 2071680  |
---------------------------------
Eval num_timesteps=2073672, episode_reward=1423.37 +/- 675.95
Episode length: 717.80 +/- 45.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2073672  |
---------------------------------
Eval num_timesteps=2075664, episode_reward=811.24 +/- 371.33
Episode length: 725.00 +/- 107.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 2075664  |
---------------------------------
Eval num_timesteps=2077656, episode_reward=1326.78 +/- 530.85
Episode length: 794.20 +/- 81.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 794      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2077656  |
---------------------------------
Eval num_timesteps=2079648, episode_reward=1013.37 +/- 898.66
Episode length: 743.00 +/- 125.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2079648  |
---------------------------------
Eval num_timesteps=2081640, episode_reward=777.43 +/- 1222.97
Episode length: 612.00 +/- 177.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 2081640  |
---------------------------------
Eval num_timesteps=2083632, episode_reward=1073.39 +/- 674.32
Episode length: 791.80 +/- 68.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2083632  |
---------------------------------
Eval num_timesteps=2085624, episode_reward=1193.65 +/- 727.55
Episode length: 727.60 +/- 89.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2085624  |
---------------------------------
Eval num_timesteps=2087616, episode_reward=428.56 +/- 480.14
Episode length: 584.00 +/- 252.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 2087616  |
---------------------------------
Eval num_timesteps=2089608, episode_reward=998.30 +/- 890.75
Episode length: 750.20 +/- 236.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 2089608  |
---------------------------------
Eval num_timesteps=2091600, episode_reward=1441.87 +/- 695.79
Episode length: 688.80 +/- 136.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2091600  |
---------------------------------
Eval num_timesteps=2093592, episode_reward=1464.95 +/- 1328.14
Episode length: 727.60 +/- 143.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2093592  |
---------------------------------
Eval num_timesteps=2095584, episode_reward=978.55 +/- 845.81
Episode length: 776.20 +/- 210.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 2095584  |
---------------------------------
Eval num_timesteps=2097576, episode_reward=1312.72 +/- 520.80
Episode length: 850.20 +/- 87.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2097576  |
---------------------------------
Eval num_timesteps=2099568, episode_reward=1274.52 +/- 986.71
Episode length: 854.60 +/- 179.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2099568  |
---------------------------------
Eval num_timesteps=2101560, episode_reward=1726.05 +/- 661.23
Episode length: 879.60 +/- 134.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 880      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 2101560  |
---------------------------------
Eval num_timesteps=2103552, episode_reward=583.76 +/- 652.20
Episode length: 581.60 +/- 178.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 2103552  |
---------------------------------
Eval num_timesteps=2105544, episode_reward=1388.99 +/- 625.68
Episode length: 836.00 +/- 77.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2105544  |
---------------------------------
Eval num_timesteps=2107536, episode_reward=542.97 +/- 446.45
Episode length: 666.20 +/- 151.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 2107536  |
---------------------------------
Eval num_timesteps=2109528, episode_reward=391.01 +/- 270.71
Episode length: 606.20 +/- 151.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 391      |
| time/              |          |
|    total_timesteps | 2109528  |
---------------------------------
Eval num_timesteps=2111520, episode_reward=1541.02 +/- 1131.67
Episode length: 836.00 +/- 65.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2111520  |
---------------------------------
Eval num_timesteps=2113512, episode_reward=1713.11 +/- 919.78
Episode length: 726.80 +/- 124.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 2113512  |
---------------------------------
Eval num_timesteps=2115504, episode_reward=639.17 +/- 836.94
Episode length: 564.20 +/- 205.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 564         |
|    mean_reward          | 639         |
| time/                   |             |
|    total_timesteps      | 2115504     |
| train/                  |             |
|    approx_kl            | 0.003996037 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 60.3        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1.41        |
|    value_loss           | 261         |
-----------------------------------------
Eval num_timesteps=2117496, episode_reward=768.25 +/- 318.18
Episode length: 697.00 +/- 111.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 2117496  |
---------------------------------
Eval num_timesteps=2119488, episode_reward=289.04 +/- 157.69
Episode length: 643.60 +/- 151.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 2119488  |
---------------------------------
Eval num_timesteps=2121480, episode_reward=506.24 +/- 391.92
Episode length: 692.80 +/- 189.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 2121480  |
---------------------------------
Eval num_timesteps=2123472, episode_reward=390.01 +/- 212.68
Episode length: 720.60 +/- 74.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 2123472  |
---------------------------------
Eval num_timesteps=2125464, episode_reward=1392.90 +/- 1177.43
Episode length: 751.60 +/- 120.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2125464  |
---------------------------------
Eval num_timesteps=2127456, episode_reward=471.88 +/- 455.04
Episode length: 592.40 +/- 152.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 2127456  |
---------------------------------
Eval num_timesteps=2129448, episode_reward=1486.92 +/- 811.58
Episode length: 813.20 +/- 74.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 2129448  |
---------------------------------
Eval num_timesteps=2131440, episode_reward=1668.52 +/- 806.07
Episode length: 837.40 +/- 72.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2131440  |
---------------------------------
Eval num_timesteps=2133432, episode_reward=1866.29 +/- 1482.24
Episode length: 715.80 +/- 136.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2133432  |
---------------------------------
Eval num_timesteps=2135424, episode_reward=681.98 +/- 589.08
Episode length: 745.40 +/- 207.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 2135424  |
---------------------------------
Eval num_timesteps=2137416, episode_reward=864.41 +/- 1004.31
Episode length: 811.00 +/- 141.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 2137416  |
---------------------------------
Eval num_timesteps=2139408, episode_reward=978.49 +/- 585.76
Episode length: 764.60 +/- 155.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 2139408  |
---------------------------------
Eval num_timesteps=2141400, episode_reward=450.80 +/- 382.06
Episode length: 676.80 +/- 122.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 2141400  |
---------------------------------
Eval num_timesteps=2143392, episode_reward=158.64 +/- 249.06
Episode length: 496.80 +/- 119.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 2143392  |
---------------------------------
Eval num_timesteps=2145384, episode_reward=387.24 +/- 214.82
Episode length: 654.40 +/- 123.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 2145384  |
---------------------------------
Eval num_timesteps=2147376, episode_reward=817.22 +/- 278.55
Episode length: 796.00 +/- 178.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 2147376  |
---------------------------------
Eval num_timesteps=2149368, episode_reward=1178.30 +/- 1467.54
Episode length: 589.00 +/- 229.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 589      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2149368  |
---------------------------------
Eval num_timesteps=2151360, episode_reward=1325.61 +/- 1010.43
Episode length: 887.00 +/- 156.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2151360  |
---------------------------------
Eval num_timesteps=2153352, episode_reward=1075.49 +/- 954.57
Episode length: 746.20 +/- 165.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2153352  |
---------------------------------
Eval num_timesteps=2155344, episode_reward=1236.44 +/- 739.27
Episode length: 839.40 +/- 138.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2155344  |
---------------------------------
Eval num_timesteps=2157336, episode_reward=1016.47 +/- 1047.38
Episode length: 694.20 +/- 181.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2157336  |
---------------------------------
Eval num_timesteps=2159328, episode_reward=725.37 +/- 498.12
Episode length: 730.20 +/- 32.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 2159328  |
---------------------------------
Eval num_timesteps=2161320, episode_reward=1679.61 +/- 1682.94
Episode length: 677.80 +/- 115.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2161320  |
---------------------------------
Eval num_timesteps=2163312, episode_reward=695.92 +/- 695.10
Episode length: 818.40 +/- 189.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | 696          |
| time/                   |              |
|    total_timesteps      | 2163312      |
| train/                  |              |
|    approx_kl            | 0.0048739873 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.07        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 152          |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.42         |
|    value_loss           | 327          |
------------------------------------------
Eval num_timesteps=2165304, episode_reward=443.45 +/- 548.61
Episode length: 624.00 +/- 193.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 2165304  |
---------------------------------
Eval num_timesteps=2167296, episode_reward=877.71 +/- 870.13
Episode length: 693.20 +/- 186.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 2167296  |
---------------------------------
Eval num_timesteps=2169288, episode_reward=1007.83 +/- 823.60
Episode length: 654.80 +/- 69.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2169288  |
---------------------------------
Eval num_timesteps=2171280, episode_reward=1847.91 +/- 1080.85
Episode length: 789.00 +/- 72.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2171280  |
---------------------------------
Eval num_timesteps=2173272, episode_reward=1165.22 +/- 926.60
Episode length: 735.60 +/- 110.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2173272  |
---------------------------------
Eval num_timesteps=2175264, episode_reward=959.38 +/- 510.56
Episode length: 740.20 +/- 201.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 2175264  |
---------------------------------
Eval num_timesteps=2177256, episode_reward=1240.61 +/- 566.34
Episode length: 649.00 +/- 110.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2177256  |
---------------------------------
Eval num_timesteps=2179248, episode_reward=1629.96 +/- 421.80
Episode length: 804.00 +/- 118.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2179248  |
---------------------------------
Eval num_timesteps=2181240, episode_reward=1571.92 +/- 626.40
Episode length: 904.40 +/- 102.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2181240  |
---------------------------------
Eval num_timesteps=2183232, episode_reward=1073.72 +/- 808.00
Episode length: 663.40 +/- 121.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2183232  |
---------------------------------
Eval num_timesteps=2185224, episode_reward=1675.44 +/- 1135.76
Episode length: 724.40 +/- 71.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2185224  |
---------------------------------
Eval num_timesteps=2187216, episode_reward=1632.72 +/- 855.50
Episode length: 790.40 +/- 120.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2187216  |
---------------------------------
Eval num_timesteps=2189208, episode_reward=2671.97 +/- 748.77
Episode length: 866.40 +/- 37.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 866      |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 2189208  |
---------------------------------
New best mean reward!
Eval num_timesteps=2191200, episode_reward=1219.80 +/- 1038.03
Episode length: 738.80 +/- 128.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2191200  |
---------------------------------
Eval num_timesteps=2193192, episode_reward=1847.87 +/- 944.67
Episode length: 709.60 +/- 104.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2193192  |
---------------------------------
Eval num_timesteps=2195184, episode_reward=1369.69 +/- 655.43
Episode length: 832.00 +/- 51.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2195184  |
---------------------------------
Eval num_timesteps=2197176, episode_reward=1741.11 +/- 637.48
Episode length: 840.80 +/- 51.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2197176  |
---------------------------------
Eval num_timesteps=2199168, episode_reward=1871.47 +/- 1198.67
Episode length: 778.60 +/- 86.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2199168  |
---------------------------------
Eval num_timesteps=2201160, episode_reward=1713.93 +/- 912.43
Episode length: 784.40 +/- 76.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 2201160  |
---------------------------------
Eval num_timesteps=2203152, episode_reward=1907.43 +/- 1528.71
Episode length: 817.00 +/- 137.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 2203152  |
---------------------------------
Eval num_timesteps=2205144, episode_reward=1131.34 +/- 1757.36
Episode length: 608.80 +/- 182.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2205144  |
---------------------------------
Eval num_timesteps=2207136, episode_reward=839.47 +/- 607.93
Episode length: 749.80 +/- 196.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 2207136  |
---------------------------------
Eval num_timesteps=2209128, episode_reward=2186.89 +/- 977.79
Episode length: 741.40 +/- 49.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 2209128  |
---------------------------------
Eval num_timesteps=2211120, episode_reward=1980.14 +/- 1201.84
Episode length: 814.00 +/- 84.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2211120  |
---------------------------------
Eval num_timesteps=2213112, episode_reward=1482.74 +/- 757.43
Episode length: 748.20 +/- 117.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 748          |
|    mean_reward          | 1.48e+03     |
| time/                   |              |
|    total_timesteps      | 2213112      |
| train/                  |              |
|    approx_kl            | 0.0035629997 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.1         |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 410          |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000884    |
|    std                  | 1.43         |
|    value_loss           | 570          |
------------------------------------------
Eval num_timesteps=2215104, episode_reward=1311.95 +/- 995.47
Episode length: 802.80 +/- 76.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2215104  |
---------------------------------
Eval num_timesteps=2217096, episode_reward=1557.00 +/- 802.61
Episode length: 805.20 +/- 22.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2217096  |
---------------------------------
Eval num_timesteps=2219088, episode_reward=1436.78 +/- 531.25
Episode length: 924.00 +/- 103.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2219088  |
---------------------------------
Eval num_timesteps=2221080, episode_reward=1686.97 +/- 926.79
Episode length: 792.20 +/- 181.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2221080  |
---------------------------------
Eval num_timesteps=2223072, episode_reward=1897.14 +/- 1154.81
Episode length: 798.00 +/- 88.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2223072  |
---------------------------------
Eval num_timesteps=2225064, episode_reward=2090.50 +/- 622.22
Episode length: 848.80 +/- 69.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 2225064  |
---------------------------------
Eval num_timesteps=2227056, episode_reward=1212.57 +/- 747.84
Episode length: 824.40 +/- 148.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2227056  |
---------------------------------
Eval num_timesteps=2229048, episode_reward=2136.51 +/- 1207.34
Episode length: 805.40 +/- 45.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 805      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 2229048  |
---------------------------------
Eval num_timesteps=2231040, episode_reward=2923.45 +/- 634.79
Episode length: 818.00 +/- 34.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 2231040  |
---------------------------------
New best mean reward!
Eval num_timesteps=2233032, episode_reward=2309.16 +/- 1067.13
Episode length: 798.80 +/- 113.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2233032  |
---------------------------------
Eval num_timesteps=2235024, episode_reward=2060.75 +/- 841.92
Episode length: 775.20 +/- 111.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2235024  |
---------------------------------
Eval num_timesteps=2237016, episode_reward=1344.72 +/- 867.65
Episode length: 787.80 +/- 120.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2237016  |
---------------------------------
Eval num_timesteps=2239008, episode_reward=1275.22 +/- 722.45
Episode length: 727.00 +/- 80.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2239008  |
---------------------------------
Eval num_timesteps=2241000, episode_reward=2116.48 +/- 270.69
Episode length: 889.60 +/- 124.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 890      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2241000  |
---------------------------------
Eval num_timesteps=2242992, episode_reward=1694.00 +/- 619.29
Episode length: 774.80 +/- 29.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2242992  |
---------------------------------
Eval num_timesteps=2244984, episode_reward=1899.91 +/- 1104.77
Episode length: 730.00 +/- 91.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 730      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2244984  |
---------------------------------
Eval num_timesteps=2246976, episode_reward=1724.12 +/- 828.68
Episode length: 750.00 +/- 89.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2246976  |
---------------------------------
Eval num_timesteps=2248968, episode_reward=2382.33 +/- 1049.19
Episode length: 764.60 +/- 112.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2248968  |
---------------------------------
Eval num_timesteps=2250960, episode_reward=1774.52 +/- 1196.00
Episode length: 777.20 +/- 35.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2250960  |
---------------------------------
Eval num_timesteps=2252952, episode_reward=1224.96 +/- 709.80
Episode length: 788.00 +/- 38.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2252952  |
---------------------------------
Eval num_timesteps=2254944, episode_reward=1629.18 +/- 1235.64
Episode length: 755.40 +/- 70.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2254944  |
---------------------------------
Eval num_timesteps=2256936, episode_reward=1816.44 +/- 1335.76
Episode length: 851.40 +/- 37.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2256936  |
---------------------------------
Eval num_timesteps=2258928, episode_reward=1054.61 +/- 271.07
Episode length: 766.00 +/- 83.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2258928  |
---------------------------------
Eval num_timesteps=2260920, episode_reward=735.65 +/- 395.99
Episode length: 727.20 +/- 124.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 2260920  |
---------------------------------
Eval num_timesteps=2262912, episode_reward=2129.00 +/- 1495.41
Episode length: 721.20 +/- 95.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 721          |
|    mean_reward          | 2.13e+03     |
| time/                   |              |
|    total_timesteps      | 2262912      |
| train/                  |              |
|    approx_kl            | 0.0029367618 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 161          |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.45         |
|    value_loss           | 497          |
------------------------------------------
Eval num_timesteps=2264904, episode_reward=2038.65 +/- 1248.74
Episode length: 736.20 +/- 37.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2264904  |
---------------------------------
Eval num_timesteps=2266896, episode_reward=1861.20 +/- 1008.04
Episode length: 772.40 +/- 211.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 2266896  |
---------------------------------
Eval num_timesteps=2268888, episode_reward=2323.28 +/- 1324.50
Episode length: 866.20 +/- 154.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 866      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2268888  |
---------------------------------
Eval num_timesteps=2270880, episode_reward=1917.55 +/- 1431.12
Episode length: 774.00 +/- 110.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2270880  |
---------------------------------
Eval num_timesteps=2272872, episode_reward=1948.37 +/- 843.44
Episode length: 816.80 +/- 104.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2272872  |
---------------------------------
Eval num_timesteps=2274864, episode_reward=1518.52 +/- 781.05
Episode length: 710.00 +/- 159.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 2274864  |
---------------------------------
Eval num_timesteps=2276856, episode_reward=2043.72 +/- 591.79
Episode length: 869.80 +/- 119.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2276856  |
---------------------------------
Eval num_timesteps=2278848, episode_reward=1267.92 +/- 779.94
Episode length: 758.00 +/- 62.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2278848  |
---------------------------------
Eval num_timesteps=2280840, episode_reward=925.48 +/- 793.61
Episode length: 678.20 +/- 104.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 2280840  |
---------------------------------
Eval num_timesteps=2282832, episode_reward=1742.02 +/- 805.60
Episode length: 798.60 +/- 83.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2282832  |
---------------------------------
Eval num_timesteps=2284824, episode_reward=2577.69 +/- 447.51
Episode length: 808.60 +/- 69.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2284824  |
---------------------------------
Eval num_timesteps=2286816, episode_reward=2217.67 +/- 1221.36
Episode length: 844.80 +/- 88.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2286816  |
---------------------------------
Eval num_timesteps=2288808, episode_reward=1531.65 +/- 1078.66
Episode length: 733.40 +/- 187.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2288808  |
---------------------------------
Eval num_timesteps=2290800, episode_reward=1668.34 +/- 885.06
Episode length: 854.60 +/- 56.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2290800  |
---------------------------------
Eval num_timesteps=2292792, episode_reward=1568.16 +/- 1461.45
Episode length: 732.80 +/- 114.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2292792  |
---------------------------------
Eval num_timesteps=2294784, episode_reward=1408.62 +/- 883.37
Episode length: 730.80 +/- 188.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2294784  |
---------------------------------
Eval num_timesteps=2296776, episode_reward=1864.64 +/- 1646.39
Episode length: 733.20 +/- 152.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 2296776  |
---------------------------------
Eval num_timesteps=2298768, episode_reward=1651.55 +/- 364.45
Episode length: 747.20 +/- 87.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2298768  |
---------------------------------
Eval num_timesteps=2300760, episode_reward=1778.32 +/- 1026.61
Episode length: 896.00 +/- 136.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2300760  |
---------------------------------
Eval num_timesteps=2302752, episode_reward=1204.59 +/- 1351.10
Episode length: 605.20 +/- 201.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2302752  |
---------------------------------
Eval num_timesteps=2304744, episode_reward=1948.56 +/- 1132.41
Episode length: 749.20 +/- 79.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2304744  |
---------------------------------
Eval num_timesteps=2306736, episode_reward=1494.16 +/- 1026.28
Episode length: 893.00 +/- 117.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 893      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 2306736  |
---------------------------------
Eval num_timesteps=2308728, episode_reward=2292.82 +/- 690.79
Episode length: 819.40 +/- 131.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2308728  |
---------------------------------
Eval num_timesteps=2310720, episode_reward=2662.11 +/- 1080.16
Episode length: 796.40 +/- 85.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 796          |
|    mean_reward          | 2.66e+03     |
| time/                   |              |
|    total_timesteps      | 2310720      |
| train/                  |              |
|    approx_kl            | 0.0027377538 |
|    clip_fraction        | 0.00638      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 263          |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00076     |
|    std                  | 1.46         |
|    value_loss           | 707          |
------------------------------------------
Eval num_timesteps=2312712, episode_reward=1425.81 +/- 702.62
Episode length: 791.00 +/- 162.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 2312712  |
---------------------------------
Eval num_timesteps=2314704, episode_reward=1699.61 +/- 484.00
Episode length: 813.00 +/- 94.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2314704  |
---------------------------------
Eval num_timesteps=2316696, episode_reward=1180.00 +/- 890.08
Episode length: 711.80 +/- 194.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2316696  |
---------------------------------
Eval num_timesteps=2318688, episode_reward=2467.36 +/- 1008.12
Episode length: 798.20 +/- 161.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2318688  |
---------------------------------
Eval num_timesteps=2320680, episode_reward=1207.07 +/- 796.99
Episode length: 684.80 +/- 65.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2320680  |
---------------------------------
Eval num_timesteps=2322672, episode_reward=1563.19 +/- 787.14
Episode length: 807.20 +/- 152.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2322672  |
---------------------------------
Eval num_timesteps=2324664, episode_reward=1804.34 +/- 1457.31
Episode length: 637.20 +/- 223.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2324664  |
---------------------------------
Eval num_timesteps=2326656, episode_reward=1019.63 +/- 479.92
Episode length: 684.00 +/- 71.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2326656  |
---------------------------------
Eval num_timesteps=2328648, episode_reward=2036.58 +/- 1111.82
Episode length: 776.00 +/- 101.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2328648  |
---------------------------------
Eval num_timesteps=2330640, episode_reward=2218.73 +/- 1719.25
Episode length: 788.00 +/- 190.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2330640  |
---------------------------------
Eval num_timesteps=2332632, episode_reward=1082.47 +/- 509.86
Episode length: 718.40 +/- 118.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2332632  |
---------------------------------
Eval num_timesteps=2334624, episode_reward=1478.52 +/- 616.44
Episode length: 764.40 +/- 86.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 2334624  |
---------------------------------
Eval num_timesteps=2336616, episode_reward=2730.39 +/- 1055.05
Episode length: 776.40 +/- 107.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 2336616  |
---------------------------------
Eval num_timesteps=2338608, episode_reward=1782.99 +/- 858.61
Episode length: 705.60 +/- 102.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2338608  |
---------------------------------
Eval num_timesteps=2340600, episode_reward=1679.08 +/- 697.29
Episode length: 782.60 +/- 137.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2340600  |
---------------------------------
Eval num_timesteps=2342592, episode_reward=1814.28 +/- 885.89
Episode length: 839.40 +/- 71.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2342592  |
---------------------------------
Eval num_timesteps=2344584, episode_reward=2243.74 +/- 1196.95
Episode length: 690.60 +/- 79.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2344584  |
---------------------------------
Eval num_timesteps=2346576, episode_reward=1014.17 +/- 922.50
Episode length: 615.80 +/- 134.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2346576  |
---------------------------------
Eval num_timesteps=2348568, episode_reward=2074.96 +/- 925.98
Episode length: 821.20 +/- 102.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2348568  |
---------------------------------
Eval num_timesteps=2350560, episode_reward=2036.24 +/- 635.98
Episode length: 806.40 +/- 61.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2350560  |
---------------------------------
Eval num_timesteps=2352552, episode_reward=2277.17 +/- 754.08
Episode length: 886.60 +/- 124.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 2352552  |
---------------------------------
Eval num_timesteps=2354544, episode_reward=1518.47 +/- 870.62
Episode length: 736.00 +/- 75.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 2354544  |
---------------------------------
Eval num_timesteps=2356536, episode_reward=2606.61 +/- 509.82
Episode length: 821.40 +/- 26.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 2356536  |
---------------------------------
Eval num_timesteps=2358528, episode_reward=958.53 +/- 929.36
Episode length: 650.00 +/- 169.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 2358528  |
---------------------------------
Eval num_timesteps=2360520, episode_reward=3125.66 +/- 704.15
Episode length: 877.40 +/- 50.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 877          |
|    mean_reward          | 3.13e+03     |
| time/                   |              |
|    total_timesteps      | 2360520      |
| train/                  |              |
|    approx_kl            | 0.0033492267 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.19        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 400          |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.46         |
|    value_loss           | 993          |
------------------------------------------
New best mean reward!
Eval num_timesteps=2362512, episode_reward=2556.87 +/- 500.54
Episode length: 827.40 +/- 39.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2362512  |
---------------------------------
Eval num_timesteps=2364504, episode_reward=1614.73 +/- 833.70
Episode length: 738.60 +/- 79.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2364504  |
---------------------------------
Eval num_timesteps=2366496, episode_reward=1628.98 +/- 1293.80
Episode length: 788.40 +/- 158.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2366496  |
---------------------------------
Eval num_timesteps=2368488, episode_reward=2335.04 +/- 1152.58
Episode length: 761.60 +/- 95.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2368488  |
---------------------------------
Eval num_timesteps=2370480, episode_reward=2563.89 +/- 718.85
Episode length: 814.00 +/- 29.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2370480  |
---------------------------------
Eval num_timesteps=2372472, episode_reward=2320.83 +/- 653.91
Episode length: 833.80 +/- 31.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 834      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2372472  |
---------------------------------
Eval num_timesteps=2374464, episode_reward=1432.69 +/- 1069.73
Episode length: 774.60 +/- 214.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 2374464  |
---------------------------------
Eval num_timesteps=2376456, episode_reward=3073.48 +/- 1470.46
Episode length: 816.20 +/- 67.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 816      |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 2376456  |
---------------------------------
Eval num_timesteps=2378448, episode_reward=2173.39 +/- 990.84
Episode length: 753.00 +/- 128.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 2378448  |
---------------------------------
Eval num_timesteps=2380440, episode_reward=1891.99 +/- 666.65
Episode length: 794.60 +/- 52.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 795      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2380440  |
---------------------------------
Eval num_timesteps=2382432, episode_reward=2760.82 +/- 945.38
Episode length: 831.80 +/- 51.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 2382432  |
---------------------------------
Eval num_timesteps=2384424, episode_reward=1681.89 +/- 1111.11
Episode length: 805.60 +/- 97.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2384424  |
---------------------------------
Eval num_timesteps=2386416, episode_reward=1546.91 +/- 1491.99
Episode length: 587.60 +/- 190.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2386416  |
---------------------------------
Eval num_timesteps=2388408, episode_reward=1396.97 +/- 681.57
Episode length: 741.40 +/- 89.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 2388408  |
---------------------------------
Eval num_timesteps=2390400, episode_reward=2174.51 +/- 805.37
Episode length: 883.40 +/- 177.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 2390400  |
---------------------------------
Eval num_timesteps=2392392, episode_reward=1225.22 +/- 1046.26
Episode length: 659.40 +/- 119.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2392392  |
---------------------------------
Eval num_timesteps=2394384, episode_reward=1682.78 +/- 990.17
Episode length: 744.40 +/- 221.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2394384  |
---------------------------------
Eval num_timesteps=2396376, episode_reward=1309.95 +/- 780.99
Episode length: 721.40 +/- 186.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2396376  |
---------------------------------
Eval num_timesteps=2398368, episode_reward=1039.18 +/- 543.28
Episode length: 738.00 +/- 193.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2398368  |
---------------------------------
Eval num_timesteps=2400360, episode_reward=1194.43 +/- 791.52
Episode length: 666.60 +/- 178.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2400360  |
---------------------------------
Eval num_timesteps=2402352, episode_reward=1775.56 +/- 1478.67
Episode length: 795.60 +/- 56.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 796      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2402352  |
---------------------------------
Eval num_timesteps=2404344, episode_reward=1578.15 +/- 859.32
Episode length: 712.40 +/- 141.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 2404344  |
---------------------------------
Eval num_timesteps=2406336, episode_reward=2249.72 +/- 1193.54
Episode length: 777.00 +/- 40.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 777      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 2406336  |
---------------------------------
Eval num_timesteps=2408328, episode_reward=1754.01 +/- 1112.70
Episode length: 805.60 +/- 85.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2408328  |
---------------------------------
Eval num_timesteps=2410320, episode_reward=1569.55 +/- 763.90
Episode length: 698.00 +/- 149.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 698          |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 2410320      |
| train/                  |              |
|    approx_kl            | 0.0030611258 |
|    clip_fraction        | 0.0073       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 293          |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000865    |
|    std                  | 1.48         |
|    value_loss           | 656          |
------------------------------------------
Eval num_timesteps=2412312, episode_reward=2071.76 +/- 1211.13
Episode length: 694.00 +/- 165.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2412312  |
---------------------------------
Eval num_timesteps=2414304, episode_reward=525.34 +/- 357.79
Episode length: 558.60 +/- 103.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 2414304  |
---------------------------------
Eval num_timesteps=2416296, episode_reward=1570.14 +/- 1345.09
Episode length: 596.80 +/- 170.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2416296  |
---------------------------------
Eval num_timesteps=2418288, episode_reward=1232.70 +/- 808.40
Episode length: 656.20 +/- 112.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2418288  |
---------------------------------
Eval num_timesteps=2420280, episode_reward=1693.47 +/- 1554.29
Episode length: 710.20 +/- 132.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2420280  |
---------------------------------
Eval num_timesteps=2422272, episode_reward=1343.40 +/- 1531.27
Episode length: 704.00 +/- 316.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2422272  |
---------------------------------
Eval num_timesteps=2424264, episode_reward=1981.55 +/- 1117.93
Episode length: 726.40 +/- 87.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2424264  |
---------------------------------
Eval num_timesteps=2426256, episode_reward=2006.07 +/- 1655.47
Episode length: 640.00 +/- 182.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2426256  |
---------------------------------
Eval num_timesteps=2428248, episode_reward=1137.44 +/- 855.72
Episode length: 618.00 +/- 82.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2428248  |
---------------------------------
Eval num_timesteps=2430240, episode_reward=2259.95 +/- 1299.00
Episode length: 714.00 +/- 104.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2430240  |
---------------------------------
Eval num_timesteps=2432232, episode_reward=1616.30 +/- 1650.85
Episode length: 616.60 +/- 229.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2432232  |
---------------------------------
Eval num_timesteps=2434224, episode_reward=1506.28 +/- 1238.43
Episode length: 702.80 +/- 204.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2434224  |
---------------------------------
Eval num_timesteps=2436216, episode_reward=944.31 +/- 575.28
Episode length: 679.80 +/- 106.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 2436216  |
---------------------------------
Eval num_timesteps=2438208, episode_reward=2647.65 +/- 1131.34
Episode length: 768.00 +/- 100.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 2438208  |
---------------------------------
Eval num_timesteps=2440200, episode_reward=1154.15 +/- 622.05
Episode length: 679.40 +/- 104.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2440200  |
---------------------------------
Eval num_timesteps=2442192, episode_reward=1587.36 +/- 1114.79
Episode length: 673.00 +/- 144.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2442192  |
---------------------------------
Eval num_timesteps=2444184, episode_reward=973.13 +/- 679.26
Episode length: 594.20 +/- 159.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 2444184  |
---------------------------------
Eval num_timesteps=2446176, episode_reward=1310.48 +/- 817.11
Episode length: 704.40 +/- 120.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2446176  |
---------------------------------
Eval num_timesteps=2448168, episode_reward=1479.45 +/- 1225.37
Episode length: 681.00 +/- 188.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 2448168  |
---------------------------------
Eval num_timesteps=2450160, episode_reward=993.95 +/- 1020.21
Episode length: 585.40 +/- 181.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 2450160  |
---------------------------------
Eval num_timesteps=2452152, episode_reward=1799.97 +/- 971.50
Episode length: 757.60 +/- 95.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2452152  |
---------------------------------
Eval num_timesteps=2454144, episode_reward=787.71 +/- 989.65
Episode length: 602.80 +/- 149.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 2454144  |
---------------------------------
Eval num_timesteps=2456136, episode_reward=2412.27 +/- 1208.10
Episode length: 813.20 +/- 77.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2456136  |
---------------------------------
Eval num_timesteps=2458128, episode_reward=1814.10 +/- 1012.88
Episode length: 719.00 +/- 183.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 719          |
|    mean_reward          | 1.81e+03     |
| time/                   |              |
|    total_timesteps      | 2458128      |
| train/                  |              |
|    approx_kl            | 0.0025033015 |
|    clip_fraction        | 0.00882      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 227          |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000869    |
|    std                  | 1.49         |
|    value_loss           | 575          |
------------------------------------------
Eval num_timesteps=2460120, episode_reward=1893.89 +/- 1483.17
Episode length: 688.00 +/- 153.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2460120  |
---------------------------------
Eval num_timesteps=2462112, episode_reward=1272.30 +/- 712.28
Episode length: 642.80 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2462112  |
---------------------------------
Eval num_timesteps=2464104, episode_reward=1371.20 +/- 993.13
Episode length: 621.00 +/- 144.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2464104  |
---------------------------------
Eval num_timesteps=2466096, episode_reward=1223.03 +/- 959.29
Episode length: 583.00 +/- 136.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2466096  |
---------------------------------
Eval num_timesteps=2468088, episode_reward=1628.23 +/- 1515.66
Episode length: 680.80 +/- 113.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2468088  |
---------------------------------
Eval num_timesteps=2470080, episode_reward=997.72 +/- 617.92
Episode length: 602.60 +/- 149.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 2470080  |
---------------------------------
Eval num_timesteps=2472072, episode_reward=1363.23 +/- 869.46
Episode length: 671.60 +/- 137.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2472072  |
---------------------------------
Eval num_timesteps=2474064, episode_reward=1822.20 +/- 694.39
Episode length: 806.80 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2474064  |
---------------------------------
Eval num_timesteps=2476056, episode_reward=1933.85 +/- 1556.95
Episode length: 680.20 +/- 141.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 2476056  |
---------------------------------
Eval num_timesteps=2478048, episode_reward=2666.66 +/- 432.63
Episode length: 844.60 +/- 102.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 2478048  |
---------------------------------
Eval num_timesteps=2480040, episode_reward=1887.84 +/- 1317.87
Episode length: 682.00 +/- 156.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2480040  |
---------------------------------
Eval num_timesteps=2482032, episode_reward=1574.59 +/- 1367.21
Episode length: 612.20 +/- 153.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2482032  |
---------------------------------
Eval num_timesteps=2484024, episode_reward=2211.12 +/- 812.82
Episode length: 745.80 +/- 93.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2484024  |
---------------------------------
Eval num_timesteps=2486016, episode_reward=2616.04 +/- 1133.52
Episode length: 738.60 +/- 90.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2486016  |
---------------------------------
Eval num_timesteps=2488008, episode_reward=2468.31 +/- 1015.63
Episode length: 734.20 +/- 86.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2488008  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=1458.57 +/- 824.11
Episode length: 714.60 +/- 127.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2490000  |
---------------------------------
Eval num_timesteps=2491992, episode_reward=2001.01 +/- 962.53
Episode length: 775.00 +/- 102.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2491992  |
---------------------------------
Eval num_timesteps=2493984, episode_reward=1162.17 +/- 1046.87
Episode length: 644.40 +/- 146.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2493984  |
---------------------------------
Eval num_timesteps=2495976, episode_reward=2539.08 +/- 817.75
Episode length: 773.60 +/- 104.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2495976  |
---------------------------------
Eval num_timesteps=2497968, episode_reward=1073.64 +/- 888.68
Episode length: 659.00 +/- 168.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2497968  |
---------------------------------
Eval num_timesteps=2499960, episode_reward=2762.31 +/- 160.31
Episode length: 844.40 +/- 33.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 2499960  |
---------------------------------
Eval num_timesteps=2501952, episode_reward=2996.39 +/- 892.45
Episode length: 800.80 +/- 85.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2501952  |
---------------------------------
Eval num_timesteps=2503944, episode_reward=2046.68 +/- 969.14
Episode length: 781.40 +/- 115.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2503944  |
---------------------------------
Eval num_timesteps=2505936, episode_reward=1555.17 +/- 1350.86
Episode length: 709.40 +/- 147.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2505936  |
---------------------------------
Eval num_timesteps=2507928, episode_reward=1569.73 +/- 1357.78
Episode length: 633.80 +/- 164.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 634          |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 2507928      |
| train/                  |              |
|    approx_kl            | 0.0023216007 |
|    clip_fraction        | 0.00681      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 302          |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00103     |
|    std                  | 1.5          |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=2509920, episode_reward=1276.42 +/- 941.82
Episode length: 644.20 +/- 127.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2509920  |
---------------------------------
Eval num_timesteps=2511912, episode_reward=1624.57 +/- 1038.32
Episode length: 715.20 +/- 58.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2511912  |
---------------------------------
Eval num_timesteps=2513904, episode_reward=1319.59 +/- 1091.51
Episode length: 644.80 +/- 147.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2513904  |
---------------------------------
Eval num_timesteps=2515896, episode_reward=791.13 +/- 738.35
Episode length: 535.80 +/- 168.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 2515896  |
---------------------------------
Eval num_timesteps=2517888, episode_reward=1808.08 +/- 1228.39
Episode length: 609.60 +/- 165.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2517888  |
---------------------------------
Eval num_timesteps=2519880, episode_reward=1170.45 +/- 969.69
Episode length: 671.80 +/- 96.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2519880  |
---------------------------------
Eval num_timesteps=2521872, episode_reward=1189.03 +/- 1238.31
Episode length: 585.20 +/- 164.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2521872  |
---------------------------------
Eval num_timesteps=2523864, episode_reward=1303.83 +/- 1120.67
Episode length: 624.20 +/- 194.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2523864  |
---------------------------------
Eval num_timesteps=2525856, episode_reward=1237.37 +/- 814.40
Episode length: 664.00 +/- 197.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2525856  |
---------------------------------
Eval num_timesteps=2527848, episode_reward=1450.84 +/- 1099.72
Episode length: 574.40 +/- 84.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 2527848  |
---------------------------------
Eval num_timesteps=2529840, episode_reward=996.13 +/- 1085.72
Episode length: 607.00 +/- 129.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 2529840  |
---------------------------------
Eval num_timesteps=2531832, episode_reward=1742.73 +/- 1101.74
Episode length: 748.80 +/- 88.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2531832  |
---------------------------------
Eval num_timesteps=2533824, episode_reward=2220.26 +/- 1246.66
Episode length: 664.80 +/- 110.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2533824  |
---------------------------------
Eval num_timesteps=2535816, episode_reward=1376.14 +/- 1049.19
Episode length: 655.40 +/- 131.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 2535816  |
---------------------------------
Eval num_timesteps=2537808, episode_reward=1825.16 +/- 1058.94
Episode length: 740.60 +/- 35.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 2537808  |
---------------------------------
Eval num_timesteps=2539800, episode_reward=1703.94 +/- 1097.91
Episode length: 731.80 +/- 147.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2539800  |
---------------------------------
Eval num_timesteps=2541792, episode_reward=2193.25 +/- 1045.70
Episode length: 697.60 +/- 88.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 2541792  |
---------------------------------
Eval num_timesteps=2543784, episode_reward=2025.27 +/- 760.81
Episode length: 713.40 +/- 69.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 2543784  |
---------------------------------
Eval num_timesteps=2545776, episode_reward=1035.55 +/- 768.36
Episode length: 570.40 +/- 134.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2545776  |
---------------------------------
Eval num_timesteps=2547768, episode_reward=1345.95 +/- 866.93
Episode length: 683.20 +/- 164.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2547768  |
---------------------------------
Eval num_timesteps=2549760, episode_reward=2309.26 +/- 830.48
Episode length: 783.60 +/- 29.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2549760  |
---------------------------------
Eval num_timesteps=2551752, episode_reward=1249.00 +/- 884.46
Episode length: 716.20 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2551752  |
---------------------------------
Eval num_timesteps=2553744, episode_reward=832.26 +/- 1002.80
Episode length: 582.00 +/- 95.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 2553744  |
---------------------------------
Eval num_timesteps=2555736, episode_reward=2069.35 +/- 1164.80
Episode length: 730.60 +/- 113.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2555736  |
---------------------------------
Eval num_timesteps=2557728, episode_reward=2703.01 +/- 1195.42
Episode length: 747.60 +/- 49.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 748         |
|    mean_reward          | 2.7e+03     |
| time/                   |             |
|    total_timesteps      | 2557728     |
| train/                  |             |
|    approx_kl            | 0.004576167 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.3        |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 186         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00125    |
|    std                  | 1.5         |
|    value_loss           | 531         |
-----------------------------------------
Eval num_timesteps=2559720, episode_reward=1400.86 +/- 589.29
Episode length: 685.80 +/- 60.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 2559720  |
---------------------------------
Eval num_timesteps=2561712, episode_reward=1539.62 +/- 1099.37
Episode length: 615.80 +/- 134.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2561712  |
---------------------------------
Eval num_timesteps=2563704, episode_reward=2479.63 +/- 842.65
Episode length: 765.80 +/- 44.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2563704  |
---------------------------------
Eval num_timesteps=2565696, episode_reward=1788.29 +/- 817.09
Episode length: 726.80 +/- 31.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2565696  |
---------------------------------
Eval num_timesteps=2567688, episode_reward=1413.45 +/- 1050.52
Episode length: 692.80 +/- 103.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2567688  |
---------------------------------
Eval num_timesteps=2569680, episode_reward=1405.33 +/- 995.17
Episode length: 722.80 +/- 105.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2569680  |
---------------------------------
Eval num_timesteps=2571672, episode_reward=1934.63 +/- 871.21
Episode length: 679.80 +/- 129.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 2571672  |
---------------------------------
Eval num_timesteps=2573664, episode_reward=2399.64 +/- 1099.10
Episode length: 709.60 +/- 43.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 2573664  |
---------------------------------
Eval num_timesteps=2575656, episode_reward=1829.39 +/- 730.43
Episode length: 757.00 +/- 48.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 2575656  |
---------------------------------
Eval num_timesteps=2577648, episode_reward=2213.74 +/- 693.44
Episode length: 756.40 +/- 41.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2577648  |
---------------------------------
Eval num_timesteps=2579640, episode_reward=1773.72 +/- 694.26
Episode length: 742.60 +/- 38.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2579640  |
---------------------------------
Eval num_timesteps=2581632, episode_reward=982.36 +/- 590.76
Episode length: 609.60 +/- 148.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 2581632  |
---------------------------------
Eval num_timesteps=2583624, episode_reward=1593.24 +/- 889.84
Episode length: 622.40 +/- 143.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2583624  |
---------------------------------
Eval num_timesteps=2585616, episode_reward=1593.00 +/- 883.07
Episode length: 724.40 +/- 60.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2585616  |
---------------------------------
Eval num_timesteps=2587608, episode_reward=1745.19 +/- 1312.40
Episode length: 682.40 +/- 160.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2587608  |
---------------------------------
Eval num_timesteps=2589600, episode_reward=2128.69 +/- 844.24
Episode length: 737.00 +/- 87.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2589600  |
---------------------------------
Eval num_timesteps=2591592, episode_reward=3345.80 +/- 926.60
Episode length: 793.20 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | 3.35e+03 |
| time/              |          |
|    total_timesteps | 2591592  |
---------------------------------
New best mean reward!
Eval num_timesteps=2593584, episode_reward=1643.87 +/- 976.52
Episode length: 633.00 +/- 50.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2593584  |
---------------------------------
Eval num_timesteps=2595576, episode_reward=2634.69 +/- 974.51
Episode length: 803.40 +/- 69.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 2595576  |
---------------------------------
Eval num_timesteps=2597568, episode_reward=2292.88 +/- 910.52
Episode length: 730.60 +/- 85.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2597568  |
---------------------------------
Eval num_timesteps=2599560, episode_reward=2618.56 +/- 889.44
Episode length: 716.80 +/- 89.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2599560  |
---------------------------------
Eval num_timesteps=2601552, episode_reward=1981.65 +/- 1152.04
Episode length: 681.00 +/- 114.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2601552  |
---------------------------------
Eval num_timesteps=2603544, episode_reward=1986.10 +/- 688.47
Episode length: 729.20 +/- 49.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2603544  |
---------------------------------
Eval num_timesteps=2605536, episode_reward=1565.71 +/- 961.04
Episode length: 614.00 +/- 121.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 614          |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 2605536      |
| train/                  |              |
|    approx_kl            | 0.0024912416 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 260          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000813    |
|    std                  | 1.51         |
|    value_loss           | 580          |
------------------------------------------
Eval num_timesteps=2607528, episode_reward=902.78 +/- 1002.35
Episode length: 542.00 +/- 166.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2607528  |
---------------------------------
Eval num_timesteps=2609520, episode_reward=1198.09 +/- 973.52
Episode length: 608.00 +/- 166.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2609520  |
---------------------------------
Eval num_timesteps=2611512, episode_reward=2623.76 +/- 750.56
Episode length: 684.00 +/- 78.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2611512  |
---------------------------------
Eval num_timesteps=2613504, episode_reward=2392.38 +/- 714.75
Episode length: 773.80 +/- 30.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 2613504  |
---------------------------------
Eval num_timesteps=2615496, episode_reward=1337.90 +/- 993.13
Episode length: 647.40 +/- 137.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2615496  |
---------------------------------
Eval num_timesteps=2617488, episode_reward=1655.77 +/- 912.69
Episode length: 663.60 +/- 115.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 2617488  |
---------------------------------
Eval num_timesteps=2619480, episode_reward=1881.61 +/- 1422.07
Episode length: 699.00 +/- 59.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 2619480  |
---------------------------------
Eval num_timesteps=2621472, episode_reward=1357.54 +/- 1041.57
Episode length: 645.60 +/- 148.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2621472  |
---------------------------------
Eval num_timesteps=2623464, episode_reward=2476.84 +/- 1398.29
Episode length: 749.60 +/- 56.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2623464  |
---------------------------------
Eval num_timesteps=2625456, episode_reward=1603.67 +/- 1116.51
Episode length: 651.20 +/- 57.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2625456  |
---------------------------------
Eval num_timesteps=2627448, episode_reward=2015.30 +/- 958.21
Episode length: 675.60 +/- 63.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2627448  |
---------------------------------
Eval num_timesteps=2629440, episode_reward=1872.58 +/- 804.56
Episode length: 691.40 +/- 31.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2629440  |
---------------------------------
Eval num_timesteps=2631432, episode_reward=1305.34 +/- 831.46
Episode length: 659.60 +/- 82.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2631432  |
---------------------------------
Eval num_timesteps=2633424, episode_reward=1790.81 +/- 451.14
Episode length: 681.80 +/- 56.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2633424  |
---------------------------------
Eval num_timesteps=2635416, episode_reward=2477.28 +/- 827.83
Episode length: 736.80 +/- 26.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2635416  |
---------------------------------
Eval num_timesteps=2637408, episode_reward=1354.68 +/- 1249.81
Episode length: 527.20 +/- 144.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 527      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2637408  |
---------------------------------
Eval num_timesteps=2639400, episode_reward=1152.04 +/- 915.36
Episode length: 649.40 +/- 158.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2639400  |
---------------------------------
Eval num_timesteps=2641392, episode_reward=2000.75 +/- 1410.35
Episode length: 707.60 +/- 48.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2641392  |
---------------------------------
Eval num_timesteps=2643384, episode_reward=1172.46 +/- 829.30
Episode length: 645.80 +/- 86.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2643384  |
---------------------------------
Eval num_timesteps=2645376, episode_reward=1669.73 +/- 1172.03
Episode length: 704.40 +/- 54.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2645376  |
---------------------------------
Eval num_timesteps=2647368, episode_reward=1725.07 +/- 1157.75
Episode length: 647.60 +/- 145.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 2647368  |
---------------------------------
Eval num_timesteps=2649360, episode_reward=2125.72 +/- 1254.71
Episode length: 744.80 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 745      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2649360  |
---------------------------------
Eval num_timesteps=2651352, episode_reward=2765.68 +/- 984.08
Episode length: 715.80 +/- 60.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 2651352  |
---------------------------------
Eval num_timesteps=2653344, episode_reward=1815.35 +/- 1027.96
Episode length: 662.80 +/- 130.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2653344  |
---------------------------------
Eval num_timesteps=2655336, episode_reward=1476.84 +/- 883.68
Episode length: 696.80 +/- 89.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 697         |
|    mean_reward          | 1.48e+03    |
| time/                   |             |
|    total_timesteps      | 2655336     |
| train/                  |             |
|    approx_kl            | 0.002402011 |
|    clip_fraction        | 0.00497     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 205         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0009     |
|    std                  | 1.52        |
|    value_loss           | 656         |
-----------------------------------------
Eval num_timesteps=2657328, episode_reward=2310.24 +/- 1109.88
Episode length: 663.00 +/- 116.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2657328  |
---------------------------------
Eval num_timesteps=2659320, episode_reward=2285.30 +/- 892.57
Episode length: 699.80 +/- 58.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2659320  |
---------------------------------
Eval num_timesteps=2661312, episode_reward=2069.07 +/- 916.03
Episode length: 682.40 +/- 53.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2661312  |
---------------------------------
Eval num_timesteps=2663304, episode_reward=2027.51 +/- 747.48
Episode length: 738.20 +/- 71.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 2663304  |
---------------------------------
Eval num_timesteps=2665296, episode_reward=916.85 +/- 828.14
Episode length: 610.60 +/- 169.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 2665296  |
---------------------------------
Eval num_timesteps=2667288, episode_reward=2498.00 +/- 615.14
Episode length: 752.00 +/- 29.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2667288  |
---------------------------------
Eval num_timesteps=2669280, episode_reward=2286.23 +/- 819.37
Episode length: 774.20 +/- 38.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2669280  |
---------------------------------
Eval num_timesteps=2671272, episode_reward=1476.01 +/- 1170.64
Episode length: 661.00 +/- 87.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 2671272  |
---------------------------------
Eval num_timesteps=2673264, episode_reward=2506.48 +/- 1251.43
Episode length: 762.80 +/- 19.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 2673264  |
---------------------------------
Eval num_timesteps=2675256, episode_reward=1588.15 +/- 1099.87
Episode length: 687.40 +/- 102.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2675256  |
---------------------------------
Eval num_timesteps=2677248, episode_reward=1224.19 +/- 1142.64
Episode length: 612.60 +/- 153.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2677248  |
---------------------------------
Eval num_timesteps=2679240, episode_reward=1653.99 +/- 1071.17
Episode length: 672.40 +/- 69.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2679240  |
---------------------------------
Eval num_timesteps=2681232, episode_reward=2155.48 +/- 690.75
Episode length: 709.40 +/- 47.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 2681232  |
---------------------------------
Eval num_timesteps=2683224, episode_reward=2206.28 +/- 1462.73
Episode length: 680.00 +/- 96.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2683224  |
---------------------------------
Eval num_timesteps=2685216, episode_reward=1306.31 +/- 924.31
Episode length: 692.40 +/- 71.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2685216  |
---------------------------------
Eval num_timesteps=2687208, episode_reward=1642.58 +/- 748.46
Episode length: 725.80 +/- 63.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2687208  |
---------------------------------
Eval num_timesteps=2689200, episode_reward=2597.88 +/- 1308.47
Episode length: 734.20 +/- 46.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2689200  |
---------------------------------
Eval num_timesteps=2691192, episode_reward=2674.19 +/- 728.84
Episode length: 751.60 +/- 43.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 2691192  |
---------------------------------
Eval num_timesteps=2693184, episode_reward=1414.14 +/- 312.57
Episode length: 652.80 +/- 48.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2693184  |
---------------------------------
Eval num_timesteps=2695176, episode_reward=2734.03 +/- 1582.22
Episode length: 692.60 +/- 131.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 2695176  |
---------------------------------
Eval num_timesteps=2697168, episode_reward=2366.69 +/- 773.77
Episode length: 686.60 +/- 67.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 2697168  |
---------------------------------
Eval num_timesteps=2699160, episode_reward=1760.78 +/- 1075.00
Episode length: 714.80 +/- 94.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 2699160  |
---------------------------------
Eval num_timesteps=2701152, episode_reward=2520.69 +/- 956.79
Episode length: 691.00 +/- 53.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 2701152  |
---------------------------------
Eval num_timesteps=2703144, episode_reward=2501.50 +/- 903.08
Episode length: 706.00 +/- 90.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2703144  |
---------------------------------
Eval num_timesteps=2705136, episode_reward=1129.73 +/- 791.02
Episode length: 617.40 +/- 133.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 617          |
|    mean_reward          | 1.13e+03     |
| time/                   |              |
|    total_timesteps      | 2705136      |
| train/                  |              |
|    approx_kl            | 0.0031359515 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000859    |
|    std                  | 1.52         |
|    value_loss           | 774          |
------------------------------------------
Eval num_timesteps=2707128, episode_reward=1961.15 +/- 1076.91
Episode length: 670.00 +/- 136.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2707128  |
---------------------------------
Eval num_timesteps=2709120, episode_reward=1573.25 +/- 916.04
Episode length: 732.60 +/- 45.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2709120  |
---------------------------------
Eval num_timesteps=2711112, episode_reward=1513.23 +/- 1204.68
Episode length: 617.00 +/- 163.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 2711112  |
---------------------------------
Eval num_timesteps=2713104, episode_reward=1385.83 +/- 1337.55
Episode length: 590.20 +/- 165.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2713104  |
---------------------------------
Eval num_timesteps=2715096, episode_reward=1702.00 +/- 896.33
Episode length: 694.00 +/- 79.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 2715096  |
---------------------------------
Eval num_timesteps=2717088, episode_reward=2509.32 +/- 1115.83
Episode length: 667.60 +/- 93.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 2717088  |
---------------------------------
Eval num_timesteps=2719080, episode_reward=1259.25 +/- 754.39
Episode length: 653.80 +/- 104.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2719080  |
---------------------------------
Eval num_timesteps=2721072, episode_reward=2055.57 +/- 1263.39
Episode length: 616.20 +/- 150.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2721072  |
---------------------------------
Eval num_timesteps=2723064, episode_reward=1469.04 +/- 841.85
Episode length: 679.80 +/- 96.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2723064  |
---------------------------------
Eval num_timesteps=2725056, episode_reward=1945.89 +/- 1208.96
Episode length: 678.80 +/- 79.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2725056  |
---------------------------------
Eval num_timesteps=2727048, episode_reward=1773.69 +/- 834.58
Episode length: 668.80 +/- 88.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2727048  |
---------------------------------
Eval num_timesteps=2729040, episode_reward=2399.38 +/- 1052.22
Episode length: 706.20 +/- 62.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 2729040  |
---------------------------------
Eval num_timesteps=2731032, episode_reward=2232.33 +/- 880.82
Episode length: 697.60 +/- 59.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 2731032  |
---------------------------------
Eval num_timesteps=2733024, episode_reward=2130.50 +/- 1376.90
Episode length: 692.60 +/- 63.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2733024  |
---------------------------------
Eval num_timesteps=2735016, episode_reward=1248.61 +/- 502.12
Episode length: 607.60 +/- 95.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2735016  |
---------------------------------
Eval num_timesteps=2737008, episode_reward=1793.49 +/- 1000.52
Episode length: 684.80 +/- 78.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 2737008  |
---------------------------------
Eval num_timesteps=2739000, episode_reward=2265.70 +/- 1374.30
Episode length: 639.60 +/- 145.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2739000  |
---------------------------------
Eval num_timesteps=2740992, episode_reward=1937.15 +/- 1343.31
Episode length: 670.00 +/- 89.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 2740992  |
---------------------------------
Eval num_timesteps=2742984, episode_reward=1129.85 +/- 734.78
Episode length: 649.40 +/- 83.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2742984  |
---------------------------------
Eval num_timesteps=2744976, episode_reward=1301.93 +/- 1146.80
Episode length: 603.80 +/- 98.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2744976  |
---------------------------------
Eval num_timesteps=2746968, episode_reward=1439.81 +/- 1222.47
Episode length: 586.80 +/- 161.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2746968  |
---------------------------------
Eval num_timesteps=2748960, episode_reward=425.96 +/- 284.07
Episode length: 487.40 +/- 86.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 2748960  |
---------------------------------
Eval num_timesteps=2750952, episode_reward=1642.02 +/- 465.37
Episode length: 718.40 +/- 82.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2750952  |
---------------------------------
Eval num_timesteps=2752944, episode_reward=1189.20 +/- 942.21
Episode length: 586.00 +/- 136.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 586         |
|    mean_reward          | 1.19e+03    |
| time/                   |             |
|    total_timesteps      | 2752944     |
| train/                  |             |
|    approx_kl            | 0.002104205 |
|    clip_fraction        | 0.00414     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.37       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 478         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000661   |
|    std                  | 1.53        |
|    value_loss           | 759         |
-----------------------------------------
Eval num_timesteps=2754936, episode_reward=1007.31 +/- 667.13
Episode length: 553.00 +/- 109.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2754936  |
---------------------------------
Eval num_timesteps=2756928, episode_reward=1436.43 +/- 838.10
Episode length: 693.80 +/- 93.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2756928  |
---------------------------------
Eval num_timesteps=2758920, episode_reward=1890.41 +/- 1489.37
Episode length: 688.40 +/- 112.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 2758920  |
---------------------------------
Eval num_timesteps=2760912, episode_reward=1501.59 +/- 1689.82
Episode length: 568.60 +/- 141.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 2760912  |
---------------------------------
Eval num_timesteps=2762904, episode_reward=2305.88 +/- 1826.69
Episode length: 672.20 +/- 61.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2762904  |
---------------------------------
Eval num_timesteps=2764896, episode_reward=1453.64 +/- 1732.98
Episode length: 554.60 +/- 72.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 2764896  |
---------------------------------
Eval num_timesteps=2766888, episode_reward=1048.76 +/- 673.97
Episode length: 608.60 +/- 142.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2766888  |
---------------------------------
Eval num_timesteps=2768880, episode_reward=1391.15 +/- 1433.07
Episode length: 528.40 +/- 140.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 528      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2768880  |
---------------------------------
Eval num_timesteps=2770872, episode_reward=2419.06 +/- 1381.79
Episode length: 690.60 +/- 56.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2770872  |
---------------------------------
Eval num_timesteps=2772864, episode_reward=878.64 +/- 223.80
Episode length: 670.80 +/- 125.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 2772864  |
---------------------------------
Eval num_timesteps=2774856, episode_reward=532.73 +/- 600.95
Episode length: 520.20 +/- 148.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 2774856  |
---------------------------------
Eval num_timesteps=2776848, episode_reward=689.17 +/- 538.32
Episode length: 558.80 +/- 180.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 2776848  |
---------------------------------
Eval num_timesteps=2778840, episode_reward=2214.80 +/- 1111.45
Episode length: 669.40 +/- 41.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 2778840  |
---------------------------------
Eval num_timesteps=2780832, episode_reward=1549.58 +/- 905.13
Episode length: 609.60 +/- 99.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2780832  |
---------------------------------
Eval num_timesteps=2782824, episode_reward=3727.14 +/- 762.74
Episode length: 717.40 +/- 36.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 3.73e+03 |
| time/              |          |
|    total_timesteps | 2782824  |
---------------------------------
New best mean reward!
Eval num_timesteps=2784816, episode_reward=1642.73 +/- 829.35
Episode length: 623.20 +/- 113.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2784816  |
---------------------------------
Eval num_timesteps=2786808, episode_reward=2378.03 +/- 1200.54
Episode length: 680.80 +/- 88.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2786808  |
---------------------------------
Eval num_timesteps=2788800, episode_reward=1157.45 +/- 607.97
Episode length: 660.80 +/- 64.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2788800  |
---------------------------------
Eval num_timesteps=2790792, episode_reward=1462.36 +/- 1043.22
Episode length: 619.20 +/- 136.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2790792  |
---------------------------------
Eval num_timesteps=2792784, episode_reward=2122.93 +/- 1143.36
Episode length: 705.60 +/- 101.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2792784  |
---------------------------------
Eval num_timesteps=2794776, episode_reward=1174.01 +/- 414.04
Episode length: 601.40 +/- 122.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2794776  |
---------------------------------
Eval num_timesteps=2796768, episode_reward=2270.01 +/- 940.35
Episode length: 715.00 +/- 63.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2796768  |
---------------------------------
Eval num_timesteps=2798760, episode_reward=1738.06 +/- 1406.78
Episode length: 611.20 +/- 131.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2798760  |
---------------------------------
Eval num_timesteps=2800752, episode_reward=1148.24 +/- 741.04
Episode length: 580.60 +/- 136.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2800752  |
---------------------------------
Eval num_timesteps=2802744, episode_reward=1426.68 +/- 567.73
Episode length: 655.40 +/- 109.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 655         |
|    mean_reward          | 1.43e+03    |
| time/                   |             |
|    total_timesteps      | 2802744     |
| train/                  |             |
|    approx_kl            | 0.003580453 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.39       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 201         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00101    |
|    std                  | 1.54        |
|    value_loss           | 615         |
-----------------------------------------
Eval num_timesteps=2804736, episode_reward=1442.13 +/- 679.30
Episode length: 612.80 +/- 123.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 2804736  |
---------------------------------
Eval num_timesteps=2806728, episode_reward=2037.96 +/- 1000.14
Episode length: 687.60 +/- 98.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2806728  |
---------------------------------
Eval num_timesteps=2808720, episode_reward=1518.06 +/- 808.08
Episode length: 725.40 +/- 80.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 2808720  |
---------------------------------
Eval num_timesteps=2810712, episode_reward=1969.29 +/- 967.43
Episode length: 671.60 +/- 98.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 2810712  |
---------------------------------
Eval num_timesteps=2812704, episode_reward=2298.40 +/- 955.73
Episode length: 735.00 +/- 53.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 2812704  |
---------------------------------
Eval num_timesteps=2814696, episode_reward=1816.58 +/- 1148.01
Episode length: 704.60 +/- 94.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2814696  |
---------------------------------
Eval num_timesteps=2816688, episode_reward=1392.16 +/- 879.36
Episode length: 710.40 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2816688  |
---------------------------------
Eval num_timesteps=2818680, episode_reward=2377.35 +/- 827.70
Episode length: 749.20 +/- 60.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2818680  |
---------------------------------
Eval num_timesteps=2820672, episode_reward=2220.82 +/- 1489.51
Episode length: 630.20 +/- 122.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2820672  |
---------------------------------
Eval num_timesteps=2822664, episode_reward=1445.14 +/- 829.61
Episode length: 766.40 +/- 23.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 2822664  |
---------------------------------
Eval num_timesteps=2824656, episode_reward=1980.27 +/- 786.14
Episode length: 737.80 +/- 36.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2824656  |
---------------------------------
Eval num_timesteps=2826648, episode_reward=1159.01 +/- 1062.37
Episode length: 611.00 +/- 128.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2826648  |
---------------------------------
Eval num_timesteps=2828640, episode_reward=1722.66 +/- 983.66
Episode length: 731.80 +/- 50.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2828640  |
---------------------------------
Eval num_timesteps=2830632, episode_reward=1121.41 +/- 1129.68
Episode length: 629.20 +/- 198.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2830632  |
---------------------------------
Eval num_timesteps=2832624, episode_reward=2832.89 +/- 719.17
Episode length: 699.60 +/- 67.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 2832624  |
---------------------------------
Eval num_timesteps=2834616, episode_reward=1257.84 +/- 752.05
Episode length: 676.00 +/- 85.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2834616  |
---------------------------------
Eval num_timesteps=2836608, episode_reward=1415.73 +/- 791.09
Episode length: 695.20 +/- 68.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2836608  |
---------------------------------
Eval num_timesteps=2838600, episode_reward=1587.04 +/- 665.47
Episode length: 626.80 +/- 95.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2838600  |
---------------------------------
Eval num_timesteps=2840592, episode_reward=1622.71 +/- 846.62
Episode length: 709.00 +/- 65.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2840592  |
---------------------------------
Eval num_timesteps=2842584, episode_reward=2101.67 +/- 827.61
Episode length: 714.40 +/- 84.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2842584  |
---------------------------------
Eval num_timesteps=2844576, episode_reward=1902.05 +/- 785.18
Episode length: 693.20 +/- 51.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2844576  |
---------------------------------
Eval num_timesteps=2846568, episode_reward=2242.58 +/- 614.83
Episode length: 697.20 +/- 53.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2846568  |
---------------------------------
Eval num_timesteps=2848560, episode_reward=2472.61 +/- 1377.54
Episode length: 638.60 +/- 139.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2848560  |
---------------------------------
Eval num_timesteps=2850552, episode_reward=1983.62 +/- 1392.22
Episode length: 661.40 +/- 74.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2850552  |
---------------------------------
Eval num_timesteps=2852544, episode_reward=2414.65 +/- 1590.11
Episode length: 750.20 +/- 26.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 750         |
|    mean_reward          | 2.41e+03    |
| time/                   |             |
|    total_timesteps      | 2852544     |
| train/                  |             |
|    approx_kl            | 0.001732119 |
|    clip_fraction        | 0.00387     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.41       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 291         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.000445   |
|    std                  | 1.55        |
|    value_loss           | 880         |
-----------------------------------------
Eval num_timesteps=2854536, episode_reward=1709.23 +/- 1126.96
Episode length: 657.80 +/- 122.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 2854536  |
---------------------------------
Eval num_timesteps=2856528, episode_reward=1667.69 +/- 1132.49
Episode length: 709.80 +/- 127.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2856528  |
---------------------------------
Eval num_timesteps=2858520, episode_reward=2158.33 +/- 913.41
Episode length: 748.40 +/- 92.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 2858520  |
---------------------------------
Eval num_timesteps=2860512, episode_reward=1344.15 +/- 807.64
Episode length: 647.80 +/- 84.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2860512  |
---------------------------------
Eval num_timesteps=2862504, episode_reward=2234.16 +/- 743.87
Episode length: 715.60 +/- 78.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 2862504  |
---------------------------------
Eval num_timesteps=2864496, episode_reward=950.37 +/- 668.20
Episode length: 619.80 +/- 201.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 2864496  |
---------------------------------
Eval num_timesteps=2866488, episode_reward=2310.58 +/- 1185.24
Episode length: 685.00 +/- 84.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2866488  |
---------------------------------
Eval num_timesteps=2868480, episode_reward=1621.08 +/- 1138.32
Episode length: 645.40 +/- 110.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2868480  |
---------------------------------
Eval num_timesteps=2870472, episode_reward=2325.64 +/- 1004.97
Episode length: 686.80 +/- 136.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 2870472  |
---------------------------------
Eval num_timesteps=2872464, episode_reward=1533.05 +/- 1191.36
Episode length: 606.00 +/- 81.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2872464  |
---------------------------------
Eval num_timesteps=2874456, episode_reward=3183.26 +/- 1186.51
Episode length: 734.60 +/- 48.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 2874456  |
---------------------------------
Eval num_timesteps=2876448, episode_reward=1929.20 +/- 731.76
Episode length: 698.00 +/- 83.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 2876448  |
---------------------------------
Eval num_timesteps=2878440, episode_reward=2219.63 +/- 1131.17
Episode length: 730.60 +/- 82.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 2878440  |
---------------------------------
Eval num_timesteps=2880432, episode_reward=1409.48 +/- 894.73
Episode length: 627.00 +/- 90.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2880432  |
---------------------------------
Eval num_timesteps=2882424, episode_reward=2331.81 +/- 1388.75
Episode length: 698.80 +/- 77.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 2882424  |
---------------------------------
Eval num_timesteps=2884416, episode_reward=1998.18 +/- 1465.82
Episode length: 644.20 +/- 140.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2884416  |
---------------------------------
Eval num_timesteps=2886408, episode_reward=2268.03 +/- 860.32
Episode length: 728.60 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2886408  |
---------------------------------
Eval num_timesteps=2888400, episode_reward=2351.42 +/- 1111.80
Episode length: 693.60 +/- 91.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2888400  |
---------------------------------
Eval num_timesteps=2890392, episode_reward=1605.19 +/- 834.94
Episode length: 659.80 +/- 96.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2890392  |
---------------------------------
Eval num_timesteps=2892384, episode_reward=1470.51 +/- 945.31
Episode length: 630.00 +/- 112.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 2892384  |
---------------------------------
Eval num_timesteps=2894376, episode_reward=2250.48 +/- 818.00
Episode length: 749.80 +/- 34.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 750      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 2894376  |
---------------------------------
Eval num_timesteps=2896368, episode_reward=1583.96 +/- 553.90
Episode length: 727.80 +/- 60.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 2896368  |
---------------------------------
Eval num_timesteps=2898360, episode_reward=2559.48 +/- 596.24
Episode length: 751.60 +/- 93.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2898360  |
---------------------------------
Eval num_timesteps=2900352, episode_reward=2473.98 +/- 999.96
Episode length: 701.60 +/- 26.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 702          |
|    mean_reward          | 2.47e+03     |
| time/                   |              |
|    total_timesteps      | 2900352      |
| train/                  |              |
|    approx_kl            | 0.0023808917 |
|    clip_fraction        | 0.00692      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000738    |
|    std                  | 1.56         |
|    value_loss           | 664          |
------------------------------------------
Eval num_timesteps=2902344, episode_reward=1917.30 +/- 750.03
Episode length: 632.40 +/- 95.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2902344  |
---------------------------------
Eval num_timesteps=2904336, episode_reward=1608.42 +/- 1300.42
Episode length: 616.20 +/- 89.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 2904336  |
---------------------------------
Eval num_timesteps=2906328, episode_reward=1259.12 +/- 1025.96
Episode length: 626.60 +/- 125.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2906328  |
---------------------------------
Eval num_timesteps=2908320, episode_reward=1983.35 +/- 1376.65
Episode length: 654.20 +/- 106.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2908320  |
---------------------------------
Eval num_timesteps=2910312, episode_reward=1356.30 +/- 740.68
Episode length: 677.80 +/- 97.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2910312  |
---------------------------------
Eval num_timesteps=2912304, episode_reward=2443.35 +/- 1032.43
Episode length: 721.40 +/- 111.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2912304  |
---------------------------------
Eval num_timesteps=2914296, episode_reward=1279.44 +/- 1127.93
Episode length: 604.60 +/- 110.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2914296  |
---------------------------------
Eval num_timesteps=2916288, episode_reward=2181.49 +/- 1410.27
Episode length: 606.80 +/- 137.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 2916288  |
---------------------------------
Eval num_timesteps=2918280, episode_reward=1970.77 +/- 1377.07
Episode length: 757.40 +/- 44.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 2918280  |
---------------------------------
Eval num_timesteps=2920272, episode_reward=1897.68 +/- 1052.32
Episode length: 723.80 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2920272  |
---------------------------------
Eval num_timesteps=2922264, episode_reward=2011.25 +/- 1294.70
Episode length: 639.80 +/- 79.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2922264  |
---------------------------------
Eval num_timesteps=2924256, episode_reward=2202.38 +/- 1020.77
Episode length: 625.20 +/- 34.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 2924256  |
---------------------------------
Eval num_timesteps=2926248, episode_reward=2002.88 +/- 1234.05
Episode length: 669.20 +/- 152.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2926248  |
---------------------------------
Eval num_timesteps=2928240, episode_reward=2232.74 +/- 1149.50
Episode length: 731.60 +/- 47.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 2928240  |
---------------------------------
Eval num_timesteps=2930232, episode_reward=2069.54 +/- 836.68
Episode length: 693.00 +/- 60.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2930232  |
---------------------------------
Eval num_timesteps=2932224, episode_reward=2237.03 +/- 1993.36
Episode length: 610.20 +/- 144.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2932224  |
---------------------------------
Eval num_timesteps=2934216, episode_reward=1949.02 +/- 1193.54
Episode length: 685.80 +/- 88.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2934216  |
---------------------------------
Eval num_timesteps=2936208, episode_reward=1386.02 +/- 854.37
Episode length: 628.40 +/- 117.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2936208  |
---------------------------------
Eval num_timesteps=2938200, episode_reward=856.05 +/- 1046.48
Episode length: 560.40 +/- 152.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 2938200  |
---------------------------------
Eval num_timesteps=2940192, episode_reward=1952.52 +/- 1370.45
Episode length: 645.20 +/- 134.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 2940192  |
---------------------------------
Eval num_timesteps=2942184, episode_reward=1270.70 +/- 1386.50
Episode length: 617.60 +/- 138.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2942184  |
---------------------------------
Eval num_timesteps=2944176, episode_reward=2336.90 +/- 1289.70
Episode length: 647.40 +/- 97.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2944176  |
---------------------------------
Eval num_timesteps=2946168, episode_reward=1920.90 +/- 861.22
Episode length: 667.80 +/- 44.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2946168  |
---------------------------------
Eval num_timesteps=2948160, episode_reward=1984.10 +/- 1401.88
Episode length: 624.20 +/- 127.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 2948160  |
---------------------------------
Eval num_timesteps=2950152, episode_reward=1834.77 +/- 1243.75
Episode length: 675.40 +/- 47.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 1.83e+03     |
| time/                   |              |
|    total_timesteps      | 2950152      |
| train/                  |              |
|    approx_kl            | 0.0032099048 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.000848    |
|    std                  | 1.59         |
|    value_loss           | 551          |
------------------------------------------
Eval num_timesteps=2952144, episode_reward=1903.16 +/- 743.95
Episode length: 758.20 +/- 22.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 2952144  |
---------------------------------
Eval num_timesteps=2954136, episode_reward=1462.02 +/- 1106.62
Episode length: 601.60 +/- 138.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 2954136  |
---------------------------------
Eval num_timesteps=2956128, episode_reward=2089.49 +/- 677.00
Episode length: 703.00 +/- 41.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 2956128  |
---------------------------------
Eval num_timesteps=2958120, episode_reward=2134.96 +/- 1301.10
Episode length: 672.00 +/- 47.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2958120  |
---------------------------------
Eval num_timesteps=2960112, episode_reward=780.21 +/- 471.90
Episode length: 596.40 +/- 151.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 2960112  |
---------------------------------
Eval num_timesteps=2962104, episode_reward=1762.14 +/- 1085.82
Episode length: 720.40 +/- 24.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 2962104  |
---------------------------------
Eval num_timesteps=2964096, episode_reward=1282.48 +/- 690.09
Episode length: 603.60 +/- 127.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2964096  |
---------------------------------
Eval num_timesteps=2966088, episode_reward=1685.33 +/- 1236.56
Episode length: 681.40 +/- 130.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2966088  |
---------------------------------
Eval num_timesteps=2968080, episode_reward=2268.00 +/- 1792.12
Episode length: 656.00 +/- 33.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2968080  |
---------------------------------
Eval num_timesteps=2970072, episode_reward=2078.26 +/- 1153.91
Episode length: 701.20 +/- 62.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2970072  |
---------------------------------
Eval num_timesteps=2972064, episode_reward=1652.40 +/- 801.20
Episode length: 715.80 +/- 53.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2972064  |
---------------------------------
Eval num_timesteps=2974056, episode_reward=1364.18 +/- 583.08
Episode length: 678.60 +/- 24.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2974056  |
---------------------------------
Eval num_timesteps=2976048, episode_reward=1669.15 +/- 825.08
Episode length: 672.80 +/- 93.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2976048  |
---------------------------------
Eval num_timesteps=2978040, episode_reward=1260.38 +/- 457.82
Episode length: 644.60 +/- 106.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2978040  |
---------------------------------
Eval num_timesteps=2980032, episode_reward=2662.36 +/- 1076.94
Episode length: 704.60 +/- 75.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 2980032  |
---------------------------------
Eval num_timesteps=2982024, episode_reward=1854.55 +/- 1043.47
Episode length: 712.00 +/- 90.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2982024  |
---------------------------------
Eval num_timesteps=2984016, episode_reward=1717.04 +/- 921.45
Episode length: 686.00 +/- 106.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2984016  |
---------------------------------
Eval num_timesteps=2986008, episode_reward=1781.98 +/- 1183.34
Episode length: 664.80 +/- 190.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2986008  |
---------------------------------
Eval num_timesteps=2988000, episode_reward=2043.85 +/- 844.02
Episode length: 658.00 +/- 57.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2989992, episode_reward=2186.78 +/- 1035.12
Episode length: 690.20 +/- 79.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 2989992  |
---------------------------------
Eval num_timesteps=2991984, episode_reward=1568.39 +/- 1097.46
Episode length: 701.20 +/- 99.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2991984  |
---------------------------------
Eval num_timesteps=2993976, episode_reward=1687.76 +/- 1123.83
Episode length: 649.20 +/- 118.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2993976  |
---------------------------------
Eval num_timesteps=2995968, episode_reward=1848.99 +/- 767.51
Episode length: 699.40 +/- 42.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2995968  |
---------------------------------
Eval num_timesteps=2997960, episode_reward=2311.48 +/- 1359.14
Episode length: 645.20 +/- 83.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2997960  |
---------------------------------
Eval num_timesteps=2999952, episode_reward=2233.58 +/- 1099.36
Episode length: 729.80 +/- 19.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 730        |
|    mean_reward          | 2.23e+03   |
| time/                   |            |
|    total_timesteps      | 2999952    |
| train/                  |            |
|    approx_kl            | 0.00271362 |
|    clip_fraction        | 0.00825    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.55      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.001      |
|    loss                 | 195        |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.000847  |
|    std                  | 1.61       |
|    value_loss           | 833        |
----------------------------------------
Eval num_timesteps=3001944, episode_reward=1667.81 +/- 1171.12
Episode length: 671.60 +/- 95.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3001944  |
---------------------------------
Eval num_timesteps=3003936, episode_reward=1543.63 +/- 1210.23
Episode length: 695.80 +/- 69.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3003936  |
---------------------------------
Eval num_timesteps=3005928, episode_reward=2024.00 +/- 632.33
Episode length: 690.40 +/- 71.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3005928  |
---------------------------------
Eval num_timesteps=3007920, episode_reward=1455.77 +/- 870.91
Episode length: 695.60 +/- 90.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3007920  |
---------------------------------
Eval num_timesteps=3009912, episode_reward=1655.45 +/- 1089.67
Episode length: 719.60 +/- 49.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3009912  |
---------------------------------
Eval num_timesteps=3011904, episode_reward=1184.51 +/- 988.04
Episode length: 702.20 +/- 70.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3011904  |
---------------------------------
Eval num_timesteps=3013896, episode_reward=1730.60 +/- 1421.58
Episode length: 726.00 +/- 47.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3013896  |
---------------------------------
Eval num_timesteps=3015888, episode_reward=1041.11 +/- 370.95
Episode length: 679.80 +/- 87.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3015888  |
---------------------------------
Eval num_timesteps=3017880, episode_reward=1950.36 +/- 926.97
Episode length: 718.60 +/- 57.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3017880  |
---------------------------------
Eval num_timesteps=3019872, episode_reward=1858.53 +/- 1051.88
Episode length: 724.00 +/- 41.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 3019872  |
---------------------------------
Eval num_timesteps=3021864, episode_reward=1643.35 +/- 976.08
Episode length: 734.40 +/- 37.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3021864  |
---------------------------------
Eval num_timesteps=3023856, episode_reward=1675.36 +/- 1033.24
Episode length: 694.80 +/- 75.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3023856  |
---------------------------------
Eval num_timesteps=3025848, episode_reward=1988.85 +/- 951.50
Episode length: 752.60 +/- 63.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 753      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3025848  |
---------------------------------
Eval num_timesteps=3027840, episode_reward=1715.28 +/- 1327.05
Episode length: 675.00 +/- 72.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3027840  |
---------------------------------
Eval num_timesteps=3029832, episode_reward=2271.95 +/- 1126.33
Episode length: 693.00 +/- 51.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3029832  |
---------------------------------
Eval num_timesteps=3031824, episode_reward=1022.56 +/- 496.30
Episode length: 672.00 +/- 61.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3031824  |
---------------------------------
Eval num_timesteps=3033816, episode_reward=1201.30 +/- 342.98
Episode length: 717.60 +/- 38.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3033816  |
---------------------------------
Eval num_timesteps=3035808, episode_reward=1566.42 +/- 723.30
Episode length: 666.20 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3035808  |
---------------------------------
Eval num_timesteps=3037800, episode_reward=1313.16 +/- 410.42
Episode length: 719.00 +/- 67.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3037800  |
---------------------------------
Eval num_timesteps=3039792, episode_reward=2471.10 +/- 1202.34
Episode length: 726.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 3039792  |
---------------------------------
Eval num_timesteps=3041784, episode_reward=1183.30 +/- 1049.64
Episode length: 693.40 +/- 44.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3041784  |
---------------------------------
Eval num_timesteps=3043776, episode_reward=1680.92 +/- 1178.05
Episode length: 714.20 +/- 56.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3043776  |
---------------------------------
Eval num_timesteps=3045768, episode_reward=946.81 +/- 396.22
Episode length: 710.60 +/- 22.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 3045768  |
---------------------------------
Eval num_timesteps=3047760, episode_reward=1584.22 +/- 746.31
Episode length: 717.00 +/- 69.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 717          |
|    mean_reward          | 1.58e+03     |
| time/                   |              |
|    total_timesteps      | 3047760      |
| train/                  |              |
|    approx_kl            | 0.0031095315 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 269          |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000602    |
|    std                  | 1.61         |
|    value_loss           | 691          |
------------------------------------------
Eval num_timesteps=3049752, episode_reward=919.85 +/- 758.08
Episode length: 670.00 +/- 49.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 3049752  |
---------------------------------
Eval num_timesteps=3051744, episode_reward=2086.24 +/- 1348.75
Episode length: 721.00 +/- 39.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3051744  |
---------------------------------
Eval num_timesteps=3053736, episode_reward=1408.59 +/- 811.71
Episode length: 709.20 +/- 67.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3053736  |
---------------------------------
Eval num_timesteps=3055728, episode_reward=1947.00 +/- 998.37
Episode length: 700.60 +/- 36.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3055728  |
---------------------------------
Eval num_timesteps=3057720, episode_reward=1588.31 +/- 911.36
Episode length: 676.40 +/- 47.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3057720  |
---------------------------------
Eval num_timesteps=3059712, episode_reward=1696.61 +/- 787.04
Episode length: 649.40 +/- 85.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3059712  |
---------------------------------
Eval num_timesteps=3061704, episode_reward=1030.99 +/- 388.95
Episode length: 684.20 +/- 71.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3061704  |
---------------------------------
Eval num_timesteps=3063696, episode_reward=1583.05 +/- 854.34
Episode length: 641.60 +/- 77.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3063696  |
---------------------------------
Eval num_timesteps=3065688, episode_reward=1355.55 +/- 1289.03
Episode length: 621.40 +/- 157.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3065688  |
---------------------------------
Eval num_timesteps=3067680, episode_reward=1222.42 +/- 445.49
Episode length: 714.20 +/- 84.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3067680  |
---------------------------------
Eval num_timesteps=3069672, episode_reward=1431.89 +/- 1032.89
Episode length: 706.40 +/- 42.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3069672  |
---------------------------------
Eval num_timesteps=3071664, episode_reward=1729.15 +/- 1094.11
Episode length: 712.40 +/- 31.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3071664  |
---------------------------------
Eval num_timesteps=3073656, episode_reward=2142.08 +/- 874.66
Episode length: 657.60 +/- 72.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3073656  |
---------------------------------
Eval num_timesteps=3075648, episode_reward=1397.93 +/- 1249.57
Episode length: 710.40 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3075648  |
---------------------------------
Eval num_timesteps=3077640, episode_reward=1604.57 +/- 743.68
Episode length: 721.40 +/- 104.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3077640  |
---------------------------------
Eval num_timesteps=3079632, episode_reward=1963.34 +/- 1101.18
Episode length: 731.20 +/- 19.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3079632  |
---------------------------------
Eval num_timesteps=3081624, episode_reward=2545.14 +/- 1544.63
Episode length: 747.00 +/- 41.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3081624  |
---------------------------------
Eval num_timesteps=3083616, episode_reward=1297.90 +/- 628.28
Episode length: 683.20 +/- 69.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3083616  |
---------------------------------
Eval num_timesteps=3085608, episode_reward=2645.08 +/- 1056.75
Episode length: 728.80 +/- 27.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3085608  |
---------------------------------
Eval num_timesteps=3087600, episode_reward=2025.59 +/- 1419.14
Episode length: 654.20 +/- 115.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 3087600  |
---------------------------------
Eval num_timesteps=3089592, episode_reward=1054.26 +/- 793.00
Episode length: 677.40 +/- 141.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3089592  |
---------------------------------
Eval num_timesteps=3091584, episode_reward=1744.64 +/- 644.70
Episode length: 726.20 +/- 82.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3091584  |
---------------------------------
Eval num_timesteps=3093576, episode_reward=1060.70 +/- 1146.73
Episode length: 693.80 +/- 94.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3093576  |
---------------------------------
Eval num_timesteps=3095568, episode_reward=1949.85 +/- 1007.74
Episode length: 695.40 +/- 84.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3095568  |
---------------------------------
Eval num_timesteps=3097560, episode_reward=2265.18 +/- 901.04
Episode length: 710.00 +/- 79.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 710          |
|    mean_reward          | 2.27e+03     |
| time/                   |              |
|    total_timesteps      | 3097560      |
| train/                  |              |
|    approx_kl            | 0.0029297404 |
|    clip_fraction        | 0.00965      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.61        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 257          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.63         |
|    value_loss           | 827          |
------------------------------------------
Eval num_timesteps=3099552, episode_reward=1028.65 +/- 372.31
Episode length: 666.00 +/- 80.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3099552  |
---------------------------------
Eval num_timesteps=3101544, episode_reward=799.48 +/- 806.29
Episode length: 531.00 +/- 165.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 531      |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 3101544  |
---------------------------------
Eval num_timesteps=3103536, episode_reward=2030.53 +/- 926.72
Episode length: 677.40 +/- 88.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 3103536  |
---------------------------------
Eval num_timesteps=3105528, episode_reward=1716.55 +/- 1390.13
Episode length: 597.80 +/- 146.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3105528  |
---------------------------------
Eval num_timesteps=3107520, episode_reward=2012.25 +/- 1292.00
Episode length: 629.00 +/- 100.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3107520  |
---------------------------------
Eval num_timesteps=3109512, episode_reward=2294.03 +/- 537.41
Episode length: 671.20 +/- 34.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 3109512  |
---------------------------------
Eval num_timesteps=3111504, episode_reward=1842.32 +/- 1266.92
Episode length: 669.60 +/- 96.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3111504  |
---------------------------------
Eval num_timesteps=3113496, episode_reward=2187.90 +/- 686.32
Episode length: 684.40 +/- 57.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3113496  |
---------------------------------
Eval num_timesteps=3115488, episode_reward=2341.47 +/- 625.93
Episode length: 697.80 +/- 37.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3115488  |
---------------------------------
Eval num_timesteps=3117480, episode_reward=1857.24 +/- 964.83
Episode length: 756.20 +/- 47.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 3117480  |
---------------------------------
Eval num_timesteps=3119472, episode_reward=3569.37 +/- 1189.75
Episode length: 706.60 +/- 27.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 3.57e+03 |
| time/              |          |
|    total_timesteps | 3119472  |
---------------------------------
Eval num_timesteps=3121464, episode_reward=3198.15 +/- 1199.12
Episode length: 708.20 +/- 53.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 3121464  |
---------------------------------
Eval num_timesteps=3123456, episode_reward=1330.71 +/- 922.34
Episode length: 653.80 +/- 147.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3123456  |
---------------------------------
Eval num_timesteps=3125448, episode_reward=1573.41 +/- 819.04
Episode length: 629.20 +/- 71.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3125448  |
---------------------------------
Eval num_timesteps=3127440, episode_reward=1893.07 +/- 747.55
Episode length: 788.60 +/- 42.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3127440  |
---------------------------------
Eval num_timesteps=3129432, episode_reward=1756.09 +/- 1315.27
Episode length: 667.00 +/- 55.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3129432  |
---------------------------------
Eval num_timesteps=3131424, episode_reward=2968.91 +/- 831.00
Episode length: 714.20 +/- 47.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 3131424  |
---------------------------------
Eval num_timesteps=3133416, episode_reward=3035.81 +/- 1846.42
Episode length: 711.40 +/- 62.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 3133416  |
---------------------------------
Eval num_timesteps=3135408, episode_reward=2114.02 +/- 966.71
Episode length: 687.60 +/- 83.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 3135408  |
---------------------------------
Eval num_timesteps=3137400, episode_reward=2055.74 +/- 615.34
Episode length: 707.60 +/- 62.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 3137400  |
---------------------------------
Eval num_timesteps=3139392, episode_reward=1765.23 +/- 470.03
Episode length: 639.60 +/- 38.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3139392  |
---------------------------------
Eval num_timesteps=3141384, episode_reward=1638.05 +/- 1095.37
Episode length: 737.20 +/- 32.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3141384  |
---------------------------------
Eval num_timesteps=3143376, episode_reward=2332.66 +/- 791.86
Episode length: 725.60 +/- 89.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3143376  |
---------------------------------
Eval num_timesteps=3145368, episode_reward=1508.32 +/- 1051.20
Episode length: 664.80 +/- 143.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3145368  |
---------------------------------
Eval num_timesteps=3147360, episode_reward=2162.31 +/- 1132.00
Episode length: 664.80 +/- 74.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 665          |
|    mean_reward          | 2.16e+03     |
| time/                   |              |
|    total_timesteps      | 3147360      |
| train/                  |              |
|    approx_kl            | 0.0036976656 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 320          |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.63         |
|    value_loss           | 908          |
------------------------------------------
Eval num_timesteps=3149352, episode_reward=1818.63 +/- 852.35
Episode length: 736.80 +/- 23.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3149352  |
---------------------------------
Eval num_timesteps=3151344, episode_reward=2168.99 +/- 736.90
Episode length: 719.60 +/- 66.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3151344  |
---------------------------------
Eval num_timesteps=3153336, episode_reward=3015.58 +/- 1419.79
Episode length: 656.80 +/- 122.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 3153336  |
---------------------------------
Eval num_timesteps=3155328, episode_reward=1398.98 +/- 896.59
Episode length: 664.40 +/- 85.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3155328  |
---------------------------------
Eval num_timesteps=3157320, episode_reward=1678.14 +/- 1087.31
Episode length: 625.80 +/- 152.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3157320  |
---------------------------------
Eval num_timesteps=3159312, episode_reward=1978.61 +/- 1571.67
Episode length: 718.80 +/- 51.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3159312  |
---------------------------------
Eval num_timesteps=3161304, episode_reward=2548.06 +/- 1217.57
Episode length: 689.40 +/- 60.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3161304  |
---------------------------------
Eval num_timesteps=3163296, episode_reward=2532.15 +/- 1217.66
Episode length: 687.00 +/- 70.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3163296  |
---------------------------------
Eval num_timesteps=3165288, episode_reward=795.37 +/- 823.28
Episode length: 574.20 +/- 112.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 3165288  |
---------------------------------
Eval num_timesteps=3167280, episode_reward=1956.56 +/- 1267.35
Episode length: 659.60 +/- 128.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3167280  |
---------------------------------
Eval num_timesteps=3169272, episode_reward=1742.61 +/- 1657.93
Episode length: 595.80 +/- 147.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3169272  |
---------------------------------
Eval num_timesteps=3171264, episode_reward=1679.95 +/- 1200.16
Episode length: 660.00 +/- 139.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3171264  |
---------------------------------
Eval num_timesteps=3173256, episode_reward=2019.71 +/- 1160.97
Episode length: 669.80 +/- 62.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3173256  |
---------------------------------
Eval num_timesteps=3175248, episode_reward=2268.13 +/- 1535.50
Episode length: 643.20 +/- 102.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3175248  |
---------------------------------
Eval num_timesteps=3177240, episode_reward=1429.17 +/- 948.39
Episode length: 550.40 +/- 118.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3177240  |
---------------------------------
Eval num_timesteps=3179232, episode_reward=1500.33 +/- 388.09
Episode length: 662.60 +/- 26.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 3179232  |
---------------------------------
Eval num_timesteps=3181224, episode_reward=1308.30 +/- 508.01
Episode length: 688.80 +/- 89.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3181224  |
---------------------------------
Eval num_timesteps=3183216, episode_reward=1256.02 +/- 1025.19
Episode length: 633.00 +/- 118.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3183216  |
---------------------------------
Eval num_timesteps=3185208, episode_reward=1566.87 +/- 821.93
Episode length: 675.40 +/- 55.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3185208  |
---------------------------------
Eval num_timesteps=3187200, episode_reward=1657.19 +/- 916.41
Episode length: 668.80 +/- 79.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3187200  |
---------------------------------
Eval num_timesteps=3189192, episode_reward=1270.52 +/- 934.53
Episode length: 632.80 +/- 97.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3189192  |
---------------------------------
Eval num_timesteps=3191184, episode_reward=1951.19 +/- 1201.66
Episode length: 687.80 +/- 74.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3191184  |
---------------------------------
Eval num_timesteps=3193176, episode_reward=1345.08 +/- 740.45
Episode length: 653.20 +/- 51.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3193176  |
---------------------------------
Eval num_timesteps=3195168, episode_reward=2597.82 +/- 1353.44
Episode length: 729.80 +/- 37.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 730          |
|    mean_reward          | 2.6e+03      |
| time/                   |              |
|    total_timesteps      | 3195168      |
| train/                  |              |
|    approx_kl            | 0.0037502695 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 108          |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.64         |
|    value_loss           | 660          |
------------------------------------------
Eval num_timesteps=3197160, episode_reward=1333.34 +/- 198.95
Episode length: 695.80 +/- 31.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3197160  |
---------------------------------
Eval num_timesteps=3199152, episode_reward=3044.96 +/- 881.69
Episode length: 723.20 +/- 46.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 3199152  |
---------------------------------
Eval num_timesteps=3201144, episode_reward=2400.71 +/- 1629.87
Episode length: 741.80 +/- 58.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3201144  |
---------------------------------
Eval num_timesteps=3203136, episode_reward=1774.99 +/- 1023.67
Episode length: 678.20 +/- 78.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3203136  |
---------------------------------
Eval num_timesteps=3205128, episode_reward=2421.71 +/- 655.37
Episode length: 699.80 +/- 33.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 700      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 3205128  |
---------------------------------
Eval num_timesteps=3207120, episode_reward=2123.19 +/- 1217.24
Episode length: 652.60 +/- 71.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3207120  |
---------------------------------
Eval num_timesteps=3209112, episode_reward=1297.58 +/- 1029.01
Episode length: 621.20 +/- 120.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3209112  |
---------------------------------
Eval num_timesteps=3211104, episode_reward=2266.65 +/- 1283.24
Episode length: 715.20 +/- 45.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3211104  |
---------------------------------
Eval num_timesteps=3213096, episode_reward=2020.71 +/- 980.84
Episode length: 719.20 +/- 39.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3213096  |
---------------------------------
Eval num_timesteps=3215088, episode_reward=3010.34 +/- 509.52
Episode length: 683.40 +/- 28.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 3215088  |
---------------------------------
Eval num_timesteps=3217080, episode_reward=1433.84 +/- 1489.47
Episode length: 631.20 +/- 98.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3217080  |
---------------------------------
Eval num_timesteps=3219072, episode_reward=2144.11 +/- 1433.32
Episode length: 718.80 +/- 20.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3219072  |
---------------------------------
Eval num_timesteps=3221064, episode_reward=1857.39 +/- 942.16
Episode length: 751.00 +/- 29.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 3221064  |
---------------------------------
Eval num_timesteps=3223056, episode_reward=1891.56 +/- 849.75
Episode length: 706.60 +/- 22.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3223056  |
---------------------------------
Eval num_timesteps=3225048, episode_reward=1597.18 +/- 1267.00
Episode length: 710.80 +/- 26.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3225048  |
---------------------------------
Eval num_timesteps=3227040, episode_reward=1508.84 +/- 969.29
Episode length: 708.40 +/- 43.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3227040  |
---------------------------------
Eval num_timesteps=3229032, episode_reward=1035.42 +/- 260.30
Episode length: 635.80 +/- 68.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3229032  |
---------------------------------
Eval num_timesteps=3231024, episode_reward=1565.26 +/- 1054.51
Episode length: 677.80 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3231024  |
---------------------------------
Eval num_timesteps=3233016, episode_reward=1667.80 +/- 1361.18
Episode length: 713.00 +/- 30.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 713      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3233016  |
---------------------------------
Eval num_timesteps=3235008, episode_reward=2046.77 +/- 1400.67
Episode length: 693.60 +/- 63.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3235008  |
---------------------------------
Eval num_timesteps=3237000, episode_reward=1163.88 +/- 1007.07
Episode length: 636.80 +/- 103.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3237000  |
---------------------------------
Eval num_timesteps=3238992, episode_reward=3171.74 +/- 1029.42
Episode length: 695.40 +/- 9.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 3238992  |
---------------------------------
Eval num_timesteps=3240984, episode_reward=1521.72 +/- 920.07
Episode length: 720.20 +/- 32.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3240984  |
---------------------------------
Eval num_timesteps=3242976, episode_reward=1435.63 +/- 995.92
Episode length: 671.40 +/- 111.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3242976  |
---------------------------------
Eval num_timesteps=3244968, episode_reward=1837.56 +/- 1470.89
Episode length: 714.60 +/- 48.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 715          |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 3244968      |
| train/                  |              |
|    approx_kl            | 0.0023198056 |
|    clip_fraction        | 0.00598      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 715          |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000736    |
|    std                  | 1.65         |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=3246960, episode_reward=1464.52 +/- 316.03
Episode length: 686.40 +/- 69.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3246960  |
---------------------------------
Eval num_timesteps=3248952, episode_reward=2625.05 +/- 1476.22
Episode length: 735.60 +/- 22.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 3248952  |
---------------------------------
Eval num_timesteps=3250944, episode_reward=1466.22 +/- 809.64
Episode length: 703.80 +/- 69.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3250944  |
---------------------------------
Eval num_timesteps=3252936, episode_reward=2847.99 +/- 1051.71
Episode length: 661.00 +/- 38.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 3252936  |
---------------------------------
Eval num_timesteps=3254928, episode_reward=2677.16 +/- 1173.78
Episode length: 699.20 +/- 35.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3254928  |
---------------------------------
Eval num_timesteps=3256920, episode_reward=2706.53 +/- 1003.05
Episode length: 724.00 +/- 34.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3256920  |
---------------------------------
Eval num_timesteps=3258912, episode_reward=1933.42 +/- 914.26
Episode length: 680.40 +/- 89.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3258912  |
---------------------------------
Eval num_timesteps=3260904, episode_reward=1956.80 +/- 1057.53
Episode length: 690.40 +/- 57.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3260904  |
---------------------------------
Eval num_timesteps=3262896, episode_reward=1650.03 +/- 1185.91
Episode length: 673.40 +/- 101.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3262896  |
---------------------------------
Eval num_timesteps=3264888, episode_reward=2621.02 +/- 627.88
Episode length: 737.00 +/- 12.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 3264888  |
---------------------------------
Eval num_timesteps=3266880, episode_reward=1909.88 +/- 1301.42
Episode length: 715.00 +/- 32.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3266880  |
---------------------------------
Eval num_timesteps=3268872, episode_reward=2091.73 +/- 1427.96
Episode length: 666.80 +/- 89.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3268872  |
---------------------------------
Eval num_timesteps=3270864, episode_reward=1474.58 +/- 856.51
Episode length: 754.40 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3270864  |
---------------------------------
Eval num_timesteps=3272856, episode_reward=1267.19 +/- 1031.88
Episode length: 696.80 +/- 63.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3272856  |
---------------------------------
Eval num_timesteps=3274848, episode_reward=1476.35 +/- 1123.03
Episode length: 707.80 +/- 45.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 3274848  |
---------------------------------
Eval num_timesteps=3276840, episode_reward=1380.21 +/- 601.71
Episode length: 721.40 +/- 42.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3276840  |
---------------------------------
Eval num_timesteps=3278832, episode_reward=2749.07 +/- 1015.14
Episode length: 716.40 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3278832  |
---------------------------------
Eval num_timesteps=3280824, episode_reward=3108.30 +/- 1797.90
Episode length: 701.20 +/- 38.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 3280824  |
---------------------------------
Eval num_timesteps=3282816, episode_reward=1740.44 +/- 912.82
Episode length: 733.40 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3282816  |
---------------------------------
Eval num_timesteps=3284808, episode_reward=1893.34 +/- 1046.82
Episode length: 717.40 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3284808  |
---------------------------------
Eval num_timesteps=3286800, episode_reward=1645.89 +/- 1165.66
Episode length: 646.00 +/- 79.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3286800  |
---------------------------------
Eval num_timesteps=3288792, episode_reward=1545.09 +/- 989.76
Episode length: 604.80 +/- 114.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3288792  |
---------------------------------
Eval num_timesteps=3290784, episode_reward=2243.46 +/- 1027.61
Episode length: 678.60 +/- 82.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3290784  |
---------------------------------
Eval num_timesteps=3292776, episode_reward=2264.59 +/- 1342.68
Episode length: 680.40 +/- 45.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3292776  |
---------------------------------
Eval num_timesteps=3294768, episode_reward=1878.49 +/- 1090.34
Episode length: 675.00 +/- 45.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 675          |
|    mean_reward          | 1.88e+03     |
| time/                   |              |
|    total_timesteps      | 3294768      |
| train/                  |              |
|    approx_kl            | 0.0017697094 |
|    clip_fraction        | 0.00462      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 538          |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.000447    |
|    std                  | 1.66         |
|    value_loss           | 774          |
------------------------------------------
Eval num_timesteps=3296760, episode_reward=1032.83 +/- 458.78
Episode length: 726.20 +/- 45.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3296760  |
---------------------------------
Eval num_timesteps=3298752, episode_reward=1783.86 +/- 1329.84
Episode length: 662.20 +/- 76.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3298752  |
---------------------------------
Eval num_timesteps=3300744, episode_reward=1550.82 +/- 1307.54
Episode length: 662.80 +/- 78.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3300744  |
---------------------------------
Eval num_timesteps=3302736, episode_reward=1610.78 +/- 1145.56
Episode length: 648.60 +/- 76.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3302736  |
---------------------------------
Eval num_timesteps=3304728, episode_reward=2481.98 +/- 1214.10
Episode length: 754.20 +/- 30.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 754      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3304728  |
---------------------------------
Eval num_timesteps=3306720, episode_reward=2059.83 +/- 805.11
Episode length: 720.80 +/- 39.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 3306720  |
---------------------------------
Eval num_timesteps=3308712, episode_reward=1755.65 +/- 1172.74
Episode length: 731.20 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3308712  |
---------------------------------
Eval num_timesteps=3310704, episode_reward=1221.28 +/- 1040.46
Episode length: 656.00 +/- 126.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3310704  |
---------------------------------
Eval num_timesteps=3312696, episode_reward=2392.74 +/- 1241.21
Episode length: 738.20 +/- 38.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3312696  |
---------------------------------
Eval num_timesteps=3314688, episode_reward=1232.53 +/- 966.96
Episode length: 709.60 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3314688  |
---------------------------------
Eval num_timesteps=3316680, episode_reward=2282.83 +/- 1065.93
Episode length: 693.60 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 3316680  |
---------------------------------
Eval num_timesteps=3318672, episode_reward=2518.78 +/- 818.97
Episode length: 722.60 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 723      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3318672  |
---------------------------------
Eval num_timesteps=3320664, episode_reward=2515.75 +/- 1021.77
Episode length: 680.00 +/- 50.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3320664  |
---------------------------------
Eval num_timesteps=3322656, episode_reward=1565.99 +/- 927.71
Episode length: 714.00 +/- 18.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3322656  |
---------------------------------
Eval num_timesteps=3324648, episode_reward=1627.43 +/- 928.92
Episode length: 700.80 +/- 33.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3324648  |
---------------------------------
Eval num_timesteps=3326640, episode_reward=1684.03 +/- 913.93
Episode length: 682.80 +/- 48.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3326640  |
---------------------------------
Eval num_timesteps=3328632, episode_reward=1422.43 +/- 632.43
Episode length: 698.00 +/- 56.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3328632  |
---------------------------------
Eval num_timesteps=3330624, episode_reward=1358.52 +/- 359.32
Episode length: 734.80 +/- 29.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3330624  |
---------------------------------
Eval num_timesteps=3332616, episode_reward=1045.42 +/- 501.30
Episode length: 720.80 +/- 55.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3332616  |
---------------------------------
Eval num_timesteps=3334608, episode_reward=3043.00 +/- 1559.30
Episode length: 672.00 +/- 122.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 3334608  |
---------------------------------
Eval num_timesteps=3336600, episode_reward=2385.80 +/- 961.58
Episode length: 734.80 +/- 39.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3336600  |
---------------------------------
Eval num_timesteps=3338592, episode_reward=1994.99 +/- 932.43
Episode length: 703.80 +/- 60.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3338592  |
---------------------------------
Eval num_timesteps=3340584, episode_reward=751.19 +/- 369.33
Episode length: 658.80 +/- 56.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 3340584  |
---------------------------------
Eval num_timesteps=3342576, episode_reward=2158.74 +/- 1364.78
Episode length: 698.80 +/- 39.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 699          |
|    mean_reward          | 2.16e+03     |
| time/                   |              |
|    total_timesteps      | 3342576      |
| train/                  |              |
|    approx_kl            | 0.0028378943 |
|    clip_fraction        | 0.00868      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 316          |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.67         |
|    value_loss           | 767          |
------------------------------------------
Eval num_timesteps=3344568, episode_reward=2388.68 +/- 1167.10
Episode length: 705.80 +/- 52.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3344568  |
---------------------------------
Eval num_timesteps=3346560, episode_reward=913.17 +/- 246.32
Episode length: 666.60 +/- 105.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 3346560  |
---------------------------------
Eval num_timesteps=3348552, episode_reward=1384.12 +/- 974.70
Episode length: 697.40 +/- 32.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3348552  |
---------------------------------
Eval num_timesteps=3350544, episode_reward=1206.57 +/- 1074.00
Episode length: 693.00 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3350544  |
---------------------------------
Eval num_timesteps=3352536, episode_reward=2134.53 +/- 1401.86
Episode length: 674.00 +/- 41.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3352536  |
---------------------------------
Eval num_timesteps=3354528, episode_reward=2214.28 +/- 1319.94
Episode length: 644.00 +/- 52.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 3354528  |
---------------------------------
Eval num_timesteps=3356520, episode_reward=2544.93 +/- 1293.56
Episode length: 690.00 +/- 42.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 3356520  |
---------------------------------
Eval num_timesteps=3358512, episode_reward=1156.62 +/- 1098.68
Episode length: 639.00 +/- 49.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3358512  |
---------------------------------
Eval num_timesteps=3360504, episode_reward=874.30 +/- 442.10
Episode length: 659.60 +/- 76.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 3360504  |
---------------------------------
Eval num_timesteps=3362496, episode_reward=2271.16 +/- 1603.83
Episode length: 649.60 +/- 75.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3362496  |
---------------------------------
Eval num_timesteps=3364488, episode_reward=1317.61 +/- 1543.63
Episode length: 670.60 +/- 91.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3364488  |
---------------------------------
Eval num_timesteps=3366480, episode_reward=1537.79 +/- 990.27
Episode length: 618.80 +/- 98.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3366480  |
---------------------------------
Eval num_timesteps=3368472, episode_reward=1528.45 +/- 742.04
Episode length: 687.40 +/- 38.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3368472  |
---------------------------------
Eval num_timesteps=3370464, episode_reward=1492.58 +/- 1271.68
Episode length: 630.80 +/- 77.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 3370464  |
---------------------------------
Eval num_timesteps=3372456, episode_reward=1972.84 +/- 1837.31
Episode length: 644.00 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3372456  |
---------------------------------
Eval num_timesteps=3374448, episode_reward=1204.30 +/- 1069.51
Episode length: 546.00 +/- 71.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3374448  |
---------------------------------
Eval num_timesteps=3376440, episode_reward=1393.41 +/- 1109.06
Episode length: 609.80 +/- 152.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3376440  |
---------------------------------
Eval num_timesteps=3378432, episode_reward=1447.68 +/- 884.07
Episode length: 670.80 +/- 44.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3378432  |
---------------------------------
Eval num_timesteps=3380424, episode_reward=745.76 +/- 458.15
Episode length: 685.60 +/- 41.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 3380424  |
---------------------------------
Eval num_timesteps=3382416, episode_reward=1459.80 +/- 1348.84
Episode length: 590.80 +/- 106.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3382416  |
---------------------------------
Eval num_timesteps=3384408, episode_reward=753.52 +/- 363.08
Episode length: 653.00 +/- 102.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 3384408  |
---------------------------------
Eval num_timesteps=3386400, episode_reward=1041.47 +/- 484.93
Episode length: 625.60 +/- 67.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3386400  |
---------------------------------
Eval num_timesteps=3388392, episode_reward=1211.95 +/- 517.13
Episode length: 691.20 +/- 46.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3388392  |
---------------------------------
Eval num_timesteps=3390384, episode_reward=2027.79 +/- 1390.99
Episode length: 681.60 +/- 76.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 3390384  |
---------------------------------
Eval num_timesteps=3392376, episode_reward=1477.03 +/- 1065.64
Episode length: 714.80 +/- 20.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 715          |
|    mean_reward          | 1.48e+03     |
| time/                   |              |
|    total_timesteps      | 3392376      |
| train/                  |              |
|    approx_kl            | 0.0031288776 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 221          |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000965    |
|    std                  | 1.68         |
|    value_loss           | 788          |
------------------------------------------
Eval num_timesteps=3394368, episode_reward=1512.12 +/- 940.14
Episode length: 723.60 +/- 26.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3394368  |
---------------------------------
Eval num_timesteps=3396360, episode_reward=1180.11 +/- 709.53
Episode length: 669.40 +/- 38.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3396360  |
---------------------------------
Eval num_timesteps=3398352, episode_reward=1628.07 +/- 1536.98
Episode length: 706.80 +/- 37.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3398352  |
---------------------------------
Eval num_timesteps=3400344, episode_reward=2960.45 +/- 1774.34
Episode length: 684.00 +/- 29.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 3400344  |
---------------------------------
Eval num_timesteps=3402336, episode_reward=724.42 +/- 188.10
Episode length: 706.00 +/- 11.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 3402336  |
---------------------------------
Eval num_timesteps=3404328, episode_reward=2019.05 +/- 1565.97
Episode length: 662.40 +/- 59.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3404328  |
---------------------------------
Eval num_timesteps=3406320, episode_reward=768.62 +/- 301.40
Episode length: 708.60 +/- 32.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 3406320  |
---------------------------------
Eval num_timesteps=3408312, episode_reward=2197.35 +/- 688.48
Episode length: 700.60 +/- 22.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3408312  |
---------------------------------
Eval num_timesteps=3410304, episode_reward=1820.13 +/- 1587.56
Episode length: 702.60 +/- 43.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3410304  |
---------------------------------
Eval num_timesteps=3412296, episode_reward=3462.66 +/- 1832.99
Episode length: 703.60 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3412296  |
---------------------------------
Eval num_timesteps=3414288, episode_reward=1605.75 +/- 1326.81
Episode length: 714.60 +/- 61.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3414288  |
---------------------------------
Eval num_timesteps=3416280, episode_reward=821.99 +/- 397.57
Episode length: 724.80 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 3416280  |
---------------------------------
Eval num_timesteps=3418272, episode_reward=1201.91 +/- 1535.02
Episode length: 717.60 +/- 49.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3418272  |
---------------------------------
Eval num_timesteps=3420264, episode_reward=1365.05 +/- 884.54
Episode length: 688.40 +/- 13.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3420264  |
---------------------------------
Eval num_timesteps=3422256, episode_reward=1586.53 +/- 1232.83
Episode length: 711.80 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3422256  |
---------------------------------
Eval num_timesteps=3424248, episode_reward=2082.68 +/- 909.32
Episode length: 678.60 +/- 48.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 3424248  |
---------------------------------
Eval num_timesteps=3426240, episode_reward=1801.75 +/- 1024.68
Episode length: 725.80 +/- 54.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3426240  |
---------------------------------
Eval num_timesteps=3428232, episode_reward=786.17 +/- 264.07
Episode length: 664.80 +/- 52.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 3428232  |
---------------------------------
Eval num_timesteps=3430224, episode_reward=1085.92 +/- 160.63
Episode length: 695.20 +/- 8.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3430224  |
---------------------------------
Eval num_timesteps=3432216, episode_reward=2046.22 +/- 1238.98
Episode length: 670.80 +/- 56.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3432216  |
---------------------------------
Eval num_timesteps=3434208, episode_reward=1763.28 +/- 1178.99
Episode length: 692.00 +/- 11.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3434208  |
---------------------------------
Eval num_timesteps=3436200, episode_reward=1318.89 +/- 898.95
Episode length: 718.20 +/- 12.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3436200  |
---------------------------------
Eval num_timesteps=3438192, episode_reward=1181.46 +/- 1016.52
Episode length: 704.40 +/- 43.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3438192  |
---------------------------------
Eval num_timesteps=3440184, episode_reward=1975.10 +/- 1379.64
Episode length: 625.80 +/- 80.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3440184  |
---------------------------------
Eval num_timesteps=3442176, episode_reward=2363.11 +/- 1207.86
Episode length: 630.00 +/- 61.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 630          |
|    mean_reward          | 2.36e+03     |
| time/                   |              |
|    total_timesteps      | 3442176      |
| train/                  |              |
|    approx_kl            | 0.0027988602 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 164          |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.7          |
|    value_loss           | 643          |
------------------------------------------
Eval num_timesteps=3444168, episode_reward=2038.96 +/- 1323.67
Episode length: 674.00 +/- 47.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3444168  |
---------------------------------
Eval num_timesteps=3446160, episode_reward=914.82 +/- 665.74
Episode length: 676.00 +/- 60.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 3446160  |
---------------------------------
Eval num_timesteps=3448152, episode_reward=1843.09 +/- 1523.33
Episode length: 693.00 +/- 21.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3448152  |
---------------------------------
Eval num_timesteps=3450144, episode_reward=1608.84 +/- 1361.84
Episode length: 695.00 +/- 43.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3450144  |
---------------------------------
Eval num_timesteps=3452136, episode_reward=1732.15 +/- 1177.02
Episode length: 668.20 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3452136  |
---------------------------------
Eval num_timesteps=3454128, episode_reward=2429.31 +/- 1784.09
Episode length: 686.00 +/- 28.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3454128  |
---------------------------------
Eval num_timesteps=3456120, episode_reward=2022.77 +/- 1238.45
Episode length: 699.20 +/- 56.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3456120  |
---------------------------------
Eval num_timesteps=3458112, episode_reward=1607.40 +/- 1160.87
Episode length: 634.40 +/- 92.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3458112  |
---------------------------------
Eval num_timesteps=3460104, episode_reward=1557.86 +/- 1093.95
Episode length: 654.60 +/- 57.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3460104  |
---------------------------------
Eval num_timesteps=3462096, episode_reward=1305.49 +/- 973.48
Episode length: 675.80 +/- 60.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3462096  |
---------------------------------
Eval num_timesteps=3464088, episode_reward=1627.75 +/- 1681.38
Episode length: 658.60 +/- 69.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3464088  |
---------------------------------
Eval num_timesteps=3466080, episode_reward=1877.52 +/- 1140.83
Episode length: 693.80 +/- 26.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3466080  |
---------------------------------
Eval num_timesteps=3468072, episode_reward=1368.66 +/- 1414.54
Episode length: 663.60 +/- 44.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3468072  |
---------------------------------
Eval num_timesteps=3470064, episode_reward=1524.95 +/- 919.38
Episode length: 682.20 +/- 34.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3470064  |
---------------------------------
Eval num_timesteps=3472056, episode_reward=1132.33 +/- 481.74
Episode length: 668.80 +/- 43.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3472056  |
---------------------------------
Eval num_timesteps=3474048, episode_reward=1509.63 +/- 854.08
Episode length: 685.00 +/- 59.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3474048  |
---------------------------------
Eval num_timesteps=3476040, episode_reward=1328.66 +/- 1209.73
Episode length: 655.40 +/- 64.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3476040  |
---------------------------------
Eval num_timesteps=3478032, episode_reward=2148.95 +/- 1503.46
Episode length: 705.20 +/- 26.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3478032  |
---------------------------------
Eval num_timesteps=3480024, episode_reward=1684.04 +/- 1663.49
Episode length: 703.00 +/- 26.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3480024  |
---------------------------------
Eval num_timesteps=3482016, episode_reward=1893.72 +/- 1575.06
Episode length: 709.00 +/- 40.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3482016  |
---------------------------------
Eval num_timesteps=3484008, episode_reward=1750.93 +/- 509.01
Episode length: 630.20 +/- 94.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3484008  |
---------------------------------
Eval num_timesteps=3486000, episode_reward=1851.13 +/- 1214.85
Episode length: 697.00 +/- 32.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3486000  |
---------------------------------
Eval num_timesteps=3487992, episode_reward=752.47 +/- 386.31
Episode length: 633.40 +/- 79.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 3487992  |
---------------------------------
Eval num_timesteps=3489984, episode_reward=3225.30 +/- 817.80
Episode length: 704.20 +/- 52.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 704         |
|    mean_reward          | 3.23e+03    |
| time/                   |             |
|    total_timesteps      | 3489984     |
| train/                  |             |
|    approx_kl            | 0.003232899 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.8        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 559         |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00104    |
|    std                  | 1.7         |
|    value_loss           | 993         |
-----------------------------------------
Eval num_timesteps=3491976, episode_reward=2343.29 +/- 1262.85
Episode length: 696.60 +/- 27.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3491976  |
---------------------------------
Eval num_timesteps=3493968, episode_reward=1799.29 +/- 1232.33
Episode length: 666.20 +/- 85.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3493968  |
---------------------------------
Eval num_timesteps=3495960, episode_reward=1244.80 +/- 1055.32
Episode length: 599.00 +/- 110.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3495960  |
---------------------------------
Eval num_timesteps=3497952, episode_reward=2140.40 +/- 1466.16
Episode length: 632.40 +/- 106.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3497952  |
---------------------------------
Eval num_timesteps=3499944, episode_reward=1822.62 +/- 1432.47
Episode length: 700.80 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3499944  |
---------------------------------
Eval num_timesteps=3501936, episode_reward=1627.06 +/- 1027.19
Episode length: 666.20 +/- 29.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3501936  |
---------------------------------
Eval num_timesteps=3503928, episode_reward=2762.47 +/- 1372.20
Episode length: 690.80 +/- 30.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3503928  |
---------------------------------
Eval num_timesteps=3505920, episode_reward=2245.38 +/- 972.02
Episode length: 650.40 +/- 53.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 3505920  |
---------------------------------
Eval num_timesteps=3507912, episode_reward=2421.27 +/- 1382.42
Episode length: 634.80 +/- 87.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 3507912  |
---------------------------------
Eval num_timesteps=3509904, episode_reward=803.26 +/- 362.79
Episode length: 596.00 +/- 103.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 3509904  |
---------------------------------
Eval num_timesteps=3511896, episode_reward=1516.14 +/- 928.06
Episode length: 682.20 +/- 70.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3511896  |
---------------------------------
Eval num_timesteps=3513888, episode_reward=3014.88 +/- 1084.37
Episode length: 677.00 +/- 34.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 3513888  |
---------------------------------
Eval num_timesteps=3515880, episode_reward=1462.88 +/- 939.13
Episode length: 685.40 +/- 95.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3515880  |
---------------------------------
Eval num_timesteps=3517872, episode_reward=1970.04 +/- 1261.21
Episode length: 668.40 +/- 57.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3517872  |
---------------------------------
Eval num_timesteps=3519864, episode_reward=2317.93 +/- 1140.61
Episode length: 683.60 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3519864  |
---------------------------------
Eval num_timesteps=3521856, episode_reward=3242.81 +/- 1215.61
Episode length: 679.80 +/- 39.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 3521856  |
---------------------------------
Eval num_timesteps=3523848, episode_reward=1159.57 +/- 169.68
Episode length: 664.20 +/- 75.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3523848  |
---------------------------------
Eval num_timesteps=3525840, episode_reward=1658.28 +/- 1387.10
Episode length: 660.80 +/- 65.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3525840  |
---------------------------------
Eval num_timesteps=3527832, episode_reward=1780.01 +/- 1326.01
Episode length: 659.80 +/- 48.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3527832  |
---------------------------------
Eval num_timesteps=3529824, episode_reward=1216.76 +/- 533.72
Episode length: 707.60 +/- 38.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3529824  |
---------------------------------
Eval num_timesteps=3531816, episode_reward=2140.03 +/- 1367.42
Episode length: 604.60 +/- 135.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3531816  |
---------------------------------
Eval num_timesteps=3533808, episode_reward=1132.73 +/- 595.02
Episode length: 638.20 +/- 88.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3533808  |
---------------------------------
Eval num_timesteps=3535800, episode_reward=2486.38 +/- 1091.88
Episode length: 679.20 +/- 47.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3535800  |
---------------------------------
Eval num_timesteps=3537792, episode_reward=1539.76 +/- 1022.35
Episode length: 634.00 +/- 131.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3537792  |
---------------------------------
Eval num_timesteps=3539784, episode_reward=2121.39 +/- 869.56
Episode length: 687.80 +/- 44.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 688          |
|    mean_reward          | 2.12e+03     |
| time/                   |              |
|    total_timesteps      | 3539784      |
| train/                  |              |
|    approx_kl            | 0.0020817125 |
|    clip_fraction        | 0.00749      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.84        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 259          |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000584    |
|    std                  | 1.73         |
|    value_loss           | 634          |
------------------------------------------
Eval num_timesteps=3541776, episode_reward=1985.23 +/- 1101.64
Episode length: 668.40 +/- 77.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3541776  |
---------------------------------
Eval num_timesteps=3543768, episode_reward=2373.24 +/- 1367.31
Episode length: 704.80 +/- 53.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 3543768  |
---------------------------------
Eval num_timesteps=3545760, episode_reward=1362.17 +/- 988.51
Episode length: 673.40 +/- 61.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3545760  |
---------------------------------
Eval num_timesteps=3547752, episode_reward=2107.67 +/- 1253.73
Episode length: 646.80 +/- 116.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 3547752  |
---------------------------------
Eval num_timesteps=3549744, episode_reward=2078.91 +/- 1345.70
Episode length: 696.40 +/- 40.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 3549744  |
---------------------------------
Eval num_timesteps=3551736, episode_reward=958.71 +/- 334.70
Episode length: 660.00 +/- 88.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 3551736  |
---------------------------------
Eval num_timesteps=3553728, episode_reward=1912.55 +/- 721.24
Episode length: 680.60 +/- 30.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3553728  |
---------------------------------
Eval num_timesteps=3555720, episode_reward=802.55 +/- 825.42
Episode length: 652.60 +/- 81.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 3555720  |
---------------------------------
Eval num_timesteps=3557712, episode_reward=1187.71 +/- 1095.79
Episode length: 613.00 +/- 139.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3557712  |
---------------------------------
Eval num_timesteps=3559704, episode_reward=1602.77 +/- 1544.14
Episode length: 581.80 +/- 113.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3559704  |
---------------------------------
Eval num_timesteps=3561696, episode_reward=2076.79 +/- 670.65
Episode length: 703.80 +/- 34.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 3561696  |
---------------------------------
Eval num_timesteps=3563688, episode_reward=1152.09 +/- 433.58
Episode length: 635.40 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3563688  |
---------------------------------
Eval num_timesteps=3565680, episode_reward=1560.78 +/- 1240.08
Episode length: 588.20 +/- 137.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3565680  |
---------------------------------
Eval num_timesteps=3567672, episode_reward=1838.73 +/- 836.25
Episode length: 669.40 +/- 65.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3567672  |
---------------------------------
Eval num_timesteps=3569664, episode_reward=1838.22 +/- 1241.56
Episode length: 643.40 +/- 54.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3569664  |
---------------------------------
Eval num_timesteps=3571656, episode_reward=664.95 +/- 589.58
Episode length: 515.60 +/- 130.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 3571656  |
---------------------------------
Eval num_timesteps=3573648, episode_reward=945.67 +/- 447.69
Episode length: 636.80 +/- 80.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 3573648  |
---------------------------------
Eval num_timesteps=3575640, episode_reward=1650.41 +/- 1332.22
Episode length: 633.60 +/- 81.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3575640  |
---------------------------------
Eval num_timesteps=3577632, episode_reward=1725.38 +/- 900.01
Episode length: 670.40 +/- 64.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3577632  |
---------------------------------
Eval num_timesteps=3579624, episode_reward=1888.38 +/- 724.23
Episode length: 627.20 +/- 56.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3579624  |
---------------------------------
Eval num_timesteps=3581616, episode_reward=1253.93 +/- 865.87
Episode length: 602.60 +/- 158.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3581616  |
---------------------------------
Eval num_timesteps=3583608, episode_reward=2077.67 +/- 1850.11
Episode length: 654.80 +/- 46.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 3583608  |
---------------------------------
Eval num_timesteps=3585600, episode_reward=2344.96 +/- 1087.84
Episode length: 673.40 +/- 43.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3585600  |
---------------------------------
Eval num_timesteps=3587592, episode_reward=2250.62 +/- 1140.32
Episode length: 652.60 +/- 34.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 3587592  |
---------------------------------
Eval num_timesteps=3589584, episode_reward=1156.87 +/- 1179.00
Episode length: 537.20 +/- 158.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 1.16e+03     |
| time/                   |              |
|    total_timesteps      | 3589584      |
| train/                  |              |
|    approx_kl            | 0.0036514106 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 410          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.000957    |
|    std                  | 1.75         |
|    value_loss           | 845          |
------------------------------------------
Eval num_timesteps=3591576, episode_reward=2231.18 +/- 2046.52
Episode length: 644.20 +/- 92.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3591576  |
---------------------------------
Eval num_timesteps=3593568, episode_reward=2669.87 +/- 763.33
Episode length: 725.40 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3593568  |
---------------------------------
Eval num_timesteps=3595560, episode_reward=2180.27 +/- 1636.99
Episode length: 674.80 +/- 64.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3595560  |
---------------------------------
Eval num_timesteps=3597552, episode_reward=2168.30 +/- 879.93
Episode length: 687.00 +/- 42.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3597552  |
---------------------------------
Eval num_timesteps=3599544, episode_reward=1995.88 +/- 858.74
Episode length: 625.60 +/- 27.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 3599544  |
---------------------------------
Eval num_timesteps=3601536, episode_reward=1954.56 +/- 991.08
Episode length: 660.80 +/- 27.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3601536  |
---------------------------------
Eval num_timesteps=3603528, episode_reward=2266.62 +/- 1566.46
Episode length: 626.40 +/- 145.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3603528  |
---------------------------------
Eval num_timesteps=3605520, episode_reward=2240.02 +/- 1496.21
Episode length: 664.40 +/- 151.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3605520  |
---------------------------------
Eval num_timesteps=3607512, episode_reward=2175.95 +/- 1344.51
Episode length: 674.20 +/- 65.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3607512  |
---------------------------------
Eval num_timesteps=3609504, episode_reward=2415.96 +/- 1299.91
Episode length: 661.40 +/- 78.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 3609504  |
---------------------------------
Eval num_timesteps=3611496, episode_reward=1872.80 +/- 1074.87
Episode length: 625.20 +/- 111.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3611496  |
---------------------------------
Eval num_timesteps=3613488, episode_reward=1726.91 +/- 725.84
Episode length: 670.60 +/- 87.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3613488  |
---------------------------------
Eval num_timesteps=3615480, episode_reward=1456.38 +/- 297.85
Episode length: 664.40 +/- 36.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3615480  |
---------------------------------
Eval num_timesteps=3617472, episode_reward=1835.49 +/- 1043.75
Episode length: 619.00 +/- 141.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3617472  |
---------------------------------
Eval num_timesteps=3619464, episode_reward=2545.14 +/- 1180.48
Episode length: 736.40 +/- 46.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 736      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3619464  |
---------------------------------
Eval num_timesteps=3621456, episode_reward=592.08 +/- 256.59
Episode length: 624.00 +/- 153.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 3621456  |
---------------------------------
Eval num_timesteps=3623448, episode_reward=1287.67 +/- 1016.48
Episode length: 605.80 +/- 88.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3623448  |
---------------------------------
Eval num_timesteps=3625440, episode_reward=2373.00 +/- 877.18
Episode length: 720.60 +/- 41.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 3625440  |
---------------------------------
Eval num_timesteps=3627432, episode_reward=1262.59 +/- 1087.86
Episode length: 596.60 +/- 165.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3627432  |
---------------------------------
Eval num_timesteps=3629424, episode_reward=2102.11 +/- 1675.90
Episode length: 565.40 +/- 172.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3629424  |
---------------------------------
Eval num_timesteps=3631416, episode_reward=1418.23 +/- 520.53
Episode length: 657.40 +/- 59.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3631416  |
---------------------------------
Eval num_timesteps=3633408, episode_reward=2434.46 +/- 1568.82
Episode length: 717.40 +/- 38.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3633408  |
---------------------------------
Eval num_timesteps=3635400, episode_reward=2164.39 +/- 2063.75
Episode length: 561.40 +/- 125.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3635400  |
---------------------------------
Eval num_timesteps=3637392, episode_reward=1568.60 +/- 426.22
Episode length: 652.60 +/- 61.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 653         |
|    mean_reward          | 1.57e+03    |
| time/                   |             |
|    total_timesteps      | 3637392     |
| train/                  |             |
|    approx_kl            | 0.002819007 |
|    clip_fraction        | 0.00822     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 297         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.000606   |
|    std                  | 1.76        |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=3639384, episode_reward=1710.01 +/- 1724.75
Episode length: 580.40 +/- 146.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 3639384  |
---------------------------------
Eval num_timesteps=3641376, episode_reward=1043.84 +/- 1226.81
Episode length: 489.80 +/- 155.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 3641376  |
---------------------------------
Eval num_timesteps=3643368, episode_reward=1974.30 +/- 1410.50
Episode length: 611.60 +/- 113.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3643368  |
---------------------------------
Eval num_timesteps=3645360, episode_reward=1142.32 +/- 402.62
Episode length: 598.40 +/- 24.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3645360  |
---------------------------------
Eval num_timesteps=3647352, episode_reward=2124.17 +/- 1051.53
Episode length: 641.80 +/- 61.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3647352  |
---------------------------------
Eval num_timesteps=3649344, episode_reward=1759.50 +/- 1328.57
Episode length: 599.60 +/- 173.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3649344  |
---------------------------------
Eval num_timesteps=3651336, episode_reward=1109.37 +/- 517.42
Episode length: 623.40 +/- 72.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3651336  |
---------------------------------
Eval num_timesteps=3653328, episode_reward=1687.25 +/- 1118.43
Episode length: 597.60 +/- 65.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 3653328  |
---------------------------------
Eval num_timesteps=3655320, episode_reward=1394.19 +/- 1349.31
Episode length: 547.40 +/- 142.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3655320  |
---------------------------------
Eval num_timesteps=3657312, episode_reward=1452.69 +/- 1170.56
Episode length: 639.60 +/- 113.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3657312  |
---------------------------------
Eval num_timesteps=3659304, episode_reward=530.01 +/- 475.39
Episode length: 507.20 +/- 145.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 3659304  |
---------------------------------
Eval num_timesteps=3661296, episode_reward=1292.24 +/- 921.43
Episode length: 576.80 +/- 88.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3661296  |
---------------------------------
Eval num_timesteps=3663288, episode_reward=532.51 +/- 333.81
Episode length: 530.20 +/- 113.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 3663288  |
---------------------------------
Eval num_timesteps=3665280, episode_reward=3098.03 +/- 1745.82
Episode length: 710.80 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 3665280  |
---------------------------------
Eval num_timesteps=3667272, episode_reward=1406.15 +/- 837.70
Episode length: 711.80 +/- 82.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3667272  |
---------------------------------
Eval num_timesteps=3669264, episode_reward=736.47 +/- 319.10
Episode length: 584.40 +/- 128.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 3669264  |
---------------------------------
Eval num_timesteps=3671256, episode_reward=1979.85 +/- 1213.65
Episode length: 637.40 +/- 74.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3671256  |
---------------------------------
Eval num_timesteps=3673248, episode_reward=2476.55 +/- 1285.52
Episode length: 639.40 +/- 117.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3673248  |
---------------------------------
Eval num_timesteps=3675240, episode_reward=2236.61 +/- 1374.31
Episode length: 664.20 +/- 76.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3675240  |
---------------------------------
Eval num_timesteps=3677232, episode_reward=2529.40 +/- 1792.23
Episode length: 601.60 +/- 117.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3677232  |
---------------------------------
Eval num_timesteps=3679224, episode_reward=1498.76 +/- 1264.64
Episode length: 529.00 +/- 127.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 3679224  |
---------------------------------
Eval num_timesteps=3681216, episode_reward=1316.00 +/- 1242.66
Episode length: 536.60 +/- 71.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3681216  |
---------------------------------
Eval num_timesteps=3683208, episode_reward=1673.80 +/- 1215.94
Episode length: 577.60 +/- 133.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3683208  |
---------------------------------
Eval num_timesteps=3685200, episode_reward=1521.37 +/- 1149.03
Episode length: 592.60 +/- 125.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3685200  |
---------------------------------
Eval num_timesteps=3687192, episode_reward=1879.08 +/- 1222.75
Episode length: 620.60 +/- 136.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 621          |
|    mean_reward          | 1.88e+03     |
| time/                   |              |
|    total_timesteps      | 3687192      |
| train/                  |              |
|    approx_kl            | 0.0025334593 |
|    clip_fraction        | 0.00549      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.96        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 392          |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.000814    |
|    std                  | 1.78         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=3689184, episode_reward=683.17 +/- 341.55
Episode length: 494.00 +/- 69.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 3689184  |
---------------------------------
Eval num_timesteps=3691176, episode_reward=1139.71 +/- 1029.47
Episode length: 597.00 +/- 122.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3691176  |
---------------------------------
Eval num_timesteps=3693168, episode_reward=1045.37 +/- 982.47
Episode length: 511.80 +/- 126.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 3693168  |
---------------------------------
Eval num_timesteps=3695160, episode_reward=1770.19 +/- 1043.68
Episode length: 639.40 +/- 99.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3695160  |
---------------------------------
Eval num_timesteps=3697152, episode_reward=2203.05 +/- 1277.29
Episode length: 637.00 +/- 114.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3697152  |
---------------------------------
Eval num_timesteps=3699144, episode_reward=1678.46 +/- 1001.08
Episode length: 607.80 +/- 69.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3699144  |
---------------------------------
Eval num_timesteps=3701136, episode_reward=1729.13 +/- 1422.40
Episode length: 563.60 +/- 110.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3701136  |
---------------------------------
Eval num_timesteps=3703128, episode_reward=2426.56 +/- 1700.43
Episode length: 629.80 +/- 50.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3703128  |
---------------------------------
Eval num_timesteps=3705120, episode_reward=3695.64 +/- 732.75
Episode length: 647.60 +/- 43.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 3.7e+03  |
| time/              |          |
|    total_timesteps | 3705120  |
---------------------------------
Eval num_timesteps=3707112, episode_reward=1816.97 +/- 968.21
Episode length: 606.40 +/- 119.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3707112  |
---------------------------------
Eval num_timesteps=3709104, episode_reward=1450.62 +/- 1270.59
Episode length: 553.60 +/- 154.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3709104  |
---------------------------------
Eval num_timesteps=3711096, episode_reward=2655.25 +/- 767.91
Episode length: 716.20 +/- 52.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3711096  |
---------------------------------
Eval num_timesteps=3713088, episode_reward=862.74 +/- 894.19
Episode length: 535.00 +/- 137.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 3713088  |
---------------------------------
Eval num_timesteps=3715080, episode_reward=1014.51 +/- 671.87
Episode length: 560.60 +/- 112.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3715080  |
---------------------------------
Eval num_timesteps=3717072, episode_reward=2221.82 +/- 1746.59
Episode length: 540.60 +/- 156.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3717072  |
---------------------------------
Eval num_timesteps=3719064, episode_reward=1018.62 +/- 1078.05
Episode length: 554.40 +/- 120.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 3719064  |
---------------------------------
Eval num_timesteps=3721056, episode_reward=2255.14 +/- 1192.28
Episode length: 682.40 +/- 83.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3721056  |
---------------------------------
Eval num_timesteps=3723048, episode_reward=2456.64 +/- 1336.23
Episode length: 614.40 +/- 149.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 3723048  |
---------------------------------
Eval num_timesteps=3725040, episode_reward=2170.89 +/- 1281.47
Episode length: 667.40 +/- 46.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3725040  |
---------------------------------
Eval num_timesteps=3727032, episode_reward=2045.57 +/- 888.71
Episode length: 619.00 +/- 55.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3727032  |
---------------------------------
Eval num_timesteps=3729024, episode_reward=1569.74 +/- 1111.05
Episode length: 584.00 +/- 97.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3729024  |
---------------------------------
Eval num_timesteps=3731016, episode_reward=1334.56 +/- 1429.02
Episode length: 554.40 +/- 91.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3731016  |
---------------------------------
Eval num_timesteps=3733008, episode_reward=1671.57 +/- 833.77
Episode length: 572.40 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3733008  |
---------------------------------
Eval num_timesteps=3735000, episode_reward=995.70 +/- 849.31
Episode length: 596.40 +/- 97.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 3735000  |
---------------------------------
Eval num_timesteps=3736992, episode_reward=2203.36 +/- 1258.89
Episode length: 619.60 +/- 59.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 620          |
|    mean_reward          | 2.2e+03      |
| time/                   |              |
|    total_timesteps      | 3736992      |
| train/                  |              |
|    approx_kl            | 0.0021967248 |
|    clip_fraction        | 0.00521      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 229          |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.000612    |
|    std                  | 1.79         |
|    value_loss           | 844          |
------------------------------------------
Eval num_timesteps=3738984, episode_reward=949.41 +/- 609.29
Episode length: 572.80 +/- 133.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 3738984  |
---------------------------------
Eval num_timesteps=3740976, episode_reward=2220.95 +/- 1298.83
Episode length: 630.20 +/- 54.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3740976  |
---------------------------------
Eval num_timesteps=3742968, episode_reward=1683.44 +/- 1319.03
Episode length: 660.20 +/- 65.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 3742968  |
---------------------------------
Eval num_timesteps=3744960, episode_reward=1639.71 +/- 1126.35
Episode length: 670.00 +/- 55.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3744960  |
---------------------------------
Eval num_timesteps=3746952, episode_reward=2159.06 +/- 1454.53
Episode length: 621.80 +/- 133.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3746952  |
---------------------------------
Eval num_timesteps=3748944, episode_reward=2952.71 +/- 854.75
Episode length: 671.00 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 3748944  |
---------------------------------
Eval num_timesteps=3750936, episode_reward=1640.90 +/- 881.90
Episode length: 694.60 +/- 69.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3750936  |
---------------------------------
Eval num_timesteps=3752928, episode_reward=1834.83 +/- 1134.57
Episode length: 586.00 +/- 50.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3752928  |
---------------------------------
Eval num_timesteps=3754920, episode_reward=1306.31 +/- 1270.52
Episode length: 675.20 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3754920  |
---------------------------------
Eval num_timesteps=3756912, episode_reward=1781.95 +/- 1194.83
Episode length: 619.00 +/- 65.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 3756912  |
---------------------------------
Eval num_timesteps=3758904, episode_reward=2504.71 +/- 1367.64
Episode length: 662.20 +/- 38.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3758904  |
---------------------------------
Eval num_timesteps=3760896, episode_reward=1862.30 +/- 1085.94
Episode length: 638.40 +/- 54.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 3760896  |
---------------------------------
Eval num_timesteps=3762888, episode_reward=2191.89 +/- 958.48
Episode length: 679.40 +/- 39.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3762888  |
---------------------------------
Eval num_timesteps=3764880, episode_reward=1288.46 +/- 1032.56
Episode length: 565.80 +/- 80.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3764880  |
---------------------------------
Eval num_timesteps=3766872, episode_reward=1358.36 +/- 857.60
Episode length: 607.20 +/- 65.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3766872  |
---------------------------------
Eval num_timesteps=3768864, episode_reward=2201.25 +/- 1254.36
Episode length: 625.60 +/- 95.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3768864  |
---------------------------------
Eval num_timesteps=3770856, episode_reward=2296.80 +/- 870.00
Episode length: 603.40 +/- 30.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3770856  |
---------------------------------
Eval num_timesteps=3772848, episode_reward=1090.25 +/- 422.89
Episode length: 639.20 +/- 66.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3772848  |
---------------------------------
Eval num_timesteps=3774840, episode_reward=2758.57 +/- 1683.67
Episode length: 659.20 +/- 39.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3774840  |
---------------------------------
Eval num_timesteps=3776832, episode_reward=1608.38 +/- 1386.83
Episode length: 627.80 +/- 35.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3776832  |
---------------------------------
Eval num_timesteps=3778824, episode_reward=1289.66 +/- 1120.21
Episode length: 554.40 +/- 90.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3778824  |
---------------------------------
Eval num_timesteps=3780816, episode_reward=1318.69 +/- 1107.90
Episode length: 672.80 +/- 12.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 673      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3780816  |
---------------------------------
Eval num_timesteps=3782808, episode_reward=1499.19 +/- 813.89
Episode length: 622.80 +/- 47.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 3782808  |
---------------------------------
Eval num_timesteps=3784800, episode_reward=2346.76 +/- 1517.57
Episode length: 661.40 +/- 27.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 661          |
|    mean_reward          | 2.35e+03     |
| time/                   |              |
|    total_timesteps      | 3784800      |
| train/                  |              |
|    approx_kl            | 0.0028578436 |
|    clip_fraction        | 0.00826      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 386          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.000738    |
|    std                  | 1.81         |
|    value_loss           | 872          |
------------------------------------------
Eval num_timesteps=3786792, episode_reward=1132.96 +/- 594.16
Episode length: 676.60 +/- 52.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3786792  |
---------------------------------
Eval num_timesteps=3788784, episode_reward=1634.48 +/- 861.45
Episode length: 541.20 +/- 98.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3788784  |
---------------------------------
Eval num_timesteps=3790776, episode_reward=1450.02 +/- 674.97
Episode length: 593.20 +/- 84.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3790776  |
---------------------------------
Eval num_timesteps=3792768, episode_reward=1549.15 +/- 941.69
Episode length: 694.80 +/- 44.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 695      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3792768  |
---------------------------------
Eval num_timesteps=3794760, episode_reward=1694.46 +/- 914.88
Episode length: 606.80 +/- 82.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 3794760  |
---------------------------------
Eval num_timesteps=3796752, episode_reward=1593.45 +/- 326.05
Episode length: 617.80 +/- 46.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3796752  |
---------------------------------
Eval num_timesteps=3798744, episode_reward=2565.44 +/- 1355.07
Episode length: 655.80 +/- 90.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 3798744  |
---------------------------------
Eval num_timesteps=3800736, episode_reward=2038.99 +/- 1650.43
Episode length: 658.60 +/- 72.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3800736  |
---------------------------------
Eval num_timesteps=3802728, episode_reward=1233.28 +/- 1071.18
Episode length: 642.60 +/- 102.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3802728  |
---------------------------------
Eval num_timesteps=3804720, episode_reward=1981.96 +/- 1002.42
Episode length: 641.80 +/- 87.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3804720  |
---------------------------------
Eval num_timesteps=3806712, episode_reward=1329.46 +/- 1163.28
Episode length: 612.00 +/- 159.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3806712  |
---------------------------------
Eval num_timesteps=3808704, episode_reward=1975.67 +/- 1051.15
Episode length: 650.20 +/- 69.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3808704  |
---------------------------------
Eval num_timesteps=3810696, episode_reward=1544.07 +/- 763.52
Episode length: 664.20 +/- 65.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3810696  |
---------------------------------
Eval num_timesteps=3812688, episode_reward=2869.97 +/- 1399.26
Episode length: 681.20 +/- 59.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 3812688  |
---------------------------------
Eval num_timesteps=3814680, episode_reward=1890.31 +/- 1320.97
Episode length: 626.00 +/- 86.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3814680  |
---------------------------------
Eval num_timesteps=3816672, episode_reward=2381.27 +/- 962.72
Episode length: 651.60 +/- 53.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 3816672  |
---------------------------------
Eval num_timesteps=3818664, episode_reward=891.69 +/- 368.13
Episode length: 646.40 +/- 115.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 3818664  |
---------------------------------
Eval num_timesteps=3820656, episode_reward=1608.35 +/- 914.95
Episode length: 609.40 +/- 124.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3820656  |
---------------------------------
Eval num_timesteps=3822648, episode_reward=921.04 +/- 391.36
Episode length: 627.20 +/- 80.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 3822648  |
---------------------------------
Eval num_timesteps=3824640, episode_reward=2307.45 +/- 677.19
Episode length: 655.40 +/- 41.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 3824640  |
---------------------------------
Eval num_timesteps=3826632, episode_reward=2132.59 +/- 857.66
Episode length: 614.00 +/- 49.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3826632  |
---------------------------------
Eval num_timesteps=3828624, episode_reward=1685.33 +/- 810.59
Episode length: 653.00 +/- 80.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 3828624  |
---------------------------------
Eval num_timesteps=3830616, episode_reward=2330.11 +/- 862.83
Episode length: 652.40 +/- 68.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3830616  |
---------------------------------
Eval num_timesteps=3832608, episode_reward=1879.40 +/- 836.62
Episode length: 622.60 +/- 35.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3832608  |
---------------------------------
Eval num_timesteps=3834600, episode_reward=2078.02 +/- 1205.53
Episode length: 617.60 +/- 66.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | 2.08e+03     |
| time/                   |              |
|    total_timesteps      | 3834600      |
| train/                  |              |
|    approx_kl            | 0.0019875742 |
|    clip_fraction        | 0.00514      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 301          |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000608    |
|    std                  | 1.83         |
|    value_loss           | 848          |
------------------------------------------
Eval num_timesteps=3836592, episode_reward=1091.94 +/- 991.22
Episode length: 513.60 +/- 79.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3836592  |
---------------------------------
Eval num_timesteps=3838584, episode_reward=1515.93 +/- 846.14
Episode length: 681.00 +/- 46.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3838584  |
---------------------------------
Eval num_timesteps=3840576, episode_reward=1101.13 +/- 1058.77
Episode length: 686.80 +/- 131.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3840576  |
---------------------------------
Eval num_timesteps=3842568, episode_reward=1983.21 +/- 757.90
Episode length: 659.60 +/- 46.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3842568  |
---------------------------------
Eval num_timesteps=3844560, episode_reward=1915.69 +/- 1459.22
Episode length: 688.40 +/- 55.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3844560  |
---------------------------------
Eval num_timesteps=3846552, episode_reward=1965.17 +/- 1182.12
Episode length: 650.40 +/- 79.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3846552  |
---------------------------------
Eval num_timesteps=3848544, episode_reward=2641.56 +/- 1842.64
Episode length: 608.60 +/- 143.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 3848544  |
---------------------------------
Eval num_timesteps=3850536, episode_reward=970.39 +/- 288.18
Episode length: 643.20 +/- 86.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 3850536  |
---------------------------------
Eval num_timesteps=3852528, episode_reward=1589.32 +/- 1307.58
Episode length: 633.40 +/- 94.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3852528  |
---------------------------------
Eval num_timesteps=3854520, episode_reward=1965.40 +/- 1342.23
Episode length: 608.80 +/- 57.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3854520  |
---------------------------------
Eval num_timesteps=3856512, episode_reward=2236.54 +/- 1160.86
Episode length: 691.60 +/- 54.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3856512  |
---------------------------------
Eval num_timesteps=3858504, episode_reward=1870.81 +/- 1817.42
Episode length: 647.20 +/- 142.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3858504  |
---------------------------------
Eval num_timesteps=3860496, episode_reward=2548.77 +/- 1228.39
Episode length: 692.00 +/- 40.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3860496  |
---------------------------------
Eval num_timesteps=3862488, episode_reward=1733.32 +/- 1024.15
Episode length: 702.20 +/- 66.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3862488  |
---------------------------------
Eval num_timesteps=3864480, episode_reward=2481.03 +/- 1284.97
Episode length: 666.40 +/- 69.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3864480  |
---------------------------------
Eval num_timesteps=3866472, episode_reward=1766.30 +/- 1150.42
Episode length: 664.00 +/- 124.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3866472  |
---------------------------------
Eval num_timesteps=3868464, episode_reward=1308.37 +/- 1021.89
Episode length: 641.60 +/- 63.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3868464  |
---------------------------------
Eval num_timesteps=3870456, episode_reward=1757.54 +/- 1100.19
Episode length: 584.80 +/- 110.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3870456  |
---------------------------------
Eval num_timesteps=3872448, episode_reward=1302.56 +/- 994.74
Episode length: 652.40 +/- 96.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3872448  |
---------------------------------
Eval num_timesteps=3874440, episode_reward=1330.01 +/- 1132.59
Episode length: 638.80 +/- 128.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3874440  |
---------------------------------
Eval num_timesteps=3876432, episode_reward=860.38 +/- 275.34
Episode length: 606.80 +/- 51.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 3876432  |
---------------------------------
Eval num_timesteps=3878424, episode_reward=2385.88 +/- 1206.11
Episode length: 603.20 +/- 118.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3878424  |
---------------------------------
Eval num_timesteps=3880416, episode_reward=1720.19 +/- 359.72
Episode length: 627.60 +/- 46.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3880416  |
---------------------------------
Eval num_timesteps=3882408, episode_reward=1009.47 +/- 551.62
Episode length: 688.40 +/- 65.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3882408  |
---------------------------------
Eval num_timesteps=3884400, episode_reward=1089.29 +/- 597.84
Episode length: 641.20 +/- 97.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 641         |
|    mean_reward          | 1.09e+03    |
| time/                   |             |
|    total_timesteps      | 3884400     |
| train/                  |             |
|    approx_kl            | 0.003597419 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 321         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.000875   |
|    std                  | 1.84        |
|    value_loss           | 774         |
-----------------------------------------
Eval num_timesteps=3886392, episode_reward=1471.04 +/- 1390.94
Episode length: 611.00 +/- 117.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3886392  |
---------------------------------
Eval num_timesteps=3888384, episode_reward=2137.19 +/- 967.17
Episode length: 679.20 +/- 35.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3888384  |
---------------------------------
Eval num_timesteps=3890376, episode_reward=1261.30 +/- 1102.81
Episode length: 627.80 +/- 146.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3890376  |
---------------------------------
Eval num_timesteps=3892368, episode_reward=2559.28 +/- 1029.47
Episode length: 716.40 +/- 57.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3892368  |
---------------------------------
Eval num_timesteps=3894360, episode_reward=2674.48 +/- 1233.87
Episode length: 683.80 +/- 19.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3894360  |
---------------------------------
Eval num_timesteps=3896352, episode_reward=1520.25 +/- 1170.62
Episode length: 610.80 +/- 110.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3896352  |
---------------------------------
Eval num_timesteps=3898344, episode_reward=2532.71 +/- 905.98
Episode length: 639.60 +/- 64.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3898344  |
---------------------------------
Eval num_timesteps=3900336, episode_reward=1981.70 +/- 1521.59
Episode length: 603.60 +/- 116.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3900336  |
---------------------------------
Eval num_timesteps=3902328, episode_reward=2032.35 +/- 1852.25
Episode length: 570.20 +/- 144.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 3902328  |
---------------------------------
Eval num_timesteps=3904320, episode_reward=1109.84 +/- 1138.75
Episode length: 639.00 +/- 81.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3904320  |
---------------------------------
Eval num_timesteps=3906312, episode_reward=2107.40 +/- 1262.22
Episode length: 611.40 +/- 55.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 3906312  |
---------------------------------
Eval num_timesteps=3908304, episode_reward=812.55 +/- 453.74
Episode length: 603.80 +/- 121.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 3908304  |
---------------------------------
Eval num_timesteps=3910296, episode_reward=1466.80 +/- 1446.72
Episode length: 612.20 +/- 111.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 3910296  |
---------------------------------
Eval num_timesteps=3912288, episode_reward=1506.64 +/- 883.71
Episode length: 620.20 +/- 86.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 3912288  |
---------------------------------
Eval num_timesteps=3914280, episode_reward=2385.39 +/- 1178.06
Episode length: 674.00 +/- 36.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3914280  |
---------------------------------
Eval num_timesteps=3916272, episode_reward=2224.39 +/- 1028.65
Episode length: 629.40 +/- 73.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3916272  |
---------------------------------
Eval num_timesteps=3918264, episode_reward=1829.58 +/- 1183.27
Episode length: 635.20 +/- 55.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3918264  |
---------------------------------
Eval num_timesteps=3920256, episode_reward=1056.98 +/- 496.90
Episode length: 653.40 +/- 43.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3920256  |
---------------------------------
Eval num_timesteps=3922248, episode_reward=1931.83 +/- 1289.27
Episode length: 702.60 +/- 31.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3922248  |
---------------------------------
Eval num_timesteps=3924240, episode_reward=724.12 +/- 643.70
Episode length: 547.80 +/- 133.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 3924240  |
---------------------------------
Eval num_timesteps=3926232, episode_reward=1364.04 +/- 1048.29
Episode length: 630.80 +/- 116.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3926232  |
---------------------------------
Eval num_timesteps=3928224, episode_reward=1529.43 +/- 850.27
Episode length: 697.20 +/- 64.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3928224  |
---------------------------------
Eval num_timesteps=3930216, episode_reward=1581.97 +/- 1280.42
Episode length: 643.20 +/- 72.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3930216  |
---------------------------------
Eval num_timesteps=3932208, episode_reward=405.73 +/- 241.71
Episode length: 438.40 +/- 86.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 3932208      |
| train/                  |              |
|    approx_kl            | 0.0027430933 |
|    clip_fraction        | 0.00429      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000667    |
|    std                  | 1.85         |
|    value_loss           | 713          |
------------------------------------------
Eval num_timesteps=3934200, episode_reward=1531.54 +/- 2077.98
Episode length: 510.20 +/- 183.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3934200  |
---------------------------------
Eval num_timesteps=3936192, episode_reward=1409.19 +/- 1271.86
Episode length: 543.60 +/- 98.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 544      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3936192  |
---------------------------------
Eval num_timesteps=3938184, episode_reward=3156.26 +/- 1325.31
Episode length: 697.60 +/- 29.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 3938184  |
---------------------------------
Eval num_timesteps=3940176, episode_reward=309.67 +/- 285.55
Episode length: 454.80 +/- 109.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 455      |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 3940176  |
---------------------------------
Eval num_timesteps=3942168, episode_reward=1534.08 +/- 975.60
Episode length: 623.80 +/- 75.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3942168  |
---------------------------------
Eval num_timesteps=3944160, episode_reward=897.64 +/- 1082.07
Episode length: 490.80 +/- 113.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 3944160  |
---------------------------------
Eval num_timesteps=3946152, episode_reward=2585.84 +/- 1237.99
Episode length: 652.40 +/- 76.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 3946152  |
---------------------------------
Eval num_timesteps=3948144, episode_reward=1541.39 +/- 945.49
Episode length: 600.60 +/- 89.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3948144  |
---------------------------------
Eval num_timesteps=3950136, episode_reward=1173.37 +/- 1399.40
Episode length: 545.80 +/- 170.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3950136  |
---------------------------------
Eval num_timesteps=3952128, episode_reward=2355.56 +/- 1307.01
Episode length: 646.20 +/- 143.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 3952128  |
---------------------------------
Eval num_timesteps=3954120, episode_reward=1520.34 +/- 1401.63
Episode length: 552.80 +/- 175.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3954120  |
---------------------------------
Eval num_timesteps=3956112, episode_reward=490.92 +/- 324.12
Episode length: 470.40 +/- 113.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 3956112  |
---------------------------------
Eval num_timesteps=3958104, episode_reward=1318.59 +/- 1021.76
Episode length: 632.40 +/- 101.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3958104  |
---------------------------------
Eval num_timesteps=3960096, episode_reward=1898.91 +/- 1427.84
Episode length: 606.20 +/- 25.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 3960096  |
---------------------------------
Eval num_timesteps=3962088, episode_reward=2054.47 +/- 1797.02
Episode length: 572.80 +/- 77.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3962088  |
---------------------------------
Eval num_timesteps=3964080, episode_reward=1199.30 +/- 1111.99
Episode length: 610.60 +/- 146.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3964080  |
---------------------------------
Eval num_timesteps=3966072, episode_reward=1574.25 +/- 1401.33
Episode length: 587.00 +/- 95.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3966072  |
---------------------------------
Eval num_timesteps=3968064, episode_reward=768.03 +/- 953.96
Episode length: 531.60 +/- 128.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 3968064  |
---------------------------------
Eval num_timesteps=3970056, episode_reward=789.51 +/- 590.78
Episode length: 623.60 +/- 168.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 3970056  |
---------------------------------
Eval num_timesteps=3972048, episode_reward=1700.07 +/- 1663.64
Episode length: 499.80 +/- 135.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3972048  |
---------------------------------
Eval num_timesteps=3974040, episode_reward=1918.14 +/- 1186.43
Episode length: 627.20 +/- 130.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3974040  |
---------------------------------
Eval num_timesteps=3976032, episode_reward=1161.09 +/- 1012.46
Episode length: 550.60 +/- 66.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3976032  |
---------------------------------
Eval num_timesteps=3978024, episode_reward=1223.47 +/- 1081.60
Episode length: 582.60 +/- 97.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3978024  |
---------------------------------
Eval num_timesteps=3980016, episode_reward=1526.54 +/- 994.64
Episode length: 610.20 +/- 83.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3980016  |
---------------------------------
Eval num_timesteps=3982008, episode_reward=1292.89 +/- 931.89
Episode length: 586.20 +/- 108.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 586          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 3982008      |
| train/                  |              |
|    approx_kl            | 0.0028295221 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 271          |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000909    |
|    std                  | 1.86         |
|    value_loss           | 553          |
------------------------------------------
Eval num_timesteps=3984000, episode_reward=1955.19 +/- 719.11
Episode length: 717.20 +/- 60.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3984000  |
---------------------------------
Eval num_timesteps=3985992, episode_reward=2204.05 +/- 1015.98
Episode length: 670.00 +/- 62.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3985992  |
---------------------------------
Eval num_timesteps=3987984, episode_reward=1519.74 +/- 1085.86
Episode length: 668.80 +/- 112.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3987984  |
---------------------------------
Eval num_timesteps=3989976, episode_reward=2833.90 +/- 1724.13
Episode length: 627.40 +/- 124.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 3989976  |
---------------------------------
Eval num_timesteps=3991968, episode_reward=1345.83 +/- 968.74
Episode length: 647.40 +/- 82.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3991968  |
---------------------------------
Eval num_timesteps=3993960, episode_reward=1715.39 +/- 690.47
Episode length: 687.80 +/- 52.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3993960  |
---------------------------------
Eval num_timesteps=3995952, episode_reward=1641.94 +/- 448.03
Episode length: 665.40 +/- 77.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3995952  |
---------------------------------
Eval num_timesteps=3997944, episode_reward=1306.81 +/- 1132.59
Episode length: 645.00 +/- 90.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3997944  |
---------------------------------
Eval num_timesteps=3999936, episode_reward=2539.94 +/- 1587.38
Episode length: 648.40 +/- 73.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 3999936  |
---------------------------------
Eval num_timesteps=4001928, episode_reward=1248.76 +/- 953.33
Episode length: 624.00 +/- 90.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4001928  |
---------------------------------
Eval num_timesteps=4003920, episode_reward=1713.99 +/- 1223.97
Episode length: 629.40 +/- 85.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4003920  |
---------------------------------
Eval num_timesteps=4005912, episode_reward=2036.16 +/- 1881.36
Episode length: 600.40 +/- 165.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4005912  |
---------------------------------
Eval num_timesteps=4007904, episode_reward=1633.78 +/- 1008.07
Episode length: 696.80 +/- 39.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 4007904  |
---------------------------------
Eval num_timesteps=4009896, episode_reward=1738.57 +/- 1105.64
Episode length: 694.40 +/- 55.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4009896  |
---------------------------------
Eval num_timesteps=4011888, episode_reward=892.29 +/- 346.46
Episode length: 626.20 +/- 120.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 4011888  |
---------------------------------
Eval num_timesteps=4013880, episode_reward=1612.30 +/- 889.95
Episode length: 644.20 +/- 97.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4013880  |
---------------------------------
Eval num_timesteps=4015872, episode_reward=1060.50 +/- 955.57
Episode length: 631.80 +/- 119.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 4015872  |
---------------------------------
Eval num_timesteps=4017864, episode_reward=801.35 +/- 583.14
Episode length: 617.60 +/- 150.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 4017864  |
---------------------------------
Eval num_timesteps=4019856, episode_reward=3495.92 +/- 1256.39
Episode length: 656.80 +/- 49.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 3.5e+03  |
| time/              |          |
|    total_timesteps | 4019856  |
---------------------------------
Eval num_timesteps=4021848, episode_reward=2167.09 +/- 833.27
Episode length: 699.40 +/- 60.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4021848  |
---------------------------------
Eval num_timesteps=4023840, episode_reward=1390.42 +/- 911.31
Episode length: 663.40 +/- 100.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4023840  |
---------------------------------
Eval num_timesteps=4025832, episode_reward=1746.29 +/- 1368.98
Episode length: 635.80 +/- 157.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 4025832  |
---------------------------------
Eval num_timesteps=4027824, episode_reward=2387.31 +/- 1603.58
Episode length: 678.80 +/- 62.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4027824  |
---------------------------------
Eval num_timesteps=4029816, episode_reward=1310.28 +/- 1029.16
Episode length: 583.00 +/- 121.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4029816  |
---------------------------------
Eval num_timesteps=4031808, episode_reward=1491.95 +/- 1070.53
Episode length: 604.20 +/- 132.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 604          |
|    mean_reward          | 1.49e+03     |
| time/                   |              |
|    total_timesteps      | 4031808      |
| train/                  |              |
|    approx_kl            | 0.0034231711 |
|    clip_fraction        | 0.00909      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 265          |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00069     |
|    std                  | 1.88         |
|    value_loss           | 529          |
------------------------------------------
Eval num_timesteps=4033800, episode_reward=2049.67 +/- 1405.91
Episode length: 667.20 +/- 93.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 4033800  |
---------------------------------
Eval num_timesteps=4035792, episode_reward=1187.04 +/- 987.23
Episode length: 638.40 +/- 115.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4035792  |
---------------------------------
Eval num_timesteps=4037784, episode_reward=2441.17 +/- 938.66
Episode length: 738.20 +/- 28.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 4037784  |
---------------------------------
Eval num_timesteps=4039776, episode_reward=1863.15 +/- 1955.29
Episode length: 539.60 +/- 151.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 4039776  |
---------------------------------
Eval num_timesteps=4041768, episode_reward=1621.19 +/- 1197.30
Episode length: 589.20 +/- 93.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 589      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4041768  |
---------------------------------
Eval num_timesteps=4043760, episode_reward=1942.94 +/- 1075.64
Episode length: 649.60 +/- 61.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 4043760  |
---------------------------------
Eval num_timesteps=4045752, episode_reward=686.71 +/- 407.82
Episode length: 630.60 +/- 74.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 4045752  |
---------------------------------
Eval num_timesteps=4047744, episode_reward=1710.11 +/- 1154.06
Episode length: 658.20 +/- 88.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4047744  |
---------------------------------
Eval num_timesteps=4049736, episode_reward=805.32 +/- 515.29
Episode length: 580.80 +/- 131.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 4049736  |
---------------------------------
Eval num_timesteps=4051728, episode_reward=1475.94 +/- 1346.88
Episode length: 659.00 +/- 62.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4051728  |
---------------------------------
Eval num_timesteps=4053720, episode_reward=1821.34 +/- 2033.64
Episode length: 669.00 +/- 38.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4053720  |
---------------------------------
Eval num_timesteps=4055712, episode_reward=1776.71 +/- 1000.04
Episode length: 684.80 +/- 68.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 4055712  |
---------------------------------
Eval num_timesteps=4057704, episode_reward=1891.55 +/- 1190.32
Episode length: 681.60 +/- 65.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 4057704  |
---------------------------------
Eval num_timesteps=4059696, episode_reward=1453.61 +/- 837.19
Episode length: 657.20 +/- 47.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4059696  |
---------------------------------
Eval num_timesteps=4061688, episode_reward=2182.53 +/- 1119.99
Episode length: 693.80 +/- 95.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 4061688  |
---------------------------------
Eval num_timesteps=4063680, episode_reward=1329.54 +/- 859.75
Episode length: 594.40 +/- 60.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4063680  |
---------------------------------
Eval num_timesteps=4065672, episode_reward=1484.26 +/- 1306.59
Episode length: 568.60 +/- 115.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4065672  |
---------------------------------
Eval num_timesteps=4067664, episode_reward=1994.45 +/- 1014.00
Episode length: 681.80 +/- 73.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 4067664  |
---------------------------------
Eval num_timesteps=4069656, episode_reward=614.16 +/- 458.03
Episode length: 560.40 +/- 134.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 4069656  |
---------------------------------
Eval num_timesteps=4071648, episode_reward=1328.95 +/- 1202.10
Episode length: 574.00 +/- 150.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4071648  |
---------------------------------
Eval num_timesteps=4073640, episode_reward=964.04 +/- 1159.10
Episode length: 597.00 +/- 137.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 4073640  |
---------------------------------
Eval num_timesteps=4075632, episode_reward=2169.46 +/- 1498.63
Episode length: 671.80 +/- 83.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4075632  |
---------------------------------
Eval num_timesteps=4077624, episode_reward=982.48 +/- 665.83
Episode length: 610.00 +/- 68.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 4077624  |
---------------------------------
Eval num_timesteps=4079616, episode_reward=1333.01 +/- 1427.92
Episode length: 589.20 +/- 106.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 589      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4079616  |
---------------------------------
Eval num_timesteps=4081608, episode_reward=1564.93 +/- 1446.84
Episode length: 625.80 +/- 98.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 626          |
|    mean_reward          | 1.56e+03     |
| time/                   |              |
|    total_timesteps      | 4081608      |
| train/                  |              |
|    approx_kl            | 0.0029672924 |
|    clip_fraction        | 0.00948      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 189          |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000819    |
|    std                  | 1.9          |
|    value_loss           | 590          |
------------------------------------------
Eval num_timesteps=4083600, episode_reward=1498.71 +/- 963.77
Episode length: 659.00 +/- 85.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 4083600  |
---------------------------------
Eval num_timesteps=4085592, episode_reward=990.24 +/- 1300.06
Episode length: 487.40 +/- 115.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 4085592  |
---------------------------------
Eval num_timesteps=4087584, episode_reward=1819.32 +/- 1759.75
Episode length: 588.40 +/- 203.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4087584  |
---------------------------------
Eval num_timesteps=4089576, episode_reward=1143.26 +/- 824.76
Episode length: 534.80 +/- 78.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4089576  |
---------------------------------
Eval num_timesteps=4091568, episode_reward=1323.86 +/- 1094.26
Episode length: 592.60 +/- 128.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4091568  |
---------------------------------
Eval num_timesteps=4093560, episode_reward=1594.66 +/- 1307.54
Episode length: 565.00 +/- 133.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 4093560  |
---------------------------------
Eval num_timesteps=4095552, episode_reward=613.08 +/- 352.12
Episode length: 571.60 +/- 131.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 572      |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 4095552  |
---------------------------------
Eval num_timesteps=4097544, episode_reward=1739.27 +/- 1294.29
Episode length: 612.00 +/- 141.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4097544  |
---------------------------------
Eval num_timesteps=4099536, episode_reward=1716.10 +/- 1364.38
Episode length: 579.80 +/- 122.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4099536  |
---------------------------------
Eval num_timesteps=4101528, episode_reward=1134.77 +/- 1138.94
Episode length: 587.40 +/- 141.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4101528  |
---------------------------------
Eval num_timesteps=4103520, episode_reward=1982.68 +/- 803.23
Episode length: 628.60 +/- 58.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 4103520  |
---------------------------------
Eval num_timesteps=4105512, episode_reward=2163.06 +/- 2163.51
Episode length: 628.00 +/- 119.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 4105512  |
---------------------------------
Eval num_timesteps=4107504, episode_reward=1877.90 +/- 832.34
Episode length: 632.40 +/- 46.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4107504  |
---------------------------------
Eval num_timesteps=4109496, episode_reward=1689.23 +/- 1525.31
Episode length: 573.00 +/- 136.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 573      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4109496  |
---------------------------------
Eval num_timesteps=4111488, episode_reward=1706.72 +/- 970.31
Episode length: 631.80 +/- 70.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4111488  |
---------------------------------
Eval num_timesteps=4113480, episode_reward=985.85 +/- 1295.94
Episode length: 473.00 +/- 163.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 4113480  |
---------------------------------
Eval num_timesteps=4115472, episode_reward=989.08 +/- 1422.68
Episode length: 456.80 +/- 138.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 4115472  |
---------------------------------
Eval num_timesteps=4117464, episode_reward=1723.28 +/- 1395.20
Episode length: 599.20 +/- 145.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4117464  |
---------------------------------
Eval num_timesteps=4119456, episode_reward=2814.61 +/- 595.36
Episode length: 731.60 +/- 29.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4119456  |
---------------------------------
Eval num_timesteps=4121448, episode_reward=995.73 +/- 1064.84
Episode length: 564.80 +/- 132.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 4121448  |
---------------------------------
Eval num_timesteps=4123440, episode_reward=1732.51 +/- 1253.33
Episode length: 603.80 +/- 152.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4123440  |
---------------------------------
Eval num_timesteps=4125432, episode_reward=961.42 +/- 1182.29
Episode length: 551.40 +/- 128.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 4125432  |
---------------------------------
Eval num_timesteps=4127424, episode_reward=1337.58 +/- 1567.26
Episode length: 520.40 +/- 149.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4127424  |
---------------------------------
Eval num_timesteps=4129416, episode_reward=3130.91 +/- 569.40
Episode length: 667.40 +/- 64.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 667          |
|    mean_reward          | 3.13e+03     |
| time/                   |              |
|    total_timesteps      | 4129416      |
| train/                  |              |
|    approx_kl            | 0.0039789174 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.23        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 226          |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.9          |
|    value_loss           | 682          |
------------------------------------------
Eval num_timesteps=4131408, episode_reward=2197.95 +/- 1158.08
Episode length: 646.80 +/- 78.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 4131408  |
---------------------------------
Eval num_timesteps=4133400, episode_reward=1410.49 +/- 1053.66
Episode length: 619.20 +/- 124.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4133400  |
---------------------------------
Eval num_timesteps=4135392, episode_reward=2213.72 +/- 1196.27
Episode length: 558.20 +/- 119.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 4135392  |
---------------------------------
Eval num_timesteps=4137384, episode_reward=1952.66 +/- 1334.33
Episode length: 665.40 +/- 36.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4137384  |
---------------------------------
Eval num_timesteps=4139376, episode_reward=2644.86 +/- 1317.06
Episode length: 607.00 +/- 134.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 4139376  |
---------------------------------
Eval num_timesteps=4141368, episode_reward=1793.26 +/- 1050.85
Episode length: 682.40 +/- 27.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 4141368  |
---------------------------------
Eval num_timesteps=4143360, episode_reward=2505.63 +/- 968.35
Episode length: 686.60 +/- 63.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 4143360  |
---------------------------------
Eval num_timesteps=4145352, episode_reward=1214.31 +/- 455.76
Episode length: 670.00 +/- 99.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4145352  |
---------------------------------
Eval num_timesteps=4147344, episode_reward=1571.87 +/- 1105.94
Episode length: 641.60 +/- 135.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4147344  |
---------------------------------
Eval num_timesteps=4149336, episode_reward=1827.34 +/- 654.92
Episode length: 682.00 +/- 53.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 4149336  |
---------------------------------
Eval num_timesteps=4151328, episode_reward=1442.61 +/- 896.91
Episode length: 628.80 +/- 124.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4151328  |
---------------------------------
Eval num_timesteps=4153320, episode_reward=2324.50 +/- 1140.95
Episode length: 696.00 +/- 35.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 4153320  |
---------------------------------
Eval num_timesteps=4155312, episode_reward=1554.32 +/- 852.28
Episode length: 672.40 +/- 43.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 4155312  |
---------------------------------
Eval num_timesteps=4157304, episode_reward=1195.84 +/- 769.02
Episode length: 649.80 +/- 33.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 4157304  |
---------------------------------
Eval num_timesteps=4159296, episode_reward=1559.93 +/- 1331.54
Episode length: 600.40 +/- 128.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4159296  |
---------------------------------
Eval num_timesteps=4161288, episode_reward=1951.34 +/- 1184.79
Episode length: 643.60 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4161288  |
---------------------------------
Eval num_timesteps=4163280, episode_reward=1278.60 +/- 1224.55
Episode length: 626.00 +/- 85.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4163280  |
---------------------------------
Eval num_timesteps=4165272, episode_reward=2743.98 +/- 1352.15
Episode length: 674.40 +/- 39.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4165272  |
---------------------------------
Eval num_timesteps=4167264, episode_reward=1951.89 +/- 833.73
Episode length: 656.60 +/- 34.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4167264  |
---------------------------------
Eval num_timesteps=4169256, episode_reward=2800.78 +/- 1140.10
Episode length: 709.60 +/- 28.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4169256  |
---------------------------------
Eval num_timesteps=4171248, episode_reward=1712.29 +/- 1141.63
Episode length: 601.40 +/- 120.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4171248  |
---------------------------------
Eval num_timesteps=4173240, episode_reward=1253.57 +/- 413.76
Episode length: 699.40 +/- 79.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4173240  |
---------------------------------
Eval num_timesteps=4175232, episode_reward=652.16 +/- 593.41
Episode length: 656.20 +/- 48.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 656      |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 4175232  |
---------------------------------
Eval num_timesteps=4177224, episode_reward=1898.96 +/- 1183.09
Episode length: 647.60 +/- 60.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 4177224  |
---------------------------------
Eval num_timesteps=4179216, episode_reward=2492.10 +/- 1679.87
Episode length: 662.20 +/- 56.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 662          |
|    mean_reward          | 2.49e+03     |
| time/                   |              |
|    total_timesteps      | 4179216      |
| train/                  |              |
|    approx_kl            | 0.0026412432 |
|    clip_fraction        | 0.0078       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.25        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 537          |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.91         |
|    value_loss           | 644          |
------------------------------------------
Eval num_timesteps=4181208, episode_reward=2328.55 +/- 1596.92
Episode length: 637.80 +/- 88.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 4181208  |
---------------------------------
Eval num_timesteps=4183200, episode_reward=2316.64 +/- 1289.08
Episode length: 631.60 +/- 69.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 4183200  |
---------------------------------
Eval num_timesteps=4185192, episode_reward=1116.44 +/- 1610.48
Episode length: 498.40 +/- 146.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4185192  |
---------------------------------
Eval num_timesteps=4187184, episode_reward=1533.10 +/- 1589.50
Episode length: 557.80 +/- 91.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4187184  |
---------------------------------
Eval num_timesteps=4189176, episode_reward=2414.31 +/- 1899.81
Episode length: 605.00 +/- 143.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4189176  |
---------------------------------
Eval num_timesteps=4191168, episode_reward=1650.03 +/- 1569.03
Episode length: 596.80 +/- 69.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4191168  |
---------------------------------
Eval num_timesteps=4193160, episode_reward=988.58 +/- 610.83
Episode length: 600.80 +/- 47.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 4193160  |
---------------------------------
Eval num_timesteps=4195152, episode_reward=2974.40 +/- 1486.45
Episode length: 724.00 +/- 30.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4195152  |
---------------------------------
Eval num_timesteps=4197144, episode_reward=1460.01 +/- 1042.09
Episode length: 629.20 +/- 40.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4197144  |
---------------------------------
Eval num_timesteps=4199136, episode_reward=2350.33 +/- 1140.49
Episode length: 636.80 +/- 67.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 4199136  |
---------------------------------
Eval num_timesteps=4201128, episode_reward=1119.62 +/- 629.31
Episode length: 627.20 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4201128  |
---------------------------------
Eval num_timesteps=4203120, episode_reward=1174.67 +/- 1635.96
Episode length: 505.40 +/- 112.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4203120  |
---------------------------------
Eval num_timesteps=4205112, episode_reward=1433.03 +/- 801.01
Episode length: 592.80 +/- 43.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4205112  |
---------------------------------
Eval num_timesteps=4207104, episode_reward=1880.17 +/- 1190.26
Episode length: 603.60 +/- 150.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4207104  |
---------------------------------
Eval num_timesteps=4209096, episode_reward=1263.80 +/- 675.51
Episode length: 607.40 +/- 64.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4209096  |
---------------------------------
Eval num_timesteps=4211088, episode_reward=588.42 +/- 381.02
Episode length: 569.20 +/- 111.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 4211088  |
---------------------------------
Eval num_timesteps=4213080, episode_reward=2776.37 +/- 1659.76
Episode length: 637.40 +/- 93.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4213080  |
---------------------------------
Eval num_timesteps=4215072, episode_reward=2053.10 +/- 1224.26
Episode length: 598.20 +/- 140.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 4215072  |
---------------------------------
Eval num_timesteps=4217064, episode_reward=1649.43 +/- 891.21
Episode length: 639.60 +/- 152.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4217064  |
---------------------------------
Eval num_timesteps=4219056, episode_reward=1080.24 +/- 1198.15
Episode length: 539.60 +/- 94.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4219056  |
---------------------------------
Eval num_timesteps=4221048, episode_reward=2012.55 +/- 1503.70
Episode length: 615.80 +/- 102.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 4221048  |
---------------------------------
Eval num_timesteps=4223040, episode_reward=1844.46 +/- 1405.24
Episode length: 636.40 +/- 79.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 4223040  |
---------------------------------
Eval num_timesteps=4225032, episode_reward=2483.70 +/- 1201.11
Episode length: 627.00 +/- 96.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 4225032  |
---------------------------------
Eval num_timesteps=4227024, episode_reward=1234.78 +/- 1149.04
Episode length: 522.00 +/- 114.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4227024  |
---------------------------------
Eval num_timesteps=4229016, episode_reward=1858.72 +/- 1024.56
Episode length: 643.60 +/- 52.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 644          |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 4229016      |
| train/                  |              |
|    approx_kl            | 0.0029339318 |
|    clip_fraction        | 0.00851      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 260          |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.000953    |
|    std                  | 1.93         |
|    value_loss           | 735          |
------------------------------------------
Eval num_timesteps=4231008, episode_reward=1928.17 +/- 1018.22
Episode length: 619.60 +/- 104.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 4231008  |
---------------------------------
Eval num_timesteps=4233000, episode_reward=2262.21 +/- 1053.86
Episode length: 610.20 +/- 76.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 4233000  |
---------------------------------
Eval num_timesteps=4234992, episode_reward=3344.80 +/- 391.51
Episode length: 651.40 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 4234992  |
---------------------------------
Eval num_timesteps=4236984, episode_reward=1532.80 +/- 1590.28
Episode length: 595.00 +/- 80.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4236984  |
---------------------------------
Eval num_timesteps=4238976, episode_reward=1428.21 +/- 1051.44
Episode length: 608.60 +/- 64.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4238976  |
---------------------------------
Eval num_timesteps=4240968, episode_reward=1898.84 +/- 1492.32
Episode length: 663.80 +/- 38.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 4240968  |
---------------------------------
Eval num_timesteps=4242960, episode_reward=1315.08 +/- 1083.21
Episode length: 619.20 +/- 124.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4242960  |
---------------------------------
Eval num_timesteps=4244952, episode_reward=2314.44 +/- 1317.63
Episode length: 677.00 +/- 41.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 4244952  |
---------------------------------
Eval num_timesteps=4246944, episode_reward=1328.99 +/- 951.53
Episode length: 597.00 +/- 60.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4246944  |
---------------------------------
Eval num_timesteps=4248936, episode_reward=1747.41 +/- 1317.62
Episode length: 614.60 +/- 49.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 4248936  |
---------------------------------
Eval num_timesteps=4250928, episode_reward=2516.42 +/- 556.53
Episode length: 624.20 +/- 70.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 4250928  |
---------------------------------
Eval num_timesteps=4252920, episode_reward=766.68 +/- 498.50
Episode length: 622.20 +/- 113.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 4252920  |
---------------------------------
Eval num_timesteps=4254912, episode_reward=1991.94 +/- 1144.76
Episode length: 633.40 +/- 78.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 633      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 4254912  |
---------------------------------
Eval num_timesteps=4256904, episode_reward=2085.54 +/- 1178.87
Episode length: 658.20 +/- 30.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 4256904  |
---------------------------------
Eval num_timesteps=4258896, episode_reward=2351.88 +/- 1054.43
Episode length: 666.00 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 4258896  |
---------------------------------
Eval num_timesteps=4260888, episode_reward=1621.38 +/- 958.64
Episode length: 604.80 +/- 66.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4260888  |
---------------------------------
Eval num_timesteps=4262880, episode_reward=1701.25 +/- 1203.30
Episode length: 654.60 +/- 64.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 4262880  |
---------------------------------
Eval num_timesteps=4264872, episode_reward=2213.30 +/- 1471.90
Episode length: 629.40 +/- 32.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 4264872  |
---------------------------------
Eval num_timesteps=4266864, episode_reward=1563.53 +/- 1357.53
Episode length: 570.20 +/- 133.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4266864  |
---------------------------------
Eval num_timesteps=4268856, episode_reward=1989.62 +/- 1465.25
Episode length: 613.80 +/- 102.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 4268856  |
---------------------------------
Eval num_timesteps=4270848, episode_reward=1294.92 +/- 1158.59
Episode length: 604.20 +/- 110.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4270848  |
---------------------------------
Eval num_timesteps=4272840, episode_reward=1706.63 +/- 1085.50
Episode length: 634.80 +/- 33.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4272840  |
---------------------------------
Eval num_timesteps=4274832, episode_reward=1562.95 +/- 1412.06
Episode length: 576.80 +/- 150.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4274832  |
---------------------------------
Eval num_timesteps=4276824, episode_reward=1767.15 +/- 1208.77
Episode length: 673.00 +/- 54.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 673          |
|    mean_reward          | 1.77e+03     |
| time/                   |              |
|    total_timesteps      | 4276824      |
| train/                  |              |
|    approx_kl            | 0.0026120727 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.31        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 301          |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.94         |
|    value_loss           | 722          |
------------------------------------------
Eval num_timesteps=4278816, episode_reward=2717.57 +/- 1166.62
Episode length: 631.00 +/- 40.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4278816  |
---------------------------------
Eval num_timesteps=4280808, episode_reward=1984.82 +/- 1115.73
Episode length: 675.00 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 4280808  |
---------------------------------
Eval num_timesteps=4282800, episode_reward=1741.98 +/- 1118.89
Episode length: 625.00 +/- 29.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4282800  |
---------------------------------
Eval num_timesteps=4284792, episode_reward=1961.42 +/- 1167.69
Episode length: 603.20 +/- 53.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 4284792  |
---------------------------------
Eval num_timesteps=4286784, episode_reward=994.85 +/- 768.63
Episode length: 624.80 +/- 82.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 4286784  |
---------------------------------
Eval num_timesteps=4288776, episode_reward=1276.15 +/- 666.80
Episode length: 650.80 +/- 41.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4288776  |
---------------------------------
Eval num_timesteps=4290768, episode_reward=609.24 +/- 286.29
Episode length: 694.40 +/- 39.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 4290768  |
---------------------------------
Eval num_timesteps=4292760, episode_reward=1094.18 +/- 866.66
Episode length: 656.60 +/- 29.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4292760  |
---------------------------------
Eval num_timesteps=4294752, episode_reward=725.00 +/- 445.14
Episode length: 638.00 +/- 86.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 4294752  |
---------------------------------
Eval num_timesteps=4296744, episode_reward=1762.83 +/- 1006.66
Episode length: 625.00 +/- 61.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 4296744  |
---------------------------------
Eval num_timesteps=4298736, episode_reward=1631.35 +/- 1029.52
Episode length: 638.20 +/- 37.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 4298736  |
---------------------------------
Eval num_timesteps=4300728, episode_reward=752.56 +/- 429.06
Episode length: 678.20 +/- 33.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 4300728  |
---------------------------------
Eval num_timesteps=4302720, episode_reward=977.40 +/- 827.84
Episode length: 589.80 +/- 59.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 4302720  |
---------------------------------
Eval num_timesteps=4304712, episode_reward=1492.57 +/- 1522.05
Episode length: 618.20 +/- 120.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 4304712  |
---------------------------------
Eval num_timesteps=4306704, episode_reward=852.84 +/- 557.78
Episode length: 624.80 +/- 65.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 4306704  |
---------------------------------
Eval num_timesteps=4308696, episode_reward=1753.52 +/- 1184.14
Episode length: 652.00 +/- 38.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 4308696  |
---------------------------------
Eval num_timesteps=4310688, episode_reward=1956.29 +/- 1693.26
Episode length: 652.00 +/- 36.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 4310688  |
---------------------------------
Eval num_timesteps=4312680, episode_reward=1745.18 +/- 1291.41
Episode length: 677.80 +/- 9.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 678      |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 4312680  |
---------------------------------
Eval num_timesteps=4314672, episode_reward=1174.53 +/- 395.82
Episode length: 586.60 +/- 100.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 4314672  |
---------------------------------
Eval num_timesteps=4316664, episode_reward=659.78 +/- 295.02
Episode length: 623.00 +/- 59.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 4316664  |
---------------------------------
Eval num_timesteps=4318656, episode_reward=746.06 +/- 560.01
Episode length: 665.20 +/- 13.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 4318656  |
---------------------------------
Eval num_timesteps=4320648, episode_reward=1317.02 +/- 1093.03
Episode length: 670.40 +/- 55.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4320648  |
---------------------------------
Eval num_timesteps=4322640, episode_reward=1376.98 +/- 1098.94
Episode length: 673.80 +/- 45.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4322640  |
---------------------------------
Eval num_timesteps=4324632, episode_reward=1782.39 +/- 1216.35
Episode length: 599.60 +/- 67.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 4324632  |
---------------------------------
Eval num_timesteps=4326624, episode_reward=1303.57 +/- 853.63
Episode length: 578.80 +/- 36.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 579          |
|    mean_reward          | 1.3e+03      |
| time/                   |              |
|    total_timesteps      | 4326624      |
| train/                  |              |
|    approx_kl            | 0.0026070683 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.34        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 177          |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00091     |
|    std                  | 1.96         |
|    value_loss           | 555          |
------------------------------------------
Eval num_timesteps=4328616, episode_reward=1263.62 +/- 1035.13
Episode length: 601.40 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4328616  |
---------------------------------
Eval num_timesteps=4330608, episode_reward=572.91 +/- 532.22
Episode length: 538.80 +/- 110.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 4330608  |
---------------------------------
Eval num_timesteps=4332600, episode_reward=1523.41 +/- 1233.19
Episode length: 645.80 +/- 45.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4332600  |
---------------------------------
Eval num_timesteps=4334592, episode_reward=1691.21 +/- 1296.59
Episode length: 576.80 +/- 62.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4334592  |
---------------------------------
Eval num_timesteps=4336584, episode_reward=1565.07 +/- 1284.73
Episode length: 658.40 +/- 41.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 658      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4336584  |
---------------------------------
Eval num_timesteps=4338576, episode_reward=1564.52 +/- 877.01
Episode length: 549.80 +/- 74.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4338576  |
---------------------------------
Eval num_timesteps=4340568, episode_reward=926.98 +/- 321.42
Episode length: 611.20 +/- 49.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 4340568  |
---------------------------------
Eval num_timesteps=4342560, episode_reward=1900.50 +/- 967.23
Episode length: 592.40 +/- 70.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 4342560  |
---------------------------------
Eval num_timesteps=4344552, episode_reward=1151.54 +/- 1571.65
Episode length: 628.80 +/- 71.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4344552  |
---------------------------------
Eval num_timesteps=4346544, episode_reward=808.39 +/- 551.10
Episode length: 592.00 +/- 46.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 4346544  |
---------------------------------
Eval num_timesteps=4348536, episode_reward=1276.36 +/- 1200.05
Episode length: 603.60 +/- 74.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 4348536  |
---------------------------------
Eval num_timesteps=4350528, episode_reward=1670.55 +/- 750.64
Episode length: 620.60 +/- 40.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4350528  |
---------------------------------
Eval num_timesteps=4352520, episode_reward=1593.97 +/- 1136.44
Episode length: 601.60 +/- 74.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 4352520  |
---------------------------------
Eval num_timesteps=4354512, episode_reward=838.49 +/- 804.95
Episode length: 568.60 +/- 93.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 4354512  |
---------------------------------
Eval num_timesteps=4356504, episode_reward=1437.07 +/- 1674.29
Episode length: 556.40 +/- 107.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4356504  |
---------------------------------
Eval num_timesteps=4358496, episode_reward=915.86 +/- 229.31
Episode length: 602.40 +/- 87.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 4358496  |
---------------------------------
Eval num_timesteps=4360488, episode_reward=1916.10 +/- 1232.01
Episode length: 608.60 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 4360488  |
---------------------------------
Eval num_timesteps=4362480, episode_reward=684.07 +/- 956.91
Episode length: 554.00 +/- 121.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 4362480  |
---------------------------------
Eval num_timesteps=4364472, episode_reward=1480.94 +/- 761.81
Episode length: 563.20 +/- 56.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4364472  |
---------------------------------
Eval num_timesteps=4366464, episode_reward=1533.17 +/- 1342.75
Episode length: 625.80 +/- 56.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4366464  |
---------------------------------
Eval num_timesteps=4368456, episode_reward=1137.89 +/- 384.22
Episode length: 587.20 +/- 71.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4368456  |
---------------------------------
Eval num_timesteps=4370448, episode_reward=593.87 +/- 426.25
Episode length: 606.80 +/- 76.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 594      |
| time/              |          |
|    total_timesteps | 4370448  |
---------------------------------
Eval num_timesteps=4372440, episode_reward=1027.74 +/- 1300.09
Episode length: 607.00 +/- 85.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4372440  |
---------------------------------
Eval num_timesteps=4374432, episode_reward=1129.20 +/- 206.73
Episode length: 599.40 +/- 79.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4374432  |
---------------------------------
Eval num_timesteps=4376424, episode_reward=1450.24 +/- 1046.35
Episode length: 602.60 +/- 60.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 603          |
|    mean_reward          | 1.45e+03     |
| time/                   |              |
|    total_timesteps      | 4376424      |
| train/                  |              |
|    approx_kl            | 0.0023817373 |
|    clip_fraction        | 0.00635      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.37        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 146          |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.000773    |
|    std                  | 1.97         |
|    value_loss           | 531          |
------------------------------------------
Eval num_timesteps=4378416, episode_reward=700.83 +/- 450.05
Episode length: 650.00 +/- 27.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 4378416  |
---------------------------------
Eval num_timesteps=4380408, episode_reward=782.13 +/- 271.35
Episode length: 639.60 +/- 37.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 4380408  |
---------------------------------
Eval num_timesteps=4382400, episode_reward=1343.69 +/- 988.55
Episode length: 593.00 +/- 72.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4382400  |
---------------------------------
Eval num_timesteps=4384392, episode_reward=1949.31 +/- 1429.35
Episode length: 576.40 +/- 55.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4384392  |
---------------------------------
Eval num_timesteps=4386384, episode_reward=1015.86 +/- 1103.56
Episode length: 664.40 +/- 39.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4386384  |
---------------------------------
Eval num_timesteps=4388376, episode_reward=1032.38 +/- 1065.68
Episode length: 603.00 +/- 47.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4388376  |
---------------------------------
Eval num_timesteps=4390368, episode_reward=1115.80 +/- 659.78
Episode length: 576.80 +/- 38.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4390368  |
---------------------------------
Eval num_timesteps=4392360, episode_reward=392.14 +/- 49.92
Episode length: 598.00 +/- 53.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 4392360  |
---------------------------------
Eval num_timesteps=4394352, episode_reward=2044.61 +/- 1257.05
Episode length: 579.60 +/- 37.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4394352  |
---------------------------------
Eval num_timesteps=4396344, episode_reward=1667.87 +/- 1098.46
Episode length: 608.00 +/- 62.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4396344  |
---------------------------------
Eval num_timesteps=4398336, episode_reward=1207.73 +/- 790.35
Episode length: 648.40 +/- 31.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4398336  |
---------------------------------
Eval num_timesteps=4400328, episode_reward=1830.00 +/- 1241.62
Episode length: 585.20 +/- 68.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 4400328  |
---------------------------------
Eval num_timesteps=4402320, episode_reward=822.10 +/- 1073.23
Episode length: 640.80 +/- 31.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 4402320  |
---------------------------------
Eval num_timesteps=4404312, episode_reward=1050.10 +/- 1113.53
Episode length: 609.60 +/- 86.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4404312  |
---------------------------------
Eval num_timesteps=4406304, episode_reward=1737.37 +/- 1385.35
Episode length: 612.20 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 4406304  |
---------------------------------
Eval num_timesteps=4408296, episode_reward=1046.10 +/- 953.30
Episode length: 619.40 +/- 57.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4408296  |
---------------------------------
Eval num_timesteps=4410288, episode_reward=1266.84 +/- 1250.84
Episode length: 641.20 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4410288  |
---------------------------------
Eval num_timesteps=4412280, episode_reward=1186.22 +/- 644.44
Episode length: 601.00 +/- 61.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 4412280  |
---------------------------------
Eval num_timesteps=4414272, episode_reward=294.92 +/- 185.62
Episode length: 622.20 +/- 33.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 4414272  |
---------------------------------
Eval num_timesteps=4416264, episode_reward=1163.53 +/- 1163.87
Episode length: 620.00 +/- 50.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4416264  |
---------------------------------
Eval num_timesteps=4418256, episode_reward=880.70 +/- 1035.99
Episode length: 607.00 +/- 45.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 4418256  |
---------------------------------
Eval num_timesteps=4420248, episode_reward=1529.11 +/- 1210.45
Episode length: 627.00 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4420248  |
---------------------------------
Eval num_timesteps=4422240, episode_reward=514.45 +/- 246.75
Episode length: 549.00 +/- 69.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 514      |
| time/              |          |
|    total_timesteps | 4422240  |
---------------------------------
Eval num_timesteps=4424232, episode_reward=1098.41 +/- 1236.97
Episode length: 597.80 +/- 72.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 598          |
|    mean_reward          | 1.1e+03      |
| time/                   |              |
|    total_timesteps      | 4424232      |
| train/                  |              |
|    approx_kl            | 0.0027170936 |
|    clip_fraction        | 0.00773      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.000578    |
|    std                  | 1.98         |
|    value_loss           | 464          |
------------------------------------------
Eval num_timesteps=4426224, episode_reward=1349.95 +/- 1396.53
Episode length: 491.60 +/- 108.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 4426224  |
---------------------------------
Eval num_timesteps=4428216, episode_reward=1810.84 +/- 1117.87
Episode length: 602.00 +/- 61.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 4428216  |
---------------------------------
Eval num_timesteps=4430208, episode_reward=1826.55 +/- 654.92
Episode length: 583.40 +/- 90.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 4430208  |
---------------------------------
Eval num_timesteps=4432200, episode_reward=2040.84 +/- 905.22
Episode length: 567.40 +/- 70.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4432200  |
---------------------------------
Eval num_timesteps=4434192, episode_reward=573.52 +/- 293.59
Episode length: 621.40 +/- 32.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 4434192  |
---------------------------------
Eval num_timesteps=4436184, episode_reward=1464.63 +/- 1323.49
Episode length: 617.80 +/- 54.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4436184  |
---------------------------------
Eval num_timesteps=4438176, episode_reward=2129.89 +/- 1117.19
Episode length: 563.60 +/- 53.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 4438176  |
---------------------------------
Eval num_timesteps=4440168, episode_reward=2260.96 +/- 1144.08
Episode length: 574.60 +/- 87.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 4440168  |
---------------------------------
Eval num_timesteps=4442160, episode_reward=1029.91 +/- 544.45
Episode length: 529.80 +/- 41.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4442160  |
---------------------------------
Eval num_timesteps=4444152, episode_reward=1512.87 +/- 1472.40
Episode length: 593.60 +/- 43.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4444152  |
---------------------------------
Eval num_timesteps=4446144, episode_reward=1516.39 +/- 1088.65
Episode length: 561.40 +/- 66.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4446144  |
---------------------------------
Eval num_timesteps=4448136, episode_reward=1609.58 +/- 1395.99
Episode length: 580.00 +/- 105.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4448136  |
---------------------------------
Eval num_timesteps=4450128, episode_reward=1647.93 +/- 1306.56
Episode length: 599.20 +/- 70.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4450128  |
---------------------------------
Eval num_timesteps=4452120, episode_reward=1791.35 +/- 1337.69
Episode length: 515.40 +/- 85.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 4452120  |
---------------------------------
Eval num_timesteps=4454112, episode_reward=2154.02 +/- 1065.67
Episode length: 624.40 +/- 34.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 4454112  |
---------------------------------
Eval num_timesteps=4456104, episode_reward=2036.66 +/- 1645.47
Episode length: 604.00 +/- 70.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4456104  |
---------------------------------
Eval num_timesteps=4458096, episode_reward=1559.30 +/- 1353.16
Episode length: 590.20 +/- 79.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4458096  |
---------------------------------
Eval num_timesteps=4460088, episode_reward=1108.26 +/- 1226.45
Episode length: 631.80 +/- 53.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4460088  |
---------------------------------
Eval num_timesteps=4462080, episode_reward=1771.13 +/- 1190.50
Episode length: 597.40 +/- 30.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 4462080  |
---------------------------------
Eval num_timesteps=4464072, episode_reward=844.27 +/- 575.23
Episode length: 605.40 +/- 73.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 4464072  |
---------------------------------
Eval num_timesteps=4466064, episode_reward=2104.79 +/- 1136.21
Episode length: 615.00 +/- 40.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 4466064  |
---------------------------------
Eval num_timesteps=4468056, episode_reward=987.74 +/- 1223.74
Episode length: 604.00 +/- 107.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 4468056  |
---------------------------------
Eval num_timesteps=4470048, episode_reward=2034.21 +/- 1210.88
Episode length: 586.60 +/- 57.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 4470048  |
---------------------------------
Eval num_timesteps=4472040, episode_reward=1778.66 +/- 1479.71
Episode length: 621.00 +/- 49.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 4472040  |
---------------------------------
Eval num_timesteps=4474032, episode_reward=2040.67 +/- 785.00
Episode length: 592.20 +/- 10.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 592          |
|    mean_reward          | 2.04e+03     |
| time/                   |              |
|    total_timesteps      | 4474032      |
| train/                  |              |
|    approx_kl            | 0.0026947705 |
|    clip_fraction        | 0.00653      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 85.2         |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000666    |
|    std                  | 2            |
|    value_loss           | 482          |
------------------------------------------
Eval num_timesteps=4476024, episode_reward=1147.99 +/- 1071.87
Episode length: 595.40 +/- 62.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4476024  |
---------------------------------
Eval num_timesteps=4478016, episode_reward=2122.87 +/- 1147.96
Episode length: 592.80 +/- 58.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 4478016  |
---------------------------------
Eval num_timesteps=4480008, episode_reward=2152.81 +/- 1067.12
Episode length: 630.80 +/- 69.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 4480008  |
---------------------------------
Eval num_timesteps=4482000, episode_reward=1323.09 +/- 789.77
Episode length: 586.00 +/- 45.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4482000  |
---------------------------------
Eval num_timesteps=4483992, episode_reward=1155.66 +/- 604.35
Episode length: 589.60 +/- 72.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4483992  |
---------------------------------
Eval num_timesteps=4485984, episode_reward=2394.13 +/- 1161.33
Episode length: 622.60 +/- 50.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4485984  |
---------------------------------
Eval num_timesteps=4487976, episode_reward=1309.39 +/- 1185.76
Episode length: 545.40 +/- 120.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 545      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4487976  |
---------------------------------
Eval num_timesteps=4489968, episode_reward=1909.02 +/- 738.36
Episode length: 566.20 +/- 24.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 4489968  |
---------------------------------
Eval num_timesteps=4491960, episode_reward=1527.09 +/- 986.69
Episode length: 596.00 +/- 65.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4491960  |
---------------------------------
Eval num_timesteps=4493952, episode_reward=1008.54 +/- 230.86
Episode length: 614.00 +/- 57.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 4493952  |
---------------------------------
Eval num_timesteps=4495944, episode_reward=1326.15 +/- 803.66
Episode length: 606.40 +/- 77.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4495944  |
---------------------------------
Eval num_timesteps=4497936, episode_reward=1082.90 +/- 1093.85
Episode length: 538.60 +/- 68.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4497936  |
---------------------------------
Eval num_timesteps=4499928, episode_reward=2166.29 +/- 1595.17
Episode length: 593.60 +/- 155.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4499928  |
---------------------------------
Eval num_timesteps=4501920, episode_reward=1095.28 +/- 991.76
Episode length: 599.80 +/- 92.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4501920  |
---------------------------------
Eval num_timesteps=4503912, episode_reward=1571.66 +/- 1300.51
Episode length: 609.00 +/- 112.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4503912  |
---------------------------------
Eval num_timesteps=4505904, episode_reward=685.37 +/- 327.32
Episode length: 574.00 +/- 48.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 4505904  |
---------------------------------
Eval num_timesteps=4507896, episode_reward=1310.14 +/- 1014.56
Episode length: 554.40 +/- 61.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4507896  |
---------------------------------
Eval num_timesteps=4509888, episode_reward=2191.29 +/- 1459.89
Episode length: 675.60 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 4509888  |
---------------------------------
Eval num_timesteps=4511880, episode_reward=1876.51 +/- 806.58
Episode length: 659.00 +/- 47.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4511880  |
---------------------------------
Eval num_timesteps=4513872, episode_reward=1073.77 +/- 1288.98
Episode length: 518.00 +/- 144.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 4513872  |
---------------------------------
Eval num_timesteps=4515864, episode_reward=1477.31 +/- 671.92
Episode length: 614.20 +/- 51.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 4515864  |
---------------------------------
Eval num_timesteps=4517856, episode_reward=1015.21 +/- 351.37
Episode length: 532.40 +/- 46.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 4517856  |
---------------------------------
Eval num_timesteps=4519848, episode_reward=920.34 +/- 648.65
Episode length: 529.00 +/- 74.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 4519848  |
---------------------------------
Eval num_timesteps=4521840, episode_reward=1316.50 +/- 639.65
Episode length: 569.20 +/- 31.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4521840  |
---------------------------------
Eval num_timesteps=4523832, episode_reward=1517.60 +/- 1428.07
Episode length: 607.40 +/- 73.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 1.52e+03     |
| time/                   |              |
|    total_timesteps      | 4523832      |
| train/                  |              |
|    approx_kl            | 0.0018651448 |
|    clip_fraction        | 0.00515      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.45        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000536    |
|    std                  | 2.02         |
|    value_loss           | 552          |
------------------------------------------
Eval num_timesteps=4525824, episode_reward=2021.52 +/- 1938.64
Episode length: 616.60 +/- 55.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 4525824  |
---------------------------------
Eval num_timesteps=4527816, episode_reward=1159.97 +/- 858.99
Episode length: 550.20 +/- 78.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 4527816  |
---------------------------------
Eval num_timesteps=4529808, episode_reward=1459.78 +/- 1404.99
Episode length: 602.60 +/- 50.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4529808  |
---------------------------------
Eval num_timesteps=4531800, episode_reward=1140.67 +/- 889.70
Episode length: 600.60 +/- 117.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4531800  |
---------------------------------
Eval num_timesteps=4533792, episode_reward=1693.37 +/- 878.10
Episode length: 555.60 +/- 50.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4533792  |
---------------------------------
Eval num_timesteps=4535784, episode_reward=2216.56 +/- 1114.99
Episode length: 611.00 +/- 73.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 4535784  |
---------------------------------
Eval num_timesteps=4537776, episode_reward=1685.17 +/- 1100.73
Episode length: 635.00 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4537776  |
---------------------------------
Eval num_timesteps=4539768, episode_reward=488.65 +/- 423.73
Episode length: 505.00 +/- 41.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 4539768  |
---------------------------------
Eval num_timesteps=4541760, episode_reward=990.64 +/- 863.81
Episode length: 576.40 +/- 48.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 4541760  |
---------------------------------
Eval num_timesteps=4543752, episode_reward=720.17 +/- 497.59
Episode length: 548.00 +/- 147.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 4543752  |
---------------------------------
Eval num_timesteps=4545744, episode_reward=1976.41 +/- 1146.78
Episode length: 621.20 +/- 25.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 4545744  |
---------------------------------
Eval num_timesteps=4547736, episode_reward=1136.83 +/- 1750.49
Episode length: 493.40 +/- 137.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4547736  |
---------------------------------
Eval num_timesteps=4549728, episode_reward=1205.79 +/- 852.00
Episode length: 559.40 +/- 65.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4549728  |
---------------------------------
Eval num_timesteps=4551720, episode_reward=889.16 +/- 890.39
Episode length: 508.20 +/- 52.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 4551720  |
---------------------------------
Eval num_timesteps=4553712, episode_reward=968.13 +/- 922.66
Episode length: 597.20 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 4553712  |
---------------------------------
Eval num_timesteps=4555704, episode_reward=756.24 +/- 162.99
Episode length: 558.80 +/- 66.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 4555704  |
---------------------------------
Eval num_timesteps=4557696, episode_reward=2859.83 +/- 1162.18
Episode length: 651.00 +/- 55.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4557696  |
---------------------------------
Eval num_timesteps=4559688, episode_reward=646.74 +/- 405.58
Episode length: 586.60 +/- 102.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 4559688  |
---------------------------------
Eval num_timesteps=4561680, episode_reward=890.17 +/- 779.03
Episode length: 547.20 +/- 84.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 4561680  |
---------------------------------
Eval num_timesteps=4563672, episode_reward=1436.55 +/- 1146.99
Episode length: 582.20 +/- 42.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4563672  |
---------------------------------
Eval num_timesteps=4565664, episode_reward=2144.89 +/- 968.62
Episode length: 623.60 +/- 51.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 4565664  |
---------------------------------
Eval num_timesteps=4567656, episode_reward=1773.22 +/- 1064.49
Episode length: 592.60 +/- 71.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 593      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 4567656  |
---------------------------------
Eval num_timesteps=4569648, episode_reward=1335.80 +/- 1108.42
Episode length: 651.00 +/- 45.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 4569648  |
---------------------------------
Eval num_timesteps=4571640, episode_reward=1161.56 +/- 780.25
Episode length: 593.20 +/- 84.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 593          |
|    mean_reward          | 1.16e+03     |
| time/                   |              |
|    total_timesteps      | 4571640      |
| train/                  |              |
|    approx_kl            | 0.0026416008 |
|    clip_fraction        | 0.00774      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.48        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 218          |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.03         |
|    value_loss           | 699          |
------------------------------------------
Eval num_timesteps=4573632, episode_reward=2002.50 +/- 1139.04
Episode length: 602.60 +/- 71.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 4573632  |
---------------------------------
Eval num_timesteps=4575624, episode_reward=2128.05 +/- 1629.57
Episode length: 606.80 +/- 59.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 607      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 4575624  |
---------------------------------
Eval num_timesteps=4577616, episode_reward=1253.02 +/- 1341.73
Episode length: 615.80 +/- 80.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4577616  |
---------------------------------
Eval num_timesteps=4579608, episode_reward=1059.41 +/- 1235.06
Episode length: 504.00 +/- 140.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 4579608  |
---------------------------------
Eval num_timesteps=4581600, episode_reward=1048.93 +/- 807.70
Episode length: 613.80 +/- 52.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4581600  |
---------------------------------
Eval num_timesteps=4583592, episode_reward=1286.44 +/- 1146.50
Episode length: 533.40 +/- 44.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4583592  |
---------------------------------
Eval num_timesteps=4585584, episode_reward=1509.76 +/- 1013.45
Episode length: 606.00 +/- 83.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4585584  |
---------------------------------
Eval num_timesteps=4587576, episode_reward=486.19 +/- 607.34
Episode length: 605.20 +/- 123.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 4587576  |
---------------------------------
Eval num_timesteps=4589568, episode_reward=1117.22 +/- 1142.18
Episode length: 594.00 +/- 65.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4589568  |
---------------------------------
Eval num_timesteps=4591560, episode_reward=1129.91 +/- 986.74
Episode length: 519.00 +/- 75.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4591560  |
---------------------------------
Eval num_timesteps=4593552, episode_reward=1512.02 +/- 942.20
Episode length: 591.00 +/- 74.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4593552  |
---------------------------------
Eval num_timesteps=4595544, episode_reward=1365.01 +/- 1126.74
Episode length: 608.60 +/- 114.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 4595544  |
---------------------------------
Eval num_timesteps=4597536, episode_reward=3135.98 +/- 989.94
Episode length: 636.00 +/- 43.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4597536  |
---------------------------------
Eval num_timesteps=4599528, episode_reward=1582.85 +/- 1224.83
Episode length: 648.40 +/- 36.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4599528  |
---------------------------------
Eval num_timesteps=4601520, episode_reward=1877.87 +/- 1074.75
Episode length: 616.20 +/- 49.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4601520  |
---------------------------------
Eval num_timesteps=4603512, episode_reward=2135.33 +/- 1884.66
Episode length: 627.40 +/- 64.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 4603512  |
---------------------------------
Eval num_timesteps=4605504, episode_reward=1102.62 +/- 1161.05
Episode length: 602.40 +/- 73.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 4605504  |
---------------------------------
Eval num_timesteps=4607496, episode_reward=512.10 +/- 216.53
Episode length: 539.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 4607496  |
---------------------------------
Eval num_timesteps=4609488, episode_reward=433.81 +/- 255.68
Episode length: 608.60 +/- 87.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 4609488  |
---------------------------------
Eval num_timesteps=4611480, episode_reward=1993.16 +/- 1006.19
Episode length: 579.00 +/- 64.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 4611480  |
---------------------------------
Eval num_timesteps=4613472, episode_reward=1256.48 +/- 999.04
Episode length: 599.20 +/- 43.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4613472  |
---------------------------------
Eval num_timesteps=4615464, episode_reward=1595.07 +/- 1034.67
Episode length: 641.60 +/- 27.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4615464  |
---------------------------------
Eval num_timesteps=4617456, episode_reward=1911.84 +/- 1548.32
Episode length: 631.00 +/- 26.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 4617456  |
---------------------------------
Eval num_timesteps=4619448, episode_reward=1888.62 +/- 892.67
Episode length: 624.60 +/- 66.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 4619448  |
---------------------------------
Eval num_timesteps=4621440, episode_reward=1612.99 +/- 1330.18
Episode length: 544.20 +/- 51.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 544        |
|    mean_reward          | 1.61e+03   |
| time/                   |            |
|    total_timesteps      | 4621440    |
| train/                  |            |
|    approx_kl            | 0.00253974 |
|    clip_fraction        | 0.0106     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.53      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.001      |
|    loss                 | 170        |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.000955  |
|    std                  | 2.06       |
|    value_loss           | 526        |
----------------------------------------
Eval num_timesteps=4623432, episode_reward=1672.12 +/- 1362.73
Episode length: 578.20 +/- 60.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4623432  |
---------------------------------
Eval num_timesteps=4625424, episode_reward=1315.84 +/- 753.90
Episode length: 574.80 +/- 83.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4625424  |
---------------------------------
Eval num_timesteps=4627416, episode_reward=794.23 +/- 455.74
Episode length: 516.20 +/- 82.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 4627416  |
---------------------------------
Eval num_timesteps=4629408, episode_reward=1180.35 +/- 641.48
Episode length: 556.80 +/- 85.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4629408  |
---------------------------------
Eval num_timesteps=4631400, episode_reward=1635.22 +/- 1129.83
Episode length: 667.00 +/- 23.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 4631400  |
---------------------------------
Eval num_timesteps=4633392, episode_reward=1568.00 +/- 1327.04
Episode length: 589.00 +/- 78.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 589      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 4633392  |
---------------------------------
Eval num_timesteps=4635384, episode_reward=2357.37 +/- 1112.15
Episode length: 663.60 +/- 130.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 4635384  |
---------------------------------
Eval num_timesteps=4637376, episode_reward=1947.56 +/- 1401.24
Episode length: 622.40 +/- 69.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4637376  |
---------------------------------
Eval num_timesteps=4639368, episode_reward=1084.00 +/- 698.22
Episode length: 516.00 +/- 65.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4639368  |
---------------------------------
Eval num_timesteps=4641360, episode_reward=1000.11 +/- 363.41
Episode length: 585.20 +/- 71.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4641360  |
---------------------------------
Eval num_timesteps=4643352, episode_reward=1608.08 +/- 1469.14
Episode length: 597.40 +/- 122.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4643352  |
---------------------------------
Eval num_timesteps=4645344, episode_reward=1146.22 +/- 878.15
Episode length: 531.00 +/- 73.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 531      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4645344  |
---------------------------------
Eval num_timesteps=4647336, episode_reward=463.04 +/- 349.84
Episode length: 577.20 +/- 137.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 4647336  |
---------------------------------
Eval num_timesteps=4649328, episode_reward=1319.55 +/- 1075.17
Episode length: 620.40 +/- 98.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 4649328  |
---------------------------------
Eval num_timesteps=4651320, episode_reward=618.38 +/- 151.62
Episode length: 555.20 +/- 37.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 4651320  |
---------------------------------
Eval num_timesteps=4653312, episode_reward=1532.25 +/- 1058.53
Episode length: 553.40 +/- 69.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 553      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4653312  |
---------------------------------
Eval num_timesteps=4655304, episode_reward=1874.38 +/- 1289.33
Episode length: 557.40 +/- 130.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 4655304  |
---------------------------------
Eval num_timesteps=4657296, episode_reward=925.73 +/- 828.57
Episode length: 581.40 +/- 69.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 4657296  |
---------------------------------
Eval num_timesteps=4659288, episode_reward=1233.97 +/- 938.67
Episode length: 546.80 +/- 46.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4659288  |
---------------------------------
Eval num_timesteps=4661280, episode_reward=1808.59 +/- 1344.21
Episode length: 548.60 +/- 116.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 4661280  |
---------------------------------
Eval num_timesteps=4663272, episode_reward=1584.74 +/- 1301.14
Episode length: 606.20 +/- 49.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4663272  |
---------------------------------
Eval num_timesteps=4665264, episode_reward=2347.95 +/- 1338.90
Episode length: 601.60 +/- 102.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 4665264  |
---------------------------------
Eval num_timesteps=4667256, episode_reward=1206.94 +/- 1355.74
Episode length: 590.80 +/- 77.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 4667256  |
---------------------------------
Eval num_timesteps=4669248, episode_reward=1254.81 +/- 1018.82
Episode length: 600.60 +/- 70.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 4669248  |
---------------------------------
Eval num_timesteps=4671240, episode_reward=1950.56 +/- 1800.63
Episode length: 575.80 +/- 82.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 576          |
|    mean_reward          | 1.95e+03     |
| time/                   |              |
|    total_timesteps      | 4671240      |
| train/                  |              |
|    approx_kl            | 0.0026431463 |
|    clip_fraction        | 0.00883      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 130          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000886    |
|    std                  | 2.08         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=4673232, episode_reward=2876.63 +/- 1262.53
Episode length: 636.00 +/- 36.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4673232  |
---------------------------------
Eval num_timesteps=4675224, episode_reward=1328.19 +/- 849.22
Episode length: 585.00 +/- 56.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 4675224  |
---------------------------------
Eval num_timesteps=4677216, episode_reward=3157.96 +/- 577.26
Episode length: 594.20 +/- 27.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4677216  |
---------------------------------
Eval num_timesteps=4679208, episode_reward=1051.45 +/- 1473.73
Episode length: 532.60 +/- 73.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 4679208  |
---------------------------------
Eval num_timesteps=4681200, episode_reward=1399.53 +/- 1331.65
Episode length: 546.00 +/- 126.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4681200  |
---------------------------------
Eval num_timesteps=4683192, episode_reward=1516.10 +/- 920.97
Episode length: 552.20 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4683192  |
---------------------------------
Eval num_timesteps=4685184, episode_reward=1405.26 +/- 900.81
Episode length: 552.00 +/- 56.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4685184  |
---------------------------------
Eval num_timesteps=4687176, episode_reward=1665.62 +/- 1530.44
Episode length: 581.60 +/- 85.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4687176  |
---------------------------------
Eval num_timesteps=4689168, episode_reward=1115.75 +/- 1287.90
Episode length: 626.60 +/- 90.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4689168  |
---------------------------------
Eval num_timesteps=4691160, episode_reward=840.48 +/- 216.95
Episode length: 587.40 +/- 93.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 4691160  |
---------------------------------
Eval num_timesteps=4693152, episode_reward=1307.30 +/- 910.95
Episode length: 542.80 +/- 64.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 543      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 4693152  |
---------------------------------
Eval num_timesteps=4695144, episode_reward=1088.38 +/- 1170.07
Episode length: 478.40 +/- 108.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4695144  |
---------------------------------
Eval num_timesteps=4697136, episode_reward=1876.21 +/- 1111.22
Episode length: 618.20 +/- 52.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4697136  |
---------------------------------
Eval num_timesteps=4699128, episode_reward=869.42 +/- 411.74
Episode length: 590.80 +/- 115.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 4699128  |
---------------------------------
Eval num_timesteps=4701120, episode_reward=1770.00 +/- 1070.52
Episode length: 569.40 +/- 99.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 4701120  |
---------------------------------
Eval num_timesteps=4703112, episode_reward=954.62 +/- 215.87
Episode length: 498.60 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 4703112  |
---------------------------------
Eval num_timesteps=4705104, episode_reward=1488.89 +/- 903.19
Episode length: 575.60 +/- 81.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 576      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 4705104  |
---------------------------------
Eval num_timesteps=4707096, episode_reward=1115.27 +/- 633.53
Episode length: 590.40 +/- 59.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4707096  |
---------------------------------
Eval num_timesteps=4709088, episode_reward=1176.47 +/- 1031.73
Episode length: 617.20 +/- 120.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4709088  |
---------------------------------
Eval num_timesteps=4711080, episode_reward=966.99 +/- 965.38
Episode length: 528.60 +/- 108.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 4711080  |
---------------------------------
Eval num_timesteps=4713072, episode_reward=884.41 +/- 744.26
Episode length: 552.40 +/- 46.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 552      |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 4713072  |
---------------------------------
Eval num_timesteps=4715064, episode_reward=762.25 +/- 803.22
Episode length: 546.20 +/- 96.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 4715064  |
---------------------------------
Eval num_timesteps=4717056, episode_reward=1806.78 +/- 1691.92
Episode length: 624.20 +/- 108.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 4717056  |
---------------------------------
Eval num_timesteps=4719048, episode_reward=1849.02 +/- 1218.00
Episode length: 587.00 +/- 68.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 587         |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 4719048     |
| train/                  |             |
|    approx_kl            | 0.003141134 |
|    clip_fraction        | 0.00909     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.62       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.000951   |
|    std                  | 2.1         |
|    value_loss           | 480         |
-----------------------------------------
Eval num_timesteps=4721040, episode_reward=1797.31 +/- 1243.66
Episode length: 555.60 +/- 50.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 4721040  |
---------------------------------
Eval num_timesteps=4723032, episode_reward=1121.49 +/- 1065.53
Episode length: 514.40 +/- 66.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4723032  |
---------------------------------
Eval num_timesteps=4725024, episode_reward=1437.54 +/- 1005.41
Episode length: 568.20 +/- 109.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4725024  |
---------------------------------
Eval num_timesteps=4727016, episode_reward=1398.44 +/- 1003.97
Episode length: 565.00 +/- 59.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 4727016  |
---------------------------------
Eval num_timesteps=4729008, episode_reward=3091.89 +/- 1034.01
Episode length: 622.00 +/- 51.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4729008  |
---------------------------------
Eval num_timesteps=4731000, episode_reward=1230.37 +/- 1023.37
Episode length: 581.40 +/- 88.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 581      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4731000  |
---------------------------------
Eval num_timesteps=4732992, episode_reward=1798.73 +/- 1296.68
Episode length: 606.00 +/- 160.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 4732992  |
---------------------------------
Eval num_timesteps=4734984, episode_reward=1258.86 +/- 1112.53
Episode length: 547.60 +/- 115.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 548      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4734984  |
---------------------------------
Eval num_timesteps=4736976, episode_reward=1287.34 +/- 865.07
Episode length: 549.00 +/- 55.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 549      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4736976  |
---------------------------------
Eval num_timesteps=4738968, episode_reward=1118.28 +/- 887.09
Episode length: 476.00 +/- 103.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 4738968  |
---------------------------------
Eval num_timesteps=4740960, episode_reward=1078.02 +/- 320.51
Episode length: 615.60 +/- 94.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 4740960  |
---------------------------------
Eval num_timesteps=4742952, episode_reward=982.35 +/- 490.69
Episode length: 533.80 +/- 60.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 534      |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 4742952  |
---------------------------------
Eval num_timesteps=4744944, episode_reward=1112.34 +/- 412.13
Episode length: 574.60 +/- 73.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4744944  |
---------------------------------
Eval num_timesteps=4746936, episode_reward=1583.15 +/- 1280.81
Episode length: 602.40 +/- 93.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 4746936  |
---------------------------------
Eval num_timesteps=4748928, episode_reward=913.82 +/- 1091.52
Episode length: 430.40 +/- 86.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 4748928  |
---------------------------------
Eval num_timesteps=4750920, episode_reward=1135.08 +/- 1248.34
Episode length: 483.80 +/- 132.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4750920  |
---------------------------------
Eval num_timesteps=4752912, episode_reward=1601.37 +/- 1212.34
Episode length: 604.20 +/- 65.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4752912  |
---------------------------------
Eval num_timesteps=4754904, episode_reward=855.50 +/- 1344.06
Episode length: 533.40 +/- 105.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 4754904  |
---------------------------------
Eval num_timesteps=4756896, episode_reward=707.78 +/- 619.79
Episode length: 518.40 +/- 118.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 4756896  |
---------------------------------
Eval num_timesteps=4758888, episode_reward=1465.97 +/- 1281.24
Episode length: 622.20 +/- 79.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4758888  |
---------------------------------
Eval num_timesteps=4760880, episode_reward=570.92 +/- 444.36
Episode length: 453.80 +/- 97.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 4760880  |
---------------------------------
Eval num_timesteps=4762872, episode_reward=1026.60 +/- 738.91
Episode length: 495.20 +/- 93.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4762872  |
---------------------------------
Eval num_timesteps=4764864, episode_reward=1129.22 +/- 1360.87
Episode length: 567.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 4764864  |
---------------------------------
Eval num_timesteps=4766856, episode_reward=1234.59 +/- 951.63
Episode length: 582.00 +/- 117.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4766856  |
---------------------------------
Eval num_timesteps=4768848, episode_reward=1416.03 +/- 605.83
Episode length: 603.80 +/- 77.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 604          |
|    mean_reward          | 1.42e+03     |
| time/                   |              |
|    total_timesteps      | 4768848      |
| train/                  |              |
|    approx_kl            | 0.0029615648 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.64        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 157          |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.000761    |
|    std                  | 2.11         |
|    value_loss           | 488          |
------------------------------------------
Eval num_timesteps=4770840, episode_reward=1274.51 +/- 915.63
Episode length: 642.80 +/- 78.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4770840  |
---------------------------------
Eval num_timesteps=4772832, episode_reward=1509.50 +/- 1377.40
Episode length: 683.80 +/- 23.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4772832  |
---------------------------------
Eval num_timesteps=4774824, episode_reward=1505.56 +/- 1640.64
Episode length: 556.80 +/- 101.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4774824  |
---------------------------------
Eval num_timesteps=4776816, episode_reward=1729.74 +/- 954.22
Episode length: 641.20 +/- 85.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4776816  |
---------------------------------
Eval num_timesteps=4778808, episode_reward=927.83 +/- 1136.62
Episode length: 647.40 +/- 30.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 4778808  |
---------------------------------
Eval num_timesteps=4780800, episode_reward=989.25 +/- 913.63
Episode length: 583.60 +/- 101.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 4780800  |
---------------------------------
Eval num_timesteps=4782792, episode_reward=1914.10 +/- 1503.49
Episode length: 566.00 +/- 108.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 566      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 4782792  |
---------------------------------
Eval num_timesteps=4784784, episode_reward=1918.81 +/- 1218.29
Episode length: 604.80 +/- 76.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 4784784  |
---------------------------------
Eval num_timesteps=4786776, episode_reward=1455.94 +/- 1379.27
Episode length: 529.00 +/- 85.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4786776  |
---------------------------------
Eval num_timesteps=4788768, episode_reward=1821.24 +/- 821.25
Episode length: 634.20 +/- 34.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 4788768  |
---------------------------------
Eval num_timesteps=4790760, episode_reward=575.53 +/- 231.05
Episode length: 608.60 +/- 79.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 4790760  |
---------------------------------
Eval num_timesteps=4792752, episode_reward=534.80 +/- 353.20
Episode length: 616.60 +/- 93.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 4792752  |
---------------------------------
Eval num_timesteps=4794744, episode_reward=1889.61 +/- 1124.39
Episode length: 600.80 +/- 96.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 4794744  |
---------------------------------
Eval num_timesteps=4796736, episode_reward=3004.25 +/- 1016.50
Episode length: 670.40 +/- 41.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4796736  |
---------------------------------
Eval num_timesteps=4798728, episode_reward=783.57 +/- 604.87
Episode length: 626.80 +/- 128.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 4798728  |
---------------------------------
Eval num_timesteps=4800720, episode_reward=812.94 +/- 566.73
Episode length: 535.40 +/- 71.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 535      |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 4800720  |
---------------------------------
Eval num_timesteps=4802712, episode_reward=1562.32 +/- 1395.38
Episode length: 577.80 +/- 69.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4802712  |
---------------------------------
Eval num_timesteps=4804704, episode_reward=1230.14 +/- 329.74
Episode length: 547.00 +/- 61.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4804704  |
---------------------------------
Eval num_timesteps=4806696, episode_reward=1561.42 +/- 1095.41
Episode length: 649.60 +/- 66.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4806696  |
---------------------------------
Eval num_timesteps=4808688, episode_reward=1356.05 +/- 1252.74
Episode length: 542.40 +/- 94.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4808688  |
---------------------------------
Eval num_timesteps=4810680, episode_reward=2541.31 +/- 1069.49
Episode length: 648.80 +/- 50.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4810680  |
---------------------------------
Eval num_timesteps=4812672, episode_reward=1531.50 +/- 1106.99
Episode length: 640.40 +/- 22.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4812672  |
---------------------------------
Eval num_timesteps=4814664, episode_reward=2132.27 +/- 1496.54
Episode length: 541.20 +/- 75.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 4814664  |
---------------------------------
Eval num_timesteps=4816656, episode_reward=2251.93 +/- 1300.13
Episode length: 659.00 +/- 35.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 659      |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 4816656  |
---------------------------------
Eval num_timesteps=4818648, episode_reward=1630.01 +/- 1077.80
Episode length: 607.40 +/- 134.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 1.63e+03     |
| time/                   |              |
|    total_timesteps      | 4818648      |
| train/                  |              |
|    approx_kl            | 0.0019493039 |
|    clip_fraction        | 0.00637      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.67        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 90.9         |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.000554    |
|    std                  | 2.12         |
|    value_loss           | 468          |
------------------------------------------
Eval num_timesteps=4820640, episode_reward=1498.60 +/- 771.08
Episode length: 584.40 +/- 81.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 4820640  |
---------------------------------
Eval num_timesteps=4822632, episode_reward=2064.83 +/- 1103.37
Episode length: 579.60 +/- 62.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 4822632  |
---------------------------------
Eval num_timesteps=4824624, episode_reward=1772.38 +/- 1673.14
Episode length: 653.20 +/- 95.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 4824624  |
---------------------------------
Eval num_timesteps=4826616, episode_reward=999.68 +/- 1214.90
Episode length: 451.20 +/- 102.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 451      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 4826616  |
---------------------------------
Eval num_timesteps=4828608, episode_reward=1712.63 +/- 1251.78
Episode length: 616.80 +/- 45.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 4828608  |
---------------------------------
Eval num_timesteps=4830600, episode_reward=1964.02 +/- 1149.80
Episode length: 647.00 +/- 53.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 4830600  |
---------------------------------
Eval num_timesteps=4832592, episode_reward=1973.29 +/- 1697.06
Episode length: 660.20 +/- 28.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 4832592  |
---------------------------------
Eval num_timesteps=4834584, episode_reward=1525.61 +/- 790.02
Episode length: 581.60 +/- 59.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 4834584  |
---------------------------------
Eval num_timesteps=4836576, episode_reward=711.87 +/- 340.13
Episode length: 638.80 +/- 49.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 4836576  |
---------------------------------
Eval num_timesteps=4838568, episode_reward=838.83 +/- 521.02
Episode length: 579.80 +/- 97.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 4838568  |
---------------------------------
Eval num_timesteps=4840560, episode_reward=1849.67 +/- 1798.99
Episode length: 570.80 +/- 108.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 4840560  |
---------------------------------
Eval num_timesteps=4842552, episode_reward=1733.88 +/- 935.41
Episode length: 611.20 +/- 47.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4842552  |
---------------------------------
Eval num_timesteps=4844544, episode_reward=1114.28 +/- 619.51
Episode length: 708.40 +/- 156.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4844544  |
---------------------------------
Eval num_timesteps=4846536, episode_reward=1642.47 +/- 1284.22
Episode length: 662.60 +/- 84.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 4846536  |
---------------------------------
Eval num_timesteps=4848528, episode_reward=1732.21 +/- 1059.63
Episode length: 650.00 +/- 43.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4848528  |
---------------------------------
Eval num_timesteps=4850520, episode_reward=2538.92 +/- 1065.79
Episode length: 631.00 +/- 34.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4850520  |
---------------------------------
Eval num_timesteps=4852512, episode_reward=1522.17 +/- 1791.45
Episode length: 574.20 +/- 224.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4852512  |
---------------------------------
Eval num_timesteps=4854504, episode_reward=1432.80 +/- 1038.01
Episode length: 527.20 +/- 102.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 527      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 4854504  |
---------------------------------
Eval num_timesteps=4856496, episode_reward=2256.68 +/- 1148.61
Episode length: 622.60 +/- 63.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 4856496  |
---------------------------------
Eval num_timesteps=4858488, episode_reward=897.29 +/- 646.24
Episode length: 556.60 +/- 155.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 557      |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 4858488  |
---------------------------------
Eval num_timesteps=4860480, episode_reward=1453.86 +/- 732.73
Episode length: 586.60 +/- 62.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 587      |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 4860480  |
---------------------------------
Eval num_timesteps=4862472, episode_reward=1294.56 +/- 1626.65
Episode length: 532.80 +/- 67.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 533      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4862472  |
---------------------------------
Eval num_timesteps=4864464, episode_reward=1413.45 +/- 1057.35
Episode length: 591.40 +/- 126.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4864464  |
---------------------------------
Eval num_timesteps=4866456, episode_reward=2112.03 +/- 1538.10
Episode length: 558.80 +/- 74.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 559         |
|    mean_reward          | 2.11e+03    |
| time/                   |             |
|    total_timesteps      | 4866456     |
| train/                  |             |
|    approx_kl            | 0.002530374 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.72       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 79.2        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.000726   |
|    std                  | 2.16        |
|    value_loss           | 545         |
-----------------------------------------
Eval num_timesteps=4868448, episode_reward=1106.62 +/- 1202.46
Episode length: 546.80 +/- 83.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 4868448  |
---------------------------------
Eval num_timesteps=4870440, episode_reward=2120.04 +/- 1799.76
Episode length: 670.00 +/- 25.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 4870440  |
---------------------------------
Eval num_timesteps=4872432, episode_reward=1540.46 +/- 749.17
Episode length: 569.20 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 569      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 4872432  |
---------------------------------
Eval num_timesteps=4874424, episode_reward=2008.85 +/- 1258.72
Episode length: 579.20 +/- 71.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 4874424  |
---------------------------------
Eval num_timesteps=4876416, episode_reward=1031.66 +/- 769.18
Episode length: 667.00 +/- 100.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 4876416  |
---------------------------------
Eval num_timesteps=4878408, episode_reward=1884.66 +/- 1145.37
Episode length: 560.00 +/- 94.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4878408  |
---------------------------------
Eval num_timesteps=4880400, episode_reward=2541.07 +/- 997.90
Episode length: 602.20 +/- 77.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4880400  |
---------------------------------
Eval num_timesteps=4882392, episode_reward=2182.08 +/- 1875.41
Episode length: 518.80 +/- 145.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 4882392  |
---------------------------------
Eval num_timesteps=4884384, episode_reward=1806.16 +/- 932.71
Episode length: 637.60 +/- 39.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 4884384  |
---------------------------------
Eval num_timesteps=4886376, episode_reward=1894.44 +/- 1261.29
Episode length: 603.20 +/- 42.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 4886376  |
---------------------------------
Eval num_timesteps=4888368, episode_reward=1595.04 +/- 1096.90
Episode length: 651.60 +/- 45.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 4888368  |
---------------------------------
Eval num_timesteps=4890360, episode_reward=1228.27 +/- 402.12
Episode length: 590.20 +/- 76.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 590      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 4890360  |
---------------------------------
Eval num_timesteps=4892352, episode_reward=2278.73 +/- 1218.64
Episode length: 681.20 +/- 29.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4892352  |
---------------------------------
Eval num_timesteps=4894344, episode_reward=422.58 +/- 256.71
Episode length: 525.40 +/- 137.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 4894344  |
---------------------------------
Eval num_timesteps=4896336, episode_reward=1385.72 +/- 1326.70
Episode length: 528.60 +/- 92.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 529      |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 4896336  |
---------------------------------
Eval num_timesteps=4898328, episode_reward=1356.09 +/- 1092.04
Episode length: 568.20 +/- 113.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 4898328  |
---------------------------------
Eval num_timesteps=4900320, episode_reward=2117.63 +/- 1526.41
Episode length: 602.40 +/- 102.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 602      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 4900320  |
---------------------------------
Eval num_timesteps=4902312, episode_reward=1443.75 +/- 921.62
Episode length: 578.40 +/- 54.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 578      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4902312  |
---------------------------------
Eval num_timesteps=4904304, episode_reward=1470.68 +/- 863.20
Episode length: 605.60 +/- 80.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 4904304  |
---------------------------------
Eval num_timesteps=4906296, episode_reward=2096.83 +/- 1357.46
Episode length: 567.20 +/- 76.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 4906296  |
---------------------------------
Eval num_timesteps=4908288, episode_reward=1200.99 +/- 903.98
Episode length: 584.80 +/- 87.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 4908288  |
---------------------------------
Eval num_timesteps=4910280, episode_reward=1651.22 +/- 1411.02
Episode length: 625.80 +/- 26.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4910280  |
---------------------------------
Eval num_timesteps=4912272, episode_reward=1612.22 +/- 1255.69
Episode length: 648.20 +/- 68.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 648      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 4912272  |
---------------------------------
Eval num_timesteps=4914264, episode_reward=1286.74 +/- 1079.24
Episode length: 547.00 +/- 100.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 547      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 4914264  |
---------------------------------
Eval num_timesteps=4916256, episode_reward=1946.47 +/- 1355.28
Episode length: 589.60 +/- 70.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 590          |
|    mean_reward          | 1.95e+03     |
| time/                   |              |
|    total_timesteps      | 4916256      |
| train/                  |              |
|    approx_kl            | 0.0024901729 |
|    clip_fraction        | 0.00524      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.77        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 282          |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 2.19         |
|    value_loss           | 638          |
------------------------------------------
Eval num_timesteps=4918248, episode_reward=1650.86 +/- 1160.26
Episode length: 567.00 +/- 71.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 4918248  |
---------------------------------
Eval num_timesteps=4920240, episode_reward=1379.79 +/- 1067.97
Episode length: 670.80 +/- 177.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 4920240  |
---------------------------------
Eval num_timesteps=4922232, episode_reward=1144.32 +/- 1188.50
Episode length: 595.40 +/- 123.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4922232  |
---------------------------------
Eval num_timesteps=4924224, episode_reward=1515.13 +/- 1257.71
Episode length: 674.60 +/- 230.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 4924224  |
---------------------------------
Eval num_timesteps=4926216, episode_reward=921.47 +/- 804.22
Episode length: 618.80 +/- 77.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 4926216  |
---------------------------------
Eval num_timesteps=4928208, episode_reward=1759.96 +/- 1450.45
Episode length: 546.00 +/- 76.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 546      |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 4928208  |
---------------------------------
Eval num_timesteps=4930200, episode_reward=1618.81 +/- 995.00
Episode length: 541.00 +/- 73.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4930200  |
---------------------------------
Eval num_timesteps=4932192, episode_reward=1721.89 +/- 258.61
Episode length: 711.40 +/- 141.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 4932192  |
---------------------------------
Eval num_timesteps=4934184, episode_reward=2281.24 +/- 1594.08
Episode length: 622.20 +/- 44.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4934184  |
---------------------------------
Eval num_timesteps=4936176, episode_reward=1914.00 +/- 1636.78
Episode length: 599.00 +/- 87.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 4936176  |
---------------------------------
Eval num_timesteps=4938168, episode_reward=2242.88 +/- 1797.62
Episode length: 574.80 +/- 211.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 4938168  |
---------------------------------
Eval num_timesteps=4940160, episode_reward=2040.54 +/- 790.07
Episode length: 636.20 +/- 127.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4940160  |
---------------------------------
Eval num_timesteps=4942152, episode_reward=2119.46 +/- 1714.21
Episode length: 561.00 +/- 130.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 561      |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 4942152  |
---------------------------------
Eval num_timesteps=4944144, episode_reward=1541.25 +/- 1114.84
Episode length: 653.40 +/- 25.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 4944144  |
---------------------------------
Eval num_timesteps=4946136, episode_reward=1459.08 +/- 1144.54
Episode length: 626.40 +/- 84.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 4946136  |
---------------------------------
Eval num_timesteps=4948128, episode_reward=1671.35 +/- 1306.81
Episode length: 662.40 +/- 79.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 4948128  |
---------------------------------
Eval num_timesteps=4950120, episode_reward=2027.59 +/- 1457.23
Episode length: 628.20 +/- 138.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 4950120  |
---------------------------------
Eval num_timesteps=4952112, episode_reward=1135.58 +/- 609.32
Episode length: 642.80 +/- 61.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 4952112  |
---------------------------------
Eval num_timesteps=4954104, episode_reward=996.48 +/- 666.76
Episode length: 607.60 +/- 52.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 4954104  |
---------------------------------
Eval num_timesteps=4956096, episode_reward=2650.62 +/- 1881.50
Episode length: 615.80 +/- 129.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4956096  |
---------------------------------
Eval num_timesteps=4958088, episode_reward=2566.24 +/- 1167.37
Episode length: 653.40 +/- 70.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 653      |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 4958088  |
---------------------------------
Eval num_timesteps=4960080, episode_reward=1271.92 +/- 1390.93
Episode length: 681.60 +/- 43.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 4960080  |
---------------------------------
Eval num_timesteps=4962072, episode_reward=1180.31 +/- 1377.38
Episode length: 583.00 +/- 131.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 583      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 4962072  |
---------------------------------
Eval num_timesteps=4964064, episode_reward=382.69 +/- 442.29
Episode length: 514.40 +/- 105.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 4964064  |
---------------------------------
Eval num_timesteps=4966056, episode_reward=750.05 +/- 688.97
Episode length: 766.80 +/- 156.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 767          |
|    mean_reward          | 750          |
| time/                   |              |
|    total_timesteps      | 4966056      |
| train/                  |              |
|    approx_kl            | 0.0027669037 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.82        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 260          |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000814    |
|    std                  | 2.21         |
|    value_loss           | 609          |
------------------------------------------
Eval num_timesteps=4968048, episode_reward=1438.00 +/- 1095.72
Episode length: 652.20 +/- 45.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 4968048  |
---------------------------------
Eval num_timesteps=4970040, episode_reward=1011.82 +/- 1153.27
Episode length: 621.00 +/- 143.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 4970040  |
---------------------------------
Eval num_timesteps=4972032, episode_reward=1619.90 +/- 1587.80
Episode length: 630.80 +/- 101.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4972032  |
---------------------------------
Eval num_timesteps=4974024, episode_reward=1690.00 +/- 801.23
Episode length: 646.80 +/- 75.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4974024  |
---------------------------------
Eval num_timesteps=4976016, episode_reward=818.88 +/- 573.97
Episode length: 687.80 +/- 79.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 4976016  |
---------------------------------
Eval num_timesteps=4978008, episode_reward=2128.70 +/- 1245.85
Episode length: 688.20 +/- 92.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 4978008  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=1409.73 +/- 836.77
Episode length: 620.20 +/- 66.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4981992, episode_reward=1944.77 +/- 1422.61
Episode length: 608.40 +/- 147.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 4981992  |
---------------------------------
Eval num_timesteps=4983984, episode_reward=1152.25 +/- 1116.42
Episode length: 699.40 +/- 74.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 4983984  |
---------------------------------
Eval num_timesteps=4985976, episode_reward=1256.72 +/- 963.78
Episode length: 656.60 +/- 68.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 4985976  |
---------------------------------
Eval num_timesteps=4987968, episode_reward=629.14 +/- 598.28
Episode length: 558.00 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 4987968  |
---------------------------------
Eval num_timesteps=4989960, episode_reward=976.05 +/- 980.82
Episode length: 600.00 +/- 113.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 4989960  |
---------------------------------
Eval num_timesteps=4991952, episode_reward=2582.86 +/- 1066.02
Episode length: 598.00 +/- 48.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4991952  |
---------------------------------
Eval num_timesteps=4993944, episode_reward=1843.84 +/- 1126.77
Episode length: 675.40 +/- 82.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 4993944  |
---------------------------------
Eval num_timesteps=4995936, episode_reward=835.71 +/- 1066.97
Episode length: 614.60 +/- 102.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 4995936  |
---------------------------------
Eval num_timesteps=4997928, episode_reward=1689.19 +/- 1676.80
Episode length: 689.60 +/- 55.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4997928  |
---------------------------------
Eval num_timesteps=4999920, episode_reward=1511.17 +/- 1235.88
Episode length: 685.40 +/- 53.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 4999920  |
---------------------------------
Eval num_timesteps=5001912, episode_reward=1265.06 +/- 1461.23
Episode length: 624.40 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 5001912  |
---------------------------------
Eval num_timesteps=5003904, episode_reward=424.22 +/- 252.73
Episode length: 644.00 +/- 69.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 5003904  |
---------------------------------
Eval num_timesteps=5005896, episode_reward=744.14 +/- 596.58
Episode length: 621.60 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 5005896  |
---------------------------------
Eval num_timesteps=5007888, episode_reward=1473.47 +/- 1301.36
Episode length: 663.20 +/- 60.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 5007888  |
---------------------------------
Eval num_timesteps=5009880, episode_reward=782.42 +/- 385.81
Episode length: 648.80 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 5009880  |
---------------------------------
Eval num_timesteps=5011872, episode_reward=1789.36 +/- 1303.29
Episode length: 649.80 +/- 129.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 5011872  |
---------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 567, in <module>
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 351, in run_full
    wandb_callback
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 301, in learn
    callback.on_training_end()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 117, in on_training_end
    self._on_training_end()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 228, in _on_training_end
    callback.on_training_end()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 117, in on_training_end
    self._on_training_end()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\integration\sb3\sb3.py", line 147, in _on_training_end
    self.save_model()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\integration\sb3\sb3.py", line 151, in save_model
    wandb.save(self.path, base_path=self.model_save_path)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\sdk\wandb_run.py", line 371, in wrapper_fn
    return func(self, *args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\sdk\wandb_run.py", line 361, in wrapper
    return func(self, *args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\sdk\wandb_run.py", line 1852, in save
    return self._save(glob_str, base_path, policy)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\wandb\sdk\wandb_run.py", line 1906, in _save
    os.symlink(abs_path, wandb_path)
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\Files\\Egyetem\\Szakdolgozat\\RL\\Sol\\model_chkpts\\save-01.03.2024_21.46.55\\wand\\model.zip' -> 'C:\\Files\\Egyetem\\Szakdolgozat\\RL\\Sol\\wandb\\run-20240103_214647-16dwofpk\\files\\model.zip'