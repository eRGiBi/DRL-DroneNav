AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO_109
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-255.43 +/- 30.29
Episode length: 218.00 +/- 55.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-276.60 +/- 5.86
Episode length: 190.20 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-246.24 +/- 55.44
Episode length: 275.00 +/- 110.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
New best mean reward!
Eval num_timesteps=7968, episode_reward=-272.41 +/- 21.37
Episode length: 170.60 +/- 30.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-266.11 +/- 14.44
Episode length: 189.60 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-276.04 +/- 14.43
Episode length: 241.20 +/- 45.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-270.30 +/- 18.87
Episode length: 219.20 +/- 29.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Eval num_timesteps=15936, episode_reward=-276.46 +/- 18.61
Episode length: 247.60 +/- 66.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 15936    |
---------------------------------
Eval num_timesteps=17928, episode_reward=-279.05 +/- 16.74
Episode length: 244.20 +/- 48.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 17928    |
---------------------------------
Eval num_timesteps=19920, episode_reward=-278.16 +/- 13.82
Episode length: 229.40 +/- 39.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 19920    |
---------------------------------
Eval num_timesteps=21912, episode_reward=-279.18 +/- 7.91
Episode length: 209.60 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 21912    |
---------------------------------
Eval num_timesteps=23904, episode_reward=-255.65 +/- 11.39
Episode length: 200.60 +/- 55.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 23904    |
---------------------------------
Eval num_timesteps=25896, episode_reward=-286.78 +/- 10.53
Episode length: 265.80 +/- 73.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 25896    |
---------------------------------
Eval num_timesteps=27888, episode_reward=-261.50 +/- 24.54
Episode length: 227.40 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 27888    |
---------------------------------
Eval num_timesteps=29880, episode_reward=-281.71 +/- 10.84
Episode length: 198.60 +/- 35.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 29880    |
---------------------------------
Eval num_timesteps=31872, episode_reward=-266.67 +/- 44.77
Episode length: 232.00 +/- 54.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 31872    |
---------------------------------
Eval num_timesteps=33864, episode_reward=-280.93 +/- 17.10
Episode length: 194.80 +/- 40.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 33864    |
---------------------------------
Eval num_timesteps=35856, episode_reward=-267.11 +/- 35.47
Episode length: 256.00 +/- 108.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 35856    |
---------------------------------
Eval num_timesteps=37848, episode_reward=-272.14 +/- 24.24
Episode length: 215.20 +/- 44.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 37848    |
---------------------------------
Eval num_timesteps=39840, episode_reward=-270.05 +/- 23.83
Episode length: 248.00 +/- 65.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 39840    |
---------------------------------
Eval num_timesteps=41832, episode_reward=-267.76 +/- 24.96
Episode length: 228.80 +/- 45.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 41832    |
---------------------------------
Eval num_timesteps=43824, episode_reward=-284.23 +/- 14.82
Episode length: 218.40 +/- 91.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 43824    |
---------------------------------
Eval num_timesteps=45816, episode_reward=-274.26 +/- 15.56
Episode length: 194.20 +/- 34.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 45816    |
---------------------------------
Eval num_timesteps=47808, episode_reward=-266.54 +/- 18.99
Episode length: 210.40 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 47808    |
---------------------------------
Eval num_timesteps=49800, episode_reward=-270.01 +/- 18.95
Episode length: 238.40 +/- 67.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 49800       |
| train/                  |             |
|    approx_kl            | 0.004047296 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.74       |
|    explained_variance   | -0.0399     |
|    learning_rate        | 0.001       |
|    loss                 | 1.01        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.02        |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=51792, episode_reward=-291.81 +/- 3.76
Episode length: 194.20 +/- 44.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 51792    |
---------------------------------
Eval num_timesteps=53784, episode_reward=-286.87 +/- 9.65
Episode length: 204.80 +/- 39.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 53784    |
---------------------------------
Eval num_timesteps=55776, episode_reward=-278.10 +/- 19.28
Episode length: 218.60 +/- 44.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 55776    |
---------------------------------
Eval num_timesteps=57768, episode_reward=-267.74 +/- 18.56
Episode length: 234.60 +/- 32.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 57768    |
---------------------------------
Eval num_timesteps=59760, episode_reward=-264.37 +/- 25.65
Episode length: 220.20 +/- 72.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 59760    |
---------------------------------
Eval num_timesteps=61752, episode_reward=-283.44 +/- 5.86
Episode length: 208.00 +/- 36.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 61752    |
---------------------------------
Eval num_timesteps=63744, episode_reward=-279.84 +/- 23.75
Episode length: 231.20 +/- 78.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 63744    |
---------------------------------
Eval num_timesteps=65736, episode_reward=-270.28 +/- 15.89
Episode length: 214.20 +/- 36.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 65736    |
---------------------------------
Eval num_timesteps=67728, episode_reward=-265.30 +/- 9.51
Episode length: 195.20 +/- 37.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 67728    |
---------------------------------
Eval num_timesteps=69720, episode_reward=-262.75 +/- 27.42
Episode length: 213.60 +/- 43.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 69720    |
---------------------------------
Eval num_timesteps=71712, episode_reward=-273.16 +/- 12.14
Episode length: 228.40 +/- 48.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 71712    |
---------------------------------
Eval num_timesteps=73704, episode_reward=-272.35 +/- 35.55
Episode length: 215.60 +/- 42.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 73704    |
---------------------------------
Eval num_timesteps=75696, episode_reward=-286.05 +/- 9.08
Episode length: 194.60 +/- 27.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 75696    |
---------------------------------
Eval num_timesteps=77688, episode_reward=-261.36 +/- 20.01
Episode length: 211.60 +/- 38.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 77688    |
---------------------------------
Eval num_timesteps=79680, episode_reward=-285.01 +/- 13.61
Episode length: 232.00 +/- 58.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 79680    |
---------------------------------
Eval num_timesteps=81672, episode_reward=-278.22 +/- 9.15
Episode length: 233.20 +/- 63.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 81672    |
---------------------------------
Eval num_timesteps=83664, episode_reward=-252.03 +/- 34.32
Episode length: 248.00 +/- 52.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 83664    |
---------------------------------
Eval num_timesteps=85656, episode_reward=-260.78 +/- 16.51
Episode length: 190.60 +/- 26.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 85656    |
---------------------------------
Eval num_timesteps=87648, episode_reward=-268.35 +/- 27.59
Episode length: 224.20 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 87648    |
---------------------------------
Eval num_timesteps=89640, episode_reward=-270.48 +/- 21.74
Episode length: 205.60 +/- 42.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 89640    |
---------------------------------
Eval num_timesteps=91632, episode_reward=-278.53 +/- 14.22
Episode length: 188.40 +/- 13.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 91632    |
---------------------------------
Eval num_timesteps=93624, episode_reward=-265.68 +/- 36.19
Episode length: 205.60 +/- 52.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 93624    |
---------------------------------
Eval num_timesteps=95616, episode_reward=-267.63 +/- 11.95
Episode length: 193.80 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 95616    |
---------------------------------
Eval num_timesteps=97608, episode_reward=-283.15 +/- 17.34
Episode length: 240.00 +/- 39.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 97608    |
---------------------------------
Eval num_timesteps=99600, episode_reward=-286.05 +/- 6.10
Episode length: 260.60 +/- 48.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 99600        |
| train/                  |              |
|    approx_kl            | 0.0053272564 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.81        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.001        |
|    loss                 | 0.201        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00348     |
|    std                  | 1.04         |
|    value_loss           | 0.63         |
------------------------------------------
Eval num_timesteps=101592, episode_reward=-275.82 +/- 11.20
Episode length: 242.40 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 101592   |
---------------------------------
Eval num_timesteps=103584, episode_reward=-275.91 +/- 23.19
Episode length: 186.60 +/- 32.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 103584   |
---------------------------------
Eval num_timesteps=105576, episode_reward=-284.77 +/- 11.91
Episode length: 191.80 +/- 40.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 105576   |
---------------------------------
Eval num_timesteps=107568, episode_reward=-265.16 +/- 26.87
Episode length: 217.20 +/- 56.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 107568   |
---------------------------------
Eval num_timesteps=109560, episode_reward=-277.50 +/- 12.57
Episode length: 248.80 +/- 73.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 109560   |
---------------------------------
Eval num_timesteps=111552, episode_reward=-244.59 +/- 19.37
Episode length: 241.40 +/- 49.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 111552   |
---------------------------------
New best mean reward!
Eval num_timesteps=113544, episode_reward=-259.40 +/- 11.22
Episode length: 229.20 +/- 57.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 113544   |
---------------------------------
Eval num_timesteps=115536, episode_reward=-268.12 +/- 14.74
Episode length: 217.60 +/- 36.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 115536   |
---------------------------------
Eval num_timesteps=117528, episode_reward=-252.13 +/- 49.52
Episode length: 238.20 +/- 74.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 117528   |
---------------------------------
Eval num_timesteps=119520, episode_reward=-276.28 +/- 20.04
Episode length: 227.80 +/- 80.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 119520   |
---------------------------------
Eval num_timesteps=121512, episode_reward=-288.11 +/- 13.88
Episode length: 233.80 +/- 33.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 121512   |
---------------------------------
Eval num_timesteps=123504, episode_reward=-292.07 +/- 8.63
Episode length: 255.40 +/- 71.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 123504   |
---------------------------------
Eval num_timesteps=125496, episode_reward=-279.07 +/- 22.19
Episode length: 260.80 +/- 48.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 125496   |
---------------------------------
Eval num_timesteps=127488, episode_reward=-283.81 +/- 15.63
Episode length: 251.00 +/- 55.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 127488   |
---------------------------------
Eval num_timesteps=129480, episode_reward=-240.70 +/- 34.45
Episode length: 257.20 +/- 67.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 129480   |
---------------------------------
New best mean reward!
Eval num_timesteps=131472, episode_reward=-270.84 +/- 15.93
Episode length: 197.60 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 131472   |
---------------------------------
Eval num_timesteps=133464, episode_reward=-282.36 +/- 12.65
Episode length: 229.20 +/- 44.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 133464   |
---------------------------------
Eval num_timesteps=135456, episode_reward=-273.00 +/- 21.45
Episode length: 270.60 +/- 106.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 135456   |
---------------------------------
Eval num_timesteps=137448, episode_reward=-281.35 +/- 10.14
Episode length: 204.20 +/- 24.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 137448   |
---------------------------------
Eval num_timesteps=139440, episode_reward=-280.98 +/- 8.22
Episode length: 231.20 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 139440   |
---------------------------------
Eval num_timesteps=141432, episode_reward=-263.71 +/- 21.06
Episode length: 230.20 +/- 67.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 141432   |
---------------------------------
Eval num_timesteps=143424, episode_reward=-274.38 +/- 14.72
Episode length: 211.40 +/- 27.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 143424   |
---------------------------------
Eval num_timesteps=145416, episode_reward=-269.23 +/- 21.12
Episode length: 241.80 +/- 77.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 145416   |
---------------------------------
Eval num_timesteps=147408, episode_reward=-272.37 +/- 14.80
Episode length: 223.20 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 147408   |
---------------------------------
Eval num_timesteps=149400, episode_reward=-273.21 +/- 33.43
Episode length: 263.60 +/- 76.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 149400      |
| train/                  |             |
|    approx_kl            | 0.005138514 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.85       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 0.106       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00299    |
|    std                  | 1.05        |
|    value_loss           | 0.407       |
-----------------------------------------
Eval num_timesteps=151392, episode_reward=-246.72 +/- 27.65
Episode length: 281.20 +/- 75.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 151392   |
---------------------------------
Eval num_timesteps=153384, episode_reward=-262.36 +/- 40.35
Episode length: 234.20 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 153384   |
---------------------------------
Eval num_timesteps=155376, episode_reward=-284.57 +/- 6.72
Episode length: 273.00 +/- 88.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 155376   |
---------------------------------
Eval num_timesteps=157368, episode_reward=-284.32 +/- 9.52
Episode length: 240.60 +/- 34.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 157368   |
---------------------------------
Eval num_timesteps=159360, episode_reward=-284.62 +/- 8.31
Episode length: 223.40 +/- 45.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 159360   |
---------------------------------
Eval num_timesteps=161352, episode_reward=-250.63 +/- 45.86
Episode length: 230.20 +/- 77.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 161352   |
---------------------------------
Eval num_timesteps=163344, episode_reward=-278.63 +/- 26.52
Episode length: 219.60 +/- 60.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 163344   |
---------------------------------
Eval num_timesteps=165336, episode_reward=-259.92 +/- 53.60
Episode length: 236.80 +/- 80.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 165336   |
---------------------------------
Eval num_timesteps=167328, episode_reward=-235.87 +/- 48.63
Episode length: 262.40 +/- 46.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 167328   |
---------------------------------
New best mean reward!
Eval num_timesteps=169320, episode_reward=-238.17 +/- 87.68
Episode length: 341.80 +/- 57.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 169320   |
---------------------------------
Eval num_timesteps=171312, episode_reward=-280.24 +/- 15.70
Episode length: 218.40 +/- 33.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 171312   |
---------------------------------
Eval num_timesteps=173304, episode_reward=-295.39 +/- 8.26
Episode length: 238.20 +/- 52.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 173304   |
---------------------------------
Eval num_timesteps=175296, episode_reward=-255.60 +/- 15.67
Episode length: 318.40 +/- 49.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 175296   |
---------------------------------
Eval num_timesteps=177288, episode_reward=-273.91 +/- 19.85
Episode length: 271.20 +/- 54.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 177288   |
---------------------------------
Eval num_timesteps=179280, episode_reward=-273.78 +/- 20.71
Episode length: 253.20 +/- 46.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 179280   |
---------------------------------
Eval num_timesteps=181272, episode_reward=-286.16 +/- 4.76
Episode length: 233.80 +/- 53.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 181272   |
---------------------------------
Eval num_timesteps=183264, episode_reward=-286.47 +/- 8.58
Episode length: 210.40 +/- 37.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 183264   |
---------------------------------
Eval num_timesteps=185256, episode_reward=-275.27 +/- 2.29
Episode length: 233.20 +/- 58.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 185256   |
---------------------------------
Eval num_timesteps=187248, episode_reward=-263.32 +/- 25.30
Episode length: 255.40 +/- 47.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 187248   |
---------------------------------
Eval num_timesteps=189240, episode_reward=-268.99 +/- 40.90
Episode length: 280.20 +/- 81.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 189240   |
---------------------------------
Eval num_timesteps=191232, episode_reward=-261.01 +/- 30.61
Episode length: 270.00 +/- 65.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 191232   |
---------------------------------
Eval num_timesteps=193224, episode_reward=-262.60 +/- 52.03
Episode length: 284.20 +/- 74.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 193224   |
---------------------------------
Eval num_timesteps=195216, episode_reward=-265.07 +/- 11.99
Episode length: 244.40 +/- 49.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 195216   |
---------------------------------
Eval num_timesteps=197208, episode_reward=-292.03 +/- 5.05
Episode length: 236.40 +/- 37.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | -292         |
| time/                   |              |
|    total_timesteps      | 197208       |
| train/                  |              |
|    approx_kl            | 0.0066085546 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0544       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00443     |
|    std                  | 1.06         |
|    value_loss           | 0.263        |
------------------------------------------
Eval num_timesteps=199200, episode_reward=-281.36 +/- 13.59
Episode length: 306.60 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
Eval num_timesteps=201192, episode_reward=-276.73 +/- 25.87
Episode length: 292.40 +/- 52.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 201192   |
---------------------------------
Eval num_timesteps=203184, episode_reward=-247.65 +/- 44.65
Episode length: 318.00 +/- 116.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 203184   |
---------------------------------
Eval num_timesteps=205176, episode_reward=-277.52 +/- 28.35
Episode length: 294.20 +/- 26.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 205176   |
---------------------------------
Eval num_timesteps=207168, episode_reward=-276.03 +/- 22.66
Episode length: 279.20 +/- 48.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 207168   |
---------------------------------
Eval num_timesteps=209160, episode_reward=-257.91 +/- 36.38
Episode length: 239.20 +/- 58.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 209160   |
---------------------------------
Eval num_timesteps=211152, episode_reward=-271.06 +/- 29.56
Episode length: 264.20 +/- 59.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 211152   |
---------------------------------
Eval num_timesteps=213144, episode_reward=-266.48 +/- 46.34
Episode length: 327.20 +/- 108.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 327      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 213144   |
---------------------------------
Eval num_timesteps=215136, episode_reward=-257.15 +/- 19.99
Episode length: 289.40 +/- 69.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 215136   |
---------------------------------
Eval num_timesteps=217128, episode_reward=-248.79 +/- 30.34
Episode length: 272.60 +/- 56.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 217128   |
---------------------------------
Eval num_timesteps=219120, episode_reward=-264.82 +/- 7.41
Episode length: 255.00 +/- 49.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 219120   |
---------------------------------
Eval num_timesteps=221112, episode_reward=-274.13 +/- 21.68
Episode length: 263.60 +/- 93.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 221112   |
---------------------------------
Eval num_timesteps=223104, episode_reward=-272.64 +/- 25.13
Episode length: 284.60 +/- 54.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 223104   |
---------------------------------
Eval num_timesteps=225096, episode_reward=-273.16 +/- 12.29
Episode length: 247.00 +/- 63.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 225096   |
---------------------------------
Eval num_timesteps=227088, episode_reward=-259.79 +/- 32.81
Episode length: 271.20 +/- 80.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 227088   |
---------------------------------
Eval num_timesteps=229080, episode_reward=-274.63 +/- 18.76
Episode length: 297.60 +/- 36.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 229080   |
---------------------------------
Eval num_timesteps=231072, episode_reward=-261.03 +/- 29.76
Episode length: 232.80 +/- 35.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 231072   |
---------------------------------
Eval num_timesteps=233064, episode_reward=-281.44 +/- 17.72
Episode length: 257.80 +/- 83.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 233064   |
---------------------------------
Eval num_timesteps=235056, episode_reward=-266.07 +/- 18.33
Episode length: 373.20 +/- 59.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 373      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 235056   |
---------------------------------
Eval num_timesteps=237048, episode_reward=-283.67 +/- 12.16
Episode length: 273.80 +/- 103.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 237048   |
---------------------------------
Eval num_timesteps=239040, episode_reward=-265.94 +/- 24.84
Episode length: 272.80 +/- 81.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 239040   |
---------------------------------
Eval num_timesteps=241032, episode_reward=-270.79 +/- 18.02
Episode length: 246.40 +/- 104.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 241032   |
---------------------------------
Eval num_timesteps=243024, episode_reward=-272.25 +/- 29.12
Episode length: 263.60 +/- 19.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 243024   |
---------------------------------
Eval num_timesteps=245016, episode_reward=-280.82 +/- 19.24
Episode length: 229.40 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 245016   |
---------------------------------
Eval num_timesteps=247008, episode_reward=-252.95 +/- 43.60
Episode length: 457.00 +/- 218.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 457         |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 247008      |
| train/                  |             |
|    approx_kl            | 0.008583308 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0475      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00693    |
|    std                  | 1.06        |
|    value_loss           | 0.259       |
-----------------------------------------
Eval num_timesteps=249000, episode_reward=-221.78 +/- 73.21
Episode length: 407.60 +/- 55.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
New best mean reward!
Eval num_timesteps=250992, episode_reward=-282.06 +/- 44.47
Episode length: 503.40 +/- 150.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 503      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 250992   |
---------------------------------
Eval num_timesteps=252984, episode_reward=-284.63 +/- 37.69
Episode length: 371.00 +/- 75.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 252984   |
---------------------------------
Eval num_timesteps=254976, episode_reward=-268.04 +/- 48.82
Episode length: 395.40 +/- 92.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=256968, episode_reward=-248.22 +/- 118.77
Episode length: 424.20 +/- 117.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 424      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 256968   |
---------------------------------
Eval num_timesteps=258960, episode_reward=-232.66 +/- 37.31
Episode length: 472.40 +/- 244.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 258960   |
---------------------------------
Eval num_timesteps=260952, episode_reward=-268.76 +/- 20.78
Episode length: 293.20 +/- 105.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 260952   |
---------------------------------
Eval num_timesteps=262944, episode_reward=-285.84 +/- 56.76
Episode length: 485.80 +/- 107.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 262944   |
---------------------------------
Eval num_timesteps=264936, episode_reward=-270.99 +/- 34.80
Episode length: 321.80 +/- 75.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 264936   |
---------------------------------
Eval num_timesteps=266928, episode_reward=-211.64 +/- 72.23
Episode length: 431.00 +/- 88.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 266928   |
---------------------------------
New best mean reward!
Eval num_timesteps=268920, episode_reward=-262.74 +/- 32.48
Episode length: 392.40 +/- 139.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 268920   |
---------------------------------
Eval num_timesteps=270912, episode_reward=-253.92 +/- 47.29
Episode length: 411.20 +/- 99.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 270912   |
---------------------------------
Eval num_timesteps=272904, episode_reward=-263.33 +/- 36.25
Episode length: 388.60 +/- 101.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 272904   |
---------------------------------
Eval num_timesteps=274896, episode_reward=-263.48 +/- 12.37
Episode length: 343.20 +/- 53.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 274896   |
---------------------------------
Eval num_timesteps=276888, episode_reward=-289.83 +/- 27.23
Episode length: 422.00 +/- 59.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 276888   |
---------------------------------
Eval num_timesteps=278880, episode_reward=-253.87 +/- 73.73
Episode length: 369.20 +/- 114.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 278880   |
---------------------------------
Eval num_timesteps=280872, episode_reward=-283.23 +/- 42.51
Episode length: 444.00 +/- 119.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 280872   |
---------------------------------
Eval num_timesteps=282864, episode_reward=-258.58 +/- 26.29
Episode length: 413.60 +/- 130.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 282864   |
---------------------------------
Eval num_timesteps=284856, episode_reward=-214.24 +/- 50.50
Episode length: 339.20 +/- 87.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 284856   |
---------------------------------
Eval num_timesteps=286848, episode_reward=-233.35 +/- 30.55
Episode length: 388.40 +/- 159.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 286848   |
---------------------------------
Eval num_timesteps=288840, episode_reward=-259.01 +/- 17.81
Episode length: 422.40 +/- 167.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 288840   |
---------------------------------
Eval num_timesteps=290832, episode_reward=-252.92 +/- 57.08
Episode length: 424.80 +/- 194.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 290832   |
---------------------------------
Eval num_timesteps=292824, episode_reward=-256.77 +/- 34.81
Episode length: 365.80 +/- 71.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 366      |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 292824   |
---------------------------------
Eval num_timesteps=294816, episode_reward=-247.37 +/- 30.31
Episode length: 369.40 +/- 118.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 294816   |
---------------------------------
Eval num_timesteps=296808, episode_reward=-215.32 +/- 117.44
Episode length: 568.20 +/- 203.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 568         |
|    mean_reward          | -215        |
| time/                   |             |
|    total_timesteps      | 296808      |
| train/                  |             |
|    approx_kl            | 0.009690188 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00436     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00824    |
|    std                  | 1.07        |
|    value_loss           | 0.235       |
-----------------------------------------
Eval num_timesteps=298800, episode_reward=-87.84 +/- 146.78
Episode length: 541.80 +/- 143.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 542      |
|    mean_reward     | -87.8    |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
New best mean reward!
Eval num_timesteps=300792, episode_reward=-234.15 +/- 52.49
Episode length: 437.20 +/- 87.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 437      |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 300792   |
---------------------------------
Eval num_timesteps=302784, episode_reward=-209.50 +/- 88.56
Episode length: 475.20 +/- 96.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 475      |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 302784   |
---------------------------------
Eval num_timesteps=304776, episode_reward=-120.66 +/- 137.69
Episode length: 602.80 +/- 110.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 304776   |
---------------------------------
Eval num_timesteps=306768, episode_reward=-217.63 +/- 106.47
Episode length: 592.40 +/- 160.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 306768   |
---------------------------------
Eval num_timesteps=308760, episode_reward=-213.85 +/- 148.35
Episode length: 664.60 +/- 100.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 308760   |
---------------------------------
Eval num_timesteps=310752, episode_reward=-182.69 +/- 86.32
Episode length: 497.00 +/- 103.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 310752   |
---------------------------------
Eval num_timesteps=312744, episode_reward=-232.00 +/- 44.24
Episode length: 605.60 +/- 235.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 606      |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 312744   |
---------------------------------
Eval num_timesteps=314736, episode_reward=-251.78 +/- 32.08
Episode length: 593.80 +/- 275.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 594      |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 314736   |
---------------------------------
Eval num_timesteps=316728, episode_reward=-177.80 +/- 126.81
Episode length: 550.00 +/- 127.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 550      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 316728   |
---------------------------------
Eval num_timesteps=318720, episode_reward=-172.75 +/- 66.81
Episode length: 582.40 +/- 161.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 318720   |
---------------------------------
Eval num_timesteps=320712, episode_reward=-93.80 +/- 158.86
Episode length: 562.60 +/- 60.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | -93.8    |
| time/              |          |
|    total_timesteps | 320712   |
---------------------------------
Eval num_timesteps=322704, episode_reward=-175.08 +/- 107.26
Episode length: 652.00 +/- 210.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 652      |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 322704   |
---------------------------------
Eval num_timesteps=324696, episode_reward=-188.50 +/- 10.32
Episode length: 495.20 +/- 209.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 324696   |
---------------------------------
Eval num_timesteps=326688, episode_reward=-204.28 +/- 75.37
Episode length: 507.40 +/- 99.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 326688   |
---------------------------------
Eval num_timesteps=328680, episode_reward=-202.42 +/- 116.14
Episode length: 676.80 +/- 131.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 328680   |
---------------------------------
Eval num_timesteps=330672, episode_reward=-226.82 +/- 33.24
Episode length: 451.60 +/- 110.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 330672   |
---------------------------------
Eval num_timesteps=332664, episode_reward=-115.47 +/- 201.10
Episode length: 541.00 +/- 153.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 332664   |
---------------------------------
Eval num_timesteps=334656, episode_reward=-175.79 +/- 110.75
Episode length: 588.00 +/- 120.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 334656   |
---------------------------------
Eval num_timesteps=336648, episode_reward=-191.36 +/- 98.76
Episode length: 461.80 +/- 107.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 462      |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 336648   |
---------------------------------
Eval num_timesteps=338640, episode_reward=-175.48 +/- 155.27
Episode length: 539.20 +/- 135.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 539      |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 338640   |
---------------------------------
Eval num_timesteps=340632, episode_reward=-193.77 +/- 73.10
Episode length: 558.20 +/- 216.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 340632   |
---------------------------------
Eval num_timesteps=342624, episode_reward=-113.99 +/- 121.06
Episode length: 563.80 +/- 107.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 342624   |
---------------------------------
Eval num_timesteps=344616, episode_reward=64.20 +/- 116.01
Episode length: 599.40 +/- 65.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 599          |
|    mean_reward          | 64.2         |
| time/                   |              |
|    total_timesteps      | 344616       |
| train/                  |              |
|    approx_kl            | 0.0096914945 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0219      |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00689     |
|    std                  | 1.07         |
|    value_loss           | 0.182        |
------------------------------------------
New best mean reward!
Eval num_timesteps=346608, episode_reward=108.02 +/- 175.79
Episode length: 608.80 +/- 144.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 346608   |
---------------------------------
New best mean reward!
Eval num_timesteps=348600, episode_reward=128.88 +/- 116.66
Episode length: 697.40 +/- 76.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 348600   |
---------------------------------
New best mean reward!
Eval num_timesteps=350592, episode_reward=-71.91 +/- 93.22
Episode length: 639.40 +/- 251.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | -71.9    |
| time/              |          |
|    total_timesteps | 350592   |
---------------------------------
Eval num_timesteps=352584, episode_reward=147.92 +/- 204.74
Episode length: 747.00 +/- 194.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 352584   |
---------------------------------
New best mean reward!
Eval num_timesteps=354576, episode_reward=41.67 +/- 78.65
Episode length: 560.00 +/- 84.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 560      |
|    mean_reward     | 41.7     |
| time/              |          |
|    total_timesteps | 354576   |
---------------------------------
Eval num_timesteps=356568, episode_reward=-111.03 +/- 227.23
Episode length: 616.00 +/- 137.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 356568   |
---------------------------------
Eval num_timesteps=358560, episode_reward=-0.91 +/- 129.86
Episode length: 601.20 +/- 134.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | -0.908   |
| time/              |          |
|    total_timesteps | 358560   |
---------------------------------
Eval num_timesteps=360552, episode_reward=-44.66 +/- 182.87
Episode length: 619.80 +/- 68.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | -44.7    |
| time/              |          |
|    total_timesteps | 360552   |
---------------------------------
Eval num_timesteps=362544, episode_reward=69.04 +/- 153.75
Episode length: 682.00 +/- 178.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | 69       |
| time/              |          |
|    total_timesteps | 362544   |
---------------------------------
Eval num_timesteps=364536, episode_reward=28.31 +/- 221.75
Episode length: 758.60 +/- 263.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | 28.3     |
| time/              |          |
|    total_timesteps | 364536   |
---------------------------------
Eval num_timesteps=366528, episode_reward=-18.63 +/- 161.24
Episode length: 669.00 +/- 267.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | -18.6    |
| time/              |          |
|    total_timesteps | 366528   |
---------------------------------
Eval num_timesteps=368520, episode_reward=-17.25 +/- 151.70
Episode length: 648.60 +/- 87.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | -17.2    |
| time/              |          |
|    total_timesteps | 368520   |
---------------------------------
Eval num_timesteps=370512, episode_reward=-35.19 +/- 110.68
Episode length: 688.40 +/- 98.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | -35.2    |
| time/              |          |
|    total_timesteps | 370512   |
---------------------------------
Eval num_timesteps=372504, episode_reward=181.56 +/- 150.24
Episode length: 650.00 +/- 78.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 372504   |
---------------------------------
New best mean reward!
Eval num_timesteps=374496, episode_reward=32.65 +/- 32.55
Episode length: 651.00 +/- 170.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 32.7     |
| time/              |          |
|    total_timesteps | 374496   |
---------------------------------
Eval num_timesteps=376488, episode_reward=44.08 +/- 186.39
Episode length: 651.00 +/- 85.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 44.1     |
| time/              |          |
|    total_timesteps | 376488   |
---------------------------------
Eval num_timesteps=378480, episode_reward=182.86 +/- 290.91
Episode length: 818.00 +/- 156.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 378480   |
---------------------------------
New best mean reward!
Eval num_timesteps=380472, episode_reward=150.72 +/- 103.42
Episode length: 615.40 +/- 64.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 380472   |
---------------------------------
Eval num_timesteps=382464, episode_reward=39.44 +/- 187.80
Episode length: 639.00 +/- 127.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 39.4     |
| time/              |          |
|    total_timesteps | 382464   |
---------------------------------
Eval num_timesteps=384456, episode_reward=140.99 +/- 153.34
Episode length: 598.80 +/- 63.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | 141      |
| time/              |          |
|    total_timesteps | 384456   |
---------------------------------
Eval num_timesteps=386448, episode_reward=-23.83 +/- 213.06
Episode length: 642.20 +/- 145.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 386448   |
---------------------------------
Eval num_timesteps=388440, episode_reward=61.01 +/- 35.74
Episode length: 603.60 +/- 62.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | 61       |
| time/              |          |
|    total_timesteps | 388440   |
---------------------------------
Eval num_timesteps=390432, episode_reward=139.96 +/- 184.09
Episode length: 769.40 +/- 146.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 769      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 390432   |
---------------------------------
Eval num_timesteps=392424, episode_reward=-95.27 +/- 115.85
Episode length: 483.60 +/- 160.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | -95.3    |
| time/              |          |
|    total_timesteps | 392424   |
---------------------------------
Eval num_timesteps=394416, episode_reward=215.08 +/- 176.09
Episode length: 748.00 +/- 126.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 748         |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 394416      |
| train/                  |             |
|    approx_kl            | 0.008500041 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0123     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0045     |
|    std                  | 1.07        |
|    value_loss           | 0.184       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=396408, episode_reward=285.61 +/- 307.86
Episode length: 807.20 +/- 121.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | 286      |
| time/              |          |
|    total_timesteps | 396408   |
---------------------------------
New best mean reward!
Eval num_timesteps=398400, episode_reward=277.62 +/- 118.33
Episode length: 628.20 +/- 54.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 628      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=400392, episode_reward=218.52 +/- 175.52
Episode length: 697.00 +/- 102.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 400392   |
---------------------------------
Eval num_timesteps=402384, episode_reward=61.36 +/- 127.91
Episode length: 662.00 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | 61.4     |
| time/              |          |
|    total_timesteps | 402384   |
---------------------------------
Eval num_timesteps=404376, episode_reward=189.59 +/- 144.99
Episode length: 637.60 +/- 55.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 404376   |
---------------------------------
Eval num_timesteps=406368, episode_reward=-11.38 +/- 179.60
Episode length: 739.40 +/- 132.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | -11.4    |
| time/              |          |
|    total_timesteps | 406368   |
---------------------------------
Eval num_timesteps=408360, episode_reward=-6.45 +/- 196.05
Episode length: 693.00 +/- 138.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | -6.45    |
| time/              |          |
|    total_timesteps | 408360   |
---------------------------------
Eval num_timesteps=410352, episode_reward=35.27 +/- 164.24
Episode length: 646.80 +/- 86.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 647      |
|    mean_reward     | 35.3     |
| time/              |          |
|    total_timesteps | 410352   |
---------------------------------
Eval num_timesteps=412344, episode_reward=24.13 +/- 179.97
Episode length: 644.60 +/- 101.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 412344   |
---------------------------------
Eval num_timesteps=414336, episode_reward=252.44 +/- 229.06
Episode length: 692.40 +/- 87.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 414336   |
---------------------------------
Eval num_timesteps=416328, episode_reward=122.83 +/- 239.00
Episode length: 728.00 +/- 59.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | 123      |
| time/              |          |
|    total_timesteps | 416328   |
---------------------------------
Eval num_timesteps=418320, episode_reward=239.97 +/- 255.46
Episode length: 693.20 +/- 63.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 418320   |
---------------------------------
Eval num_timesteps=420312, episode_reward=236.05 +/- 199.56
Episode length: 678.80 +/- 74.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 420312   |
---------------------------------
Eval num_timesteps=422304, episode_reward=-42.93 +/- 162.93
Episode length: 638.60 +/- 113.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | -42.9    |
| time/              |          |
|    total_timesteps | 422304   |
---------------------------------
Eval num_timesteps=424296, episode_reward=162.34 +/- 123.71
Episode length: 670.40 +/- 71.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 424296   |
---------------------------------
Eval num_timesteps=426288, episode_reward=145.61 +/- 128.15
Episode length: 676.00 +/- 66.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 146      |
| time/              |          |
|    total_timesteps | 426288   |
---------------------------------
Eval num_timesteps=428280, episode_reward=-16.50 +/- 110.06
Episode length: 661.40 +/- 112.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | -16.5    |
| time/              |          |
|    total_timesteps | 428280   |
---------------------------------
Eval num_timesteps=430272, episode_reward=136.09 +/- 192.56
Episode length: 649.60 +/- 58.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 650      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 430272   |
---------------------------------
Eval num_timesteps=432264, episode_reward=42.25 +/- 169.49
Episode length: 667.60 +/- 81.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | 42.3     |
| time/              |          |
|    total_timesteps | 432264   |
---------------------------------
Eval num_timesteps=434256, episode_reward=343.72 +/- 172.93
Episode length: 779.60 +/- 94.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 434256   |
---------------------------------
New best mean reward!
Eval num_timesteps=436248, episode_reward=-107.78 +/- 129.86
Episode length: 662.40 +/- 53.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 436248   |
---------------------------------
Eval num_timesteps=438240, episode_reward=40.29 +/- 176.73
Episode length: 595.40 +/- 51.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 595      |
|    mean_reward     | 40.3     |
| time/              |          |
|    total_timesteps | 438240   |
---------------------------------
Eval num_timesteps=440232, episode_reward=-8.73 +/- 280.65
Episode length: 681.00 +/- 60.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 681      |
|    mean_reward     | -8.73    |
| time/              |          |
|    total_timesteps | 440232   |
---------------------------------
Eval num_timesteps=442224, episode_reward=78.82 +/- 270.20
Episode length: 569.60 +/- 81.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | 78.8     |
| time/              |          |
|    total_timesteps | 442224   |
---------------------------------
Eval num_timesteps=444216, episode_reward=176.91 +/- 294.90
Episode length: 618.80 +/- 95.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 619         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 444216      |
| train/                  |             |
|    approx_kl            | 0.007814663 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00723     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00396    |
|    std                  | 1.08        |
|    value_loss           | 0.187       |
-----------------------------------------
Eval num_timesteps=446208, episode_reward=245.76 +/- 115.83
Episode length: 639.80 +/- 64.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 446208   |
---------------------------------
Eval num_timesteps=448200, episode_reward=194.34 +/- 172.89
Episode length: 638.60 +/- 54.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 448200   |
---------------------------------
Eval num_timesteps=450192, episode_reward=176.49 +/- 230.81
Episode length: 648.80 +/- 51.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 450192   |
---------------------------------
Eval num_timesteps=452184, episode_reward=-39.15 +/- 118.93
Episode length: 555.20 +/- 61.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | -39.2    |
| time/              |          |
|    total_timesteps | 452184   |
---------------------------------
Eval num_timesteps=454176, episode_reward=160.08 +/- 68.35
Episode length: 615.80 +/- 80.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 454176   |
---------------------------------
Eval num_timesteps=456168, episode_reward=182.56 +/- 228.35
Episode length: 662.80 +/- 74.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 456168   |
---------------------------------
Eval num_timesteps=458160, episode_reward=42.07 +/- 270.21
Episode length: 712.00 +/- 105.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 42.1     |
| time/              |          |
|    total_timesteps | 458160   |
---------------------------------
Eval num_timesteps=460152, episode_reward=167.27 +/- 110.85
Episode length: 611.40 +/- 35.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 460152   |
---------------------------------
Eval num_timesteps=462144, episode_reward=105.35 +/- 198.45
Episode length: 613.40 +/- 69.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 462144   |
---------------------------------
Eval num_timesteps=464136, episode_reward=82.83 +/- 181.49
Episode length: 586.40 +/- 103.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 82.8     |
| time/              |          |
|    total_timesteps | 464136   |
---------------------------------
Eval num_timesteps=466128, episode_reward=19.46 +/- 208.05
Episode length: 675.00 +/- 87.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 466128   |
---------------------------------
Eval num_timesteps=468120, episode_reward=184.48 +/- 214.63
Episode length: 630.40 +/- 73.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 468120   |
---------------------------------
Eval num_timesteps=470112, episode_reward=203.81 +/- 149.37
Episode length: 650.60 +/- 64.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 470112   |
---------------------------------
Eval num_timesteps=472104, episode_reward=66.58 +/- 139.08
Episode length: 608.00 +/- 104.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | 66.6     |
| time/              |          |
|    total_timesteps | 472104   |
---------------------------------
Eval num_timesteps=474096, episode_reward=-16.16 +/- 136.73
Episode length: 614.20 +/- 53.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | -16.2    |
| time/              |          |
|    total_timesteps | 474096   |
---------------------------------
Eval num_timesteps=476088, episode_reward=90.63 +/- 186.30
Episode length: 644.00 +/- 45.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | 90.6     |
| time/              |          |
|    total_timesteps | 476088   |
---------------------------------
Eval num_timesteps=478080, episode_reward=-19.34 +/- 194.82
Episode length: 668.00 +/- 73.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | -19.3    |
| time/              |          |
|    total_timesteps | 478080   |
---------------------------------
Eval num_timesteps=480072, episode_reward=92.14 +/- 226.86
Episode length: 625.00 +/- 54.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | 92.1     |
| time/              |          |
|    total_timesteps | 480072   |
---------------------------------
Eval num_timesteps=482064, episode_reward=111.19 +/- 144.04
Episode length: 637.60 +/- 93.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 482064   |
---------------------------------
Eval num_timesteps=484056, episode_reward=192.03 +/- 117.78
Episode length: 582.00 +/- 37.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 484056   |
---------------------------------
Eval num_timesteps=486048, episode_reward=212.25 +/- 145.61
Episode length: 587.60 +/- 32.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 588      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 486048   |
---------------------------------
Eval num_timesteps=488040, episode_reward=96.81 +/- 162.52
Episode length: 637.40 +/- 67.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 96.8     |
| time/              |          |
|    total_timesteps | 488040   |
---------------------------------
Eval num_timesteps=490032, episode_reward=68.29 +/- 286.83
Episode length: 633.60 +/- 67.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | 68.3     |
| time/              |          |
|    total_timesteps | 490032   |
---------------------------------
Eval num_timesteps=492024, episode_reward=-108.90 +/- 158.72
Episode length: 607.40 +/- 39.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 607         |
|    mean_reward          | -109        |
| time/                   |             |
|    total_timesteps      | 492024      |
| train/                  |             |
|    approx_kl            | 0.008765058 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0087     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00555    |
|    std                  | 1.09        |
|    value_loss           | 0.254       |
-----------------------------------------
Eval num_timesteps=494016, episode_reward=-75.02 +/- 189.29
Episode length: 674.80 +/- 103.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | -75      |
| time/              |          |
|    total_timesteps | 494016   |
---------------------------------
Eval num_timesteps=496008, episode_reward=-47.33 +/- 165.27
Episode length: 629.60 +/- 76.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | -47.3    |
| time/              |          |
|    total_timesteps | 496008   |
---------------------------------
Eval num_timesteps=498000, episode_reward=27.91 +/- 200.16
Episode length: 625.60 +/- 42.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=499992, episode_reward=-168.31 +/- 81.37
Episode length: 619.80 +/- 46.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 499992   |
---------------------------------
Eval num_timesteps=501984, episode_reward=-61.18 +/- 141.80
Episode length: 603.00 +/- 41.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -61.2    |
| time/              |          |
|    total_timesteps | 501984   |
---------------------------------
Eval num_timesteps=503976, episode_reward=-52.25 +/- 79.12
Episode length: 605.20 +/- 86.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | -52.3    |
| time/              |          |
|    total_timesteps | 503976   |
---------------------------------
Eval num_timesteps=505968, episode_reward=-102.34 +/- 171.12
Episode length: 621.20 +/- 22.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 505968   |
---------------------------------
Eval num_timesteps=507960, episode_reward=-126.71 +/- 116.61
Episode length: 609.40 +/- 91.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 609      |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 507960   |
---------------------------------
Eval num_timesteps=509952, episode_reward=-90.04 +/- 93.89
Episode length: 603.00 +/- 106.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -90      |
| time/              |          |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=511944, episode_reward=6.55 +/- 139.03
Episode length: 616.00 +/- 57.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 616      |
|    mean_reward     | 6.55     |
| time/              |          |
|    total_timesteps | 511944   |
---------------------------------
Eval num_timesteps=513936, episode_reward=-18.93 +/- 168.71
Episode length: 624.60 +/- 41.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | -18.9    |
| time/              |          |
|    total_timesteps | 513936   |
---------------------------------
Eval num_timesteps=515928, episode_reward=6.86 +/- 150.90
Episode length: 660.00 +/- 118.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 6.86     |
| time/              |          |
|    total_timesteps | 515928   |
---------------------------------
Eval num_timesteps=517920, episode_reward=-116.67 +/- 108.95
Episode length: 598.60 +/- 28.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 517920   |
---------------------------------
Eval num_timesteps=519912, episode_reward=-7.67 +/- 121.08
Episode length: 575.00 +/- 66.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | -7.67    |
| time/              |          |
|    total_timesteps | 519912   |
---------------------------------
Eval num_timesteps=521904, episode_reward=49.52 +/- 157.82
Episode length: 562.60 +/- 61.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 563      |
|    mean_reward     | 49.5     |
| time/              |          |
|    total_timesteps | 521904   |
---------------------------------
Eval num_timesteps=523896, episode_reward=-25.09 +/- 185.31
Episode length: 584.80 +/- 64.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 585      |
|    mean_reward     | -25.1    |
| time/              |          |
|    total_timesteps | 523896   |
---------------------------------
Eval num_timesteps=525888, episode_reward=-98.35 +/- 144.97
Episode length: 550.80 +/- 29.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 551      |
|    mean_reward     | -98.4    |
| time/              |          |
|    total_timesteps | 525888   |
---------------------------------
Eval num_timesteps=527880, episode_reward=-84.49 +/- 83.17
Episode length: 671.80 +/- 127.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | -84.5    |
| time/              |          |
|    total_timesteps | 527880   |
---------------------------------
Eval num_timesteps=529872, episode_reward=-115.13 +/- 176.59
Episode length: 571.20 +/- 38.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 571      |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 529872   |
---------------------------------
Eval num_timesteps=531864, episode_reward=-100.81 +/- 53.78
Episode length: 650.80 +/- 42.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 531864   |
---------------------------------
Eval num_timesteps=533856, episode_reward=-6.56 +/- 113.37
Episode length: 574.60 +/- 83.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | -6.56    |
| time/              |          |
|    total_timesteps | 533856   |
---------------------------------
Eval num_timesteps=535848, episode_reward=-3.49 +/- 233.47
Episode length: 598.00 +/- 66.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 598      |
|    mean_reward     | -3.49    |
| time/              |          |
|    total_timesteps | 535848   |
---------------------------------
Eval num_timesteps=537840, episode_reward=98.16 +/- 70.30
Episode length: 638.00 +/- 91.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | 98.2     |
| time/              |          |
|    total_timesteps | 537840   |
---------------------------------
Eval num_timesteps=539832, episode_reward=-43.70 +/- 195.32
Episode length: 603.20 +/- 87.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -43.7    |
| time/              |          |
|    total_timesteps | 539832   |
---------------------------------
Eval num_timesteps=541824, episode_reward=-10.07 +/- 122.51
Episode length: 574.80 +/- 48.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 575         |
|    mean_reward          | -10.1       |
| time/                   |             |
|    total_timesteps      | 541824      |
| train/                  |             |
|    approx_kl            | 0.007744161 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0228      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00353    |
|    std                  | 1.1         |
|    value_loss           | 0.305       |
-----------------------------------------
Eval num_timesteps=543816, episode_reward=-58.84 +/- 137.24
Episode length: 634.00 +/- 37.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | -58.8    |
| time/              |          |
|    total_timesteps | 543816   |
---------------------------------
Eval num_timesteps=545808, episode_reward=-17.81 +/- 148.15
Episode length: 564.00 +/- 59.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 564      |
|    mean_reward     | -17.8    |
| time/              |          |
|    total_timesteps | 545808   |
---------------------------------
Eval num_timesteps=547800, episode_reward=21.98 +/- 209.08
Episode length: 554.40 +/- 36.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 554      |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 547800   |
---------------------------------
Eval num_timesteps=549792, episode_reward=2.94 +/- 156.78
Episode length: 611.00 +/- 92.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 549792   |
---------------------------------
Eval num_timesteps=551784, episode_reward=-168.09 +/- 23.34
Episode length: 525.20 +/- 86.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 551784   |
---------------------------------
Eval num_timesteps=553776, episode_reward=-17.76 +/- 118.99
Episode length: 605.00 +/- 72.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 605      |
|    mean_reward     | -17.8    |
| time/              |          |
|    total_timesteps | 553776   |
---------------------------------
Eval num_timesteps=555768, episode_reward=-164.11 +/- 23.73
Episode length: 764.40 +/- 373.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 555768   |
---------------------------------
Eval num_timesteps=557760, episode_reward=-111.15 +/- 93.15
Episode length: 584.40 +/- 68.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 584      |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 557760   |
---------------------------------
Eval num_timesteps=559752, episode_reward=-116.53 +/- 33.66
Episode length: 632.00 +/- 84.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 632      |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 559752   |
---------------------------------
Eval num_timesteps=561744, episode_reward=-143.64 +/- 80.84
Episode length: 570.20 +/- 53.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 570      |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 561744   |
---------------------------------
Eval num_timesteps=563736, episode_reward=-96.77 +/- 58.66
Episode length: 620.60 +/- 30.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -96.8    |
| time/              |          |
|    total_timesteps | 563736   |
---------------------------------
Eval num_timesteps=565728, episode_reward=-26.48 +/- 130.46
Episode length: 524.40 +/- 69.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | -26.5    |
| time/              |          |
|    total_timesteps | 565728   |
---------------------------------
Eval num_timesteps=567720, episode_reward=-76.12 +/- 187.61
Episode length: 559.20 +/- 27.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | -76.1    |
| time/              |          |
|    total_timesteps | 567720   |
---------------------------------
Eval num_timesteps=569712, episode_reward=-154.97 +/- 53.25
Episode length: 742.40 +/- 133.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 569712   |
---------------------------------
Eval num_timesteps=571704, episode_reward=-74.86 +/- 94.02
Episode length: 579.80 +/- 168.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | -74.9    |
| time/              |          |
|    total_timesteps | 571704   |
---------------------------------
Eval num_timesteps=573696, episode_reward=-117.95 +/- 140.37
Episode length: 609.60 +/- 96.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 573696   |
---------------------------------
Eval num_timesteps=575688, episode_reward=-89.80 +/- 114.30
Episode length: 591.00 +/- 51.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 591      |
|    mean_reward     | -89.8    |
| time/              |          |
|    total_timesteps | 575688   |
---------------------------------
Eval num_timesteps=577680, episode_reward=-123.24 +/- 120.94
Episode length: 600.80 +/- 54.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 577680   |
---------------------------------
Eval num_timesteps=579672, episode_reward=-71.31 +/- 126.58
Episode length: 580.00 +/- 113.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | -71.3    |
| time/              |          |
|    total_timesteps | 579672   |
---------------------------------
Eval num_timesteps=581664, episode_reward=-184.69 +/- 32.92
Episode length: 597.00 +/- 83.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 597      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 581664   |
---------------------------------
Eval num_timesteps=583656, episode_reward=-7.21 +/- 183.14
Episode length: 636.00 +/- 31.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 636      |
|    mean_reward     | -7.21    |
| time/              |          |
|    total_timesteps | 583656   |
---------------------------------
Eval num_timesteps=585648, episode_reward=-78.90 +/- 216.08
Episode length: 580.00 +/- 53.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 580      |
|    mean_reward     | -78.9    |
| time/              |          |
|    total_timesteps | 585648   |
---------------------------------
Eval num_timesteps=587640, episode_reward=-63.72 +/- 156.69
Episode length: 585.80 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | -63.7    |
| time/              |          |
|    total_timesteps | 587640   |
---------------------------------
Eval num_timesteps=589632, episode_reward=-67.35 +/- 131.88
Episode length: 556.40 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | -67.4    |
| time/              |          |
|    total_timesteps | 589632   |
---------------------------------
Eval num_timesteps=591624, episode_reward=-134.90 +/- 114.11
Episode length: 565.00 +/- 106.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 565          |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 591624       |
| train/                  |              |
|    approx_kl            | 0.0082847625 |
|    clip_fraction        | 0.0937       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0195       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00475     |
|    std                  | 1.12         |
|    value_loss           | 0.273        |
------------------------------------------
Eval num_timesteps=593616, episode_reward=-53.30 +/- 205.55
Episode length: 654.00 +/- 112.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | -53.3    |
| time/              |          |
|    total_timesteps | 593616   |
---------------------------------
Eval num_timesteps=595608, episode_reward=-136.85 +/- 62.33
Episode length: 701.60 +/- 55.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 595608   |
---------------------------------
Eval num_timesteps=597600, episode_reward=-125.53 +/- 96.92
Episode length: 626.00 +/- 82.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 597600   |
---------------------------------
Eval num_timesteps=599592, episode_reward=-109.76 +/- 81.09
Episode length: 623.20 +/- 94.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 599592   |
---------------------------------
Eval num_timesteps=601584, episode_reward=-89.94 +/- 125.87
Episode length: 621.00 +/- 53.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -89.9    |
| time/              |          |
|    total_timesteps | 601584   |
---------------------------------
Eval num_timesteps=603576, episode_reward=-177.01 +/- 68.99
Episode length: 624.80 +/- 60.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 603576   |
---------------------------------
Eval num_timesteps=605568, episode_reward=-57.97 +/- 108.67
Episode length: 828.20 +/- 358.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | -58      |
| time/              |          |
|    total_timesteps | 605568   |
---------------------------------
Eval num_timesteps=607560, episode_reward=-52.30 +/- 127.10
Episode length: 628.60 +/- 106.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | -52.3    |
| time/              |          |
|    total_timesteps | 607560   |
---------------------------------
Eval num_timesteps=609552, episode_reward=-181.33 +/- 60.21
Episode length: 705.40 +/- 127.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 705      |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 609552   |
---------------------------------
Eval num_timesteps=611544, episode_reward=-72.64 +/- 180.57
Episode length: 615.00 +/- 54.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 615      |
|    mean_reward     | -72.6    |
| time/              |          |
|    total_timesteps | 611544   |
---------------------------------
Eval num_timesteps=613536, episode_reward=-50.98 +/- 157.92
Episode length: 755.40 +/- 377.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | -51      |
| time/              |          |
|    total_timesteps | 613536   |
---------------------------------
Eval num_timesteps=615528, episode_reward=-1.77 +/- 160.30
Episode length: 586.40 +/- 68.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | -1.77    |
| time/              |          |
|    total_timesteps | 615528   |
---------------------------------
Eval num_timesteps=617520, episode_reward=-176.30 +/- 38.56
Episode length: 656.80 +/- 120.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 657      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 617520   |
---------------------------------
Eval num_timesteps=619512, episode_reward=-118.02 +/- 124.71
Episode length: 665.00 +/- 76.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 619512   |
---------------------------------
Eval num_timesteps=621504, episode_reward=-33.40 +/- 183.54
Episode length: 649.00 +/- 200.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 649      |
|    mean_reward     | -33.4    |
| time/              |          |
|    total_timesteps | 621504   |
---------------------------------
Eval num_timesteps=623496, episode_reward=60.04 +/- 179.34
Episode length: 562.20 +/- 26.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 562      |
|    mean_reward     | 60       |
| time/              |          |
|    total_timesteps | 623496   |
---------------------------------
Eval num_timesteps=625488, episode_reward=-135.47 +/- 74.70
Episode length: 661.20 +/- 79.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 625488   |
---------------------------------
Eval num_timesteps=627480, episode_reward=-152.09 +/- 28.89
Episode length: 611.80 +/- 60.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 627480   |
---------------------------------
Eval num_timesteps=629472, episode_reward=19.30 +/- 252.50
Episode length: 677.20 +/- 84.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 629472   |
---------------------------------
Eval num_timesteps=631464, episode_reward=-65.46 +/- 135.46
Episode length: 630.40 +/- 115.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 630      |
|    mean_reward     | -65.5    |
| time/              |          |
|    total_timesteps | 631464   |
---------------------------------
Eval num_timesteps=633456, episode_reward=-68.35 +/- 119.02
Episode length: 671.00 +/- 112.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 671      |
|    mean_reward     | -68.3    |
| time/              |          |
|    total_timesteps | 633456   |
---------------------------------
Eval num_timesteps=635448, episode_reward=-182.82 +/- 72.43
Episode length: 620.60 +/- 96.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 621      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 635448   |
---------------------------------
Eval num_timesteps=637440, episode_reward=-144.80 +/- 52.19
Episode length: 667.00 +/- 42.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 637440   |
---------------------------------
Eval num_timesteps=639432, episode_reward=-190.61 +/- 34.70
Episode length: 686.80 +/- 142.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 687         |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 639432      |
| train/                  |             |
|    approx_kl            | 0.008156505 |
|    clip_fraction        | 0.0947      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0133      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00485    |
|    std                  | 1.13        |
|    value_loss           | 0.225       |
-----------------------------------------
Eval num_timesteps=641424, episode_reward=-141.80 +/- 45.46
Episode length: 685.00 +/- 76.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 685      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 641424   |
---------------------------------
Eval num_timesteps=643416, episode_reward=-7.78 +/- 140.72
Episode length: 700.60 +/- 129.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | -7.78    |
| time/              |          |
|    total_timesteps | 643416   |
---------------------------------
Eval num_timesteps=645408, episode_reward=-112.29 +/- 109.67
Episode length: 689.80 +/- 91.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 690      |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 645408   |
---------------------------------
Eval num_timesteps=647400, episode_reward=-202.90 +/- 31.26
Episode length: 683.80 +/- 106.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 647400   |
---------------------------------
Eval num_timesteps=649392, episode_reward=-59.14 +/- 177.35
Episode length: 676.80 +/- 188.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 677      |
|    mean_reward     | -59.1    |
| time/              |          |
|    total_timesteps | 649392   |
---------------------------------
Eval num_timesteps=651384, episode_reward=-175.67 +/- 35.92
Episode length: 691.40 +/- 85.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 651384   |
---------------------------------
Eval num_timesteps=653376, episode_reward=-17.80 +/- 283.88
Episode length: 701.20 +/- 131.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | -17.8    |
| time/              |          |
|    total_timesteps | 653376   |
---------------------------------
Eval num_timesteps=655368, episode_reward=-109.81 +/- 71.79
Episode length: 665.00 +/- 104.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 655368   |
---------------------------------
Eval num_timesteps=657360, episode_reward=-88.31 +/- 125.84
Episode length: 639.40 +/- 65.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 639      |
|    mean_reward     | -88.3    |
| time/              |          |
|    total_timesteps | 657360   |
---------------------------------
Eval num_timesteps=659352, episode_reward=-152.40 +/- 52.61
Episode length: 651.20 +/- 50.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 659352   |
---------------------------------
Eval num_timesteps=661344, episode_reward=-153.85 +/- 45.20
Episode length: 662.40 +/- 83.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 662      |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 661344   |
---------------------------------
Eval num_timesteps=663336, episode_reward=-121.01 +/- 102.33
Episode length: 698.60 +/- 212.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 663336   |
---------------------------------
Eval num_timesteps=665328, episode_reward=-185.10 +/- 61.33
Episode length: 666.00 +/- 104.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 665328   |
---------------------------------
Eval num_timesteps=667320, episode_reward=-145.35 +/- 72.66
Episode length: 710.20 +/- 127.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 710      |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 667320   |
---------------------------------
Eval num_timesteps=669312, episode_reward=-90.89 +/- 117.92
Episode length: 642.40 +/- 114.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | -90.9    |
| time/              |          |
|    total_timesteps | 669312   |
---------------------------------
Eval num_timesteps=671304, episode_reward=-57.52 +/- 160.61
Episode length: 576.80 +/- 88.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 577      |
|    mean_reward     | -57.5    |
| time/              |          |
|    total_timesteps | 671304   |
---------------------------------
Eval num_timesteps=673296, episode_reward=-91.71 +/- 114.37
Episode length: 634.20 +/- 114.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 634      |
|    mean_reward     | -91.7    |
| time/              |          |
|    total_timesteps | 673296   |
---------------------------------
Eval num_timesteps=675288, episode_reward=-138.53 +/- 99.78
Episode length: 688.60 +/- 205.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 675288   |
---------------------------------
Eval num_timesteps=677280, episode_reward=-163.03 +/- 55.46
Episode length: 619.80 +/- 34.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 620      |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 677280   |
---------------------------------
Eval num_timesteps=679272, episode_reward=-145.88 +/- 59.48
Episode length: 797.00 +/- 132.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 679272   |
---------------------------------
Eval num_timesteps=681264, episode_reward=-133.74 +/- 106.11
Episode length: 664.60 +/- 54.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 681264   |
---------------------------------
Eval num_timesteps=683256, episode_reward=-200.27 +/- 33.58
Episode length: 629.00 +/- 75.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 629      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 683256   |
---------------------------------
Eval num_timesteps=685248, episode_reward=-138.02 +/- 79.66
Episode length: 619.00 +/- 121.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 685248   |
---------------------------------
Eval num_timesteps=687240, episode_reward=-134.40 +/- 90.73
Episode length: 678.80 +/- 97.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 687240   |
---------------------------------
Eval num_timesteps=689232, episode_reward=-160.72 +/- 68.79
Episode length: 636.60 +/- 95.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 637        |
|    mean_reward          | -161       |
| time/                   |            |
|    total_timesteps      | 689232     |
| train/                  |            |
|    approx_kl            | 0.00998279 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.18      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0218    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.00457   |
|    std                  | 1.14       |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=691224, episode_reward=-181.42 +/- 22.59
Episode length: 610.20 +/- 82.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 691224   |
---------------------------------
Eval num_timesteps=693216, episode_reward=-199.99 +/- 29.80
Episode length: 611.60 +/- 118.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 693216   |
---------------------------------
Eval num_timesteps=695208, episode_reward=-93.85 +/- 62.49
Episode length: 654.20 +/- 63.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | -93.8    |
| time/              |          |
|    total_timesteps | 695208   |
---------------------------------
Eval num_timesteps=697200, episode_reward=-206.82 +/- 16.22
Episode length: 678.80 +/- 105.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 697200   |
---------------------------------
Eval num_timesteps=699192, episode_reward=-158.66 +/- 45.41
Episode length: 641.20 +/- 70.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 641      |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 699192   |
---------------------------------
Eval num_timesteps=701184, episode_reward=-187.60 +/- 36.93
Episode length: 710.60 +/- 249.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 711      |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 701184   |
---------------------------------
Eval num_timesteps=703176, episode_reward=-106.10 +/- 91.49
Episode length: 675.60 +/- 138.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 703176   |
---------------------------------
Eval num_timesteps=705168, episode_reward=-185.39 +/- 37.50
Episode length: 640.40 +/- 69.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 705168   |
---------------------------------
Eval num_timesteps=707160, episode_reward=-206.42 +/- 30.17
Episode length: 645.60 +/- 79.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 707160   |
---------------------------------
Eval num_timesteps=709152, episode_reward=-181.92 +/- 35.06
Episode length: 625.20 +/- 29.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 625      |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 709152   |
---------------------------------
Eval num_timesteps=711144, episode_reward=-204.06 +/- 38.57
Episode length: 516.40 +/- 156.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 711144   |
---------------------------------
Eval num_timesteps=713136, episode_reward=-143.78 +/- 52.79
Episode length: 644.80 +/- 106.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 645      |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 713136   |
---------------------------------
Eval num_timesteps=715128, episode_reward=-170.32 +/- 79.76
Episode length: 529.60 +/- 112.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 530      |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 715128   |
---------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 562, in <module>
    # sim.run_test()
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 349, in run_full
    model.learn(total_timesteps=int(5e6),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 194, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 205, in step
    self.step_async(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 369, in step_async
    self.venv.step_async(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 125, in step_async
    remote.send(("step", action))
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 280, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
BrokenPipeError: [WinError 232] The pipe is being closed