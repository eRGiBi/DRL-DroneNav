AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py:155: UserWarning: You have specified a mini-batch size of 49152, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=2048 and n_envs=1)
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO 01.08.2024_21.20.45_1
Eval num_timesteps=2000, episode_reward=-112.84 +/- 56.78
Episode length: 246.60 +/- 70.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-140.09 +/- 51.36
Episode length: 215.40 +/- 43.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 215       |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 4000      |
| train/                  |           |
|    approx_kl            | 0.0085294 |
|    clip_fraction        | 0.0397    |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.68     |
|    explained_variance   | -2.63e-05 |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+03  |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.00697  |
|    std                  | 1         |
|    value_loss           | 3.26e+03  |
---------------------------------------
Eval num_timesteps=6000, episode_reward=-125.34 +/- 43.88
Episode length: 230.40 +/- 41.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 6000        |
| train/                  |             |
|    approx_kl            | 0.002884013 |
|    clip_fraction        | 0.00356     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.69       |
|    explained_variance   | 0.0563      |
|    learning_rate        | 0.001       |
|    loss                 | 1.7e+03     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00203    |
|    std                  | 1           |
|    value_loss           | 3.44e+03    |
-----------------------------------------
Eval num_timesteps=8000, episode_reward=-120.92 +/- 27.06
Episode length: 246.20 +/- 31.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 8000         |
| train/                  |              |
|    approx_kl            | 0.0033121533 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0316       |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1            |
|    value_loss           | 2.79e+03     |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-118.83 +/- 21.22
Episode length: 246.00 +/- 30.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -119         |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0046253726 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+03      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00323     |
|    std                  | 0.999        |
|    value_loss           | 3.42e+03     |
------------------------------------------
Eval num_timesteps=12000, episode_reward=-106.67 +/- 19.19
Episode length: 305.20 +/- 30.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | -107         |
| time/                   |              |
|    total_timesteps      | 12000        |
| train/                  |              |
|    approx_kl            | 0.0026280973 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.0706       |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+03     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000919    |
|    std                  | 1            |
|    value_loss           | 3.35e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=14000, episode_reward=-119.79 +/- 40.50
Episode length: 269.80 +/- 89.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | -120        |
| time/                   |             |
|    total_timesteps      | 14000       |
| train/                  |             |
|    approx_kl            | 0.003670984 |
|    clip_fraction        | 0.00571     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.68       |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.001       |
|    loss                 | 1.52e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00285    |
|    std                  | 1           |
|    value_loss           | 3.11e+03    |
-----------------------------------------
Eval num_timesteps=16000, episode_reward=-129.81 +/- 55.11
Episode length: 249.40 +/- 79.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 16000        |
| train/                  |              |
|    approx_kl            | 0.0014394647 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000972    |
|    std                  | 1            |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=-106.61 +/- 23.13
Episode length: 281.00 +/- 38.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -107         |
| time/                   |              |
|    total_timesteps      | 18000        |
| train/                  |              |
|    approx_kl            | 0.0008402217 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.01         |
|    value_loss           | 2.45e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-130.84 +/- 53.84
Episode length: 237.80 +/- 56.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0012924202 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000803    |
|    std                  | 1.01         |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=22000, episode_reward=-114.21 +/- 24.13
Episode length: 241.20 +/- 21.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -114         |
| time/                   |              |
|    total_timesteps      | 22000        |
| train/                  |              |
|    approx_kl            | 0.0007878692 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+03     |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.01         |
|    value_loss           | 2.64e+03     |
------------------------------------------
Eval num_timesteps=24000, episode_reward=-123.88 +/- 13.96
Episode length: 241.20 +/- 27.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 0.0012771247 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.01         |
|    value_loss           | 2.29e+03     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-71.97 +/- 42.83
Episode length: 324.80 +/- 44.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 325           |
|    mean_reward          | -72           |
| time/                   |               |
|    total_timesteps      | 26000         |
| train/                  |               |
|    approx_kl            | 0.00063823024 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.72         |
|    explained_variance   | 0.344         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 120           |
|    policy_gradient_loss | -0.000461     |
|    std                  | 1.01          |
|    value_loss           | 2.38e+03      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=28000, episode_reward=-98.71 +/- 48.44
Episode length: 291.20 +/- 55.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | -98.7         |
| time/                   |               |
|    total_timesteps      | 28000         |
| train/                  |               |
|    approx_kl            | 0.00093551417 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.73         |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 951           |
|    n_updates            | 130           |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1.01          |
|    value_loss           | 1.97e+03      |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-80.09 +/- 72.29
Episode length: 298.80 +/- 83.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | -80.1        |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0022884116 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.01         |
|    value_loss           | 2.55e+03     |
------------------------------------------
Eval num_timesteps=32000, episode_reward=-136.84 +/- 39.02
Episode length: 245.60 +/- 66.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 32000        |
| train/                  |              |
|    approx_kl            | 0.0024649557 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.02         |
|    value_loss           | 2.14e+03     |
------------------------------------------
Eval num_timesteps=34000, episode_reward=-135.41 +/- 40.29
Episode length: 236.40 +/- 67.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 0.0012685017 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.74        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000543    |
|    std                  | 1.02         |
|    value_loss           | 2.54e+03     |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-158.54 +/- 7.12
Episode length: 206.80 +/- 15.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 207           |
|    mean_reward          | -159          |
| time/                   |               |
|    total_timesteps      | 36000         |
| train/                  |               |
|    approx_kl            | 0.00019056987 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.74         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+03      |
|    n_updates            | 170           |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.02          |
|    value_loss           | 2.97e+03      |
-------------------------------------------
Eval num_timesteps=38000, episode_reward=-129.72 +/- 61.64
Episode length: 229.80 +/- 61.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 230           |
|    mean_reward          | -130          |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 0.00072515244 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.75         |
|    explained_variance   | 0.446         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000736     |
|    std                  | 1.02          |
|    value_loss           | 2.33e+03      |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-140.93 +/- 49.46
Episode length: 217.60 +/- 33.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | -141         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0022775386 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.76        |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.02         |
|    value_loss           | 2.61e+03     |
------------------------------------------
Eval num_timesteps=42000, episode_reward=29.62 +/- 120.61
Episode length: 400.20 +/- 128.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 42000        |
| train/                  |              |
|    approx_kl            | 0.0025565843 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.001        |
|    loss                 | 989          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.03         |
|    value_loss           | 2.01e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=44000, episode_reward=-45.50 +/- 128.27
Episode length: 415.80 +/- 191.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 416        |
|    mean_reward          | -45.5      |
| time/                   |            |
|    total_timesteps      | 44000      |
| train/                  |            |
|    approx_kl            | 0.00266018 |
|    clip_fraction        | 0.00195    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.79      |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.001      |
|    loss                 | 704        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00186   |
|    std                  | 1.03       |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=46000, episode_reward=-31.43 +/- 145.40
Episode length: 355.40 +/- 130.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | -31.4        |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0033044978 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.03         |
|    value_loss           | 2.17e+03     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-58.25 +/- 55.81
Episode length: 372.60 +/- 86.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 373         |
|    mean_reward          | -58.3       |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.004963712 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.001       |
|    loss                 | 883         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00375    |
|    std                  | 1.04        |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-57.48 +/- 59.58
Episode length: 364.20 +/- 75.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | -57.5         |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00069006794 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 0.53          |
|    learning_rate        | 0.001         |
|    loss                 | 675           |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.04          |
|    value_loss           | 1.38e+03      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=-28.47 +/- 75.87
Episode length: 345.80 +/- 66.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | -28.5        |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0011701885 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.001        |
|    loss                 | 810          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00161     |
|    std                  | 1.05         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-49.71 +/- 37.95
Episode length: 389.40 +/- 87.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 389         |
|    mean_reward          | -49.7       |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.002844796 |
|    clip_fraction        | 0.00459     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.001       |
|    loss                 | 792         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00397    |
|    std                  | 1.05        |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=-12.62 +/- 39.87
Episode length: 451.60 +/- 91.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 452         |
|    mean_reward          | -12.6       |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.003957567 |
|    clip_fraction        | 0.00933     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.001       |
|    loss                 | 543         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00129    |
|    std                  | 1.06        |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=-65.15 +/- 62.42
Episode length: 399.80 +/- 158.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -65.2       |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.003340132 |
|    clip_fraction        | 0.0063      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.001       |
|    loss                 | 676         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0035     |
|    std                  | 1.06        |
|    value_loss           | 1.36e+03    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-15.90 +/- 34.99
Episode length: 493.00 +/- 50.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 493          |
|    mean_reward          | -15.9        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0020839903 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.615        |
|    learning_rate        | 0.001        |
|    loss                 | 789          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000827    |
|    std                  | 1.07         |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-61.33 +/- 52.72
Episode length: 387.40 +/- 114.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | -61.3        |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0008005854 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.61         |
|    learning_rate        | 0.001        |
|    loss                 | 903          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.000487    |
|    std                  | 1.07         |
|    value_loss           | 1.83e+03     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-82.14 +/- 52.73
Episode length: 370.60 +/- 128.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | -82.1       |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.002002548 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.001       |
|    loss                 | 658         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00226    |
|    std                  | 1.07        |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=66000, episode_reward=-53.02 +/- 62.00
Episode length: 426.20 +/- 129.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | -53          |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0058808094 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.665        |
|    learning_rate        | 0.001        |
|    loss                 | 527          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.07         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=68000, episode_reward=54.57 +/- 94.08
Episode length: 612.20 +/- 164.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 612         |
|    mean_reward          | 54.6        |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.010085985 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.001       |
|    loss                 | 639         |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00475    |
|    std                  | 1.08        |
|    value_loss           | 1.29e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=70000, episode_reward=592.87 +/- 672.19
Episode length: 1056.40 +/- 407.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.06e+03    |
|    mean_reward          | 593         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.007968308 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.001       |
|    loss                 | 501         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.004      |
|    std                  | 1.08        |
|    value_loss           | 1.01e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=72000, episode_reward=175.37 +/- 91.56
Episode length: 665.00 +/- 233.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 665         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.007044426 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.99       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.001       |
|    loss                 | 497         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00314    |
|    std                  | 1.08        |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=74000, episode_reward=97.57 +/- 143.71
Episode length: 589.80 +/- 249.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 590          |
|    mean_reward          | 97.6         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0022421712 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6           |
|    explained_variance   | 0.655        |
|    learning_rate        | 0.001        |
|    loss                 | 480          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.09         |
|    value_loss           | 979          |
------------------------------------------
Eval num_timesteps=76000, episode_reward=-53.05 +/- 14.91
Episode length: 394.20 +/- 45.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 394         |
|    mean_reward          | -53.1       |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.004527779 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.001       |
|    loss                 | 602         |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00153    |
|    std                  | 1.09        |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=78000, episode_reward=-84.30 +/- 96.07
Episode length: 303.60 +/- 124.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | -84.3        |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0033008484 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.692        |
|    learning_rate        | 0.001        |
|    loss                 | 724          |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00196     |
|    std                  | 1.09         |
|    value_loss           | 1.47e+03     |
------------------------------------------
Eval num_timesteps=80000, episode_reward=-70.90 +/- 62.12
Episode length: 351.60 +/- 74.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | -70.9        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0039663166 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.684        |
|    learning_rate        | 0.001        |
|    loss                 | 846          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00114     |
|    std                  | 1.09         |
|    value_loss           | 1.7e+03      |
------------------------------------------
Eval num_timesteps=82000, episode_reward=-90.71 +/- 34.63
Episode length: 310.00 +/- 57.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | -90.7        |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0031284676 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.04        |
|    explained_variance   | 0.697        |
|    learning_rate        | 0.001        |
|    loss                 | 714          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.1          |
|    value_loss           | 1.44e+03     |
------------------------------------------
Eval num_timesteps=84000, episode_reward=-82.03 +/- 66.89
Episode length: 337.60 +/- 99.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 338           |
|    mean_reward          | -82           |
| time/                   |               |
|    total_timesteps      | 84000         |
| train/                  |               |
|    approx_kl            | 0.00092794525 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.05         |
|    explained_variance   | 0.633         |
|    learning_rate        | 0.001         |
|    loss                 | 851           |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000678     |
|    std                  | 1.1           |
|    value_loss           | 1.77e+03      |
-------------------------------------------
Eval num_timesteps=86000, episode_reward=-109.96 +/- 51.34
Episode length: 292.60 +/- 81.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-108.95 +/- 18.63
Episode length: 276.00 +/- 30.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | -109          |
| time/                   |               |
|    total_timesteps      | 88000         |
| train/                  |               |
|    approx_kl            | 0.00090581377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.06         |
|    explained_variance   | 0.698         |
|    learning_rate        | 0.001         |
|    loss                 | 828           |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000936     |
|    std                  | 1.1           |
|    value_loss           | 1.67e+03      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-74.95 +/- 39.56
Episode length: 343.00 +/- 54.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | -75          |
| time/                   |              |
|    total_timesteps      | 90000        |
| train/                  |              |
|    approx_kl            | 0.0066915057 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.691        |
|    learning_rate        | 0.001        |
|    loss                 | 821          |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.1          |
|    value_loss           | 1.66e+03     |
------------------------------------------
Eval num_timesteps=92000, episode_reward=-93.63 +/- 52.64
Episode length: 347.20 +/- 85.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | -93.6        |
| time/                   |              |
|    total_timesteps      | 92000        |
| train/                  |              |
|    approx_kl            | 0.0041964566 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.001        |
|    loss                 | 692          |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00333     |
|    std                  | 1.1          |
|    value_loss           | 1.4e+03      |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-100.69 +/- 20.73
Episode length: 334.60 +/- 38.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | -101        |
| time/                   |             |
|    total_timesteps      | 94000       |
| train/                  |             |
|    approx_kl            | 0.005684389 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.06       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.001       |
|    loss                 | 573         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00148    |
|    std                  | 1.1         |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=-84.47 +/- 55.73
Episode length: 335.60 +/- 99.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 336          |
|    mean_reward          | -84.5        |
| time/                   |              |
|    total_timesteps      | 96000        |
| train/                  |              |
|    approx_kl            | 0.0051658507 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.001        |
|    loss                 | 675          |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.0027      |
|    std                  | 1.11         |
|    value_loss           | 1.37e+03     |
------------------------------------------
Eval num_timesteps=98000, episode_reward=-39.96 +/- 68.02
Episode length: 493.20 +/- 156.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | -40         |
| time/                   |             |
|    total_timesteps      | 98000       |
| train/                  |             |
|    approx_kl            | 0.006832922 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.001       |
|    loss                 | 662         |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00365    |
|    std                  | 1.12        |
|    value_loss           | 1.34e+03    |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-94.94 +/- 20.44
Episode length: 351.20 +/- 84.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | -94.9        |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0024446642 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.12        |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.001        |
|    loss                 | 548          |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000548    |
|    std                  | 1.12         |
|    value_loss           | 1.11e+03     |
------------------------------------------
Eval num_timesteps=102000, episode_reward=-124.24 +/- 32.11
Episode length: 263.20 +/- 66.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 102000       |
| train/                  |              |
|    approx_kl            | 0.0028782738 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.12        |
|    explained_variance   | 0.729        |
|    learning_rate        | 0.001        |
|    loss                 | 713          |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.12         |
|    value_loss           | 1.44e+03     |
------------------------------------------
Eval num_timesteps=104000, episode_reward=-125.12 +/- 36.54
Episode length: 254.80 +/- 55.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 104000       |
| train/                  |              |
|    approx_kl            | 0.0043989224 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 651          |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00318     |
|    std                  | 1.12         |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=106000, episode_reward=-110.56 +/- 49.52
Episode length: 310.80 +/- 109.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | -111         |
| time/                   |              |
|    total_timesteps      | 106000       |
| train/                  |              |
|    approx_kl            | 0.0015363225 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.001        |
|    loss                 | 853          |
|    n_updates            | 510          |
|    policy_gradient_loss | 0.000328     |
|    std                  | 1.12         |
|    value_loss           | 1.72e+03     |
------------------------------------------
Eval num_timesteps=108000, episode_reward=-138.31 +/- 27.52
Episode length: 257.20 +/- 50.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 257           |
|    mean_reward          | -138          |
| time/                   |               |
|    total_timesteps      | 108000        |
| train/                  |               |
|    approx_kl            | 0.00034285523 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.13         |
|    explained_variance   | 0.754         |
|    learning_rate        | 0.001         |
|    loss                 | 750           |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.000347     |
|    std                  | 1.12          |
|    value_loss           | 1.5e+03       |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-120.16 +/- 63.82
Episode length: 284.60 +/- 101.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | -120         |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0010788239 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.001        |
|    loss                 | 641          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.12         |
|    value_loss           | 1.3e+03      |
------------------------------------------
Eval num_timesteps=112000, episode_reward=-87.41 +/- 34.69
Episode length: 327.80 +/- 61.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 328          |
|    mean_reward          | -87.4        |
| time/                   |              |
|    total_timesteps      | 112000       |
| train/                  |              |
|    approx_kl            | 0.0016548481 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.14        |
|    explained_variance   | 0.688        |
|    learning_rate        | 0.001        |
|    loss                 | 792          |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.13         |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=114000, episode_reward=-91.25 +/- 37.70
Episode length: 346.80 +/- 98.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | -91.2        |
| time/                   |              |
|    total_timesteps      | 114000       |
| train/                  |              |
|    approx_kl            | 0.0033065986 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.15        |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.001        |
|    loss                 | 738          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00402     |
|    std                  | 1.13         |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=116000, episode_reward=-68.41 +/- 27.61
Episode length: 391.60 +/- 84.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | -68.4       |
| time/                   |             |
|    total_timesteps      | 116000      |
| train/                  |             |
|    approx_kl            | 0.006658118 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.001       |
|    loss                 | 623         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00361    |
|    std                  | 1.13        |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=118000, episode_reward=-20.75 +/- 37.39
Episode length: 427.40 +/- 83.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | -20.8       |
| time/                   |             |
|    total_timesteps      | 118000      |
| train/                  |             |
|    approx_kl            | 0.004550959 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.001       |
|    loss                 | 416         |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00364    |
|    std                  | 1.13        |
|    value_loss           | 848         |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=-16.16 +/- 29.57
Episode length: 422.40 +/- 97.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | -16.2       |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.009081168 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.18       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.001       |
|    loss                 | 414         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00643    |
|    std                  | 1.14        |
|    value_loss           | 835         |
-----------------------------------------
Eval num_timesteps=122000, episode_reward=-4.37 +/- 77.10
Episode length: 433.20 +/- 127.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | -4.37        |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0075409748 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.23        |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.001        |
|    loss                 | 512          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000958    |
|    std                  | 1.15         |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=124000, episode_reward=-22.90 +/- 65.44
Episode length: 420.00 +/- 170.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | -22.9       |
| time/                   |             |
|    total_timesteps      | 124000      |
| train/                  |             |
|    approx_kl            | 0.002939091 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 497         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00308    |
|    std                  | 1.16        |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=126000, episode_reward=8.91 +/- 95.36
Episode length: 406.80 +/- 86.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 8.91         |
| time/                   |              |
|    total_timesteps      | 126000       |
| train/                  |              |
|    approx_kl            | 0.0013531385 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.27        |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.001        |
|    loss                 | 615          |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00103     |
|    std                  | 1.16         |
|    value_loss           | 1.28e+03     |
------------------------------------------
Eval num_timesteps=128000, episode_reward=157.08 +/- 107.84
Episode length: 546.00 +/- 92.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 157          |
| time/                   |              |
|    total_timesteps      | 128000       |
| train/                  |              |
|    approx_kl            | 0.0006601422 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.001        |
|    loss                 | 400          |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.17         |
|    value_loss           | 864          |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-5.45 +/- 68.34
Episode length: 413.80 +/- 113.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | -5.45        |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0017266046 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 600          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000479    |
|    std                  | 1.17         |
|    value_loss           | 1.25e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-24.80 +/- 33.73
Episode length: 399.60 +/- 35.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | -24.8         |
| time/                   |               |
|    total_timesteps      | 132000        |
| train/                  |               |
|    approx_kl            | 0.00040327676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.29         |
|    explained_variance   | 0.771         |
|    learning_rate        | 0.001         |
|    loss                 | 611           |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000265     |
|    std                  | 1.17          |
|    value_loss           | 1.32e+03      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=-38.90 +/- 53.17
Episode length: 429.20 +/- 114.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | -38.9       |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.001034955 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.001       |
|    loss                 | 406         |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00184    |
|    std                  | 1.17        |
|    value_loss           | 844         |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=55.13 +/- 125.13
Episode length: 568.20 +/- 167.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 568         |
|    mean_reward          | 55.1        |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.002287236 |
|    clip_fraction        | 0.0041      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.001       |
|    loss                 | 430         |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.0011      |
|    std                  | 1.17        |
|    value_loss           | 963         |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=23.74 +/- 81.08
Episode length: 507.60 +/- 194.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 138000        |
| train/                  |               |
|    approx_kl            | 0.00048621092 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.3          |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 473           |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000238     |
|    std                  | 1.17          |
|    value_loss           | 992           |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-86.33 +/- 30.75
Episode length: 337.00 +/- 49.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 337         |
|    mean_reward          | -86.3       |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.000528386 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.001       |
|    loss                 | 507         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0017     |
|    std                  | 1.17        |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=-28.06 +/- 68.58
Episode length: 416.80 +/- 90.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | -28.1        |
| time/                   |              |
|    total_timesteps      | 142000       |
| train/                  |              |
|    approx_kl            | 0.0004339578 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 410          |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.17         |
|    value_loss           | 848          |
------------------------------------------
Eval num_timesteps=144000, episode_reward=15.02 +/- 153.41
Episode length: 438.80 +/- 128.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 144000       |
| train/                  |              |
|    approx_kl            | 0.0014944379 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 515          |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.17         |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=51.78 +/- 134.27
Episode length: 515.60 +/- 139.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 51.8         |
| time/                   |              |
|    total_timesteps      | 146000       |
| train/                  |              |
|    approx_kl            | 0.0016976992 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 424          |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.17         |
|    value_loss           | 879          |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-16.31 +/- 50.70
Episode length: 463.00 +/- 97.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | -16.3        |
| time/                   |              |
|    total_timesteps      | 148000       |
| train/                  |              |
|    approx_kl            | 0.0009883032 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.001        |
|    loss                 | 414          |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000779    |
|    std                  | 1.17         |
|    value_loss           | 862          |
------------------------------------------
Eval num_timesteps=150000, episode_reward=16.14 +/- 72.65
Episode length: 588.40 +/- 133.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 588          |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0010754963 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.31        |
|    explained_variance   | 0.724        |
|    learning_rate        | 0.001        |
|    loss                 | 404          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.17         |
|    value_loss           | 861          |
------------------------------------------
Eval num_timesteps=152000, episode_reward=-9.94 +/- 17.18
Episode length: 530.00 +/- 30.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 530          |
|    mean_reward          | -9.94        |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0007753528 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.31        |
|    explained_variance   | 0.765        |
|    learning_rate        | 0.001        |
|    loss                 | 473          |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0012      |
|    std                  | 1.18         |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=154000, episode_reward=-15.87 +/- 82.41
Episode length: 456.20 +/- 150.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | -15.9        |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0012509122 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.001        |
|    loss                 | 490          |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.18         |
|    value_loss           | 995          |
------------------------------------------
Eval num_timesteps=156000, episode_reward=-7.68 +/- 59.71
Episode length: 536.60 +/- 110.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 537         |
|    mean_reward          | -7.68       |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.004731407 |
|    clip_fraction        | 0.00732     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 309         |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00197    |
|    std                  | 1.18        |
|    value_loss           | 630         |
-----------------------------------------
Eval num_timesteps=158000, episode_reward=-49.72 +/- 42.75
Episode length: 506.20 +/- 111.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | -49.7        |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0053902473 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.001        |
|    loss                 | 363          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00226     |
|    std                  | 1.18         |
|    value_loss           | 758          |
------------------------------------------
Eval num_timesteps=160000, episode_reward=-49.75 +/- 30.90
Episode length: 514.60 +/- 48.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -49.7        |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0030252119 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.33        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 448          |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.18         |
|    value_loss           | 941          |
------------------------------------------
Eval num_timesteps=162000, episode_reward=-8.04 +/- 52.83
Episode length: 504.00 +/- 113.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | -8.04       |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.008947111 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.001       |
|    loss                 | 408         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00353    |
|    std                  | 1.19        |
|    value_loss           | 915         |
-----------------------------------------
Eval num_timesteps=164000, episode_reward=28.03 +/- 36.10
Episode length: 584.40 +/- 84.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 584         |
|    mean_reward          | 28          |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.004219032 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 317         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.19        |
|    value_loss           | 643         |
-----------------------------------------
Eval num_timesteps=166000, episode_reward=-3.91 +/- 44.97
Episode length: 555.00 +/- 109.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 555         |
|    mean_reward          | -3.91       |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.003236611 |
|    clip_fraction        | 0.00288     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.001       |
|    loss                 | 399         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.19        |
|    value_loss           | 897         |
-----------------------------------------
Eval num_timesteps=168000, episode_reward=46.32 +/- 83.39
Episode length: 682.40 +/- 160.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 682         |
|    mean_reward          | 46.3        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.004566829 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.001       |
|    loss                 | 319         |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00238    |
|    std                  | 1.19        |
|    value_loss           | 659         |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=-6.67 +/- 29.26
Episode length: 566.40 +/- 44.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 566         |
|    mean_reward          | -6.67       |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.007396509 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 299         |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00704    |
|    std                  | 1.19        |
|    value_loss           | 616         |
-----------------------------------------
Eval num_timesteps=172000, episode_reward=62.09 +/- 65.27
Episode length: 711.60 +/- 109.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | 62.1     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=174000, episode_reward=13.69 +/- 50.40
Episode length: 630.60 +/- 107.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 631         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 174000      |
| train/                  |             |
|    approx_kl            | 0.009467598 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.001       |
|    loss                 | 286         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00653    |
|    std                  | 1.2         |
|    value_loss           | 582         |
-----------------------------------------
Eval num_timesteps=176000, episode_reward=88.25 +/- 151.07
Episode length: 687.20 +/- 202.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 687          |
|    mean_reward          | 88.2         |
| time/                   |              |
|    total_timesteps      | 176000       |
| train/                  |              |
|    approx_kl            | 0.0064992034 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.42        |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.001        |
|    loss                 | 400          |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00241     |
|    std                  | 1.21         |
|    value_loss           | 881          |
------------------------------------------
Eval num_timesteps=178000, episode_reward=22.15 +/- 48.27
Episode length: 599.00 +/- 112.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 599         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 178000      |
| train/                  |             |
|    approx_kl            | 0.014969565 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.001       |
|    loss                 | 285         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00655    |
|    std                  | 1.22        |
|    value_loss           | 582         |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=-20.63 +/- 30.06
Episode length: 449.00 +/- 56.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | -20.6        |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0072528766 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.833        |
|    learning_rate        | 0.001        |
|    loss                 | 289          |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00312     |
|    std                  | 1.22         |
|    value_loss           | 611          |
------------------------------------------
Eval num_timesteps=182000, episode_reward=-3.53 +/- 45.90
Episode length: 539.20 +/- 105.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 539          |
|    mean_reward          | -3.53        |
| time/                   |              |
|    total_timesteps      | 182000       |
| train/                  |              |
|    approx_kl            | 0.0037550763 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 380          |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.0027      |
|    std                  | 1.22         |
|    value_loss           | 777          |
------------------------------------------
Eval num_timesteps=184000, episode_reward=93.00 +/- 196.56
Episode length: 576.20 +/- 119.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 576          |
|    mean_reward          | 93           |
| time/                   |              |
|    total_timesteps      | 184000       |
| train/                  |              |
|    approx_kl            | 0.0027933605 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.49        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 284          |
|    n_updates            | 890          |
|    policy_gradient_loss | 5.26e-05     |
|    std                  | 1.23         |
|    value_loss           | 595          |
------------------------------------------
Eval num_timesteps=186000, episode_reward=2.44 +/- 50.11
Episode length: 578.20 +/- 131.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 578          |
|    mean_reward          | 2.44         |
| time/                   |              |
|    total_timesteps      | 186000       |
| train/                  |              |
|    approx_kl            | 0.0047151735 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.5         |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 345          |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00399     |
|    std                  | 1.23         |
|    value_loss           | 707          |
------------------------------------------
Eval num_timesteps=188000, episode_reward=-58.96 +/- 34.32
Episode length: 461.20 +/- 67.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | -59          |
| time/                   |              |
|    total_timesteps      | 188000       |
| train/                  |              |
|    approx_kl            | 0.0103610335 |
|    clip_fraction        | 0.0684       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.844        |
|    learning_rate        | 0.001        |
|    loss                 | 369          |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00464     |
|    std                  | 1.23         |
|    value_loss           | 750          |
------------------------------------------
Eval num_timesteps=190000, episode_reward=-3.97 +/- 57.83
Episode length: 464.40 +/- 90.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | -3.97       |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.008042483 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.001       |
|    loss                 | 378         |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00336    |
|    std                  | 1.24        |
|    value_loss           | 766         |
-----------------------------------------
Eval num_timesteps=192000, episode_reward=-2.14 +/- 80.92
Episode length: 493.60 +/- 48.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | -2.14       |
| time/                   |             |
|    total_timesteps      | 192000      |
| train/                  |             |
|    approx_kl            | 0.005252059 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.001       |
|    loss                 | 386         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00214    |
|    std                  | 1.24        |
|    value_loss           | 792         |
-----------------------------------------
Eval num_timesteps=194000, episode_reward=24.49 +/- 107.15
Episode length: 525.80 +/- 108.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 526         |
|    mean_reward          | 24.5        |
| time/                   |             |
|    total_timesteps      | 194000      |
| train/                  |             |
|    approx_kl            | 0.002980657 |
|    clip_fraction        | 0.00508     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.001       |
|    loss                 | 376         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00222    |
|    std                  | 1.24        |
|    value_loss           | 782         |
-----------------------------------------
Eval num_timesteps=196000, episode_reward=-32.27 +/- 96.54
Episode length: 439.60 +/- 107.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | -32.3       |
| time/                   |             |
|    total_timesteps      | 196000      |
| train/                  |             |
|    approx_kl            | 0.002824772 |
|    clip_fraction        | 0.004       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 302         |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00208    |
|    std                  | 1.24        |
|    value_loss           | 592         |
-----------------------------------------
Eval num_timesteps=198000, episode_reward=29.62 +/- 79.32
Episode length: 538.20 +/- 59.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 0.0019293777 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.52        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 389          |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.24         |
|    value_loss           | 784          |
------------------------------------------
Eval num_timesteps=200000, episode_reward=80.94 +/- 63.73
Episode length: 567.20 +/- 70.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 567         |
|    mean_reward          | 80.9        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.006677285 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00377    |
|    std                  | 1.24        |
|    value_loss           | 579         |
-----------------------------------------
Eval num_timesteps=202000, episode_reward=49.91 +/- 68.69
Episode length: 603.40 +/- 97.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 603         |
|    mean_reward          | 49.9        |
| time/                   |             |
|    total_timesteps      | 202000      |
| train/                  |             |
|    approx_kl            | 0.005343317 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.000982   |
|    std                  | 1.24        |
|    value_loss           | 583         |
-----------------------------------------
Eval num_timesteps=204000, episode_reward=-3.49 +/- 36.95
Episode length: 571.60 +/- 104.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 572         |
|    mean_reward          | -3.49       |
| time/                   |             |
|    total_timesteps      | 204000      |
| train/                  |             |
|    approx_kl            | 0.011256656 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.55       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 362         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00561    |
|    std                  | 1.26        |
|    value_loss           | 740         |
-----------------------------------------
Eval num_timesteps=206000, episode_reward=-33.74 +/- 72.13
Episode length: 393.20 +/- 118.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | -33.7       |
| time/                   |             |
|    total_timesteps      | 206000      |
| train/                  |             |
|    approx_kl            | 0.009303849 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 355         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 1.27        |
|    value_loss           | 718         |
-----------------------------------------
Eval num_timesteps=208000, episode_reward=8.47 +/- 127.55
Episode length: 508.00 +/- 92.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 8.47         |
| time/                   |              |
|    total_timesteps      | 208000       |
| train/                  |              |
|    approx_kl            | 0.0025017383 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.64        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 357          |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000819    |
|    std                  | 1.28         |
|    value_loss           | 733          |
------------------------------------------
Eval num_timesteps=210000, episode_reward=-27.59 +/- 45.77
Episode length: 507.20 +/- 86.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 507          |
|    mean_reward          | -27.6        |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0132428575 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.65        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 401          |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00459     |
|    std                  | 1.28         |
|    value_loss           | 845          |
------------------------------------------
Eval num_timesteps=212000, episode_reward=88.24 +/- 73.68
Episode length: 722.40 +/- 161.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 722          |
|    mean_reward          | 88.2         |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 0.0113701485 |
|    clip_fraction        | 0.0535       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.65        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 346          |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00588     |
|    std                  | 1.28         |
|    value_loss           | 710          |
------------------------------------------
Eval num_timesteps=214000, episode_reward=206.46 +/- 406.40
Episode length: 619.20 +/- 163.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 619         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 214000      |
| train/                  |             |
|    approx_kl            | 0.004304819 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 262         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00336    |
|    std                  | 1.28        |
|    value_loss           | 535         |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=165.25 +/- 257.73
Episode length: 706.80 +/- 229.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 707         |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.011971589 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.001       |
|    loss                 | 335         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00731    |
|    std                  | 1.28        |
|    value_loss           | 703         |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=60.37 +/- 67.06
Episode length: 634.60 +/- 151.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 635         |
|    mean_reward          | 60.4        |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.003865975 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.001       |
|    loss                 | 478         |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00177    |
|    std                  | 1.28        |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=658.30 +/- 512.02
Episode length: 799.20 +/- 136.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 799          |
|    mean_reward          | 658          |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0048356582 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.65        |
|    explained_variance   | 0.831        |
|    learning_rate        | 0.001        |
|    loss                 | 272          |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 1.28         |
|    value_loss           | 585          |
------------------------------------------
New best mean reward!
Eval num_timesteps=222000, episode_reward=20.91 +/- 63.56
Episode length: 564.80 +/- 150.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 565         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.002506582 |
|    clip_fraction        | 0.00288     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.001       |
|    loss                 | 283         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 1.28        |
|    value_loss           | 602         |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=52.83 +/- 138.65
Episode length: 578.80 +/- 63.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 579         |
|    mean_reward          | 52.8        |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.006536233 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.001       |
|    loss                 | 339         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00308    |
|    std                  | 1.28        |
|    value_loss           | 701         |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=-92.38 +/- 35.99
Episode length: 442.80 +/- 37.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | -92.4        |
| time/                   |              |
|    total_timesteps      | 226000       |
| train/                  |              |
|    approx_kl            | 0.0062896265 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.67        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 350          |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.004       |
|    std                  | 1.29         |
|    value_loss           | 712          |
------------------------------------------
Eval num_timesteps=228000, episode_reward=132.24 +/- 203.86
Episode length: 562.60 +/- 77.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 563         |
|    mean_reward          | 132         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.010249352 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.001       |
|    loss                 | 255         |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00473    |
|    std                  | 1.3         |
|    value_loss           | 521         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=44.55 +/- 255.91
Episode length: 488.40 +/- 110.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 44.5         |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0031667366 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.001        |
|    loss                 | 355          |
|    n_updates            | 1120         |
|    policy_gradient_loss | 0.000179     |
|    std                  | 1.3          |
|    value_loss           | 826          |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-19.67 +/- 91.39
Episode length: 439.80 +/- 77.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | -19.7        |
| time/                   |              |
|    total_timesteps      | 232000       |
| train/                  |              |
|    approx_kl            | 0.0019155492 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.73        |
|    explained_variance   | 0.741        |
|    learning_rate        | 0.001        |
|    loss                 | 593          |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000104    |
|    std                  | 1.31         |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=-111.00 +/- 21.29
Episode length: 437.20 +/- 33.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | -111        |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.010434477 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.74       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.001       |
|    loss                 | 424         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00415    |
|    std                  | 1.31        |
|    value_loss           | 869         |
-----------------------------------------
Eval num_timesteps=236000, episode_reward=-77.48 +/- 50.00
Episode length: 449.80 +/- 61.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -77.5       |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.008163583 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.75       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.001       |
|    loss                 | 603         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0047     |
|    std                  | 1.31        |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=238000, episode_reward=12.89 +/- 66.35
Episode length: 485.80 +/- 46.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0027735548 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.001        |
|    loss                 | 342          |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00086     |
|    std                  | 1.31         |
|    value_loss           | 926          |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-9.36 +/- 88.17
Episode length: 469.40 +/- 85.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | -9.36         |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 0.00036571862 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.76         |
|    explained_variance   | 0.835         |
|    learning_rate        | 0.001         |
|    loss                 | 447           |
|    n_updates            | 1170          |
|    policy_gradient_loss | 0.000104      |
|    std                  | 1.32          |
|    value_loss           | 1.02e+03      |
-------------------------------------------
Eval num_timesteps=242000, episode_reward=59.35 +/- 217.18
Episode length: 453.80 +/- 94.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 59.4         |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0007276825 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.001        |
|    loss                 | 465          |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.32         |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=244000, episode_reward=-37.07 +/- 63.66
Episode length: 390.40 +/- 58.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | -37.1        |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0021367394 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 347          |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 1.32         |
|    value_loss           | 744          |
------------------------------------------
Eval num_timesteps=246000, episode_reward=-50.45 +/- 59.34
Episode length: 445.40 +/- 127.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 445           |
|    mean_reward          | -50.4         |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 0.00048560387 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.76         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 412           |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.00014      |
|    std                  | 1.32          |
|    value_loss           | 925           |
-------------------------------------------
Eval num_timesteps=248000, episode_reward=1.03 +/- 149.00
Episode length: 424.80 +/- 117.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 1.03         |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0007495227 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 440          |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.31         |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=250000, episode_reward=-34.00 +/- 92.60
Episode length: 435.40 +/- 106.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | -34          |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0039146068 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 325          |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.32         |
|    value_loss           | 674          |
------------------------------------------
Eval num_timesteps=252000, episode_reward=-91.67 +/- 24.91
Episode length: 364.60 +/- 68.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | -91.7        |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0059163515 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 390          |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 1.32         |
|    value_loss           | 811          |
------------------------------------------
Eval num_timesteps=254000, episode_reward=-84.28 +/- 28.08
Episode length: 333.40 +/- 46.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | -84.3       |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.008184991 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.77       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.001       |
|    loss                 | 314         |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00375    |
|    std                  | 1.32        |
|    value_loss           | 647         |
-----------------------------------------
Eval num_timesteps=256000, episode_reward=-104.58 +/- 23.47
Episode length: 324.40 +/- 67.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-75.18 +/- 40.42
Episode length: 347.00 +/- 71.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | -75.2       |
| time/                   |             |
|    total_timesteps      | 258000      |
| train/                  |             |
|    approx_kl            | 0.002689082 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 304         |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 1.33        |
|    value_loss           | 627         |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=-66.72 +/- 36.65
Episode length: 351.00 +/- 59.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -66.7       |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.010457766 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.81       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 380         |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00549    |
|    std                  | 1.33        |
|    value_loss           | 773         |
-----------------------------------------
Eval num_timesteps=262000, episode_reward=-90.68 +/- 56.16
Episode length: 426.20 +/- 53.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | -90.7       |
| time/                   |             |
|    total_timesteps      | 262000      |
| train/                  |             |
|    approx_kl            | 0.005701866 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.82       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 374         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00325    |
|    std                  | 1.34        |
|    value_loss           | 769         |
-----------------------------------------
Eval num_timesteps=264000, episode_reward=-22.22 +/- 112.48
Episode length: 511.00 +/- 135.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | -22.2        |
| time/                   |              |
|    total_timesteps      | 264000       |
| train/                  |              |
|    approx_kl            | 0.0033536982 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 412          |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.34         |
|    value_loss           | 933          |
------------------------------------------
Eval num_timesteps=266000, episode_reward=-137.87 +/- 23.11
Episode length: 429.00 +/- 49.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | -138         |
| time/                   |              |
|    total_timesteps      | 266000       |
| train/                  |              |
|    approx_kl            | 0.0037832325 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.001        |
|    loss                 | 339          |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 1.34         |
|    value_loss           | 715          |
------------------------------------------
Eval num_timesteps=268000, episode_reward=137.22 +/- 293.21
Episode length: 676.60 +/- 408.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 677         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 268000      |
| train/                  |             |
|    approx_kl            | 0.008153516 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.001       |
|    loss                 | 398         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00325    |
|    std                  | 1.35        |
|    value_loss           | 875         |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=72.96 +/- 248.98
Episode length: 616.80 +/- 117.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 617         |
|    mean_reward          | 73          |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.008239161 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.87       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.001       |
|    loss                 | 588         |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0031     |
|    std                  | 1.35        |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=272000, episode_reward=79.56 +/- 228.29
Episode length: 552.80 +/- 123.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 79.6         |
| time/                   |              |
|    total_timesteps      | 272000       |
| train/                  |              |
|    approx_kl            | 0.0034523574 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 339          |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00287     |
|    std                  | 1.35         |
|    value_loss           | 725          |
------------------------------------------
Eval num_timesteps=274000, episode_reward=182.08 +/- 232.84
Episode length: 490.80 +/- 81.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 274000      |
| train/                  |             |
|    approx_kl            | 0.010145984 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.001       |
|    loss                 | 433         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00323    |
|    std                  | 1.35        |
|    value_loss           | 992         |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=-7.25 +/- 130.46
Episode length: 455.40 +/- 81.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | -7.25        |
| time/                   |              |
|    total_timesteps      | 276000       |
| train/                  |              |
|    approx_kl            | 0.0030530398 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.553        |
|    learning_rate        | 0.001        |
|    loss                 | 821          |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000124    |
|    std                  | 1.35         |
|    value_loss           | 2.02e+03     |
------------------------------------------
Eval num_timesteps=278000, episode_reward=-7.85 +/- 132.73
Episode length: 517.80 +/- 107.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | -7.85        |
| time/                   |              |
|    total_timesteps      | 278000       |
| train/                  |              |
|    approx_kl            | 0.0013176766 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 363          |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.000738    |
|    std                  | 1.35         |
|    value_loss           | 778          |
------------------------------------------
Eval num_timesteps=280000, episode_reward=-80.51 +/- 128.84
Episode length: 466.40 +/- 61.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | -80.5        |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0014317759 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 471          |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.000815    |
|    std                  | 1.35         |
|    value_loss           | 1.08e+03     |
------------------------------------------
Eval num_timesteps=282000, episode_reward=171.27 +/- 229.90
Episode length: 451.60 +/- 64.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 282000       |
| train/                  |              |
|    approx_kl            | 0.0024725115 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 389          |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.35         |
|    value_loss           | 846          |
------------------------------------------
Eval num_timesteps=284000, episode_reward=820.32 +/- 996.68
Episode length: 695.00 +/- 175.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 695          |
|    mean_reward          | 820          |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 0.0052456176 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 429          |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.36         |
|    value_loss           | 928          |
------------------------------------------
New best mean reward!
Eval num_timesteps=286000, episode_reward=-2.89 +/- 81.18
Episode length: 499.80 +/- 86.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | -2.89        |
| time/                   |              |
|    total_timesteps      | 286000       |
| train/                  |              |
|    approx_kl            | 0.0052021574 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 308          |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.36         |
|    value_loss           | 649          |
------------------------------------------
Eval num_timesteps=288000, episode_reward=97.32 +/- 279.52
Episode length: 526.60 +/- 150.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 527          |
|    mean_reward          | 97.3         |
| time/                   |              |
|    total_timesteps      | 288000       |
| train/                  |              |
|    approx_kl            | 0.0032719953 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 396          |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.35         |
|    value_loss           | 819          |
------------------------------------------
Eval num_timesteps=290000, episode_reward=276.83 +/- 375.40
Episode length: 761.00 +/- 152.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 761          |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0060032527 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.001        |
|    loss                 | 430          |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.35         |
|    value_loss           | 909          |
------------------------------------------
Eval num_timesteps=292000, episode_reward=128.65 +/- 202.84
Episode length: 400.20 +/- 20.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 129          |
| time/                   |              |
|    total_timesteps      | 292000       |
| train/                  |              |
|    approx_kl            | 0.0056338715 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 450          |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.36         |
|    value_loss           | 981          |
------------------------------------------
Eval num_timesteps=294000, episode_reward=86.37 +/- 211.09
Episode length: 529.00 +/- 49.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | 86.4         |
| time/                   |              |
|    total_timesteps      | 294000       |
| train/                  |              |
|    approx_kl            | 0.0064053815 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.001        |
|    loss                 | 408          |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.36         |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=296000, episode_reward=40.07 +/- 164.59
Episode length: 426.00 +/- 73.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 40.1        |
| time/                   |             |
|    total_timesteps      | 296000      |
| train/                  |             |
|    approx_kl            | 0.012793954 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.89       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.001       |
|    loss                 | 428         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00434    |
|    std                  | 1.36        |
|    value_loss           | 940         |
-----------------------------------------
Eval num_timesteps=298000, episode_reward=147.16 +/- 265.14
Episode length: 572.80 +/- 97.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 573          |
|    mean_reward          | 147          |
| time/                   |              |
|    total_timesteps      | 298000       |
| train/                  |              |
|    approx_kl            | 0.0019136166 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.9         |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 329          |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00281     |
|    std                  | 1.36         |
|    value_loss           | 721          |
------------------------------------------
Eval num_timesteps=300000, episode_reward=33.10 +/- 144.49
Episode length: 544.20 +/- 121.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 33.1         |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0073652836 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.91        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 381          |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 1.37         |
|    value_loss           | 812          |
------------------------------------------
Eval num_timesteps=302000, episode_reward=-18.29 +/- 99.63
Episode length: 725.00 +/- 379.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 725          |
|    mean_reward          | -18.3        |
| time/                   |              |
|    total_timesteps      | 302000       |
| train/                  |              |
|    approx_kl            | 0.0046824925 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.92        |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.001        |
|    loss                 | 375          |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.37         |
|    value_loss           | 788          |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-34.82 +/- 58.32
Episode length: 547.40 +/- 97.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 547         |
|    mean_reward          | -34.8       |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.010044601 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.001       |
|    loss                 | 691         |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00395    |
|    std                  | 1.37        |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=-76.32 +/- 74.66
Episode length: 534.60 +/- 151.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 535         |
|    mean_reward          | -76.3       |
| time/                   |             |
|    total_timesteps      | 306000      |
| train/                  |             |
|    approx_kl            | 0.011305543 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.001       |
|    loss                 | 2.76e+03    |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0043     |
|    std                  | 1.37        |
|    value_loss           | 5.92e+03    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=-22.19 +/- 97.40
Episode length: 512.60 +/- 64.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | -22.2       |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.010959851 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 303         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00306    |
|    std                  | 1.37        |
|    value_loss           | 625         |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=-3.43 +/- 117.18
Episode length: 454.20 +/- 45.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | -3.43       |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.010687857 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.001       |
|    loss                 | 322         |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 1.37        |
|    value_loss           | 669         |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=169.43 +/- 294.36
Episode length: 502.80 +/- 106.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 312000       |
| train/                  |              |
|    approx_kl            | 0.0029224425 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.95        |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.001        |
|    loss                 | 516          |
|    n_updates            | 1520         |
|    policy_gradient_loss | -8.54e-06    |
|    std                  | 1.38         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=126.23 +/- 289.67
Episode length: 490.60 +/- 36.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 126         |
| time/                   |             |
|    total_timesteps      | 314000      |
| train/                  |             |
|    approx_kl            | 0.006205388 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 323         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00339    |
|    std                  | 1.38        |
|    value_loss           | 694         |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=-83.20 +/- 88.35
Episode length: 502.40 +/- 88.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -83.2       |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.007982632 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.001       |
|    loss                 | 802         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00228    |
|    std                  | 1.38        |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=116.62 +/- 176.75
Episode length: 501.00 +/- 126.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 501       |
|    mean_reward          | 117       |
| time/                   |           |
|    total_timesteps      | 318000    |
| train/                  |           |
|    approx_kl            | 0.0007255 |
|    clip_fraction        | 4.88e-05  |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.95     |
|    explained_variance   | 0.526     |
|    learning_rate        | 0.001     |
|    loss                 | 901       |
|    n_updates            | 1550      |
|    policy_gradient_loss | 0.000299  |
|    std                  | 1.38      |
|    value_loss           | 2.41e+03  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-101.72 +/- 31.97
Episode length: 400.20 +/- 70.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | -102         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0015801481 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.95        |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 392          |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.38         |
|    value_loss           | 924          |
------------------------------------------
Eval num_timesteps=322000, episode_reward=748.75 +/- 1280.37
Episode length: 555.20 +/- 119.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 555          |
|    mean_reward          | 749          |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0011186021 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 396          |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00021     |
|    std                  | 1.38         |
|    value_loss           | 932          |
------------------------------------------
Eval num_timesteps=324000, episode_reward=-69.76 +/- 52.97
Episode length: 429.40 +/- 77.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | -69.8        |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 0.0010462407 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 366          |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.000779    |
|    std                  | 1.38         |
|    value_loss           | 843          |
------------------------------------------
Eval num_timesteps=326000, episode_reward=35.64 +/- 89.59
Episode length: 404.20 +/- 94.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 35.6         |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0035266143 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.001        |
|    loss                 | 343          |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.38         |
|    value_loss           | 778          |
------------------------------------------
Eval num_timesteps=328000, episode_reward=-15.71 +/- 57.92
Episode length: 381.60 +/- 26.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | -15.7        |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0011457566 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.001        |
|    loss                 | 443          |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.000183    |
|    std                  | 1.38         |
|    value_loss           | 964          |
------------------------------------------
Eval num_timesteps=330000, episode_reward=-83.85 +/- 13.47
Episode length: 340.40 +/- 50.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | -83.8        |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0008899384 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 559          |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.38         |
|    value_loss           | 1.27e+03     |
------------------------------------------
Eval num_timesteps=332000, episode_reward=-17.63 +/- 126.67
Episode length: 349.60 +/- 73.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 350           |
|    mean_reward          | -17.6         |
| time/                   |               |
|    total_timesteps      | 332000        |
| train/                  |               |
|    approx_kl            | 0.00033134982 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.97         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 403           |
|    n_updates            | 1620          |
|    policy_gradient_loss | -0.000358     |
|    std                  | 1.38          |
|    value_loss           | 898           |
-------------------------------------------
Eval num_timesteps=334000, episode_reward=-68.18 +/- 20.22
Episode length: 332.40 +/- 42.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 332          |
|    mean_reward          | -68.2        |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0010306347 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.001        |
|    loss                 | 433          |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.38         |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=336000, episode_reward=-14.74 +/- 75.33
Episode length: 353.60 +/- 99.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 354         |
|    mean_reward          | -14.7       |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.005355803 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.001       |
|    loss                 | 475         |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 1.39        |
|    value_loss           | 1.01e+03    |
-----------------------------------------
Eval num_timesteps=338000, episode_reward=-91.25 +/- 38.18
Episode length: 319.20 +/- 62.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 319          |
|    mean_reward          | -91.3        |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0015049384 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 436          |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.000981    |
|    std                  | 1.39         |
|    value_loss           | 939          |
------------------------------------------
Eval num_timesteps=340000, episode_reward=-82.54 +/- 45.65
Episode length: 307.20 +/- 65.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -82.5        |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0020216196 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.98        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 465          |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000828    |
|    std                  | 1.39         |
|    value_loss           | 1.02e+03     |
------------------------------------------
Eval num_timesteps=342000, episode_reward=-64.97 +/- 65.81
Episode length: 309.80 +/- 81.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | -65      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-48.82 +/- 40.24
Episode length: 331.40 +/- 28.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 331           |
|    mean_reward          | -48.8         |
| time/                   |               |
|    total_timesteps      | 344000        |
| train/                  |               |
|    approx_kl            | 0.00040738942 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.98         |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.001         |
|    loss                 | 481           |
|    n_updates            | 1670          |
|    policy_gradient_loss | 1.52e-05      |
|    std                  | 1.39          |
|    value_loss           | 1e+03         |
-------------------------------------------
Eval num_timesteps=346000, episode_reward=-98.93 +/- 36.65
Episode length: 298.00 +/- 73.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 298         |
|    mean_reward          | -98.9       |
| time/                   |             |
|    total_timesteps      | 346000      |
| train/                  |             |
|    approx_kl            | 0.002647344 |
|    clip_fraction        | 0.00171     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 454         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1.39        |
|    value_loss           | 950         |
-----------------------------------------
Eval num_timesteps=348000, episode_reward=-67.34 +/- 50.04
Episode length: 314.80 +/- 49.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 315          |
|    mean_reward          | -67.3        |
| time/                   |              |
|    total_timesteps      | 348000       |
| train/                  |              |
|    approx_kl            | 0.0010290489 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.99        |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.001        |
|    loss                 | 413          |
|    n_updates            | 1690         |
|    policy_gradient_loss | 9.49e-05     |
|    std                  | 1.39         |
|    value_loss           | 906          |
------------------------------------------
Eval num_timesteps=350000, episode_reward=-77.85 +/- 34.23
Episode length: 307.60 +/- 65.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | -77.9       |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.008712612 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.001       |
|    loss                 | 405         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00652    |
|    std                  | 1.4         |
|    value_loss           | 830         |
-----------------------------------------
Eval num_timesteps=352000, episode_reward=-22.32 +/- 83.68
Episode length: 335.60 +/- 52.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 336          |
|    mean_reward          | -22.3        |
| time/                   |              |
|    total_timesteps      | 352000       |
| train/                  |              |
|    approx_kl            | 0.0027823336 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.001        |
|    loss                 | 467          |
|    n_updates            | 1710         |
|    policy_gradient_loss | 0.00027      |
|    std                  | 1.4          |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=354000, episode_reward=4.02 +/- 79.81
Episode length: 508.20 +/- 152.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 508         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 354000      |
| train/                  |             |
|    approx_kl            | 0.005648261 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 355         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00419    |
|    std                  | 1.4         |
|    value_loss           | 742         |
-----------------------------------------
Eval num_timesteps=356000, episode_reward=202.18 +/- 258.69
Episode length: 579.00 +/- 217.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 579         |
|    mean_reward          | 202         |
| time/                   |             |
|    total_timesteps      | 356000      |
| train/                  |             |
|    approx_kl            | 0.005350678 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.001       |
|    loss                 | 665         |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00351    |
|    std                  | 1.4         |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=358000, episode_reward=-61.96 +/- 56.15
Episode length: 350.40 +/- 56.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 350         |
|    mean_reward          | -62         |
| time/                   |             |
|    total_timesteps      | 358000      |
| train/                  |             |
|    approx_kl            | 0.006055122 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.02       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.001       |
|    loss                 | 3.18e+03    |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00236    |
|    std                  | 1.4         |
|    value_loss           | 6.79e+03    |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=288.18 +/- 216.49
Episode length: 529.60 +/- 138.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 530         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.008128834 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.001       |
|    loss                 | 346         |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.003      |
|    std                  | 1.41        |
|    value_loss           | 744         |
-----------------------------------------
Eval num_timesteps=362000, episode_reward=151.20 +/- 287.17
Episode length: 382.60 +/- 80.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 362000      |
| train/                  |             |
|    approx_kl            | 0.008015005 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.001       |
|    loss                 | 409         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0035     |
|    std                  | 1.41        |
|    value_loss           | 937         |
-----------------------------------------
Eval num_timesteps=364000, episode_reward=230.02 +/- 243.88
Episode length: 392.60 +/- 78.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 230          |
| time/                   |              |
|    total_timesteps      | 364000       |
| train/                  |              |
|    approx_kl            | 0.0062843594 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 361          |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 1.41         |
|    value_loss           | 837          |
------------------------------------------
Eval num_timesteps=366000, episode_reward=143.30 +/- 87.22
Episode length: 356.80 +/- 53.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | 143          |
| time/                   |              |
|    total_timesteps      | 366000       |
| train/                  |              |
|    approx_kl            | 0.0076090004 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.001        |
|    loss                 | 441          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.41         |
|    value_loss           | 1.03e+03     |
------------------------------------------
Eval num_timesteps=368000, episode_reward=56.50 +/- 46.79
Episode length: 311.60 +/- 24.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 312         |
|    mean_reward          | 56.5        |
| time/                   |             |
|    total_timesteps      | 368000      |
| train/                  |             |
|    approx_kl            | 0.007895699 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.06       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.001       |
|    loss                 | 418         |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00351    |
|    std                  | 1.42        |
|    value_loss           | 867         |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=20.59 +/- 49.70
Episode length: 298.80 +/- 26.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | 20.6         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0036658656 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.08        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 359          |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000845    |
|    std                  | 1.42         |
|    value_loss           | 751          |
------------------------------------------
Eval num_timesteps=372000, episode_reward=-5.65 +/- 42.88
Episode length: 279.60 +/- 27.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | -5.65        |
| time/                   |              |
|    total_timesteps      | 372000       |
| train/                  |              |
|    approx_kl            | 0.0076539298 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.09        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 422          |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.43         |
|    value_loss           | 874          |
------------------------------------------
Eval num_timesteps=374000, episode_reward=74.99 +/- 62.15
Episode length: 335.00 +/- 39.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 75          |
| time/                   |             |
|    total_timesteps      | 374000      |
| train/                  |             |
|    approx_kl            | 0.007439552 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.001       |
|    loss                 | 407         |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00197    |
|    std                  | 1.43        |
|    value_loss           | 845         |
-----------------------------------------
Eval num_timesteps=376000, episode_reward=262.99 +/- 126.92
Episode length: 365.20 +/- 22.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 263          |
| time/                   |              |
|    total_timesteps      | 376000       |
| train/                  |              |
|    approx_kl            | 0.0071760644 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 308          |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.44         |
|    value_loss           | 670          |
------------------------------------------
Eval num_timesteps=378000, episode_reward=109.17 +/- 87.02
Episode length: 339.20 +/- 49.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 339        |
|    mean_reward          | 109        |
| time/                   |            |
|    total_timesteps      | 378000     |
| train/                  |            |
|    approx_kl            | 0.01633483 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.13      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.001      |
|    loss                 | 356        |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.00284   |
|    std                  | 1.44       |
|    value_loss           | 734        |
----------------------------------------
Eval num_timesteps=380000, episode_reward=289.72 +/- 326.70
Episode length: 424.00 +/- 64.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 290          |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0016861511 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.13        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 378          |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.000529    |
|    std                  | 1.44         |
|    value_loss           | 821          |
------------------------------------------
Eval num_timesteps=382000, episode_reward=60.72 +/- 95.12
Episode length: 316.20 +/- 49.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 316         |
|    mean_reward          | 60.7        |
| time/                   |             |
|    total_timesteps      | 382000      |
| train/                  |             |
|    approx_kl            | 0.008169267 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 350         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 1.44        |
|    value_loss           | 837         |
-----------------------------------------
Eval num_timesteps=384000, episode_reward=107.09 +/- 80.53
Episode length: 363.00 +/- 53.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 363         |
|    mean_reward          | 107         |
| time/                   |             |
|    total_timesteps      | 384000      |
| train/                  |             |
|    approx_kl            | 0.004138033 |
|    clip_fraction        | 0.00483     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.001       |
|    loss                 | 302         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00173    |
|    std                  | 1.45        |
|    value_loss           | 690         |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=125.76 +/- 111.80
Episode length: 343.60 +/- 46.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 386000       |
| train/                  |              |
|    approx_kl            | 0.0031559383 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.14        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 408          |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.45         |
|    value_loss           | 925          |
------------------------------------------
Eval num_timesteps=388000, episode_reward=325.20 +/- 381.92
Episode length: 526.00 +/- 114.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 388000       |
| train/                  |              |
|    approx_kl            | 0.0021585198 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+03     |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.45         |
|    value_loss           | 4.73e+03     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=378.61 +/- 906.10
Episode length: 514.80 +/- 91.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 379          |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0021586139 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.001        |
|    loss                 | 871          |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.45         |
|    value_loss           | 2.42e+03     |
------------------------------------------
Eval num_timesteps=392000, episode_reward=94.12 +/- 158.49
Episode length: 420.20 +/- 81.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 94.1         |
| time/                   |              |
|    total_timesteps      | 392000       |
| train/                  |              |
|    approx_kl            | 0.0050351378 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.001        |
|    loss                 | 660          |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 1.45         |
|    value_loss           | 1.4e+03      |
------------------------------------------
Eval num_timesteps=394000, episode_reward=51.28 +/- 65.75
Episode length: 427.80 +/- 83.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 51.3         |
| time/                   |              |
|    total_timesteps      | 394000       |
| train/                  |              |
|    approx_kl            | 0.0072064097 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.001        |
|    loss                 | 2.86e+03     |
|    n_updates            | 1920         |
|    policy_gradient_loss | 0.0019       |
|    std                  | 1.45         |
|    value_loss           | 6.07e+03     |
------------------------------------------
Eval num_timesteps=396000, episode_reward=45.98 +/- 70.26
Episode length: 436.00 +/- 81.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 46           |
| time/                   |              |
|    total_timesteps      | 396000       |
| train/                  |              |
|    approx_kl            | 0.0021037357 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.46         |
|    value_loss           | 714          |
------------------------------------------
Eval num_timesteps=398000, episode_reward=260.62 +/- 356.91
Episode length: 471.80 +/- 60.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 472         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 398000      |
| train/                  |             |
|    approx_kl            | 0.005576683 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 291         |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00245    |
|    std                  | 1.45        |
|    value_loss           | 656         |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=393.33 +/- 199.50
Episode length: 495.00 +/- 53.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 393         |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.003994991 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.001       |
|    loss                 | 414         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 1.45        |
|    value_loss           | 1.07e+03    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=279.45 +/- 299.82
Episode length: 415.60 +/- 46.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 279          |
| time/                   |              |
|    total_timesteps      | 402000       |
| train/                  |              |
|    approx_kl            | 0.0020899673 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 256          |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.000836    |
|    std                  | 1.45         |
|    value_loss           | 668          |
------------------------------------------
Eval num_timesteps=404000, episode_reward=63.74 +/- 76.46
Episode length: 422.20 +/- 37.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 63.7         |
| time/                   |              |
|    total_timesteps      | 404000       |
| train/                  |              |
|    approx_kl            | 0.0037494907 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 339          |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.45         |
|    value_loss           | 823          |
------------------------------------------
Eval num_timesteps=406000, episode_reward=305.74 +/- 222.90
Episode length: 523.40 +/- 132.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 523          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0025222902 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 347          |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 1.45         |
|    value_loss           | 743          |
------------------------------------------
Eval num_timesteps=408000, episode_reward=56.90 +/- 134.44
Episode length: 503.80 +/- 112.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 504        |
|    mean_reward          | 56.9       |
| time/                   |            |
|    total_timesteps      | 408000     |
| train/                  |            |
|    approx_kl            | 0.00436365 |
|    clip_fraction        | 0.00933    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.16      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.001      |
|    loss                 | 328        |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.000968  |
|    std                  | 1.45       |
|    value_loss           | 779        |
----------------------------------------
Eval num_timesteps=410000, episode_reward=50.82 +/- 132.53
Episode length: 531.20 +/- 127.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 531          |
|    mean_reward          | 50.8         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0063901255 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1.45         |
|    value_loss           | 5.12e+03     |
------------------------------------------
Eval num_timesteps=412000, episode_reward=206.51 +/- 123.77
Episode length: 406.20 +/- 35.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.009649187 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.001       |
|    loss                 | 1.58e+03    |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 1.45        |
|    value_loss           | 3.23e+03    |
-----------------------------------------
Eval num_timesteps=414000, episode_reward=267.26 +/- 157.85
Episode length: 373.40 +/- 28.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 373         |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.008500237 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.001       |
|    loss                 | 695         |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 1.46        |
|    value_loss           | 1.59e+03    |
-----------------------------------------
Eval num_timesteps=416000, episode_reward=87.25 +/- 116.52
Episode length: 372.20 +/- 75.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 372          |
|    mean_reward          | 87.2         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0038054036 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.001        |
|    loss                 | 519          |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.46         |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=418000, episode_reward=191.50 +/- 162.28
Episode length: 390.00 +/- 62.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 390         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.005688837 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.17       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 371         |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00237    |
|    std                  | 1.46        |
|    value_loss           | 838         |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=51.31 +/- 38.91
Episode length: 344.00 +/- 21.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 51.3         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0064794635 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 284          |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.000974    |
|    std                  | 1.46         |
|    value_loss           | 611          |
------------------------------------------
Eval num_timesteps=422000, episode_reward=279.95 +/- 135.57
Episode length: 382.20 +/- 40.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 382         |
|    mean_reward          | 280         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.014271381 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 317         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00288    |
|    std                  | 1.46        |
|    value_loss           | 656         |
-----------------------------------------
Eval num_timesteps=424000, episode_reward=272.59 +/- 241.32
Episode length: 470.00 +/- 98.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | 273          |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0019747429 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 466          |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.46         |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=426000, episode_reward=212.82 +/- 117.73
Episode length: 437.20 +/- 92.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 213          |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0052534235 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 310          |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.46         |
|    value_loss           | 723          |
------------------------------------------
Eval num_timesteps=428000, episode_reward=99.48 +/- 159.38
Episode length: 370.60 +/- 85.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 99.5     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=115.00 +/- 100.80
Episode length: 368.40 +/- 40.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 115         |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.012849828 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.18       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.001       |
|    loss                 | 682         |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00229    |
|    std                  | 1.46        |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=432000, episode_reward=179.18 +/- 78.65
Episode length: 366.40 +/- 31.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 366        |
|    mean_reward          | 179        |
| time/                   |            |
|    total_timesteps      | 432000     |
| train/                  |            |
|    approx_kl            | 0.00985703 |
|    clip_fraction        | 0.049      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.2       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.001      |
|    loss                 | 333        |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.00246   |
|    std                  | 1.47       |
|    value_loss           | 751        |
----------------------------------------
Eval num_timesteps=434000, episode_reward=-20.29 +/- 43.36
Episode length: 276.80 +/- 41.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | -20.3       |
| time/                   |             |
|    total_timesteps      | 434000      |
| train/                  |             |
|    approx_kl            | 0.012275461 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 445         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0032     |
|    std                  | 1.48        |
|    value_loss           | 988         |
-----------------------------------------
Eval num_timesteps=436000, episode_reward=99.14 +/- 190.46
Episode length: 306.20 +/- 53.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 306          |
|    mean_reward          | 99.1         |
| time/                   |              |
|    total_timesteps      | 436000       |
| train/                  |              |
|    approx_kl            | 0.0023390222 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.23        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 353          |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.48         |
|    value_loss           | 749          |
------------------------------------------
Eval num_timesteps=438000, episode_reward=148.52 +/- 104.00
Episode length: 354.60 +/- 47.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 438000      |
| train/                  |             |
|    approx_kl            | 0.008203257 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.001       |
|    loss                 | 339         |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00192    |
|    std                  | 1.48        |
|    value_loss           | 697         |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=49.95 +/- 94.07
Episode length: 293.00 +/- 37.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 49.9         |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0036533114 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 272          |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000571    |
|    std                  | 1.49         |
|    value_loss           | 641          |
------------------------------------------
Eval num_timesteps=442000, episode_reward=-19.36 +/- 94.68
Episode length: 295.80 +/- 75.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | -19.4        |
| time/                   |              |
|    total_timesteps      | 442000       |
| train/                  |              |
|    approx_kl            | 0.0044369786 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 308          |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000534    |
|    std                  | 1.49         |
|    value_loss           | 683          |
------------------------------------------
Eval num_timesteps=444000, episode_reward=-19.97 +/- 36.21
Episode length: 272.00 +/- 24.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | -20         |
| time/                   |             |
|    total_timesteps      | 444000      |
| train/                  |             |
|    approx_kl            | 0.002935798 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 318         |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.005      |
|    std                  | 1.49        |
|    value_loss           | 666         |
-----------------------------------------
Eval num_timesteps=446000, episode_reward=22.26 +/- 49.32
Episode length: 295.20 +/- 37.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 446000      |
| train/                  |             |
|    approx_kl            | 0.009162151 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 355         |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00301    |
|    std                  | 1.48        |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=448000, episode_reward=-27.71 +/- 81.61
Episode length: 257.80 +/- 48.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 258         |
|    mean_reward          | -27.7       |
| time/                   |             |
|    total_timesteps      | 448000      |
| train/                  |             |
|    approx_kl            | 0.005244797 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 269         |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00242    |
|    std                  | 1.48        |
|    value_loss           | 579         |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=38.47 +/- 35.23
Episode length: 297.00 +/- 28.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 297         |
|    mean_reward          | 38.5        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.009292971 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 346         |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00454    |
|    std                  | 1.48        |
|    value_loss           | 706         |
-----------------------------------------
Eval num_timesteps=452000, episode_reward=51.74 +/- 48.30
Episode length: 302.00 +/- 29.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 51.7         |
| time/                   |              |
|    total_timesteps      | 452000       |
| train/                  |              |
|    approx_kl            | 0.0070262128 |
|    clip_fraction        | 0.0925       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 273          |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00264     |
|    std                  | 1.49         |
|    value_loss           | 588          |
------------------------------------------
Eval num_timesteps=454000, episode_reward=-28.03 +/- 54.01
Episode length: 252.00 +/- 37.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | -28        |
| time/                   |            |
|    total_timesteps      | 454000     |
| train/                  |            |
|    approx_kl            | 0.00674681 |
|    clip_fraction        | 0.0243     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.25      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.001      |
|    loss                 | 340        |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.000543  |
|    std                  | 1.49       |
|    value_loss           | 696        |
----------------------------------------
Eval num_timesteps=456000, episode_reward=-14.97 +/- 65.76
Episode length: 265.20 +/- 41.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | -15         |
| time/                   |             |
|    total_timesteps      | 456000      |
| train/                  |             |
|    approx_kl            | 0.010559868 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 276         |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 1.5         |
|    value_loss           | 571         |
-----------------------------------------
Eval num_timesteps=458000, episode_reward=36.14 +/- 70.82
Episode length: 308.60 +/- 59.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 36.1        |
| time/                   |             |
|    total_timesteps      | 458000      |
| train/                  |             |
|    approx_kl            | 0.006811396 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 266         |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.00104     |
|    std                  | 1.5         |
|    value_loss           | 533         |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=-2.10 +/- 24.34
Episode length: 277.80 +/- 24.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | -2.1        |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.016590592 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.28       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 269         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00587    |
|    std                  | 1.5         |
|    value_loss           | 553         |
-----------------------------------------
Eval num_timesteps=462000, episode_reward=-7.27 +/- 47.09
Episode length: 326.00 +/- 17.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | -7.27        |
| time/                   |              |
|    total_timesteps      | 462000       |
| train/                  |              |
|    approx_kl            | 0.0061897123 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.29        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 292          |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.000176    |
|    std                  | 1.51         |
|    value_loss           | 588          |
------------------------------------------
Eval num_timesteps=464000, episode_reward=83.28 +/- 78.43
Episode length: 375.00 +/- 39.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 83.3         |
| time/                   |              |
|    total_timesteps      | 464000       |
| train/                  |              |
|    approx_kl            | 0.0072608865 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 248          |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00389     |
|    std                  | 1.51         |
|    value_loss           | 531          |
------------------------------------------
Eval num_timesteps=466000, episode_reward=55.47 +/- 69.44
Episode length: 387.20 +/- 89.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 55.5        |
| time/                   |             |
|    total_timesteps      | 466000      |
| train/                  |             |
|    approx_kl            | 0.011481993 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 247         |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0022     |
|    std                  | 1.52        |
|    value_loss           | 614         |
-----------------------------------------
Eval num_timesteps=468000, episode_reward=68.60 +/- 85.98
Episode length: 387.00 +/- 86.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 68.6        |
| time/                   |             |
|    total_timesteps      | 468000      |
| train/                  |             |
|    approx_kl            | 0.005554274 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.001       |
|    loss                 | 322         |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.52        |
|    value_loss           | 867         |
-----------------------------------------
Eval num_timesteps=470000, episode_reward=-22.59 +/- 54.20
Episode length: 313.40 +/- 31.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | -22.6       |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.008320047 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.001       |
|    loss                 | 287         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00245    |
|    std                  | 1.53        |
|    value_loss           | 709         |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=20.97 +/- 80.81
Episode length: 331.00 +/- 35.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 331         |
|    mean_reward          | 21          |
| time/                   |             |
|    total_timesteps      | 472000      |
| train/                  |             |
|    approx_kl            | 0.013751907 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 242         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 1.53        |
|    value_loss           | 519         |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=18.91 +/- 76.97
Episode length: 321.40 +/- 60.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | 18.9         |
| time/                   |              |
|    total_timesteps      | 474000       |
| train/                  |              |
|    approx_kl            | 0.0061612483 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.37        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 277          |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.53         |
|    value_loss           | 578          |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-23.73 +/- 35.12
Episode length: 343.40 +/- 23.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | -23.7        |
| time/                   |              |
|    total_timesteps      | 476000       |
| train/                  |              |
|    approx_kl            | 0.0046716877 |
|    clip_fraction        | 0.0988       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 226          |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00465     |
|    std                  | 1.53         |
|    value_loss           | 471          |
------------------------------------------
Eval num_timesteps=478000, episode_reward=69.99 +/- 158.87
Episode length: 345.40 +/- 58.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 345        |
|    mean_reward          | 70         |
| time/                   |            |
|    total_timesteps      | 478000     |
| train/                  |            |
|    approx_kl            | 0.00749333 |
|    clip_fraction        | 0.0646     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.36      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.001      |
|    loss                 | 238        |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.00109   |
|    std                  | 1.53       |
|    value_loss           | 569        |
----------------------------------------
Eval num_timesteps=480000, episode_reward=-2.03 +/- 30.94
Episode length: 325.00 +/- 31.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | -2.03        |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0043336246 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.001        |
|    loss                 | 317          |
|    n_updates            | 2340         |
|    policy_gradient_loss | 0.000355     |
|    std                  | 1.52         |
|    value_loss           | 700          |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-6.73 +/- 63.76
Episode length: 334.60 +/- 13.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 335        |
|    mean_reward          | -6.73      |
| time/                   |            |
|    total_timesteps      | 482000     |
| train/                  |            |
|    approx_kl            | 0.01001673 |
|    clip_fraction        | 0.0406     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.34      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.001      |
|    loss                 | 230        |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.00251   |
|    std                  | 1.52       |
|    value_loss           | 472        |
----------------------------------------
Eval num_timesteps=484000, episode_reward=-9.48 +/- 31.74
Episode length: 363.40 +/- 34.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 363        |
|    mean_reward          | -9.48      |
| time/                   |            |
|    total_timesteps      | 484000     |
| train/                  |            |
|    approx_kl            | 0.01712261 |
|    clip_fraction        | 0.0775     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.34      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.001      |
|    loss                 | 253        |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.00382   |
|    std                  | 1.52       |
|    value_loss           | 532        |
----------------------------------------
Eval num_timesteps=486000, episode_reward=-5.34 +/- 32.77
Episode length: 321.80 +/- 20.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | -5.34        |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0050222417 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 187          |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.53         |
|    value_loss           | 396          |
------------------------------------------
Eval num_timesteps=488000, episode_reward=143.42 +/- 127.10
Episode length: 373.80 +/- 17.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 488000      |
| train/                  |             |
|    approx_kl            | 0.003971966 |
|    clip_fraction        | 0.00796     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.38       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 1.54        |
|    value_loss           | 423         |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=228.71 +/- 166.50
Episode length: 404.40 +/- 79.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 229          |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0060114157 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 194          |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.55         |
|    value_loss           | 462          |
------------------------------------------
Eval num_timesteps=492000, episode_reward=95.55 +/- 62.60
Episode length: 327.60 +/- 38.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | 95.5        |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.008118457 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.4        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 200         |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 1.55        |
|    value_loss           | 425         |
-----------------------------------------
Eval num_timesteps=494000, episode_reward=63.33 +/- 24.46
Episode length: 314.80 +/- 15.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 315         |
|    mean_reward          | 63.3        |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.007551426 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.41       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 310         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1.55        |
|    value_loss           | 716         |
-----------------------------------------
Eval num_timesteps=496000, episode_reward=60.02 +/- 84.15
Episode length: 321.60 +/- 54.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | 60          |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.003711741 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.001       |
|    loss                 | 2.17e+03    |
|    n_updates            | 2420        |
|    policy_gradient_loss | -7.74e-05   |
|    std                  | 1.56        |
|    value_loss           | 4.69e+03    |
-----------------------------------------
Eval num_timesteps=498000, episode_reward=45.13 +/- 31.00
Episode length: 308.20 +/- 34.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | 45.1         |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0027038115 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 252          |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.000846    |
|    std                  | 1.56         |
|    value_loss           | 548          |
------------------------------------------
Eval num_timesteps=500000, episode_reward=29.88 +/- 67.71
Episode length: 293.00 +/- 19.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 29.9         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0049148216 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.43        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 272          |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.56         |
|    value_loss           | 571          |
------------------------------------------
Eval num_timesteps=502000, episode_reward=-79.71 +/- 31.20
Episode length: 217.40 +/- 22.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 217         |
|    mean_reward          | -79.7       |
| time/                   |             |
|    total_timesteps      | 502000      |
| train/                  |             |
|    approx_kl            | 0.009722265 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 277         |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00337    |
|    std                  | 1.56        |
|    value_loss           | 564         |
-----------------------------------------
Eval num_timesteps=504000, episode_reward=35.15 +/- 16.41
Episode length: 296.40 +/- 17.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 296         |
|    mean_reward          | 35.1        |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 0.008625937 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 253         |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00192    |
|    std                  | 1.56        |
|    value_loss           | 520         |
-----------------------------------------
Eval num_timesteps=506000, episode_reward=56.56 +/- 102.96
Episode length: 320.40 +/- 50.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 320         |
|    mean_reward          | 56.6        |
| time/                   |             |
|    total_timesteps      | 506000      |
| train/                  |             |
|    approx_kl            | 0.010759395 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 195         |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00278    |
|    std                  | 1.56        |
|    value_loss           | 411         |
-----------------------------------------
Eval num_timesteps=508000, episode_reward=4.55 +/- 34.58
Episode length: 290.80 +/- 37.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 4.55         |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 0.0043031573 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 225          |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 1.57         |
|    value_loss           | 478          |
------------------------------------------
Eval num_timesteps=510000, episode_reward=-6.52 +/- 66.67
Episode length: 297.20 +/- 40.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 297         |
|    mean_reward          | -6.52       |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.003318714 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.46       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 185         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 1.57        |
|    value_loss           | 387         |
-----------------------------------------
Eval num_timesteps=512000, episode_reward=31.27 +/- 88.64
Episode length: 311.00 +/- 48.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 31.3     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=514000, episode_reward=21.93 +/- 47.77
Episode length: 333.00 +/- 30.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 333          |
|    mean_reward          | 21.9         |
| time/                   |              |
|    total_timesteps      | 514000       |
| train/                  |              |
|    approx_kl            | 0.0030769943 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 203          |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.58         |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=516000, episode_reward=68.71 +/- 42.15
Episode length: 334.80 +/- 15.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 68.7        |
| time/                   |             |
|    total_timesteps      | 516000      |
| train/                  |             |
|    approx_kl            | 0.005913495 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.48       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 193         |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 1.58        |
|    value_loss           | 405         |
-----------------------------------------
Eval num_timesteps=518000, episode_reward=4.65 +/- 97.59
Episode length: 323.20 +/- 26.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | 4.65         |
| time/                   |              |
|    total_timesteps      | 518000       |
| train/                  |              |
|    approx_kl            | 0.0062351124 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 186          |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.58         |
|    value_loss           | 436          |
------------------------------------------
Eval num_timesteps=520000, episode_reward=117.38 +/- 76.56
Episode length: 373.20 +/- 27.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 117          |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0051632207 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 179          |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 1.59         |
|    value_loss           | 379          |
------------------------------------------
Eval num_timesteps=522000, episode_reward=78.08 +/- 73.39
Episode length: 346.00 +/- 10.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | 78.1         |
| time/                   |              |
|    total_timesteps      | 522000       |
| train/                  |              |
|    approx_kl            | 0.0052644736 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 179          |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 1.59         |
|    value_loss           | 387          |
------------------------------------------
Eval num_timesteps=524000, episode_reward=61.41 +/- 75.42
Episode length: 370.00 +/- 63.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 61.4        |
| time/                   |             |
|    total_timesteps      | 524000      |
| train/                  |             |
|    approx_kl            | 0.010292922 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.51       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 155         |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 1.6         |
|    value_loss           | 346         |
-----------------------------------------
Eval num_timesteps=526000, episode_reward=115.81 +/- 334.08
Episode length: 409.20 +/- 51.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 116          |
| time/                   |              |
|    total_timesteps      | 526000       |
| train/                  |              |
|    approx_kl            | 0.0038260787 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.52        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 2560         |
|    policy_gradient_loss | 0.000535     |
|    std                  | 1.6          |
|    value_loss           | 476          |
------------------------------------------
Eval num_timesteps=528000, episode_reward=43.16 +/- 112.60
Episode length: 376.80 +/- 17.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 43.2         |
| time/                   |              |
|    total_timesteps      | 528000       |
| train/                  |              |
|    approx_kl            | 0.0068743946 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 164          |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00492     |
|    std                  | 1.6          |
|    value_loss           | 343          |
------------------------------------------
Eval num_timesteps=530000, episode_reward=7.63 +/- 102.86
Episode length: 376.60 +/- 41.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 7.63         |
| time/                   |              |
|    total_timesteps      | 530000       |
| train/                  |              |
|    approx_kl            | 0.0063908016 |
|    clip_fraction        | 0.0905       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 365          |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.61         |
|    value_loss           | 792          |
------------------------------------------
Eval num_timesteps=532000, episode_reward=83.95 +/- 172.93
Episode length: 393.60 +/- 31.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 83.9         |
| time/                   |              |
|    total_timesteps      | 532000       |
| train/                  |              |
|    approx_kl            | 0.0067078434 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.61         |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=534000, episode_reward=177.92 +/- 410.71
Episode length: 453.20 +/- 73.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 534000       |
| train/                  |              |
|    approx_kl            | 0.0049908245 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.55        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00323     |
|    std                  | 1.62         |
|    value_loss           | 428          |
------------------------------------------
Eval num_timesteps=536000, episode_reward=51.94 +/- 212.51
Episode length: 440.40 +/- 76.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 51.9         |
| time/                   |              |
|    total_timesteps      | 536000       |
| train/                  |              |
|    approx_kl            | 0.0069323867 |
|    clip_fraction        | 0.0619       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 1.62         |
|    value_loss           | 326          |
------------------------------------------
Eval num_timesteps=538000, episode_reward=289.44 +/- 466.38
Episode length: 453.00 +/- 77.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 289          |
| time/                   |              |
|    total_timesteps      | 538000       |
| train/                  |              |
|    approx_kl            | 0.0075650816 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.63         |
|    value_loss           | 289          |
------------------------------------------
Eval num_timesteps=540000, episode_reward=-11.94 +/- 59.06
Episode length: 455.60 +/- 48.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | -11.9       |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.007902976 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 135         |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00129    |
|    std                  | 1.63        |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=542000, episode_reward=190.82 +/- 287.61
Episode length: 405.60 +/- 21.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 542000      |
| train/                  |             |
|    approx_kl            | 0.006455021 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 200         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000554   |
|    std                  | 1.63        |
|    value_loss           | 491         |
-----------------------------------------
Eval num_timesteps=544000, episode_reward=649.81 +/- 392.18
Episode length: 562.80 +/- 172.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 563          |
|    mean_reward          | 650          |
| time/                   |              |
|    total_timesteps      | 544000       |
| train/                  |              |
|    approx_kl            | 0.0023410493 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.000842    |
|    std                  | 1.63         |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=546000, episode_reward=66.24 +/- 83.75
Episode length: 471.60 +/- 82.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 472         |
|    mean_reward          | 66.2        |
| time/                   |             |
|    total_timesteps      | 546000      |
| train/                  |             |
|    approx_kl            | 0.006828145 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 1.62        |
|    value_loss           | 347         |
-----------------------------------------
Eval num_timesteps=548000, episode_reward=359.93 +/- 334.49
Episode length: 507.40 +/- 79.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 507          |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 548000       |
| train/                  |              |
|    approx_kl            | 0.0015022821 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 2.48e+03     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.000646    |
|    std                  | 1.63         |
|    value_loss           | 5.62e+03     |
------------------------------------------
Eval num_timesteps=550000, episode_reward=114.44 +/- 221.70
Episode length: 455.00 +/- 64.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 114           |
| time/                   |               |
|    total_timesteps      | 550000        |
| train/                  |               |
|    approx_kl            | 0.00037095856 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.58         |
|    explained_variance   | 0.685         |
|    learning_rate        | 0.001         |
|    loss                 | 2.26e+03      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.000101     |
|    std                  | 1.63          |
|    value_loss           | 5.26e+03      |
-------------------------------------------
Eval num_timesteps=552000, episode_reward=254.15 +/- 120.52
Episode length: 490.80 +/- 70.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 552000      |
| train/                  |             |
|    approx_kl            | 0.000512372 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.001       |
|    loss                 | 1.72e+03    |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.000362   |
|    std                  | 1.63        |
|    value_loss           | 3.67e+03    |
-----------------------------------------
Eval num_timesteps=554000, episode_reward=134.57 +/- 214.66
Episode length: 421.40 +/- 33.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 421        |
|    mean_reward          | 135        |
| time/                   |            |
|    total_timesteps      | 554000     |
| train/                  |            |
|    approx_kl            | 0.01440182 |
|    clip_fraction        | 0.0565     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.001      |
|    loss                 | 196        |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.00523   |
|    std                  | 1.63       |
|    value_loss           | 465        |
----------------------------------------
Eval num_timesteps=556000, episode_reward=98.25 +/- 152.20
Episode length: 356.40 +/- 35.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | 98.3        |
| time/                   |             |
|    total_timesteps      | 556000      |
| train/                  |             |
|    approx_kl            | 0.008475509 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 134         |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00276    |
|    std                  | 1.63        |
|    value_loss           | 307         |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=181.93 +/- 277.62
Episode length: 348.00 +/- 29.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 348         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 558000      |
| train/                  |             |
|    approx_kl            | 0.005685426 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 169         |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1.63        |
|    value_loss           | 451         |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=253.02 +/- 255.66
Episode length: 341.60 +/- 47.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 342          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0015030067 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 277          |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.000116    |
|    std                  | 1.63         |
|    value_loss           | 591          |
------------------------------------------
Eval num_timesteps=562000, episode_reward=195.60 +/- 174.47
Episode length: 331.60 +/- 59.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 332         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 562000      |
| train/                  |             |
|    approx_kl            | 0.006977815 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 171         |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.64        |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=564000, episode_reward=129.47 +/- 66.55
Episode length: 358.20 +/- 37.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 564000      |
| train/                  |             |
|    approx_kl            | 0.005753395 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 188         |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 1.64        |
|    value_loss           | 428         |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=328.57 +/- 381.13
Episode length: 361.60 +/- 60.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 566000      |
| train/                  |             |
|    approx_kl            | 0.003988599 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.001       |
|    loss                 | 2.43e+03    |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00271    |
|    std                  | 1.64        |
|    value_loss           | 5.34e+03    |
-----------------------------------------
Eval num_timesteps=568000, episode_reward=252.28 +/- 251.77
Episode length: 374.00 +/- 24.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | 252          |
| time/                   |              |
|    total_timesteps      | 568000       |
| train/                  |              |
|    approx_kl            | 0.0039190124 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 220          |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.64         |
|    value_loss           | 577          |
------------------------------------------
Eval num_timesteps=570000, episode_reward=266.81 +/- 171.68
Episode length: 404.00 +/- 42.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.001497282 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 218         |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.000501   |
|    std                  | 1.65        |
|    value_loss           | 562         |
-----------------------------------------
Eval num_timesteps=572000, episode_reward=176.87 +/- 243.87
Episode length: 373.40 +/- 79.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 572000       |
| train/                  |              |
|    approx_kl            | 0.0034485245 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 170          |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.65         |
|    value_loss           | 378          |
------------------------------------------
Eval num_timesteps=574000, episode_reward=375.29 +/- 208.56
Episode length: 400.40 +/- 18.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 375         |
| time/                   |             |
|    total_timesteps      | 574000      |
| train/                  |             |
|    approx_kl            | 0.006905477 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 156         |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00193    |
|    std                  | 1.65        |
|    value_loss           | 363         |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=175.43 +/- 115.20
Episode length: 371.60 +/- 18.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 372          |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 576000       |
| train/                  |              |
|    approx_kl            | 0.0067346953 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 245          |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.000812    |
|    std                  | 1.65         |
|    value_loss           | 563          |
------------------------------------------
Eval num_timesteps=578000, episode_reward=202.86 +/- 195.41
Episode length: 433.20 +/- 29.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0065986314 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.64         |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=580000, episode_reward=333.41 +/- 273.35
Episode length: 383.40 +/- 25.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 333          |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0018433788 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.001        |
|    loss                 | 2.86e+03     |
|    n_updates            | 2830         |
|    policy_gradient_loss | 0.00115      |
|    std                  | 1.64         |
|    value_loss           | 6.74e+03     |
------------------------------------------
Eval num_timesteps=582000, episode_reward=317.59 +/- 630.77
Episode length: 463.40 +/- 58.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 463           |
|    mean_reward          | 318           |
| time/                   |               |
|    total_timesteps      | 582000        |
| train/                  |               |
|    approx_kl            | 4.8945512e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.63         |
|    explained_variance   | 0.766         |
|    learning_rate        | 0.001         |
|    loss                 | 2.76e+03      |
|    n_updates            | 2840          |
|    policy_gradient_loss | -0.000186     |
|    std                  | 1.64          |
|    value_loss           | 5.69e+03      |
-------------------------------------------
Eval num_timesteps=584000, episode_reward=284.47 +/- 159.40
Episode length: 453.60 +/- 62.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 9.252876e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.704        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+03      |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.000137    |
|    std                  | 1.64         |
|    value_loss           | 6.2e+03      |
------------------------------------------
Eval num_timesteps=586000, episode_reward=267.54 +/- 360.43
Episode length: 397.00 +/- 49.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0028884679 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 157          |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.65         |
|    value_loss           | 494          |
------------------------------------------
Eval num_timesteps=588000, episode_reward=284.81 +/- 352.41
Episode length: 386.40 +/- 54.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 285          |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0053727143 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 170          |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.65         |
|    value_loss           | 490          |
------------------------------------------
Eval num_timesteps=590000, episode_reward=392.93 +/- 317.94
Episode length: 390.80 +/- 43.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 393         |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.004523746 |
|    clip_fraction        | 0.00894     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.001       |
|    loss                 | 2.69e+03    |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00251    |
|    std                  | 1.65        |
|    value_loss           | 6.32e+03    |
-----------------------------------------
Eval num_timesteps=592000, episode_reward=573.15 +/- 297.38
Episode length: 393.40 +/- 15.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 573          |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0020625563 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 203          |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 1.65         |
|    value_loss           | 514          |
------------------------------------------
Eval num_timesteps=594000, episode_reward=103.88 +/- 323.95
Episode length: 348.60 +/- 73.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 349         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.008545487 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 142         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.00426    |
|    std                  | 1.65        |
|    value_loss           | 372         |
-----------------------------------------
Eval num_timesteps=596000, episode_reward=214.10 +/- 176.54
Episode length: 377.60 +/- 47.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.006714715 |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 233         |
|    n_updates            | 2910        |
|    policy_gradient_loss | 0.000474    |
|    std                  | 1.65        |
|    value_loss           | 601         |
-----------------------------------------
Eval num_timesteps=598000, episode_reward=210.85 +/- 193.09
Episode length: 379.20 +/- 59.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
Eval num_timesteps=600000, episode_reward=220.11 +/- 188.47
Episode length: 372.40 +/- 49.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 372         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.002098673 |
|    clip_fraction        | 0.00225     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 191         |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.000878   |
|    std                  | 1.65        |
|    value_loss           | 566         |
-----------------------------------------
Eval num_timesteps=602000, episode_reward=237.08 +/- 137.86
Episode length: 370.40 +/- 46.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 602000      |
| train/                  |             |
|    approx_kl            | 0.003118408 |
|    clip_fraction        | 0.00435     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 293         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.000356   |
|    std                  | 1.65        |
|    value_loss           | 797         |
-----------------------------------------
Eval num_timesteps=604000, episode_reward=206.40 +/- 222.16
Episode length: 370.20 +/- 47.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 206          |
| time/                   |              |
|    total_timesteps      | 604000       |
| train/                  |              |
|    approx_kl            | 0.0011462811 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 626          |
|    n_updates            | 2940         |
|    policy_gradient_loss | -8.45e-05    |
|    std                  | 1.65         |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=606000, episode_reward=214.75 +/- 495.59
Episode length: 444.60 +/- 75.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 215          |
| time/                   |              |
|    total_timesteps      | 606000       |
| train/                  |              |
|    approx_kl            | 0.0035625843 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.001        |
|    loss                 | 439          |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.65         |
|    value_loss           | 1.13e+03     |
------------------------------------------
Eval num_timesteps=608000, episode_reward=281.25 +/- 177.58
Episode length: 419.60 +/- 29.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 281          |
| time/                   |              |
|    total_timesteps      | 608000       |
| train/                  |              |
|    approx_kl            | 0.0005139224 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 254          |
|    n_updates            | 2960         |
|    policy_gradient_loss | 0.000163     |
|    std                  | 1.65         |
|    value_loss           | 851          |
------------------------------------------
Eval num_timesteps=610000, episode_reward=334.43 +/- 362.86
Episode length: 466.60 +/- 30.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.007455771 |
|    clip_fraction        | 0.0214      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 150         |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 1.65        |
|    value_loss           | 418         |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=103.07 +/- 85.33
Episode length: 444.20 +/- 56.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 103          |
| time/                   |              |
|    total_timesteps      | 612000       |
| train/                  |              |
|    approx_kl            | 0.0023853064 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 227          |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 1.65         |
|    value_loss           | 672          |
------------------------------------------
Eval num_timesteps=614000, episode_reward=137.44 +/- 244.40
Episode length: 379.80 +/- 70.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 614000       |
| train/                  |              |
|    approx_kl            | 0.0012418262 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 171          |
|    n_updates            | 2990         |
|    policy_gradient_loss | 8.53e-05     |
|    std                  | 1.66         |
|    value_loss           | 455          |
------------------------------------------
Eval num_timesteps=616000, episode_reward=84.29 +/- 121.66
Episode length: 465.00 +/- 92.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 465         |
|    mean_reward          | 84.3        |
| time/                   |             |
|    total_timesteps      | 616000      |
| train/                  |             |
|    approx_kl            | 0.005266618 |
|    clip_fraction        | 0.00913     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 187         |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 1.66        |
|    value_loss           | 581         |
-----------------------------------------
Eval num_timesteps=618000, episode_reward=-14.01 +/- 87.16
Episode length: 362.00 +/- 51.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | -14         |
| time/                   |             |
|    total_timesteps      | 618000      |
| train/                  |             |
|    approx_kl            | 0.007531986 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 151         |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.00475    |
|    std                  | 1.66        |
|    value_loss           | 403         |
-----------------------------------------
Eval num_timesteps=620000, episode_reward=-110.50 +/- 21.38
Episode length: 262.20 +/- 43.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | -110         |
| time/                   |              |
|    total_timesteps      | 620000       |
| train/                  |              |
|    approx_kl            | 0.0059667053 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 162          |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.000952    |
|    std                  | 1.66         |
|    value_loss           | 425          |
------------------------------------------
Eval num_timesteps=622000, episode_reward=-89.10 +/- 11.86
Episode length: 283.60 +/- 22.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | -89.1        |
| time/                   |              |
|    total_timesteps      | 622000       |
| train/                  |              |
|    approx_kl            | 0.0050906776 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 3030         |
|    policy_gradient_loss | 1.76e-05     |
|    std                  | 1.67         |
|    value_loss           | 403          |
------------------------------------------
Eval num_timesteps=624000, episode_reward=-93.18 +/- 35.47
Episode length: 275.60 +/- 45.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | -93.2        |
| time/                   |              |
|    total_timesteps      | 624000       |
| train/                  |              |
|    approx_kl            | 0.0026231057 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.000984    |
|    std                  | 1.67         |
|    value_loss           | 424          |
------------------------------------------
Eval num_timesteps=626000, episode_reward=-57.66 +/- 25.38
Episode length: 321.00 +/- 21.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | -57.7        |
| time/                   |              |
|    total_timesteps      | 626000       |
| train/                  |              |
|    approx_kl            | 0.0048635527 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 146          |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 1.67         |
|    value_loss           | 371          |
------------------------------------------
Eval num_timesteps=628000, episode_reward=22.26 +/- 81.10
Episode length: 383.40 +/- 35.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 22.3         |
| time/                   |              |
|    total_timesteps      | 628000       |
| train/                  |              |
|    approx_kl            | 0.0075409273 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 168          |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.00365     |
|    std                  | 1.67         |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=630000, episode_reward=29.93 +/- 48.79
Episode length: 370.20 +/- 28.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 370        |
|    mean_reward          | 29.9       |
| time/                   |            |
|    total_timesteps      | 630000     |
| train/                  |            |
|    approx_kl            | 0.00709876 |
|    clip_fraction        | 0.0278     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.7       |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.001      |
|    loss                 | 142        |
|    n_updates            | 3070       |
|    policy_gradient_loss | -0.0022    |
|    std                  | 1.67       |
|    value_loss           | 305        |
----------------------------------------
Eval num_timesteps=632000, episode_reward=42.13 +/- 72.63
Episode length: 374.60 +/- 59.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 42.1         |
| time/                   |              |
|    total_timesteps      | 632000       |
| train/                  |              |
|    approx_kl            | 0.0051817484 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 158          |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.68         |
|    value_loss           | 370          |
------------------------------------------
Eval num_timesteps=634000, episode_reward=266.00 +/- 167.32
Episode length: 369.80 +/- 41.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 634000       |
| train/                  |              |
|    approx_kl            | 0.0036626072 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 133          |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.68         |
|    value_loss           | 347          |
------------------------------------------
Eval num_timesteps=636000, episode_reward=370.55 +/- 190.08
Episode length: 387.40 +/- 24.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 636000      |
| train/                  |             |
|    approx_kl            | 0.005887111 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.001       |
|    loss                 | 3.14e+03    |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00216    |
|    std                  | 1.68        |
|    value_loss           | 7.21e+03    |
-----------------------------------------
Eval num_timesteps=638000, episode_reward=333.00 +/- 256.35
Episode length: 406.80 +/- 40.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 407         |
|    mean_reward          | 333         |
| time/                   |             |
|    total_timesteps      | 638000      |
| train/                  |             |
|    approx_kl            | 0.005708698 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | 250         |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00193    |
|    std                  | 1.68        |
|    value_loss           | 576         |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=207.99 +/- 246.02
Episode length: 400.00 +/- 43.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 208          |
| time/                   |              |
|    total_timesteps      | 640000       |
| train/                  |              |
|    approx_kl            | 0.0022778912 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.001        |
|    loss                 | 6.12e+03     |
|    n_updates            | 3120         |
|    policy_gradient_loss | -0.000139    |
|    std                  | 1.68         |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=642000, episode_reward=506.34 +/- 137.56
Episode length: 381.40 +/- 16.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 642000       |
| train/                  |              |
|    approx_kl            | 0.0019699316 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 1.9e+03      |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.000755    |
|    std                  | 1.68         |
|    value_loss           | 4.38e+03     |
------------------------------------------
Eval num_timesteps=644000, episode_reward=249.30 +/- 163.34
Episode length: 406.80 +/- 49.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 644000       |
| train/                  |              |
|    approx_kl            | 0.0027516193 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 2.63e+03     |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.000405    |
|    std                  | 1.68         |
|    value_loss           | 5.57e+03     |
------------------------------------------
Eval num_timesteps=646000, episode_reward=243.45 +/- 211.49
Episode length: 340.60 +/- 34.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 341           |
|    mean_reward          | 243           |
| time/                   |               |
|    total_timesteps      | 646000        |
| train/                  |               |
|    approx_kl            | 0.00066024833 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.72         |
|    explained_variance   | 0.688         |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+03      |
|    n_updates            | 3150          |
|    policy_gradient_loss | -0.000393     |
|    std                  | 1.68          |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=648000, episode_reward=377.54 +/- 254.92
Episode length: 405.20 +/- 30.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 378          |
| time/                   |              |
|    total_timesteps      | 648000       |
| train/                  |              |
|    approx_kl            | 0.0009765569 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.66         |
|    learning_rate        | 0.001        |
|    loss                 | 2.95e+03     |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.000723    |
|    std                  | 1.68         |
|    value_loss           | 6.66e+03     |
------------------------------------------
Eval num_timesteps=650000, episode_reward=286.38 +/- 176.78
Episode length: 388.00 +/- 23.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.004170259 |
|    clip_fraction        | 0.004       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 164         |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.000897   |
|    std                  | 1.69        |
|    value_loss           | 463         |
-----------------------------------------
Eval num_timesteps=652000, episode_reward=295.26 +/- 256.90
Episode length: 443.60 +/- 32.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 295          |
| time/                   |              |
|    total_timesteps      | 652000       |
| train/                  |              |
|    approx_kl            | 0.0064671487 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.69         |
|    value_loss           | 396          |
------------------------------------------
Eval num_timesteps=654000, episode_reward=364.56 +/- 232.93
Episode length: 432.00 +/- 23.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 365          |
| time/                   |              |
|    total_timesteps      | 654000       |
| train/                  |              |
|    approx_kl            | 0.0069032917 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 163          |
|    n_updates            | 3190         |
|    policy_gradient_loss | 4.64e-05     |
|    std                  | 1.69         |
|    value_loss           | 680          |
------------------------------------------
Eval num_timesteps=656000, episode_reward=395.40 +/- 279.19
Episode length: 441.20 +/- 61.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 656000       |
| train/                  |              |
|    approx_kl            | 0.0072239414 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.745        |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+03     |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.69         |
|    value_loss           | 5.12e+03     |
------------------------------------------
Eval num_timesteps=658000, episode_reward=233.91 +/- 241.61
Episode length: 446.60 +/- 62.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 234          |
| time/                   |              |
|    total_timesteps      | 658000       |
| train/                  |              |
|    approx_kl            | 0.0059538693 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.758        |
|    learning_rate        | 0.001        |
|    loss                 | 3.22e+03     |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00356     |
|    std                  | 1.69         |
|    value_loss           | 6.43e+03     |
------------------------------------------
Eval num_timesteps=660000, episode_reward=352.72 +/- 235.88
Episode length: 443.80 +/- 29.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 444        |
|    mean_reward          | 353        |
| time/                   |            |
|    total_timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.00865654 |
|    clip_fraction        | 0.052      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.74      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 142        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.00375   |
|    std                  | 1.7        |
|    value_loss           | 446        |
----------------------------------------
Eval num_timesteps=662000, episode_reward=457.57 +/- 348.13
Episode length: 442.60 +/- 36.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 443        |
|    mean_reward          | 458        |
| time/                   |            |
|    total_timesteps      | 662000     |
| train/                  |            |
|    approx_kl            | 0.00793604 |
|    clip_fraction        | 0.0389     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.75      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.001      |
|    loss                 | 1.88e+03   |
|    n_updates            | 3230       |
|    policy_gradient_loss | 0.000572   |
|    std                  | 1.7        |
|    value_loss           | 4.01e+03   |
----------------------------------------
Eval num_timesteps=664000, episode_reward=-10.34 +/- 244.14
Episode length: 458.00 +/- 38.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | -10.3        |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0007388063 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.75        |
|    explained_variance   | 0.557        |
|    learning_rate        | 0.001        |
|    loss                 | 5.48e+03     |
|    n_updates            | 3240         |
|    policy_gradient_loss | 7.12e-05     |
|    std                  | 1.7          |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=666000, episode_reward=230.52 +/- 196.99
Episode length: 390.80 +/- 58.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 231          |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0019195988 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.75        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 399          |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.000742    |
|    std                  | 1.7          |
|    value_loss           | 1.35e+03     |
------------------------------------------
Eval num_timesteps=668000, episode_reward=220.20 +/- 57.38
Episode length: 395.20 +/- 34.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.005065169 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 2.54e+03    |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00162    |
|    std                  | 1.7         |
|    value_loss           | 5.12e+03    |
-----------------------------------------
Eval num_timesteps=670000, episode_reward=385.18 +/- 331.79
Episode length: 376.80 +/- 24.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 377         |
|    mean_reward          | 385         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.009381562 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.0032     |
|    std                  | 1.7         |
|    value_loss           | 281         |
-----------------------------------------
Eval num_timesteps=672000, episode_reward=156.24 +/- 148.84
Episode length: 387.80 +/- 69.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 156          |
| time/                   |              |
|    total_timesteps      | 672000       |
| train/                  |              |
|    approx_kl            | 0.0028725932 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 243          |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.00054     |
|    std                  | 1.71         |
|    value_loss           | 597          |
------------------------------------------
Eval num_timesteps=674000, episode_reward=308.36 +/- 199.86
Episode length: 362.00 +/- 30.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0038440595 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0.758        |
|    learning_rate        | 0.001        |
|    loss                 | 2.38e+03     |
|    n_updates            | 3290         |
|    policy_gradient_loss | 0.000893     |
|    std                  | 1.71         |
|    value_loss           | 5.34e+03     |
------------------------------------------
Eval num_timesteps=676000, episode_reward=487.33 +/- 290.96
Episode length: 381.20 +/- 46.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 487         |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.001053897 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.001       |
|    loss                 | 6.31e+03    |
|    n_updates            | 3300        |
|    policy_gradient_loss | -7.26e-05   |
|    std                  | 1.71        |
|    value_loss           | 1.29e+04    |
-----------------------------------------
Eval num_timesteps=678000, episode_reward=191.36 +/- 111.40
Episode length: 355.60 +/- 9.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 356          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0018344636 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 245          |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.71         |
|    value_loss           | 827          |
------------------------------------------
Eval num_timesteps=680000, episode_reward=205.77 +/- 399.09
Episode length: 376.20 +/- 49.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.004234924 |
|    clip_fraction        | 0.00742     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | 228         |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00215    |
|    std                  | 1.71        |
|    value_loss           | 645         |
-----------------------------------------
Eval num_timesteps=682000, episode_reward=69.88 +/- 46.01
Episode length: 351.20 +/- 34.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 69.9        |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.004585102 |
|    clip_fraction        | 0.00747     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 160         |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 1.71        |
|    value_loss           | 446         |
-----------------------------------------
Eval num_timesteps=684000, episode_reward=115.60 +/- 70.92
Episode length: 387.80 +/- 36.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | 116      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=686000, episode_reward=61.91 +/- 54.06
Episode length: 370.00 +/- 50.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 61.9         |
| time/                   |              |
|    total_timesteps      | 686000       |
| train/                  |              |
|    approx_kl            | 0.0031352788 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 132          |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.71         |
|    value_loss           | 383          |
------------------------------------------
Eval num_timesteps=688000, episode_reward=-44.17 +/- 36.56
Episode length: 312.00 +/- 50.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 312         |
|    mean_reward          | -44.2       |
| time/                   |             |
|    total_timesteps      | 688000      |
| train/                  |             |
|    approx_kl            | 0.010436354 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 126         |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 1.72        |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=690000, episode_reward=-4.71 +/- 78.84
Episode length: 311.20 +/- 42.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | -4.71        |
| time/                   |              |
|    total_timesteps      | 690000       |
| train/                  |              |
|    approx_kl            | 0.0015122214 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.72         |
|    value_loss           | 327          |
------------------------------------------
Eval num_timesteps=692000, episode_reward=1.63 +/- 81.39
Episode length: 329.60 +/- 52.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 330         |
|    mean_reward          | 1.63        |
| time/                   |             |
|    total_timesteps      | 692000      |
| train/                  |             |
|    approx_kl            | 0.005185719 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.8        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 122         |
|    n_updates            | 3370        |
|    policy_gradient_loss | -0.00109    |
|    std                  | 1.72        |
|    value_loss           | 278         |
-----------------------------------------
Eval num_timesteps=694000, episode_reward=-6.09 +/- 27.91
Episode length: 363.60 +/- 23.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 364       |
|    mean_reward          | -6.09     |
| time/                   |           |
|    total_timesteps      | 694000    |
| train/                  |           |
|    approx_kl            | 0.0109398 |
|    clip_fraction        | 0.0595    |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.8      |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.001     |
|    loss                 | 132       |
|    n_updates            | 3380      |
|    policy_gradient_loss | -0.00203  |
|    std                  | 1.72      |
|    value_loss           | 271       |
---------------------------------------
Eval num_timesteps=696000, episode_reward=-23.05 +/- 36.85
Episode length: 349.00 +/- 22.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 349          |
|    mean_reward          | -23.1        |
| time/                   |              |
|    total_timesteps      | 696000       |
| train/                  |              |
|    approx_kl            | 0.0026372788 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.81        |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+03     |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000286    |
|    std                  | 1.72         |
|    value_loss           | 6.56e+03     |
------------------------------------------
Eval num_timesteps=698000, episode_reward=-57.15 +/- 40.24
Episode length: 316.80 +/- 47.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 317          |
|    mean_reward          | -57.1        |
| time/                   |              |
|    total_timesteps      | 698000       |
| train/                  |              |
|    approx_kl            | 0.0023431464 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.82        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00202     |
|    std                  | 1.73         |
|    value_loss           | 296          |
------------------------------------------
Eval num_timesteps=700000, episode_reward=-34.86 +/- 44.79
Episode length: 322.00 +/- 34.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | -34.9       |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.008333812 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00283    |
|    std                  | 1.73        |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=-99.12 +/- 39.70
Episode length: 253.80 +/- 53.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | -99.1        |
| time/                   |              |
|    total_timesteps      | 702000       |
| train/                  |              |
|    approx_kl            | 0.0047717346 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.83        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 108          |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.73         |
|    value_loss           | 234          |
------------------------------------------
Eval num_timesteps=704000, episode_reward=-68.26 +/- 20.93
Episode length: 274.80 +/- 17.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | -68.3        |
| time/                   |              |
|    total_timesteps      | 704000       |
| train/                  |              |
|    approx_kl            | 0.0027540813 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.83        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.000432    |
|    std                  | 1.73         |
|    value_loss           | 348          |
------------------------------------------
Eval num_timesteps=706000, episode_reward=-36.79 +/- 36.80
Episode length: 324.80 +/- 23.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | -36.8       |
| time/                   |             |
|    total_timesteps      | 706000      |
| train/                  |             |
|    approx_kl            | 0.006932594 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.83       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 130         |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.00328    |
|    std                  | 1.73        |
|    value_loss           | 301         |
-----------------------------------------
Eval num_timesteps=708000, episode_reward=-11.94 +/- 41.59
Episode length: 342.00 +/- 33.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 342          |
|    mean_reward          | -11.9        |
| time/                   |              |
|    total_timesteps      | 708000       |
| train/                  |              |
|    approx_kl            | 0.0063606864 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.84        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.74         |
|    value_loss           | 421          |
------------------------------------------
Eval num_timesteps=710000, episode_reward=190.35 +/- 148.90
Episode length: 413.80 +/- 25.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 710000      |
| train/                  |             |
|    approx_kl            | 0.013188715 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.86       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 121         |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 1.75        |
|    value_loss           | 399         |
-----------------------------------------
Eval num_timesteps=712000, episode_reward=47.10 +/- 136.60
Episode length: 369.80 +/- 72.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 47.1        |
| time/                   |             |
|    total_timesteps      | 712000      |
| train/                  |             |
|    approx_kl            | 0.004572115 |
|    clip_fraction        | 0.0716      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 156         |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 1.75        |
|    value_loss           | 411         |
-----------------------------------------
Eval num_timesteps=714000, episode_reward=219.59 +/- 123.03
Episode length: 463.00 +/- 50.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 714000       |
| train/                  |              |
|    approx_kl            | 0.0069966055 |
|    clip_fraction        | 0.0785       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.88        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.76         |
|    value_loss           | 287          |
------------------------------------------
Eval num_timesteps=716000, episode_reward=213.83 +/- 136.31
Episode length: 464.80 +/- 56.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 465         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 716000      |
| train/                  |             |
|    approx_kl            | 0.008664432 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | 307         |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 1.76        |
|    value_loss           | 927         |
-----------------------------------------
Eval num_timesteps=718000, episode_reward=109.84 +/- 425.26
Episode length: 433.00 +/- 73.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 433        |
|    mean_reward          | 110        |
| time/                   |            |
|    total_timesteps      | 718000     |
| train/                  |            |
|    approx_kl            | 0.00530227 |
|    clip_fraction        | 0.0129     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.89      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.001      |
|    loss                 | 175        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.00212   |
|    std                  | 1.76       |
|    value_loss           | 464        |
----------------------------------------
Eval num_timesteps=720000, episode_reward=261.33 +/- 419.22
Episode length: 444.60 +/- 34.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0017853281 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.76         |
|    value_loss           | 376          |
------------------------------------------
Eval num_timesteps=722000, episode_reward=333.33 +/- 209.69
Episode length: 440.40 +/- 23.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 333         |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.009670485 |
|    clip_fraction        | 0.0537      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 140         |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.76        |
|    value_loss           | 355         |
-----------------------------------------
Eval num_timesteps=724000, episode_reward=294.83 +/- 319.52
Episode length: 381.20 +/- 13.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 295          |
| time/                   |              |
|    total_timesteps      | 724000       |
| train/                  |              |
|    approx_kl            | 0.0050559677 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 105          |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.00264     |
|    std                  | 1.76         |
|    value_loss           | 267          |
------------------------------------------
Eval num_timesteps=726000, episode_reward=447.89 +/- 149.85
Episode length: 383.60 +/- 27.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 448          |
| time/                   |              |
|    total_timesteps      | 726000       |
| train/                  |              |
|    approx_kl            | 0.0055720657 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.89        |
|    explained_variance   | 0.564        |
|    learning_rate        | 0.001        |
|    loss                 | 3.59e+03     |
|    n_updates            | 3540         |
|    policy_gradient_loss | 0.000774     |
|    std                  | 1.76         |
|    value_loss           | 8.35e+03     |
------------------------------------------
Eval num_timesteps=728000, episode_reward=164.34 +/- 315.24
Episode length: 401.40 +/- 40.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 164          |
| time/                   |              |
|    total_timesteps      | 728000       |
| train/                  |              |
|    approx_kl            | 0.0034855874 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.9         |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 187          |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.76         |
|    value_loss           | 573          |
------------------------------------------
Eval num_timesteps=730000, episode_reward=235.73 +/- 188.19
Episode length: 386.20 +/- 24.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 236          |
| time/                   |              |
|    total_timesteps      | 730000       |
| train/                  |              |
|    approx_kl            | 0.0056874924 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.91        |
|    explained_variance   | 0.64         |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+03     |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000992    |
|    std                  | 1.76         |
|    value_loss           | 1.2e+04      |
------------------------------------------
Eval num_timesteps=732000, episode_reward=373.13 +/- 325.76
Episode length: 424.00 +/- 34.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 373          |
| time/                   |              |
|    total_timesteps      | 732000       |
| train/                  |              |
|    approx_kl            | 0.0006004313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.91        |
|    explained_variance   | 0.671        |
|    learning_rate        | 0.001        |
|    loss                 | 7.56e+03     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.000302    |
|    std                  | 1.77         |
|    value_loss           | 1.62e+04     |
------------------------------------------
Eval num_timesteps=734000, episode_reward=231.51 +/- 167.75
Episode length: 367.40 +/- 34.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 367         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 734000      |
| train/                  |             |
|    approx_kl            | 0.009461299 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.91       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 278         |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00486    |
|    std                  | 1.77        |
|    value_loss           | 691         |
-----------------------------------------
Eval num_timesteps=736000, episode_reward=288.94 +/- 136.26
Episode length: 358.20 +/- 22.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 358          |
|    mean_reward          | 289          |
| time/                   |              |
|    total_timesteps      | 736000       |
| train/                  |              |
|    approx_kl            | 0.0067876866 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+03     |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1.77         |
|    value_loss           | 7.1e+03      |
------------------------------------------
Eval num_timesteps=738000, episode_reward=423.79 +/- 276.08
Episode length: 368.40 +/- 33.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 738000       |
| train/                  |              |
|    approx_kl            | 0.0050508557 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.77         |
|    value_loss           | 337          |
------------------------------------------
Eval num_timesteps=740000, episode_reward=269.41 +/- 350.32
Episode length: 345.60 +/- 35.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 740000       |
| train/                  |              |
|    approx_kl            | 0.0018039062 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.793        |
|    learning_rate        | 0.001        |
|    loss                 | 2.72e+03     |
|    n_updates            | 3610         |
|    policy_gradient_loss | -0.000284    |
|    std                  | 1.77         |
|    value_loss           | 5.79e+03     |
------------------------------------------
Eval num_timesteps=742000, episode_reward=372.07 +/- 381.35
Episode length: 373.60 +/- 35.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 374           |
|    mean_reward          | 372           |
| time/                   |               |
|    total_timesteps      | 742000        |
| train/                  |               |
|    approx_kl            | 0.00021388722 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.93         |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 3620          |
|    policy_gradient_loss | 0.000141      |
|    std                  | 1.77          |
|    value_loss           | 4.23e+03      |
-------------------------------------------
Eval num_timesteps=744000, episode_reward=313.58 +/- 188.80
Episode length: 341.00 +/- 21.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 341         |
|    mean_reward          | 314         |
| time/                   |             |
|    total_timesteps      | 744000      |
| train/                  |             |
|    approx_kl            | 0.004131632 |
|    clip_fraction        | 0.00454     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.93       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 1.78        |
|    value_loss           | 344         |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=285.94 +/- 171.96
Episode length: 332.60 +/- 20.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 746000      |
| train/                  |             |
|    approx_kl            | 0.012060895 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.94       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 99.1        |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.0043     |
|    std                  | 1.78        |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=748000, episode_reward=351.92 +/- 276.57
Episode length: 335.00 +/- 32.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 352         |
| time/                   |             |
|    total_timesteps      | 748000      |
| train/                  |             |
|    approx_kl            | 0.005217487 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.95       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 108         |
|    n_updates            | 3650        |
|    policy_gradient_loss | 0.00014     |
|    std                  | 1.78        |
|    value_loss           | 256         |
-----------------------------------------
Eval num_timesteps=750000, episode_reward=283.60 +/- 248.54
Episode length: 353.60 +/- 30.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 354         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.004729019 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.96       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.00341    |
|    std                  | 1.79        |
|    value_loss           | 362         |
-----------------------------------------
Eval num_timesteps=752000, episode_reward=300.15 +/- 260.92
Episode length: 336.00 +/- 25.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 336         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 752000      |
| train/                  |             |
|    approx_kl            | 0.008234691 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.96       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 178         |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 1.79        |
|    value_loss           | 409         |
-----------------------------------------
Eval num_timesteps=754000, episode_reward=363.58 +/- 228.92
Episode length: 390.80 +/- 47.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 364          |
| time/                   |              |
|    total_timesteps      | 754000       |
| train/                  |              |
|    approx_kl            | 0.0059621534 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.97        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 3.03e+03     |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.000707    |
|    std                  | 1.79         |
|    value_loss           | 6.32e+03     |
------------------------------------------
Eval num_timesteps=756000, episode_reward=241.57 +/- 281.68
Episode length: 382.20 +/- 27.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 382           |
|    mean_reward          | 242           |
| time/                   |               |
|    total_timesteps      | 756000        |
| train/                  |               |
|    approx_kl            | 0.00048051905 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.97         |
|    explained_variance   | 0.764         |
|    learning_rate        | 0.001         |
|    loss                 | 3.32e+03      |
|    n_updates            | 3690          |
|    policy_gradient_loss | 7.5e-05       |
|    std                  | 1.79          |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=758000, episode_reward=384.04 +/- 165.70
Episode length: 387.00 +/- 13.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 758000       |
| train/                  |              |
|    approx_kl            | 0.0028289773 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.97        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 200          |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.79         |
|    value_loss           | 493          |
------------------------------------------
Eval num_timesteps=760000, episode_reward=421.19 +/- 100.37
Episode length: 383.60 +/- 19.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.005969346 |
|    clip_fraction        | 0.00962     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 80.3        |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 1.8         |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=762000, episode_reward=317.87 +/- 238.78
Episode length: 365.40 +/- 34.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 318          |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0075159036 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 2.38e+03     |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000441    |
|    std                  | 1.81         |
|    value_loss           | 5.65e+03     |
------------------------------------------
Eval num_timesteps=764000, episode_reward=107.98 +/- 203.46
Episode length: 398.40 +/- 55.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 108          |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0017976437 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.000895    |
|    std                  | 1.81         |
|    value_loss           | 353          |
------------------------------------------
Eval num_timesteps=766000, episode_reward=447.78 +/- 295.60
Episode length: 410.40 +/- 19.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 448          |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0025475011 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.001        |
|    loss                 | 2.62e+03     |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000952    |
|    std                  | 1.81         |
|    value_loss           | 5.75e+03     |
------------------------------------------
Eval num_timesteps=768000, episode_reward=335.75 +/- 215.17
Episode length: 400.20 +/- 27.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=180.09 +/- 328.24
Episode length: 402.40 +/- 40.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 0.0009147645 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 170          |
|    n_updates            | 3750         |
|    policy_gradient_loss | -5.64e-05    |
|    std                  | 1.82         |
|    value_loss           | 717          |
------------------------------------------
Eval num_timesteps=772000, episode_reward=426.45 +/- 221.82
Episode length: 415.20 +/- 28.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 415         |
|    mean_reward          | 426         |
| time/                   |             |
|    total_timesteps      | 772000      |
| train/                  |             |
|    approx_kl            | 0.008421119 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 122         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.82        |
|    value_loss           | 322         |
-----------------------------------------
Eval num_timesteps=774000, episode_reward=233.40 +/- 162.75
Episode length: 435.20 +/- 56.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 435         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 774000      |
| train/                  |             |
|    approx_kl            | 0.004712295 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 119         |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00144    |
|    std                  | 1.82        |
|    value_loss           | 320         |
-----------------------------------------
Eval num_timesteps=776000, episode_reward=399.35 +/- 258.85
Episode length: 435.20 +/- 63.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 435         |
|    mean_reward          | 399         |
| time/                   |             |
|    total_timesteps      | 776000      |
| train/                  |             |
|    approx_kl            | 0.013469372 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 132         |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00376    |
|    std                  | 1.82        |
|    value_loss           | 341         |
-----------------------------------------
Eval num_timesteps=778000, episode_reward=501.45 +/- 401.99
Episode length: 433.20 +/- 69.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 501          |
| time/                   |              |
|    total_timesteps      | 778000       |
| train/                  |              |
|    approx_kl            | 0.0044331606 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.001        |
|    loss                 | 3.5e+03      |
|    n_updates            | 3790         |
|    policy_gradient_loss | 0.00192      |
|    std                  | 1.82         |
|    value_loss           | 7.42e+03     |
------------------------------------------
Eval num_timesteps=780000, episode_reward=310.63 +/- 184.69
Episode length: 387.80 +/- 21.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0015349307 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.001        |
|    loss                 | 1.82e+03     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000932    |
|    std                  | 1.82         |
|    value_loss           | 4.38e+03     |
------------------------------------------
Eval num_timesteps=782000, episode_reward=445.75 +/- 128.40
Episode length: 395.60 +/- 8.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 396           |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 782000        |
| train/                  |               |
|    approx_kl            | 0.00079090893 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.05         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 146           |
|    n_updates            | 3810          |
|    policy_gradient_loss | -0.000117     |
|    std                  | 1.83          |
|    value_loss           | 491           |
-------------------------------------------
Eval num_timesteps=784000, episode_reward=556.15 +/- 455.87
Episode length: 441.60 +/- 77.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 784000       |
| train/                  |              |
|    approx_kl            | 0.0018811149 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.05        |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.001        |
|    loss                 | 2.86e+03     |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.000688    |
|    std                  | 1.83         |
|    value_loss           | 5.98e+03     |
------------------------------------------
Eval num_timesteps=786000, episode_reward=375.71 +/- 481.66
Episode length: 416.00 +/- 60.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 376           |
| time/                   |               |
|    total_timesteps      | 786000        |
| train/                  |               |
|    approx_kl            | 0.00038168314 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.06         |
|    explained_variance   | 0.775         |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+03      |
|    n_updates            | 3830          |
|    policy_gradient_loss | -0.000441     |
|    std                  | 1.83          |
|    value_loss           | 5.94e+03      |
-------------------------------------------
Eval num_timesteps=788000, episode_reward=96.36 +/- 329.27
Episode length: 439.00 +/- 28.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 439         |
|    mean_reward          | 96.4        |
| time/                   |             |
|    total_timesteps      | 788000      |
| train/                  |             |
|    approx_kl            | 0.005016812 |
|    clip_fraction        | 0.00737     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 160         |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 1.84        |
|    value_loss           | 545         |
-----------------------------------------
Eval num_timesteps=790000, episode_reward=134.97 +/- 112.01
Episode length: 459.80 +/- 68.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 790000       |
| train/                  |              |
|    approx_kl            | 0.0014539816 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.07        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 2.81e+03     |
|    n_updates            | 3850         |
|    policy_gradient_loss | 0.00051      |
|    std                  | 1.84         |
|    value_loss           | 6.31e+03     |
------------------------------------------
Eval num_timesteps=792000, episode_reward=173.08 +/- 347.11
Episode length: 384.60 +/- 16.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 792000       |
| train/                  |              |
|    approx_kl            | 0.0008873608 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.000664    |
|    std                  | 1.85         |
|    value_loss           | 365          |
------------------------------------------
Eval num_timesteps=794000, episode_reward=125.45 +/- 181.17
Episode length: 359.20 +/- 35.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 125         |
| time/                   |             |
|    total_timesteps      | 794000      |
| train/                  |             |
|    approx_kl            | 0.007624779 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 133         |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00205    |
|    std                  | 1.85        |
|    value_loss           | 494         |
-----------------------------------------
Eval num_timesteps=796000, episode_reward=53.21 +/- 109.13
Episode length: 329.20 +/- 33.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | 53.2         |
| time/                   |              |
|    total_timesteps      | 796000       |
| train/                  |              |
|    approx_kl            | 0.0011005094 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.09        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 103          |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.000456    |
|    std                  | 1.84         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=798000, episode_reward=299.77 +/- 317.84
Episode length: 384.20 +/- 30.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 384        |
|    mean_reward          | 300        |
| time/                   |            |
|    total_timesteps      | 798000     |
| train/                  |            |
|    approx_kl            | 0.00271767 |
|    clip_fraction        | 0.000879   |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.09      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.001      |
|    loss                 | 84.4       |
|    n_updates            | 3890       |
|    policy_gradient_loss | -0.00147   |
|    std                  | 1.85       |
|    value_loss           | 253        |
----------------------------------------
Eval num_timesteps=800000, episode_reward=218.10 +/- 110.97
Episode length: 353.20 +/- 12.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.007841324 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 96.9        |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 1.84        |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=802000, episode_reward=334.58 +/- 231.73
Episode length: 399.00 +/- 114.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 335          |
| time/                   |              |
|    total_timesteps      | 802000       |
| train/                  |              |
|    approx_kl            | 0.0045647672 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.07        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 77.6         |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 1.83         |
|    value_loss           | 201          |
------------------------------------------
Eval num_timesteps=804000, episode_reward=338.63 +/- 394.04
Episode length: 411.80 +/- 30.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 412         |
|    mean_reward          | 339         |
| time/                   |             |
|    total_timesteps      | 804000      |
| train/                  |             |
|    approx_kl            | 0.006418302 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 102         |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.000428   |
|    std                  | 1.82        |
|    value_loss           | 375         |
-----------------------------------------
Eval num_timesteps=806000, episode_reward=476.10 +/- 191.01
Episode length: 406.80 +/- 21.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 476          |
| time/                   |              |
|    total_timesteps      | 806000       |
| train/                  |              |
|    approx_kl            | 0.0044350075 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 71.8         |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.82         |
|    value_loss           | 210          |
------------------------------------------
Eval num_timesteps=808000, episode_reward=135.43 +/- 173.76
Episode length: 450.00 +/- 178.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 808000      |
| train/                  |             |
|    approx_kl            | 0.009806167 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 63.8        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 1.81        |
|    value_loss           | 233         |
-----------------------------------------
Eval num_timesteps=810000, episode_reward=99.17 +/- 164.94
Episode length: 443.40 +/- 98.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 443         |
|    mean_reward          | 99.2        |
| time/                   |             |
|    total_timesteps      | 810000      |
| train/                  |             |
|    approx_kl            | 0.003599991 |
|    clip_fraction        | 0.0062      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 69.7        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00131    |
|    std                  | 1.81        |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=812000, episode_reward=176.67 +/- 274.19
Episode length: 484.60 +/- 63.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 485          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 812000       |
| train/                  |              |
|    approx_kl            | 0.0059524197 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.81         |
|    value_loss           | 477          |
------------------------------------------
Eval num_timesteps=814000, episode_reward=323.13 +/- 357.77
Episode length: 523.00 +/- 117.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 523         |
|    mean_reward          | 323         |
| time/                   |             |
|    total_timesteps      | 814000      |
| train/                  |             |
|    approx_kl            | 0.005951793 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 451         |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.000116   |
|    std                  | 1.81        |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=406.59 +/- 290.06
Episode length: 494.80 +/- 116.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 816000       |
| train/                  |              |
|    approx_kl            | 0.0011729642 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.000799    |
|    std                  | 1.81         |
|    value_loss           | 335          |
------------------------------------------
Eval num_timesteps=818000, episode_reward=30.72 +/- 109.57
Episode length: 402.00 +/- 53.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 30.7         |
| time/                   |              |
|    total_timesteps      | 818000       |
| train/                  |              |
|    approx_kl            | 0.0017475073 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.001        |
|    loss                 | 3.38e+03     |
|    n_updates            | 3990         |
|    policy_gradient_loss | 0.00109      |
|    std                  | 1.82         |
|    value_loss           | 8.01e+03     |
------------------------------------------
Eval num_timesteps=820000, episode_reward=1160.30 +/- 922.65
Episode length: 531.00 +/- 122.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 531          |
|    mean_reward          | 1.16e+03     |
| time/                   |              |
|    total_timesteps      | 820000       |
| train/                  |              |
|    approx_kl            | 0.0049420884 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 4000         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.82         |
|    value_loss           | 4.48e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=822000, episode_reward=163.34 +/- 336.86
Episode length: 385.60 +/- 25.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 386         |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 822000      |
| train/                  |             |
|    approx_kl            | 0.000803104 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.001       |
|    loss                 | 540         |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.000305   |
|    std                  | 1.82        |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=824000, episode_reward=309.87 +/- 368.56
Episode length: 432.20 +/- 60.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 310          |
| time/                   |              |
|    total_timesteps      | 824000       |
| train/                  |              |
|    approx_kl            | 0.0073150005 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 128          |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.82         |
|    value_loss           | 351          |
------------------------------------------
Eval num_timesteps=826000, episode_reward=338.28 +/- 341.78
Episode length: 594.40 +/- 159.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 594          |
|    mean_reward          | 338          |
| time/                   |              |
|    total_timesteps      | 826000       |
| train/                  |              |
|    approx_kl            | 0.0040470175 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+03     |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.000785    |
|    std                  | 1.82         |
|    value_loss           | 3.39e+03     |
------------------------------------------
Eval num_timesteps=828000, episode_reward=381.46 +/- 302.75
Episode length: 471.40 +/- 40.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 381         |
| time/                   |             |
|    total_timesteps      | 828000      |
| train/                  |             |
|    approx_kl            | 0.005135908 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.001       |
|    loss                 | 2.13e+03    |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00104    |
|    std                  | 1.82        |
|    value_loss           | 4.71e+03    |
-----------------------------------------
Eval num_timesteps=830000, episode_reward=437.76 +/- 547.51
Episode length: 528.60 +/- 39.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | 438          |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0008948375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.001        |
|    loss                 | 4.39e+03     |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.000536    |
|    std                  | 1.82         |
|    value_loss           | 9.78e+03     |
------------------------------------------
Eval num_timesteps=832000, episode_reward=25.63 +/- 81.59
Episode length: 494.80 +/- 56.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 25.6         |
| time/                   |              |
|    total_timesteps      | 832000       |
| train/                  |              |
|    approx_kl            | 0.0011952955 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.001        |
|    loss                 | 2.73e+03     |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.000854    |
|    std                  | 1.82         |
|    value_loss           | 6.24e+03     |
------------------------------------------
Eval num_timesteps=834000, episode_reward=106.17 +/- 160.33
Episode length: 471.60 +/- 73.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0023615507 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 779          |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.000973    |
|    std                  | 1.82         |
|    value_loss           | 2.03e+03     |
------------------------------------------
Eval num_timesteps=836000, episode_reward=70.77 +/- 183.08
Episode length: 458.80 +/- 37.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 70.8         |
| time/                   |              |
|    total_timesteps      | 836000       |
| train/                  |              |
|    approx_kl            | 0.0018606588 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 230          |
|    n_updates            | 4080         |
|    policy_gradient_loss | -0.000523    |
|    std                  | 1.82         |
|    value_loss           | 771          |
------------------------------------------
Eval num_timesteps=838000, episode_reward=1.14 +/- 133.34
Episode length: 504.40 +/- 113.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 1.14         |
| time/                   |              |
|    total_timesteps      | 838000       |
| train/                  |              |
|    approx_kl            | 0.0028925934 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 187          |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 1.82         |
|    value_loss           | 649          |
------------------------------------------
Eval num_timesteps=840000, episode_reward=31.46 +/- 270.21
Episode length: 410.40 +/- 93.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 31.5         |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0042382553 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 377          |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.000358    |
|    std                  | 1.83         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=842000, episode_reward=-96.97 +/- 45.40
Episode length: 451.80 +/- 109.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 452         |
|    mean_reward          | -97         |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.010666823 |
|    clip_fraction        | 0.0762      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 98.3        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 1.83        |
|    value_loss           | 317         |
-----------------------------------------
Eval num_timesteps=844000, episode_reward=-100.40 +/- 28.10
Episode length: 418.60 +/- 56.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | -100        |
| time/                   |             |
|    total_timesteps      | 844000      |
| train/                  |             |
|    approx_kl            | 0.014838394 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 195         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.00473    |
|    std                  | 1.84        |
|    value_loss           | 586         |
-----------------------------------------
Eval num_timesteps=846000, episode_reward=-77.94 +/- 60.32
Episode length: 437.20 +/- 87.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | -77.9       |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.015550625 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.001       |
|    loss                 | 122         |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00136    |
|    std                  | 1.84        |
|    value_loss           | 662         |
-----------------------------------------
Eval num_timesteps=848000, episode_reward=-117.75 +/- 23.87
Episode length: 383.00 +/- 35.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 848000       |
| train/                  |              |
|    approx_kl            | 0.0065214504 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.001        |
|    loss                 | 333          |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.84         |
|    value_loss           | 1.13e+03     |
------------------------------------------
Eval num_timesteps=850000, episode_reward=104.78 +/- 296.69
Episode length: 510.00 +/- 81.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 105         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.005600486 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 102         |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.0014     |
|    std                  | 1.85        |
|    value_loss           | 343         |
-----------------------------------------
Eval num_timesteps=852000, episode_reward=-14.68 +/- 250.62
Episode length: 542.60 +/- 113.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 543        |
|    mean_reward          | -14.7      |
| time/                   |            |
|    total_timesteps      | 852000     |
| train/                  |            |
|    approx_kl            | 0.00934226 |
|    clip_fraction        | 0.0281     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.11      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.001      |
|    loss                 | 102        |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.00363   |
|    std                  | 1.86       |
|    value_loss           | 291        |
----------------------------------------
Eval num_timesteps=854000, episode_reward=-66.14 +/- 80.74
Episode length: 436.20 +/- 100.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 436      |
|    mean_reward     | -66.1    |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
Eval num_timesteps=856000, episode_reward=4.55 +/- 130.33
Episode length: 448.40 +/- 86.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 4.55         |
| time/                   |              |
|    total_timesteps      | 856000       |
| train/                  |              |
|    approx_kl            | 0.0052460446 |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 54           |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.00249     |
|    std                  | 1.87         |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=858000, episode_reward=-28.29 +/- 114.78
Episode length: 528.80 +/- 149.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | -28.3        |
| time/                   |              |
|    total_timesteps      | 858000       |
| train/                  |              |
|    approx_kl            | 0.0042245607 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 95.8         |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.000733    |
|    std                  | 1.87         |
|    value_loss           | 364          |
------------------------------------------
Eval num_timesteps=860000, episode_reward=-68.48 +/- 44.76
Episode length: 458.60 +/- 33.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | -68.5        |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 0.0038164507 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.000702    |
|    std                  | 1.87         |
|    value_loss           | 423          |
------------------------------------------
Eval num_timesteps=862000, episode_reward=-25.79 +/- 71.87
Episode length: 427.40 +/- 33.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | -25.8       |
| time/                   |             |
|    total_timesteps      | 862000      |
| train/                  |             |
|    approx_kl            | 0.008964244 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 0.929       |
|    loss                 | 136         |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00338    |
|    std                  | 1.88        |
|    value_loss           | 409         |
-----------------------------------------
Eval num_timesteps=864000, episode_reward=-58.61 +/- 71.68
Episode length: 405.60 +/- 75.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | -58.6       |
| time/                   |             |
|    total_timesteps      | 864000      |
| train/                  |             |
|    approx_kl            | 0.006365698 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.14       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 80.9        |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 1.88        |
|    value_loss           | 243         |
-----------------------------------------
Eval num_timesteps=866000, episode_reward=-57.93 +/- 49.65
Episode length: 412.40 +/- 42.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 412        |
|    mean_reward          | -57.9      |
| time/                   |            |
|    total_timesteps      | 866000     |
| train/                  |            |
|    approx_kl            | 0.00453895 |
|    clip_fraction        | 0.0531     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.15      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.001      |
|    loss                 | 97.1       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.00155   |
|    std                  | 1.89       |
|    value_loss           | 364        |
----------------------------------------
Eval num_timesteps=868000, episode_reward=-59.26 +/- 66.53
Episode length: 388.80 +/- 49.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 389        |
|    mean_reward          | -59.3      |
| time/                   |            |
|    total_timesteps      | 868000     |
| train/                  |            |
|    approx_kl            | 0.00565137 |
|    clip_fraction        | 0.0302     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.18      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.001      |
|    loss                 | 89         |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.00229   |
|    std                  | 1.91       |
|    value_loss           | 366        |
----------------------------------------
Eval num_timesteps=870000, episode_reward=10.12 +/- 53.79
Episode length: 451.20 +/- 56.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 870000       |
| train/                  |              |
|    approx_kl            | 0.0028865342 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 68.4         |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.92         |
|    value_loss           | 205          |
------------------------------------------
Eval num_timesteps=872000, episode_reward=2.44 +/- 77.54
Episode length: 455.80 +/- 55.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 872000      |
| train/                  |             |
|    approx_kl            | 0.008099092 |
|    clip_fraction        | 0.0459      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 98.2        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 1.93        |
|    value_loss           | 302         |
-----------------------------------------
Eval num_timesteps=874000, episode_reward=15.60 +/- 120.46
Episode length: 448.40 +/- 84.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 874000       |
| train/                  |              |
|    approx_kl            | 0.0066339327 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.26        |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.001        |
|    loss                 | 99.9         |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.94         |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=876000, episode_reward=26.01 +/- 25.40
Episode length: 478.40 +/- 66.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 478        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 876000     |
| train/                  |            |
|    approx_kl            | 0.00927841 |
|    clip_fraction        | 0.065      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.26      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.001      |
|    loss                 | 65.6       |
|    n_updates            | 4270       |
|    policy_gradient_loss | -0.00637   |
|    std                  | 1.93       |
|    value_loss           | 216        |
----------------------------------------
Eval num_timesteps=878000, episode_reward=60.90 +/- 89.87
Episode length: 497.60 +/- 60.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 60.9         |
| time/                   |              |
|    total_timesteps      | 878000       |
| train/                  |              |
|    approx_kl            | 0.0038127587 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.25        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 95           |
|    n_updates            | 4280         |
|    policy_gradient_loss | 0.0012       |
|    std                  | 1.93         |
|    value_loss           | 419          |
------------------------------------------
Eval num_timesteps=880000, episode_reward=167.97 +/- 110.22
Episode length: 546.40 +/- 55.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 546         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.010006527 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 54.4        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00154    |
|    std                  | 1.93        |
|    value_loss           | 176         |
-----------------------------------------
Eval num_timesteps=882000, episode_reward=167.62 +/- 171.76
Episode length: 481.60 +/- 74.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 882000       |
| train/                  |              |
|    approx_kl            | 0.0070142597 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.25        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 87.7         |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.000207    |
|    std                  | 1.93         |
|    value_loss           | 380          |
------------------------------------------
Eval num_timesteps=884000, episode_reward=80.97 +/- 164.97
Episode length: 454.00 +/- 77.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | 81          |
| time/                   |             |
|    total_timesteps      | 884000      |
| train/                  |             |
|    approx_kl            | 0.002842033 |
|    clip_fraction        | 0.00513     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 1.93        |
|    value_loss           | 474         |
-----------------------------------------
Eval num_timesteps=886000, episode_reward=105.55 +/- 163.85
Episode length: 489.40 +/- 46.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 886000       |
| train/                  |              |
|    approx_kl            | 0.0053195143 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.26        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 93.8         |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.000542    |
|    std                  | 1.94         |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=888000, episode_reward=208.54 +/- 425.72
Episode length: 413.80 +/- 50.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 888000      |
| train/                  |             |
|    approx_kl            | 0.006207781 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 116         |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00182    |
|    std                  | 1.94        |
|    value_loss           | 499         |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=334.05 +/- 482.48
Episode length: 473.00 +/- 37.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 334          |
| time/                   |              |
|    total_timesteps      | 890000       |
| train/                  |              |
|    approx_kl            | 0.0026567876 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.27        |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.001        |
|    loss                 | 5.43e+03     |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.000223    |
|    std                  | 1.94         |
|    value_loss           | 1.25e+04     |
------------------------------------------
Eval num_timesteps=892000, episode_reward=173.73 +/- 84.69
Episode length: 478.00 +/- 81.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 174          |
| time/                   |              |
|    total_timesteps      | 892000       |
| train/                  |              |
|    approx_kl            | 0.0005888364 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.27        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 173          |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.00048     |
|    std                  | 1.94         |
|    value_loss           | 569          |
------------------------------------------
Eval num_timesteps=894000, episode_reward=401.50 +/- 234.40
Episode length: 510.80 +/- 65.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 511           |
|    mean_reward          | 402           |
| time/                   |               |
|    total_timesteps      | 894000        |
| train/                  |               |
|    approx_kl            | 0.00011622271 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.792         |
|    learning_rate        | 0.001         |
|    loss                 | 2.05e+03      |
|    n_updates            | 4360          |
|    policy_gradient_loss | 4.79e-05      |
|    std                  | 1.94          |
|    value_loss           | 4.6e+03       |
-------------------------------------------
Eval num_timesteps=896000, episode_reward=396.48 +/- 422.05
Episode length: 477.80 +/- 76.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 478           |
|    mean_reward          | 396           |
| time/                   |               |
|    total_timesteps      | 896000        |
| train/                  |               |
|    approx_kl            | 0.00024659542 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 257           |
|    n_updates            | 4370          |
|    policy_gradient_loss | -0.000193     |
|    std                  | 1.94          |
|    value_loss           | 714           |
-------------------------------------------
Eval num_timesteps=898000, episode_reward=238.28 +/- 322.62
Episode length: 470.40 +/- 78.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | 238          |
| time/                   |              |
|    total_timesteps      | 898000       |
| train/                  |              |
|    approx_kl            | 0.0028174478 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.27        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 90.6         |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.94         |
|    value_loss           | 333          |
------------------------------------------
Eval num_timesteps=900000, episode_reward=140.12 +/- 156.59
Episode length: 392.00 +/- 52.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 140          |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0068769087 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.26        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 124          |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.94         |
|    value_loss           | 373          |
------------------------------------------
Eval num_timesteps=902000, episode_reward=300.86 +/- 182.35
Episode length: 427.80 +/- 32.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 902000       |
| train/                  |              |
|    approx_kl            | 0.0037032694 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.26        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 96.4         |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 1.94         |
|    value_loss           | 387          |
------------------------------------------
Eval num_timesteps=904000, episode_reward=345.33 +/- 440.24
Episode length: 446.00 +/- 93.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 345          |
| time/                   |              |
|    total_timesteps      | 904000       |
| train/                  |              |
|    approx_kl            | 0.0010623254 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.26        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 81.7         |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.94         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=906000, episode_reward=366.83 +/- 187.38
Episode length: 399.00 +/- 17.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 906000       |
| train/                  |              |
|    approx_kl            | 0.0010450534 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.27        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.000317    |
|    std                  | 1.94         |
|    value_loss           | 559          |
------------------------------------------
Eval num_timesteps=908000, episode_reward=445.92 +/- 241.11
Episode length: 411.20 +/- 34.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 908000        |
| train/                  |               |
|    approx_kl            | 0.00030675906 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.27         |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 151           |
|    n_updates            | 4430          |
|    policy_gradient_loss | -0.000599     |
|    std                  | 1.95          |
|    value_loss           | 482           |
-------------------------------------------
Eval num_timesteps=910000, episode_reward=161.32 +/- 275.20
Episode length: 391.80 +/- 53.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0016002237 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 106          |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.00092     |
|    std                  | 1.95         |
|    value_loss           | 419          |
------------------------------------------
Eval num_timesteps=912000, episode_reward=269.83 +/- 170.41
Episode length: 391.40 +/- 29.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 912000       |
| train/                  |              |
|    approx_kl            | 0.0057980763 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 92           |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 1.95         |
|    value_loss           | 275          |
------------------------------------------
Eval num_timesteps=914000, episode_reward=223.53 +/- 84.23
Episode length: 348.20 +/- 27.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 348         |
|    mean_reward          | 224         |
| time/                   |             |
|    total_timesteps      | 914000      |
| train/                  |             |
|    approx_kl            | 0.003120049 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 139         |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 1.95        |
|    value_loss           | 405         |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=222.24 +/- 145.80
Episode length: 347.20 +/- 26.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 916000       |
| train/                  |              |
|    approx_kl            | 0.0036433944 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 173          |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.000786    |
|    std                  | 1.95         |
|    value_loss           | 528          |
------------------------------------------
Eval num_timesteps=918000, episode_reward=92.17 +/- 45.50
Episode length: 315.80 +/- 5.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | 92.2         |
| time/                   |              |
|    total_timesteps      | 918000       |
| train/                  |              |
|    approx_kl            | 0.0056636487 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 92.9         |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.00283     |
|    std                  | 1.95         |
|    value_loss           | 284          |
------------------------------------------
Eval num_timesteps=920000, episode_reward=-13.39 +/- 33.15
Episode length: 290.80 +/- 43.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | -13.4        |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0059909397 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 96           |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 1.95         |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=922000, episode_reward=-46.55 +/- 76.79
Episode length: 252.40 +/- 53.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | -46.5       |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.002082347 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 139         |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.000861   |
|    std                  | 1.95        |
|    value_loss           | 488         |
-----------------------------------------
Eval num_timesteps=924000, episode_reward=44.27 +/- 87.71
Episode length: 292.00 +/- 27.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 292          |
|    mean_reward          | 44.3         |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0023661745 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 118          |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000865    |
|    std                  | 1.95         |
|    value_loss           | 351          |
------------------------------------------
Eval num_timesteps=926000, episode_reward=70.91 +/- 181.51
Episode length: 297.20 +/- 50.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 70.9         |
| time/                   |              |
|    total_timesteps      | 926000       |
| train/                  |              |
|    approx_kl            | 0.0037680375 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 80.3         |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.95         |
|    value_loss           | 252          |
------------------------------------------
Eval num_timesteps=928000, episode_reward=179.32 +/- 95.22
Episode length: 335.40 +/- 22.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 928000      |
| train/                  |             |
|    approx_kl            | 0.008237787 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 68.4        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 1.95        |
|    value_loss           | 211         |
-----------------------------------------
Eval num_timesteps=930000, episode_reward=220.60 +/- 136.83
Episode length: 357.00 +/- 28.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 357         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.006736535 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 76.8        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.95        |
|    value_loss           | 225         |
-----------------------------------------
Eval num_timesteps=932000, episode_reward=163.45 +/- 151.68
Episode length: 352.20 +/- 38.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 163          |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 0.0076815984 |
|    clip_fraction        | 0.0714       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 85.6         |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 1.95         |
|    value_loss           | 274          |
------------------------------------------
Eval num_timesteps=934000, episode_reward=332.10 +/- 296.94
Episode length: 391.60 +/- 25.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 934000       |
| train/                  |              |
|    approx_kl            | 0.0044494653 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 76           |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.000667    |
|    std                  | 1.95         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=936000, episode_reward=351.01 +/- 306.85
Episode length: 394.00 +/- 43.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0034664585 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 4570         |
|    policy_gradient_loss | 0.0012       |
|    std                  | 1.95         |
|    value_loss           | 402          |
------------------------------------------
Eval num_timesteps=938000, episode_reward=146.90 +/- 88.77
Episode length: 361.40 +/- 17.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 361         |
|    mean_reward          | 147         |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.009028133 |
|    clip_fraction        | 0.0557      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 153         |
|    n_updates            | 4580        |
|    policy_gradient_loss | 9.67e-05    |
|    std                  | 1.96        |
|    value_loss           | 488         |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=287.09 +/- 217.32
Episode length: 379.00 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=942000, episode_reward=564.24 +/- 371.90
Episode length: 409.00 +/- 19.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 564          |
| time/                   |              |
|    total_timesteps      | 942000       |
| train/                  |              |
|    approx_kl            | 0.0046045976 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.3         |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 67.3         |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.96         |
|    value_loss           | 203          |
------------------------------------------
Eval num_timesteps=944000, episode_reward=402.85 +/- 314.56
Episode length: 361.80 +/- 51.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | 403         |
| time/                   |             |
|    total_timesteps      | 944000      |
| train/                  |             |
|    approx_kl            | 0.008728379 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 251         |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.00311    |
|    std                  | 1.96        |
|    value_loss           | 620         |
-----------------------------------------
Eval num_timesteps=946000, episode_reward=152.87 +/- 192.37
Episode length: 362.20 +/- 31.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | 153          |
| time/                   |              |
|    total_timesteps      | 946000       |
| train/                  |              |
|    approx_kl            | 0.0011384618 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.3         |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 4610         |
|    policy_gradient_loss | -3.55e-05    |
|    std                  | 1.95         |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=948000, episode_reward=156.86 +/- 275.18
Episode length: 397.00 +/- 38.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 397           |
|    mean_reward          | 157           |
| time/                   |               |
|    total_timesteps      | 948000        |
| train/                  |               |
|    approx_kl            | 0.00020301598 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.29         |
|    explained_variance   | 0.676         |
|    learning_rate        | 0.001         |
|    loss                 | 5.87e+03      |
|    n_updates            | 4620          |
|    policy_gradient_loss | -0.000313     |
|    std                  | 1.95          |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=950000, episode_reward=440.01 +/- 248.26
Episode length: 388.60 +/- 24.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 440          |
| time/                   |              |
|    total_timesteps      | 950000       |
| train/                  |              |
|    approx_kl            | 0.0011688521 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 152          |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.000968    |
|    std                  | 1.95         |
|    value_loss           | 421          |
------------------------------------------
Eval num_timesteps=952000, episode_reward=340.88 +/- 231.89
Episode length: 358.20 +/- 23.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | 341         |
| time/                   |             |
|    total_timesteps      | 952000      |
| train/                  |             |
|    approx_kl            | 0.006904775 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 102         |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 1.95        |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=954000, episode_reward=208.97 +/- 173.44
Episode length: 335.40 +/- 27.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | 209          |
| time/                   |              |
|    total_timesteps      | 954000       |
| train/                  |              |
|    approx_kl            | 0.0034744155 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.29        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 79           |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.96         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=956000, episode_reward=242.64 +/- 332.41
Episode length: 371.20 +/- 29.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 243          |
| time/                   |              |
|    total_timesteps      | 956000       |
| train/                  |              |
|    approx_kl            | 0.0039095716 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.3         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 79.5         |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.000801    |
|    std                  | 1.97         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=958000, episode_reward=227.21 +/- 128.93
Episode length: 346.60 +/- 22.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 958000      |
| train/                  |             |
|    approx_kl            | 0.008644668 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | 355         |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00103    |
|    std                  | 1.98        |
|    value_loss           | 842         |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=117.79 +/- 107.86
Episode length: 330.40 +/- 23.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | 118          |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0057790736 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.35        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 98.1         |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.98         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=962000, episode_reward=297.36 +/- 185.11
Episode length: 338.20 +/- 18.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 338         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 962000      |
| train/                  |             |
|    approx_kl            | 0.009395454 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.001       |
|    loss                 | 2.87e+03    |
|    n_updates            | 4690        |
|    policy_gradient_loss | 0.00546     |
|    std                  | 1.98        |
|    value_loss           | 6.05e+03    |
-----------------------------------------
Eval num_timesteps=964000, episode_reward=81.99 +/- 37.38
Episode length: 312.40 +/- 17.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 82           |
| time/                   |              |
|    total_timesteps      | 964000       |
| train/                  |              |
|    approx_kl            | 0.0015499992 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 61.1         |
|    n_updates            | 4700         |
|    policy_gradient_loss | -0.000624    |
|    std                  | 1.99         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=966000, episode_reward=189.53 +/- 145.72
Episode length: 333.00 +/- 24.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 966000      |
| train/                  |             |
|    approx_kl            | 0.007079603 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 58.2        |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 1.99        |
|    value_loss           | 186         |
-----------------------------------------
Eval num_timesteps=968000, episode_reward=380.37 +/- 152.99
Episode length: 351.00 +/- 23.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | 380          |
| time/                   |              |
|    total_timesteps      | 968000       |
| train/                  |              |
|    approx_kl            | 0.0020265912 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 68.6         |
|    n_updates            | 4720         |
|    policy_gradient_loss | 5.85e-05     |
|    std                  | 1.99         |
|    value_loss           | 226          |
------------------------------------------
Eval num_timesteps=970000, episode_reward=529.93 +/- 157.00
Episode length: 379.40 +/- 17.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 530         |
| time/                   |             |
|    total_timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.011848817 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 56.8        |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00419    |
|    std                  | 1.99        |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=972000, episode_reward=324.76 +/- 228.92
Episode length: 396.60 +/- 46.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 325         |
| time/                   |             |
|    total_timesteps      | 972000      |
| train/                  |             |
|    approx_kl            | 0.008359288 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 72.3        |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 1.99        |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=974000, episode_reward=589.58 +/- 261.94
Episode length: 407.20 +/- 37.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 407         |
|    mean_reward          | 590         |
| time/                   |             |
|    total_timesteps      | 974000      |
| train/                  |             |
|    approx_kl            | 0.010265937 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 155         |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00295    |
|    std                  | 1.98        |
|    value_loss           | 480         |
-----------------------------------------
Eval num_timesteps=976000, episode_reward=306.66 +/- 437.08
Episode length: 432.60 +/- 52.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 307          |
| time/                   |              |
|    total_timesteps      | 976000       |
| train/                  |              |
|    approx_kl            | 0.0023444903 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.665        |
|    learning_rate        | 0.001        |
|    loss                 | 5.17e+03     |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.98         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=978000, episode_reward=401.14 +/- 453.40
Episode length: 419.60 +/- 51.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 401           |
| time/                   |               |
|    total_timesteps      | 978000        |
| train/                  |               |
|    approx_kl            | 0.00061937695 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.35         |
|    explained_variance   | 0.832         |
|    learning_rate        | 0.001         |
|    loss                 | 2.29e+03      |
|    n_updates            | 4770          |
|    policy_gradient_loss | -0.000763     |
|    std                  | 1.98          |
|    value_loss           | 5.04e+03      |
-------------------------------------------
Eval num_timesteps=980000, episode_reward=345.24 +/- 139.09
Episode length: 432.00 +/- 30.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 432           |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 0.00059708976 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.7           |
|    learning_rate        | 0.001         |
|    loss                 | 8.09e+03      |
|    n_updates            | 4780          |
|    policy_gradient_loss | 0.000466      |
|    std                  | 1.98          |
|    value_loss           | 1.73e+04      |
-------------------------------------------
Eval num_timesteps=982000, episode_reward=660.13 +/- 312.48
Episode length: 421.20 +/- 31.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 660          |
| time/                   |              |
|    total_timesteps      | 982000       |
| train/                  |              |
|    approx_kl            | 0.0045174276 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 83           |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 1.98         |
|    value_loss           | 364          |
------------------------------------------
Eval num_timesteps=984000, episode_reward=85.58 +/- 495.44
Episode length: 417.80 +/- 44.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 85.6         |
| time/                   |              |
|    total_timesteps      | 984000       |
| train/                  |              |
|    approx_kl            | 0.0044339956 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.65         |
|    learning_rate        | 0.001        |
|    loss                 | 3.78e+03     |
|    n_updates            | 4800         |
|    policy_gradient_loss | 0.000355     |
|    std                  | 1.98         |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=986000, episode_reward=663.55 +/- 170.51
Episode length: 452.80 +/- 48.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 664          |
| time/                   |              |
|    total_timesteps      | 986000       |
| train/                  |              |
|    approx_kl            | 0.0023659293 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.35        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 245          |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00214     |
|    std                  | 1.98         |
|    value_loss           | 834          |
------------------------------------------
Eval num_timesteps=988000, episode_reward=347.37 +/- 201.15
Episode length: 458.60 +/- 50.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 347          |
| time/                   |              |
|    total_timesteps      | 988000       |
| train/                  |              |
|    approx_kl            | 0.0020727958 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 2.59e+03     |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.000498    |
|    std                  | 1.98         |
|    value_loss           | 5.66e+03     |
------------------------------------------
Eval num_timesteps=990000, episode_reward=422.91 +/- 160.10
Episode length: 393.80 +/- 33.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 423          |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 0.0062064696 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.98         |
|    value_loss           | 452          |
------------------------------------------
Eval num_timesteps=992000, episode_reward=400.90 +/- 265.53
Episode length: 408.20 +/- 19.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 992000       |
| train/                  |              |
|    approx_kl            | 0.0031830436 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+03     |
|    n_updates            | 4840         |
|    policy_gradient_loss | -0.000542    |
|    std                  | 1.98         |
|    value_loss           | 6.29e+03     |
------------------------------------------
Eval num_timesteps=994000, episode_reward=281.92 +/- 239.65
Episode length: 393.80 +/- 14.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 282          |
| time/                   |              |
|    total_timesteps      | 994000       |
| train/                  |              |
|    approx_kl            | 0.0005037404 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+03     |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.000286    |
|    std                  | 1.98         |
|    value_loss           | 5.84e+03     |
------------------------------------------
Eval num_timesteps=996000, episode_reward=328.03 +/- 242.79
Episode length: 406.20 +/- 22.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 328           |
| time/                   |               |
|    total_timesteps      | 996000        |
| train/                  |               |
|    approx_kl            | 0.00024758244 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+03      |
|    n_updates            | 4860          |
|    policy_gradient_loss | -0.00035      |
|    std                  | 1.98          |
|    value_loss           | 4.87e+03      |
-------------------------------------------
Eval num_timesteps=998000, episode_reward=374.42 +/- 251.09
Episode length: 369.20 +/- 17.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 369           |
|    mean_reward          | 374           |
| time/                   |               |
|    total_timesteps      | 998000        |
| train/                  |               |
|    approx_kl            | 0.00019965225 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.828         |
|    learning_rate        | 0.001         |
|    loss                 | 2.32e+03      |
|    n_updates            | 4870          |
|    policy_gradient_loss | -0.000324     |
|    std                  | 1.98          |
|    value_loss           | 4.97e+03      |
-------------------------------------------
Eval num_timesteps=1000000, episode_reward=29.85 +/- 285.35
Episode length: 421.40 +/- 24.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 29.8         |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0034210589 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 223          |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.98         |
|    value_loss           | 713          |
------------------------------------------
Eval num_timesteps=1002000, episode_reward=303.73 +/- 374.45
Episode length: 427.40 +/- 29.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 1002000      |
| train/                  |              |
|    approx_kl            | 0.0023788125 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.765        |
|    learning_rate        | 0.001        |
|    loss                 | 4.17e+03     |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000132    |
|    std                  | 1.98         |
|    value_loss           | 9.35e+03     |
------------------------------------------
Eval num_timesteps=1004000, episode_reward=183.74 +/- 330.37
Episode length: 439.00 +/- 55.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 184           |
| time/                   |               |
|    total_timesteps      | 1004000       |
| train/                  |               |
|    approx_kl            | 0.00018603442 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.35         |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.001         |
|    loss                 | 2.87e+03      |
|    n_updates            | 4900          |
|    policy_gradient_loss | 2.95e-05      |
|    std                  | 1.98          |
|    value_loss           | 6.51e+03      |
-------------------------------------------
Eval num_timesteps=1006000, episode_reward=-87.19 +/- 288.36
Episode length: 415.20 +/- 34.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | -87.2        |
| time/                   |              |
|    total_timesteps      | 1006000      |
| train/                  |              |
|    approx_kl            | 0.0001191113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.35        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+03     |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.000354    |
|    std                  | 1.98         |
|    value_loss           | 7.16e+03     |
------------------------------------------
Eval num_timesteps=1008000, episode_reward=362.36 +/- 256.62
Episode length: 420.00 +/- 57.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 362           |
| time/                   |               |
|    total_timesteps      | 1008000       |
| train/                  |               |
|    approx_kl            | 0.00015973332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.741         |
|    learning_rate        | 0.001         |
|    loss                 | 3.72e+03      |
|    n_updates            | 4920          |
|    policy_gradient_loss | -0.000456     |
|    std                  | 1.98          |
|    value_loss           | 7.07e+03      |
-------------------------------------------
Eval num_timesteps=1010000, episode_reward=139.60 +/- 510.03
Episode length: 428.40 +/- 47.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 140          |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 9.578586e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.695        |
|    learning_rate        | 0.001        |
|    loss                 | 4.29e+03     |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.000332    |
|    std                  | 1.98         |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=1012000, episode_reward=303.58 +/- 258.75
Episode length: 422.40 +/- 31.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 304           |
| time/                   |               |
|    total_timesteps      | 1012000       |
| train/                  |               |
|    approx_kl            | 1.8680643e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.757         |
|    learning_rate        | 0.001         |
|    loss                 | 5.46e+03      |
|    n_updates            | 4940          |
|    policy_gradient_loss | 1.04e-05      |
|    std                  | 1.98          |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=1014000, episode_reward=533.32 +/- 212.78
Episode length: 444.20 +/- 30.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 444        |
|    mean_reward          | 533        |
| time/                   |            |
|    total_timesteps      | 1014000    |
| train/                  |            |
|    approx_kl            | 0.00250187 |
|    clip_fraction        | 0.000928   |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.36      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.001      |
|    loss                 | 185        |
|    n_updates            | 4950       |
|    policy_gradient_loss | -0.0014    |
|    std                  | 1.98       |
|    value_loss           | 966        |
----------------------------------------
Eval num_timesteps=1016000, episode_reward=537.56 +/- 305.87
Episode length: 405.60 +/- 27.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 538          |
| time/                   |              |
|    total_timesteps      | 1016000      |
| train/                  |              |
|    approx_kl            | 0.0053842627 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.36        |
|    explained_variance   | 0.749        |
|    learning_rate        | 0.001        |
|    loss                 | 3.17e+03     |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.98         |
|    value_loss           | 7.13e+03     |
------------------------------------------
Eval num_timesteps=1018000, episode_reward=82.89 +/- 515.53
Episode length: 501.20 +/- 19.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 501          |
|    mean_reward          | 82.9         |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 0.0045182556 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.35        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 126          |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 1.98         |
|    value_loss           | 607          |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=371.87 +/- 650.48
Episode length: 519.60 +/- 59.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 520        |
|    mean_reward          | 372        |
| time/                   |            |
|    total_timesteps      | 1020000    |
| train/                  |            |
|    approx_kl            | 0.00199779 |
|    clip_fraction        | 0.000977   |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.36      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.001      |
|    loss                 | 174        |
|    n_updates            | 4980       |
|    policy_gradient_loss | 0.000613   |
|    std                  | 1.98       |
|    value_loss           | 685        |
----------------------------------------
Eval num_timesteps=1022000, episode_reward=629.00 +/- 403.79
Episode length: 578.00 +/- 49.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 578           |
|    mean_reward          | 629           |
| time/                   |               |
|    total_timesteps      | 1022000       |
| train/                  |               |
|    approx_kl            | 0.00040036783 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.36         |
|    explained_variance   | 0.742         |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+03      |
|    n_updates            | 4990          |
|    policy_gradient_loss | -4.92e-05     |
|    std                  | 1.99          |
|    value_loss           | 6.16e+03      |
-------------------------------------------
Eval num_timesteps=1024000, episode_reward=108.66 +/- 433.27
Episode length: 505.20 +/- 70.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1026000, episode_reward=409.22 +/- 299.02
Episode length: 499.60 +/- 70.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 500           |
|    mean_reward          | 409           |
| time/                   |               |
|    total_timesteps      | 1026000       |
| train/                  |               |
|    approx_kl            | 0.00015866745 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.37         |
|    explained_variance   | 0.716         |
|    learning_rate        | 0.001         |
|    loss                 | 3.38e+03      |
|    n_updates            | 5000          |
|    policy_gradient_loss | 0.000471      |
|    std                  | 1.99          |
|    value_loss           | 7.86e+03      |
-------------------------------------------
Eval num_timesteps=1028000, episode_reward=517.85 +/- 202.44
Episode length: 561.20 +/- 13.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 561          |
|    mean_reward          | 518          |
| time/                   |              |
|    total_timesteps      | 1028000      |
| train/                  |              |
|    approx_kl            | 0.0010316252 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.37        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 176          |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.99         |
|    value_loss           | 706          |
------------------------------------------
Eval num_timesteps=1030000, episode_reward=643.08 +/- 384.99
Episode length: 593.60 +/- 127.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 594          |
|    mean_reward          | 643          |
| time/                   |              |
|    total_timesteps      | 1030000      |
| train/                  |              |
|    approx_kl            | 0.0039886967 |
|    clip_fraction        | 0.00674      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.37        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+03     |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 1.99         |
|    value_loss           | 4.05e+03     |
------------------------------------------
Eval num_timesteps=1032000, episode_reward=579.76 +/- 326.86
Episode length: 561.40 +/- 84.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 561           |
|    mean_reward          | 580           |
| time/                   |               |
|    total_timesteps      | 1032000       |
| train/                  |               |
|    approx_kl            | 0.00046499682 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.38         |
|    explained_variance   | 0.693         |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+03      |
|    n_updates            | 5030          |
|    policy_gradient_loss | 0.000182      |
|    std                  | 1.99          |
|    value_loss           | 6.73e+03      |
-------------------------------------------
Eval num_timesteps=1034000, episode_reward=1460.60 +/- 1944.62
Episode length: 623.80 +/- 102.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 624          |
|    mean_reward          | 1.46e+03     |
| time/                   |              |
|    total_timesteps      | 1034000      |
| train/                  |              |
|    approx_kl            | 0.0025805156 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.38        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 168          |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2            |
|    value_loss           | 605          |
------------------------------------------
New best mean reward!
Eval num_timesteps=1036000, episode_reward=543.52 +/- 509.79
Episode length: 530.40 +/- 58.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 530         |
|    mean_reward          | 544         |
| time/                   |             |
|    total_timesteps      | 1036000     |
| train/                  |             |
|    approx_kl            | 0.005773083 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.39       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.001       |
|    loss                 | 2.13e+03    |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 2           |
|    value_loss           | 5.86e+03    |
-----------------------------------------
Eval num_timesteps=1038000, episode_reward=-17.31 +/- 209.62
Episode length: 501.80 +/- 57.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -17.3       |
| time/                   |             |
|    total_timesteps      | 1038000     |
| train/                  |             |
|    approx_kl            | 0.004192003 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.39       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.001       |
|    loss                 | 341         |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.002      |
|    std                  | 2           |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=1040000, episode_reward=830.50 +/- 675.62
Episode length: 548.20 +/- 36.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 548          |
|    mean_reward          | 831          |
| time/                   |              |
|    total_timesteps      | 1040000      |
| train/                  |              |
|    approx_kl            | 0.0015945234 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 1.81e+03     |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.000552    |
|    std                  | 2            |
|    value_loss           | 4.14e+03     |
------------------------------------------
Eval num_timesteps=1042000, episode_reward=30.62 +/- 566.13
Episode length: 536.60 +/- 130.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 537       |
|    mean_reward          | 30.6      |
| time/                   |           |
|    total_timesteps      | 1042000   |
| train/                  |           |
|    approx_kl            | 0.0030723 |
|    clip_fraction        | 0.00474   |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.4      |
|    explained_variance   | 0.327     |
|    learning_rate        | 0.001     |
|    loss                 | 7.71e+03  |
|    n_updates            | 5080      |
|    policy_gradient_loss | -0.000722 |
|    std                  | 2         |
|    value_loss           | 1.85e+04  |
---------------------------------------
Eval num_timesteps=1044000, episode_reward=265.33 +/- 291.32
Episode length: 483.80 +/- 94.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 265          |
| time/                   |              |
|    total_timesteps      | 1044000      |
| train/                  |              |
|    approx_kl            | 0.0045072623 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.001        |
|    loss                 | 287          |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 2            |
|    value_loss           | 973          |
------------------------------------------
Eval num_timesteps=1046000, episode_reward=344.50 +/- 243.75
Episode length: 407.20 +/- 31.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 344          |
| time/                   |              |
|    total_timesteps      | 1046000      |
| train/                  |              |
|    approx_kl            | 0.0041353772 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.376        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+04     |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.000769    |
|    std                  | 2.01         |
|    value_loss           | 2.56e+04     |
------------------------------------------
Eval num_timesteps=1048000, episode_reward=485.40 +/- 229.95
Episode length: 383.60 +/- 30.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 485          |
| time/                   |              |
|    total_timesteps      | 1048000      |
| train/                  |              |
|    approx_kl            | 0.0011649302 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.001        |
|    loss                 | 4.16e+03     |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 2.01         |
|    value_loss           | 8.51e+03     |
------------------------------------------
Eval num_timesteps=1050000, episode_reward=292.49 +/- 268.51
Episode length: 378.00 +/- 40.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0013313266 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.41        |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.001        |
|    loss                 | 2.36e+03     |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 2.01         |
|    value_loss           | 5.74e+03     |
------------------------------------------
Eval num_timesteps=1052000, episode_reward=441.94 +/- 143.55
Episode length: 398.20 +/- 18.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | 442         |
| time/                   |             |
|    total_timesteps      | 1052000     |
| train/                  |             |
|    approx_kl            | 0.004583982 |
|    clip_fraction        | 0.00942     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.41       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 217         |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00206    |
|    std                  | 2.01        |
|    value_loss           | 675         |
-----------------------------------------
Eval num_timesteps=1054000, episode_reward=76.13 +/- 190.10
Episode length: 410.40 +/- 45.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 76.1          |
| time/                   |               |
|    total_timesteps      | 1054000       |
| train/                  |               |
|    approx_kl            | 0.00067641505 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.42         |
|    explained_variance   | 0.703         |
|    learning_rate        | 0.001         |
|    loss                 | 2.71e+03      |
|    n_updates            | 5140          |
|    policy_gradient_loss | 0.000276      |
|    std                  | 2.01          |
|    value_loss           | 6.63e+03      |
-------------------------------------------
Eval num_timesteps=1056000, episode_reward=259.90 +/- 130.35
Episode length: 392.20 +/- 51.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 260          |
| time/                   |              |
|    total_timesteps      | 1056000      |
| train/                  |              |
|    approx_kl            | 0.0037061153 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 176          |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 2.01         |
|    value_loss           | 544          |
------------------------------------------
Eval num_timesteps=1058000, episode_reward=217.43 +/- 217.03
Episode length: 439.60 +/- 45.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 1058000     |
| train/                  |             |
|    approx_kl            | 0.005148532 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.42       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 199         |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 2.02        |
|    value_loss           | 533         |
-----------------------------------------
Eval num_timesteps=1060000, episode_reward=102.95 +/- 200.73
Episode length: 460.20 +/- 95.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 103          |
| time/                   |              |
|    total_timesteps      | 1060000      |
| train/                  |              |
|    approx_kl            | 0.0014250011 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 168          |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 2.02         |
|    value_loss           | 485          |
------------------------------------------
Eval num_timesteps=1062000, episode_reward=173.38 +/- 135.53
Episode length: 442.20 +/- 22.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 1062000      |
| train/                  |              |
|    approx_kl            | 0.0040908605 |
|    clip_fraction        | 0.00815      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.43        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.000678    |
|    std                  | 2.02         |
|    value_loss           | 440          |
------------------------------------------
Eval num_timesteps=1064000, episode_reward=31.03 +/- 182.17
Episode length: 412.20 +/- 34.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 412        |
|    mean_reward          | 31         |
| time/                   |            |
|    total_timesteps      | 1064000    |
| train/                  |            |
|    approx_kl            | 0.01642399 |
|    clip_fraction        | 0.0571     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.43      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.001      |
|    loss                 | 227        |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.00413   |
|    std                  | 2.03       |
|    value_loss           | 707        |
----------------------------------------
Eval num_timesteps=1066000, episode_reward=74.49 +/- 85.82
Episode length: 385.80 +/- 26.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 386         |
|    mean_reward          | 74.5        |
| time/                   |             |
|    total_timesteps      | 1066000     |
| train/                  |             |
|    approx_kl            | 0.010245239 |
|    clip_fraction        | 0.055       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.45       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 92.1        |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00428    |
|    std                  | 2.03        |
|    value_loss           | 268         |
-----------------------------------------
Eval num_timesteps=1068000, episode_reward=133.02 +/- 110.08
Episode length: 462.40 +/- 47.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 1068000      |
| train/                  |              |
|    approx_kl            | 0.0078377705 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.46        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 2.03         |
|    value_loss           | 279          |
------------------------------------------
Eval num_timesteps=1070000, episode_reward=142.26 +/- 133.56
Episode length: 445.80 +/- 15.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 446         |
|    mean_reward          | 142         |
| time/                   |             |
|    total_timesteps      | 1070000     |
| train/                  |             |
|    approx_kl            | 0.005708969 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.46       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 121         |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 2.04        |
|    value_loss           | 340         |
-----------------------------------------
Eval num_timesteps=1072000, episode_reward=170.12 +/- 82.19
Episode length: 502.20 +/- 57.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 502          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 1072000      |
| train/                  |              |
|    approx_kl            | 0.0036700917 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.47        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 66           |
|    n_updates            | 5230         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 2.05         |
|    value_loss           | 234          |
------------------------------------------
Eval num_timesteps=1074000, episode_reward=502.30 +/- 326.54
Episode length: 415.20 +/- 42.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 1074000      |
| train/                  |              |
|    approx_kl            | 0.0063167405 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 61.9         |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.000818    |
|    std                  | 2.07         |
|    value_loss           | 199          |
------------------------------------------
Eval num_timesteps=1076000, episode_reward=246.44 +/- 509.46
Episode length: 434.80 +/- 32.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 435         |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 1076000     |
| train/                  |             |
|    approx_kl            | 0.007362541 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 86.9        |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.00317    |
|    std                  | 2.07        |
|    value_loss           | 273         |
-----------------------------------------
Eval num_timesteps=1078000, episode_reward=596.70 +/- 472.99
Episode length: 422.80 +/- 52.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 597          |
| time/                   |              |
|    total_timesteps      | 1078000      |
| train/                  |              |
|    approx_kl            | 0.0055158944 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+03      |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000572    |
|    std                  | 2.08         |
|    value_loss           | 6e+03        |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=97.31 +/- 541.54
Episode length: 430.40 +/- 24.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 97.3          |
| time/                   |               |
|    total_timesteps      | 1080000       |
| train/                  |               |
|    approx_kl            | 0.00036014649 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.53         |
|    explained_variance   | 0.796         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+03      |
|    n_updates            | 5270          |
|    policy_gradient_loss | 0.000233      |
|    std                  | 2.08          |
|    value_loss           | 4.52e+03      |
-------------------------------------------
Eval num_timesteps=1082000, episode_reward=252.91 +/- 393.68
Episode length: 426.80 +/- 45.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 1082000       |
| train/                  |               |
|    approx_kl            | 0.00050176046 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.53         |
|    explained_variance   | 0.72          |
|    learning_rate        | 0.001         |
|    loss                 | 2.94e+03      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000309     |
|    std                  | 2.08          |
|    value_loss           | 7.01e+03      |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=545.26 +/- 146.96
Episode length: 427.20 +/- 51.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 545          |
| time/                   |              |
|    total_timesteps      | 1084000      |
| train/                  |              |
|    approx_kl            | 0.0010129898 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 2.08         |
|    value_loss           | 448          |
------------------------------------------
Eval num_timesteps=1086000, episode_reward=467.52 +/- 257.34
Episode length: 401.20 +/- 34.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 468         |
| time/                   |             |
|    total_timesteps      | 1086000     |
| train/                  |             |
|    approx_kl            | 0.003421396 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 176         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 2.09        |
|    value_loss           | 665         |
-----------------------------------------
Eval num_timesteps=1088000, episode_reward=178.99 +/- 380.79
Episode length: 458.20 +/- 37.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 179          |
| time/                   |              |
|    total_timesteps      | 1088000      |
| train/                  |              |
|    approx_kl            | 0.0013515658 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.001        |
|    loss                 | 5.97e+03     |
|    n_updates            | 5310         |
|    policy_gradient_loss | 0.000708     |
|    std                  | 2.09         |
|    value_loss           | 1.41e+04     |
------------------------------------------
Eval num_timesteps=1090000, episode_reward=127.88 +/- 502.99
Episode length: 452.20 +/- 28.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 128           |
| time/                   |               |
|    total_timesteps      | 1090000       |
| train/                  |               |
|    approx_kl            | 0.00033370286 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.54         |
|    explained_variance   | 0.824         |
|    learning_rate        | 0.001         |
|    loss                 | 2.37e+03      |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.000455     |
|    std                  | 2.09          |
|    value_loss           | 5.27e+03      |
-------------------------------------------
Eval num_timesteps=1092000, episode_reward=218.69 +/- 553.43
Episode length: 462.40 +/- 47.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 219           |
| time/                   |               |
|    total_timesteps      | 1092000       |
| train/                  |               |
|    approx_kl            | 0.00026105606 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.54         |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+03      |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.000442     |
|    std                  | 2.09          |
|    value_loss           | 7.27e+03      |
-------------------------------------------
Eval num_timesteps=1094000, episode_reward=-3.77 +/- 337.04
Episode length: 427.40 +/- 57.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | -3.77         |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 0.00015066238 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.54         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 3.13e+03      |
|    n_updates            | 5340          |
|    policy_gradient_loss | -0.000239     |
|    std                  | 2.09          |
|    value_loss           | 6.91e+03      |
-------------------------------------------
Eval num_timesteps=1096000, episode_reward=-77.79 +/- 309.48
Episode length: 430.20 +/- 49.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | -77.8        |
| time/                   |              |
|    total_timesteps      | 1096000      |
| train/                  |              |
|    approx_kl            | 9.461568e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+03      |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.000187    |
|    std                  | 2.09         |
|    value_loss           | 8.96e+03     |
------------------------------------------
Eval num_timesteps=1098000, episode_reward=186.97 +/- 368.08
Episode length: 427.60 +/- 32.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 1098000      |
| train/                  |              |
|    approx_kl            | 0.0026879208 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 256          |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 2.09         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=1100000, episode_reward=372.97 +/- 347.83
Episode length: 431.20 +/- 35.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 373          |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0013466561 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.807        |
|    learning_rate        | 0.001        |
|    loss                 | 2.58e+03     |
|    n_updates            | 5370         |
|    policy_gradient_loss | 0.00117      |
|    std                  | 2.09         |
|    value_loss           | 5.86e+03     |
------------------------------------------
Eval num_timesteps=1102000, episode_reward=511.31 +/- 372.21
Episode length: 405.40 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 511           |
| time/                   |               |
|    total_timesteps      | 1102000       |
| train/                  |               |
|    approx_kl            | 0.00018944032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.772         |
|    learning_rate        | 0.001         |
|    loss                 | 2.41e+03      |
|    n_updates            | 5380          |
|    policy_gradient_loss | -0.000219     |
|    std                  | 2.09          |
|    value_loss           | 5.71e+03      |
-------------------------------------------
Eval num_timesteps=1104000, episode_reward=20.47 +/- 211.95
Episode length: 403.40 +/- 39.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 403          |
|    mean_reward          | 20.5         |
| time/                   |              |
|    total_timesteps      | 1104000      |
| train/                  |              |
|    approx_kl            | 0.0021404852 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 2.09         |
|    value_loss           | 380          |
------------------------------------------
Eval num_timesteps=1106000, episode_reward=316.48 +/- 239.59
Episode length: 390.60 +/- 18.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 1106000      |
| train/                  |              |
|    approx_kl            | 0.0030552666 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 226          |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.000517    |
|    std                  | 2.1          |
|    value_loss           | 824          |
------------------------------------------
Eval num_timesteps=1108000, episode_reward=351.96 +/- 307.01
Episode length: 408.00 +/- 19.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 352         |
| time/                   |             |
|    total_timesteps      | 1108000     |
| train/                  |             |
|    approx_kl            | 0.000562942 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.001       |
|    loss                 | 2.24e+03    |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.000124   |
|    std                  | 2.1         |
|    value_loss           | 5.3e+03     |
-----------------------------------------
Eval num_timesteps=1110000, episode_reward=293.17 +/- 238.65
Episode length: 390.20 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
Eval num_timesteps=1112000, episode_reward=126.20 +/- 377.72
Episode length: 387.40 +/- 35.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 1112000      |
| train/                  |              |
|    approx_kl            | 0.0008950267 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 2.63e+03     |
|    n_updates            | 5420         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 2.1          |
|    value_loss           | 5.97e+03     |
------------------------------------------
Eval num_timesteps=1114000, episode_reward=433.56 +/- 330.30
Episode length: 343.00 +/- 57.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 343         |
|    mean_reward          | 434         |
| time/                   |             |
|    total_timesteps      | 1114000     |
| train/                  |             |
|    approx_kl            | 0.004349309 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 107         |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0017     |
|    std                  | 2.1         |
|    value_loss           | 364         |
-----------------------------------------
Eval num_timesteps=1116000, episode_reward=322.60 +/- 93.13
Episode length: 366.60 +/- 13.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 367         |
|    mean_reward          | 323         |
| time/                   |             |
|    total_timesteps      | 1116000     |
| train/                  |             |
|    approx_kl            | 0.008002391 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 95          |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 2.1         |
|    value_loss           | 258         |
-----------------------------------------
Eval num_timesteps=1118000, episode_reward=353.55 +/- 176.24
Episode length: 356.40 +/- 17.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 1118000     |
| train/                  |             |
|    approx_kl            | 0.012117705 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.001       |
|    loss                 | 5.16e+03    |
|    n_updates            | 5450        |
|    policy_gradient_loss | 0.00391     |
|    std                  | 2.1         |
|    value_loss           | 1.09e+04    |
-----------------------------------------
Eval num_timesteps=1120000, episode_reward=203.40 +/- 415.72
Episode length: 380.60 +/- 30.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 1120000     |
| train/                  |             |
|    approx_kl            | 0.005999159 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 78          |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00216    |
|    std                  | 2.1         |
|    value_loss           | 250         |
-----------------------------------------
Eval num_timesteps=1122000, episode_reward=639.06 +/- 255.98
Episode length: 396.80 +/- 30.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 639          |
| time/                   |              |
|    total_timesteps      | 1122000      |
| train/                  |              |
|    approx_kl            | 0.0025050617 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.001        |
|    loss                 | 5.7e+03      |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 2.11         |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=1124000, episode_reward=61.17 +/- 429.69
Episode length: 413.60 +/- 51.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 61.2         |
| time/                   |              |
|    total_timesteps      | 1124000      |
| train/                  |              |
|    approx_kl            | 0.0015547327 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.001        |
|    loss                 | 4.42e+03     |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 2.11         |
|    value_loss           | 9.79e+03     |
------------------------------------------
Eval num_timesteps=1126000, episode_reward=411.50 +/- 371.85
Episode length: 397.60 +/- 24.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 411          |
| time/                   |              |
|    total_timesteps      | 1126000      |
| train/                  |              |
|    approx_kl            | 0.0029432448 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 54.1         |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 2.11         |
|    value_loss           | 184          |
------------------------------------------
Eval num_timesteps=1128000, episode_reward=497.88 +/- 259.52
Episode length: 377.60 +/- 32.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 498          |
| time/                   |              |
|    total_timesteps      | 1128000      |
| train/                  |              |
|    approx_kl            | 0.0061613526 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 87.8         |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 2.11         |
|    value_loss           | 213          |
------------------------------------------
Eval num_timesteps=1130000, episode_reward=134.53 +/- 345.48
Episode length: 382.00 +/- 41.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 1130000      |
| train/                  |              |
|    approx_kl            | 0.0020746037 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.853        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.000714    |
|    std                  | 2.11         |
|    value_loss           | 3.02e+03     |
------------------------------------------
Eval num_timesteps=1132000, episode_reward=217.60 +/- 87.95
Episode length: 378.00 +/- 36.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 1132000     |
| train/                  |             |
|    approx_kl            | 0.002745658 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 414         |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00139    |
|    std                  | 2.1         |
|    value_loss           | 982         |
-----------------------------------------
Eval num_timesteps=1134000, episode_reward=399.40 +/- 133.46
Episode length: 361.60 +/- 13.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | 399          |
| time/                   |              |
|    total_timesteps      | 1134000      |
| train/                  |              |
|    approx_kl            | 0.0026141258 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.001        |
|    loss                 | 2.22e+03     |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.000194    |
|    std                  | 2.1          |
|    value_loss           | 5e+03        |
------------------------------------------
Eval num_timesteps=1136000, episode_reward=608.93 +/- 399.23
Episode length: 394.20 +/- 17.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 609          |
| time/                   |              |
|    total_timesteps      | 1136000      |
| train/                  |              |
|    approx_kl            | 0.0010755451 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+03     |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.000376    |
|    std                  | 2.1          |
|    value_loss           | 4.64e+03     |
------------------------------------------
Eval num_timesteps=1138000, episode_reward=598.71 +/- 364.90
Episode length: 391.60 +/- 14.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 599          |
| time/                   |              |
|    total_timesteps      | 1138000      |
| train/                  |              |
|    approx_kl            | 0.0035765888 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 60.6         |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2.09         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=1140000, episode_reward=388.44 +/- 170.60
Episode length: 367.60 +/- 18.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 388         |
| time/                   |             |
|    total_timesteps      | 1140000     |
| train/                  |             |
|    approx_kl            | 0.003998141 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.001       |
|    loss                 | 2.56e+03    |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00143    |
|    std                  | 2.09        |
|    value_loss           | 5.48e+03    |
-----------------------------------------
Eval num_timesteps=1142000, episode_reward=452.55 +/- 315.94
Episode length: 401.40 +/- 19.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 453         |
| time/                   |             |
|    total_timesteps      | 1142000     |
| train/                  |             |
|    approx_kl            | 0.005067462 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 60.6        |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 2.09        |
|    value_loss           | 176         |
-----------------------------------------
Eval num_timesteps=1144000, episode_reward=378.72 +/- 198.81
Episode length: 407.40 +/- 41.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 379          |
| time/                   |              |
|    total_timesteps      | 1144000      |
| train/                  |              |
|    approx_kl            | 0.0044753663 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 34.1         |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.000689    |
|    std                  | 2.08         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=1146000, episode_reward=203.56 +/- 306.92
Episode length: 400.60 +/- 29.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 1146000      |
| train/                  |              |
|    approx_kl            | 0.0048373383 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 47.8         |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 2.07         |
|    value_loss           | 169          |
------------------------------------------
Eval num_timesteps=1148000, episode_reward=343.50 +/- 152.63
Episode length: 397.00 +/- 33.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 1148000      |
| train/                  |              |
|    approx_kl            | 0.0039054703 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 5600         |
|    policy_gradient_loss | 0.00229      |
|    std                  | 2.07         |
|    value_loss           | 6.01e+03     |
------------------------------------------
Eval num_timesteps=1150000, episode_reward=60.39 +/- 121.10
Episode length: 418.80 +/- 34.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 60.4         |
| time/                   |              |
|    total_timesteps      | 1150000      |
| train/                  |              |
|    approx_kl            | 0.0007504097 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.000774    |
|    std                  | 2.06         |
|    value_loss           | 5.16e+03     |
------------------------------------------
Eval num_timesteps=1152000, episode_reward=224.99 +/- 208.69
Episode length: 407.00 +/- 14.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 225           |
| time/                   |               |
|    total_timesteps      | 1152000       |
| train/                  |               |
|    approx_kl            | 0.00040502523 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.5          |
|    explained_variance   | 0.84          |
|    learning_rate        | 0.001         |
|    loss                 | 2.37e+03      |
|    n_updates            | 5620          |
|    policy_gradient_loss | -5.2e-05      |
|    std                  | 2.06          |
|    value_loss           | 5.19e+03      |
-------------------------------------------
Eval num_timesteps=1154000, episode_reward=105.64 +/- 100.56
Episode length: 397.40 +/- 15.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 1154000      |
| train/                  |              |
|    approx_kl            | 0.0037361705 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 91.5         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.00189     |
|    std                  | 2.06         |
|    value_loss           | 392          |
------------------------------------------
Eval num_timesteps=1156000, episode_reward=61.80 +/- 240.58
Episode length: 396.40 +/- 52.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 61.8        |
| time/                   |             |
|    total_timesteps      | 1156000     |
| train/                  |             |
|    approx_kl            | 0.004710624 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.5        |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.001       |
|    loss                 | 3e+03       |
|    n_updates            | 5640        |
|    policy_gradient_loss | -5.29e-05   |
|    std                  | 2.07        |
|    value_loss           | 6.82e+03    |
-----------------------------------------
Eval num_timesteps=1158000, episode_reward=301.24 +/- 207.10
Episode length: 384.40 +/- 13.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 1158000      |
| train/                  |              |
|    approx_kl            | 0.0018377379 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.001        |
|    loss                 | 2.06e+03     |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 2.07         |
|    value_loss           | 4.63e+03     |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=438.67 +/- 236.49
Episode length: 402.60 +/- 18.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 403          |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 1160000      |
| train/                  |              |
|    approx_kl            | 0.0015699177 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.001        |
|    loss                 | 4.28e+03     |
|    n_updates            | 5660         |
|    policy_gradient_loss | 0.000303     |
|    std                  | 2.07         |
|    value_loss           | 9.16e+03     |
------------------------------------------
Eval num_timesteps=1162000, episode_reward=400.60 +/- 392.57
Episode length: 393.00 +/- 32.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 401           |
| time/                   |               |
|    total_timesteps      | 1162000       |
| train/                  |               |
|    approx_kl            | 0.00069478696 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.001         |
|    loss                 | 422           |
|    n_updates            | 5670          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 2.07          |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=1164000, episode_reward=80.31 +/- 315.11
Episode length: 402.00 +/- 36.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 80.3         |
| time/                   |              |
|    total_timesteps      | 1164000      |
| train/                  |              |
|    approx_kl            | 0.0013620559 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 5.57e+03     |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.000283    |
|    std                  | 2.07         |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=1166000, episode_reward=389.98 +/- 224.32
Episode length: 383.00 +/- 30.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 390           |
| time/                   |               |
|    total_timesteps      | 1166000       |
| train/                  |               |
|    approx_kl            | 0.00039775134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.51         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 5690          |
|    policy_gradient_loss | -1.41e-05     |
|    std                  | 2.07          |
|    value_loss           | 5.31e+03      |
-------------------------------------------
Eval num_timesteps=1168000, episode_reward=431.90 +/- 280.09
Episode length: 399.80 +/- 8.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 1168000      |
| train/                  |              |
|    approx_kl            | 0.0016920725 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 2.07         |
|    value_loss           | 358          |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=142.45 +/- 176.56
Episode length: 373.60 +/- 18.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 142         |
| time/                   |             |
|    total_timesteps      | 1170000     |
| train/                  |             |
|    approx_kl            | 0.005155253 |
|    clip_fraction        | 0.00835     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 93.6        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 2.07        |
|    value_loss           | 402         |
-----------------------------------------
Eval num_timesteps=1172000, episode_reward=128.19 +/- 220.94
Episode length: 344.20 +/- 32.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 1172000      |
| train/                  |              |
|    approx_kl            | 0.0027981964 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 46.6         |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00293     |
|    std                  | 2.07         |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=1174000, episode_reward=334.20 +/- 221.01
Episode length: 356.60 +/- 16.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | 334          |
| time/                   |              |
|    total_timesteps      | 1174000      |
| train/                  |              |
|    approx_kl            | 0.0053897067 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 38.6         |
|    n_updates            | 5730         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 2.07         |
|    value_loss           | 109          |
------------------------------------------
Eval num_timesteps=1176000, episode_reward=285.16 +/- 252.64
Episode length: 392.40 +/- 26.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 1176000     |
| train/                  |             |
|    approx_kl            | 0.007889624 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.001       |
|    loss                 | 23.2        |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00316    |
|    std                  | 2.07        |
|    value_loss           | 67.7        |
-----------------------------------------
Eval num_timesteps=1178000, episode_reward=56.17 +/- 322.69
Episode length: 390.40 +/- 47.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 390         |
|    mean_reward          | 56.2        |
| time/                   |             |
|    total_timesteps      | 1178000     |
| train/                  |             |
|    approx_kl            | 0.015535114 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.001       |
|    loss                 | 2.23e+03    |
|    n_updates            | 5750        |
|    policy_gradient_loss | 0.00215     |
|    std                  | 2.07        |
|    value_loss           | 5.61e+03    |
-----------------------------------------
Eval num_timesteps=1180000, episode_reward=342.06 +/- 315.22
Episode length: 427.00 +/- 36.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 342          |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 0.0060679293 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 49.8         |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 2.08         |
|    value_loss           | 199          |
------------------------------------------
Eval num_timesteps=1182000, episode_reward=279.25 +/- 468.60
Episode length: 442.60 +/- 11.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 443         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 1182000     |
| train/                  |             |
|    approx_kl            | 0.004854056 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 81          |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 2.08        |
|    value_loss           | 324         |
-----------------------------------------
Eval num_timesteps=1184000, episode_reward=286.72 +/- 435.95
Episode length: 478.40 +/- 43.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 1184000      |
| train/                  |              |
|    approx_kl            | 0.0026273117 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.001        |
|    loss                 | 4.72e+03     |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.00057     |
|    std                  | 2.08         |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=1186000, episode_reward=79.85 +/- 438.56
Episode length: 483.00 +/- 17.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 483         |
|    mean_reward          | 79.8        |
| time/                   |             |
|    total_timesteps      | 1186000     |
| train/                  |             |
|    approx_kl            | 0.002732074 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.001       |
|    loss                 | 2.49e+03    |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00101    |
|    std                  | 2.08        |
|    value_loss           | 5.49e+03    |
-----------------------------------------
Eval num_timesteps=1188000, episode_reward=62.85 +/- 326.00
Episode length: 460.40 +/- 53.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 62.8         |
| time/                   |              |
|    total_timesteps      | 1188000      |
| train/                  |              |
|    approx_kl            | 0.0052335127 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 213          |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 2.08         |
|    value_loss           | 936          |
------------------------------------------
Eval num_timesteps=1190000, episode_reward=82.78 +/- 536.55
Episode length: 459.80 +/- 31.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 82.8         |
| time/                   |              |
|    total_timesteps      | 1190000      |
| train/                  |              |
|    approx_kl            | 0.0031133755 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 5.2e+03      |
|    n_updates            | 5810         |
|    policy_gradient_loss | 0.000938     |
|    std                  | 2.08         |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=1192000, episode_reward=346.45 +/- 574.48
Episode length: 483.00 +/- 26.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 483         |
|    mean_reward          | 346         |
| time/                   |             |
|    total_timesteps      | 1192000     |
| train/                  |             |
|    approx_kl            | 0.008000698 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 55.9        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 2.08        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=1194000, episode_reward=113.06 +/- 279.43
Episode length: 492.80 +/- 41.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 1194000     |
| train/                  |             |
|    approx_kl            | 0.005013679 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 2.08        |
|    value_loss           | 438         |
-----------------------------------------
Eval num_timesteps=1196000, episode_reward=198.83 +/- 202.69
Episode length: 478.80 +/- 57.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 479      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 1196000  |
---------------------------------
Eval num_timesteps=1198000, episode_reward=446.23 +/- 419.88
Episode length: 468.20 +/- 54.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 1198000      |
| train/                  |              |
|    approx_kl            | 0.0010527278 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 111          |
|    n_updates            | 5840         |
|    policy_gradient_loss | 0.000348     |
|    std                  | 2.08         |
|    value_loss           | 456          |
------------------------------------------
Eval num_timesteps=1200000, episode_reward=405.96 +/- 415.99
Episode length: 428.60 +/- 63.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0018379474 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 90.6         |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.08         |
|    value_loss           | 352          |
------------------------------------------
Eval num_timesteps=1202000, episode_reward=-95.55 +/- 393.04
Episode length: 461.60 +/- 22.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | -95.6        |
| time/                   |              |
|    total_timesteps      | 1202000      |
| train/                  |              |
|    approx_kl            | 0.0043812385 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 89.6         |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.00048     |
|    std                  | 2.09         |
|    value_loss           | 320          |
------------------------------------------
Eval num_timesteps=1204000, episode_reward=284.49 +/- 523.29
Episode length: 459.60 +/- 37.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 1204000     |
| train/                  |             |
|    approx_kl            | 0.001145792 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 2.4e+03     |
|    n_updates            | 5870        |
|    policy_gradient_loss | 0.000547    |
|    std                  | 2.09        |
|    value_loss           | 5.7e+03     |
-----------------------------------------
Eval num_timesteps=1206000, episode_reward=208.72 +/- 213.35
Episode length: 427.60 +/- 61.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 428        |
|    mean_reward          | 209        |
| time/                   |            |
|    total_timesteps      | 1206000    |
| train/                  |            |
|    approx_kl            | 0.00482182 |
|    clip_fraction        | 0.0084     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.54      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.001      |
|    loss                 | 108        |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0014    |
|    std                  | 2.09       |
|    value_loss           | 422        |
----------------------------------------
Eval num_timesteps=1208000, episode_reward=353.74 +/- 400.53
Episode length: 427.20 +/- 72.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 1208000      |
| train/                  |              |
|    approx_kl            | 0.0015886337 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 2.87e+03     |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.09         |
|    value_loss           | 6.54e+03     |
------------------------------------------
Eval num_timesteps=1210000, episode_reward=185.95 +/- 79.05
Episode length: 454.60 +/- 35.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 1210000      |
| train/                  |              |
|    approx_kl            | 0.0002562313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0.76         |
|    learning_rate        | 0.001        |
|    loss                 | 4.61e+03     |
|    n_updates            | 5900         |
|    policy_gradient_loss | 0.000115     |
|    std                  | 2.1          |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=1212000, episode_reward=313.55 +/- 250.35
Episode length: 449.60 +/- 22.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 314           |
| time/                   |               |
|    total_timesteps      | 1212000       |
| train/                  |               |
|    approx_kl            | 0.00039448187 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 0.821         |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+03      |
|    n_updates            | 5910          |
|    policy_gradient_loss | -0.000415     |
|    std                  | 2.1           |
|    value_loss           | 4.71e+03      |
-------------------------------------------
Eval num_timesteps=1214000, episode_reward=367.50 +/- 460.33
Episode length: 440.40 +/- 90.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 367         |
| time/                   |             |
|    total_timesteps      | 1214000     |
| train/                  |             |
|    approx_kl            | 0.001477396 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.56       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 31.9        |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.000369   |
|    std                  | 2.1         |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=1216000, episode_reward=354.28 +/- 265.01
Episode length: 500.20 +/- 69.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 1216000     |
| train/                  |             |
|    approx_kl            | 0.005721205 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 78.9        |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 2.11        |
|    value_loss           | 270         |
-----------------------------------------
Eval num_timesteps=1218000, episode_reward=389.90 +/- 293.90
Episode length: 483.80 +/- 47.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 1218000      |
| train/                  |              |
|    approx_kl            | 0.0033298736 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 85.7         |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.000178    |
|    std                  | 2.12         |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=247.03 +/- 273.42
Episode length: 463.20 +/- 54.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 463         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 1220000     |
| train/                  |             |
|    approx_kl            | 0.002594255 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 280         |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.000801   |
|    std                  | 2.12        |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=1222000, episode_reward=147.73 +/- 185.17
Episode length: 423.60 +/- 69.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 148          |
| time/                   |              |
|    total_timesteps      | 1222000      |
| train/                  |              |
|    approx_kl            | 0.0019007148 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.6         |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 125          |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.12         |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=1224000, episode_reward=213.75 +/- 90.19
Episode length: 493.40 +/- 60.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 1224000     |
| train/                  |             |
|    approx_kl            | 0.005696075 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 125         |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.00127    |
|    std                  | 2.12        |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=1226000, episode_reward=393.13 +/- 231.57
Episode length: 476.60 +/- 72.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 393         |
| time/                   |             |
|    total_timesteps      | 1226000     |
| train/                  |             |
|    approx_kl            | 0.004718052 |
|    clip_fraction        | 0.00791     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 125         |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00184    |
|    std                  | 2.12        |
|    value_loss           | 524         |
-----------------------------------------
Eval num_timesteps=1228000, episode_reward=382.95 +/- 311.52
Episode length: 495.40 +/- 55.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 383          |
| time/                   |              |
|    total_timesteps      | 1228000      |
| train/                  |              |
|    approx_kl            | 0.0049810233 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 61.9         |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 2.12         |
|    value_loss           | 195          |
------------------------------------------
Eval num_timesteps=1230000, episode_reward=660.95 +/- 262.64
Episode length: 501.60 +/- 40.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | 661         |
| time/                   |             |
|    total_timesteps      | 1230000     |
| train/                  |             |
|    approx_kl            | 0.004415253 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 22          |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00111    |
|    std                  | 2.13        |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=1232000, episode_reward=189.42 +/- 84.36
Episode length: 465.60 +/- 46.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 1232000     |
| train/                  |             |
|    approx_kl            | 0.013008809 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.61       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.001       |
|    loss                 | 2.14e+03    |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.000225   |
|    std                  | 2.14        |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=1234000, episode_reward=169.34 +/- 156.68
Episode length: 442.00 +/- 65.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 1234000      |
| train/                  |              |
|    approx_kl            | 0.0031416854 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.63        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.000754    |
|    std                  | 2.15         |
|    value_loss           | 744          |
------------------------------------------
Eval num_timesteps=1236000, episode_reward=488.95 +/- 312.54
Episode length: 497.40 +/- 35.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 489         |
| time/                   |             |
|    total_timesteps      | 1236000     |
| train/                  |             |
|    approx_kl            | 0.009458288 |
|    clip_fraction        | 0.0756      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.63       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 34.4        |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 2.15        |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=1238000, episode_reward=831.65 +/- 442.72
Episode length: 484.40 +/- 42.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | 832         |
| time/                   |             |
|    total_timesteps      | 1238000     |
| train/                  |             |
|    approx_kl            | 0.008282304 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.64       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.001       |
|    loss                 | 5.59e+03    |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.000515   |
|    std                  | 2.15        |
|    value_loss           | 1.46e+04    |
-----------------------------------------
Eval num_timesteps=1240000, episode_reward=-9.36 +/- 372.99
Episode length: 472.00 +/- 49.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 472           |
|    mean_reward          | -9.36         |
| time/                   |               |
|    total_timesteps      | 1240000       |
| train/                  |               |
|    approx_kl            | 0.00041771468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.65         |
|    explained_variance   | 0.757         |
|    learning_rate        | 0.001         |
|    loss                 | 4.75e+03      |
|    n_updates            | 6050          |
|    policy_gradient_loss | 0.000389      |
|    std                  | 2.15          |
|    value_loss           | 1.06e+04      |
-------------------------------------------
Eval num_timesteps=1242000, episode_reward=576.07 +/- 217.75
Episode length: 521.20 +/- 22.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 521         |
|    mean_reward          | 576         |
| time/                   |             |
|    total_timesteps      | 1242000     |
| train/                  |             |
|    approx_kl            | 0.008982276 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.65       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 82.5        |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.00526    |
|    std                  | 2.16        |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=1244000, episode_reward=238.11 +/- 415.48
Episode length: 477.20 +/- 24.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 1244000     |
| train/                  |             |
|    approx_kl            | 0.004264842 |
|    clip_fraction        | 0.00596     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.65       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.000831   |
|    std                  | 2.16        |
|    value_loss           | 507         |
-----------------------------------------
Eval num_timesteps=1246000, episode_reward=424.94 +/- 275.30
Episode length: 469.20 +/- 40.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 1246000     |
| train/                  |             |
|    approx_kl            | 0.002640536 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.66       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.001       |
|    loss                 | 179         |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.000468   |
|    std                  | 2.16        |
|    value_loss           | 702         |
-----------------------------------------
Eval num_timesteps=1248000, episode_reward=99.49 +/- 76.97
Episode length: 424.40 +/- 55.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 99.5         |
| time/                   |              |
|    total_timesteps      | 1248000      |
| train/                  |              |
|    approx_kl            | 0.0022192392 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.16         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=1250000, episode_reward=170.15 +/- 572.44
Episode length: 452.60 +/- 47.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 1250000     |
| train/                  |             |
|    approx_kl            | 0.003969052 |
|    clip_fraction        | 0.00444     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.67       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 50.6        |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00177    |
|    std                  | 2.16        |
|    value_loss           | 233         |
-----------------------------------------
Eval num_timesteps=1252000, episode_reward=247.11 +/- 288.45
Episode length: 397.00 +/- 19.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 1252000     |
| train/                  |             |
|    approx_kl            | 0.002926039 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.67       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 70.2        |
|    n_updates            | 6110        |
|    policy_gradient_loss | -0.000631   |
|    std                  | 2.17        |
|    value_loss           | 346         |
-----------------------------------------
Eval num_timesteps=1254000, episode_reward=182.04 +/- 257.62
Episode length: 379.60 +/- 28.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 380         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 1254000     |
| train/                  |             |
|    approx_kl            | 0.004882155 |
|    clip_fraction        | 0.00967     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.68       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 53          |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.000748   |
|    std                  | 2.17        |
|    value_loss           | 197         |
-----------------------------------------
Eval num_timesteps=1256000, episode_reward=226.41 +/- 159.68
Episode length: 386.20 +/- 28.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 226          |
| time/                   |              |
|    total_timesteps      | 1256000      |
| train/                  |              |
|    approx_kl            | 0.0008050178 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.68        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.001        |
|    loss                 | 2.52e+03     |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.000397    |
|    std                  | 2.17         |
|    value_loss           | 5.89e+03     |
------------------------------------------
Eval num_timesteps=1258000, episode_reward=104.15 +/- 112.61
Episode length: 359.40 +/- 36.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 1258000      |
| train/                  |              |
|    approx_kl            | 0.0013750367 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.68        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 57.5         |
|    n_updates            | 6140         |
|    policy_gradient_loss | -0.000505    |
|    std                  | 2.17         |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=1260000, episode_reward=75.45 +/- 112.43
Episode length: 356.20 +/- 18.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | 75.5        |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.009244553 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.68       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 39.7        |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.00353    |
|    std                  | 2.17        |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=1262000, episode_reward=76.41 +/- 250.58
Episode length: 341.00 +/- 38.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | 76.4         |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0057211886 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.68        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 38.3         |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 2.17         |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=1264000, episode_reward=137.32 +/- 308.20
Episode length: 381.00 +/- 38.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 1264000     |
| train/                  |             |
|    approx_kl            | 0.009403357 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.67       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.001       |
|    loss                 | 31.4        |
|    n_updates            | 6170        |
|    policy_gradient_loss | -0.00252    |
|    std                  | 2.16        |
|    value_loss           | 83.6        |
-----------------------------------------
Eval num_timesteps=1266000, episode_reward=138.61 +/- 334.17
Episode length: 402.40 +/- 13.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 1266000     |
| train/                  |             |
|    approx_kl            | 0.012847677 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.66       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 40.3        |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0033     |
|    std                  | 2.16        |
|    value_loss           | 133         |
-----------------------------------------
Eval num_timesteps=1268000, episode_reward=287.18 +/- 403.33
Episode length: 401.40 +/- 21.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 1268000      |
| train/                  |              |
|    approx_kl            | 0.0032971748 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.678        |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+03     |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.000977    |
|    std                  | 2.16         |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=1270000, episode_reward=-30.47 +/- 398.58
Episode length: 401.20 +/- 14.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | -30.5        |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0006465061 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+03     |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.000317    |
|    std                  | 2.16         |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=1272000, episode_reward=141.97 +/- 383.99
Episode length: 419.40 +/- 53.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 142          |
| time/                   |              |
|    total_timesteps      | 1272000      |
| train/                  |              |
|    approx_kl            | 5.057009e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 2.06e+03     |
|    n_updates            | 6210         |
|    policy_gradient_loss | 4.73e-05     |
|    std                  | 2.16         |
|    value_loss           | 5.03e+03     |
------------------------------------------
Eval num_timesteps=1274000, episode_reward=146.77 +/- 443.60
Episode length: 434.40 +/- 31.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 147           |
| time/                   |               |
|    total_timesteps      | 1274000       |
| train/                  |               |
|    approx_kl            | 6.3865824e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.67         |
|    explained_variance   | 0.769         |
|    learning_rate        | 0.001         |
|    loss                 | 4.68e+03      |
|    n_updates            | 6220          |
|    policy_gradient_loss | -8.63e-05     |
|    std                  | 2.16          |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=1276000, episode_reward=47.97 +/- 366.92
Episode length: 400.80 +/- 33.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 48            |
| time/                   |               |
|    total_timesteps      | 1276000       |
| train/                  |               |
|    approx_kl            | 0.00013516279 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.67         |
|    explained_variance   | 0.758         |
|    learning_rate        | 0.001         |
|    loss                 | 3.74e+03      |
|    n_updates            | 6230          |
|    policy_gradient_loss | -0.000285     |
|    std                  | 2.16          |
|    value_loss           | 8.77e+03      |
-------------------------------------------
Eval num_timesteps=1278000, episode_reward=405.51 +/- 301.00
Episode length: 367.00 +/- 12.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 367            |
|    mean_reward          | 406            |
| time/                   |                |
|    total_timesteps      | 1278000        |
| train/                  |                |
|    approx_kl            | 0.000109821325 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.67          |
|    explained_variance   | 0.841          |
|    learning_rate        | 0.001          |
|    loss                 | 3.98e+03       |
|    n_updates            | 6240           |
|    policy_gradient_loss | -0.000129      |
|    std                  | 2.16           |
|    value_loss           | 8.3e+03        |
--------------------------------------------
Eval num_timesteps=1280000, episode_reward=253.82 +/- 463.93
Episode length: 401.80 +/- 41.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1282000, episode_reward=380.19 +/- 309.36
Episode length: 385.60 +/- 20.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 380           |
| time/                   |               |
|    total_timesteps      | 1282000       |
| train/                  |               |
|    approx_kl            | 0.00052125077 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.67         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+03      |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000496     |
|    std                  | 2.17          |
|    value_loss           | 3.9e+03       |
-------------------------------------------
Eval num_timesteps=1284000, episode_reward=373.15 +/- 270.31
Episode length: 385.80 +/- 23.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 373          |
| time/                   |              |
|    total_timesteps      | 1284000      |
| train/                  |              |
|    approx_kl            | 0.0011996976 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.67        |
|    explained_variance   | 0.848        |
|    learning_rate        | 0.001        |
|    loss                 | 3.1e+03      |
|    n_updates            | 6260         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 2.17         |
|    value_loss           | 6.63e+03     |
------------------------------------------
Eval num_timesteps=1286000, episode_reward=525.41 +/- 178.99
Episode length: 366.40 +/- 26.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 366           |
|    mean_reward          | 525           |
| time/                   |               |
|    total_timesteps      | 1286000       |
| train/                  |               |
|    approx_kl            | 0.00094029366 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.68         |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+03      |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.0006       |
|    std                  | 2.17          |
|    value_loss           | 4.97e+03      |
-------------------------------------------
Eval num_timesteps=1288000, episode_reward=511.05 +/- 150.93
Episode length: 365.20 +/- 21.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 365         |
|    mean_reward          | 511         |
| time/                   |             |
|    total_timesteps      | 1288000     |
| train/                  |             |
|    approx_kl            | 0.008056125 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.68       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 45.9        |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 2.18        |
|    value_loss           | 208         |
-----------------------------------------
Eval num_timesteps=1290000, episode_reward=346.66 +/- 214.83
Episode length: 380.20 +/- 22.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 347          |
| time/                   |              |
|    total_timesteps      | 1290000      |
| train/                  |              |
|    approx_kl            | 0.0016629979 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.69        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 3.89e+03     |
|    n_updates            | 6290         |
|    policy_gradient_loss | -7.68e-05    |
|    std                  | 2.18         |
|    value_loss           | 8.44e+03     |
------------------------------------------
Eval num_timesteps=1292000, episode_reward=407.03 +/- 233.13
Episode length: 390.40 +/- 20.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 390         |
|    mean_reward          | 407         |
| time/                   |             |
|    total_timesteps      | 1292000     |
| train/                  |             |
|    approx_kl            | 0.011048267 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.7        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 312         |
|    n_updates            | 6300        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 2.19        |
|    value_loss           | 710         |
-----------------------------------------
Eval num_timesteps=1294000, episode_reward=340.40 +/- 352.30
Episode length: 385.40 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 340          |
| time/                   |              |
|    total_timesteps      | 1294000      |
| train/                  |              |
|    approx_kl            | 0.0011412001 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.71        |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.001        |
|    loss                 | 5e+03        |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.00064     |
|    std                  | 2.19         |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=1296000, episode_reward=441.35 +/- 284.10
Episode length: 380.20 +/- 42.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | 441           |
| time/                   |               |
|    total_timesteps      | 1296000       |
| train/                  |               |
|    approx_kl            | 0.00032884764 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.71         |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.001         |
|    loss                 | 5.8e+03       |
|    n_updates            | 6320          |
|    policy_gradient_loss | -0.000177     |
|    std                  | 2.19          |
|    value_loss           | 1.27e+04      |
-------------------------------------------
Eval num_timesteps=1298000, episode_reward=333.66 +/- 216.08
Episode length: 412.60 +/- 47.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 334          |
| time/                   |              |
|    total_timesteps      | 1298000      |
| train/                  |              |
|    approx_kl            | 0.0004558933 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 226          |
|    n_updates            | 6330         |
|    policy_gradient_loss | -0.000271    |
|    std                  | 2.19         |
|    value_loss           | 720          |
------------------------------------------
Eval num_timesteps=1300000, episode_reward=409.78 +/- 254.60
Episode length: 376.40 +/- 20.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 410         |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.008500585 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.71       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 78.4        |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.0016     |
|    std                  | 2.19        |
|    value_loss           | 336         |
-----------------------------------------
Eval num_timesteps=1302000, episode_reward=213.43 +/- 323.72
Episode length: 379.20 +/- 28.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 1302000     |
| train/                  |             |
|    approx_kl            | 0.008025917 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.71       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.001       |
|    loss                 | 1.69e+03    |
|    n_updates            | 6350        |
|    policy_gradient_loss | 0.000953    |
|    std                  | 2.19        |
|    value_loss           | 4.26e+03    |
-----------------------------------------
Eval num_timesteps=1304000, episode_reward=476.15 +/- 380.33
Episode length: 396.40 +/- 24.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 476          |
| time/                   |              |
|    total_timesteps      | 1304000      |
| train/                  |              |
|    approx_kl            | 0.0016655887 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.71        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+04     |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.19         |
|    value_loss           | 2.26e+04     |
------------------------------------------
Eval num_timesteps=1306000, episode_reward=316.06 +/- 327.84
Episode length: 393.40 +/- 29.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 1306000      |
| train/                  |              |
|    approx_kl            | 0.0016479328 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.71        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 2.32e+03     |
|    n_updates            | 6370         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 2.19         |
|    value_loss           | 5.37e+03     |
------------------------------------------
Eval num_timesteps=1308000, episode_reward=564.77 +/- 130.29
Episode length: 382.20 +/- 11.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 565          |
| time/                   |              |
|    total_timesteps      | 1308000      |
| train/                  |              |
|    approx_kl            | 0.0017632082 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.001        |
|    loss                 | 3.32e+03     |
|    n_updates            | 6380         |
|    policy_gradient_loss | 3.76e-05     |
|    std                  | 2.2          |
|    value_loss           | 7.02e+03     |
------------------------------------------
Eval num_timesteps=1310000, episode_reward=210.80 +/- 183.78
Episode length: 373.00 +/- 24.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 211          |
| time/                   |              |
|    total_timesteps      | 1310000      |
| train/                  |              |
|    approx_kl            | 0.0008591792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 4.11e+03     |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.00068     |
|    std                  | 2.2          |
|    value_loss           | 9.05e+03     |
------------------------------------------
Eval num_timesteps=1312000, episode_reward=336.42 +/- 123.51
Episode length: 365.40 +/- 24.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 336          |
| time/                   |              |
|    total_timesteps      | 1312000      |
| train/                  |              |
|    approx_kl            | 0.0014375399 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.73        |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 903          |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 2.2          |
|    value_loss           | 3.07e+03     |
------------------------------------------
Eval num_timesteps=1314000, episode_reward=300.29 +/- 338.75
Episode length: 376.40 +/- 34.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 300          |
| time/                   |              |
|    total_timesteps      | 1314000      |
| train/                  |              |
|    approx_kl            | 0.0018467635 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.73        |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 6.84e+03     |
|    n_updates            | 6410         |
|    policy_gradient_loss | -0.000603    |
|    std                  | 2.2          |
|    value_loss           | 1.42e+04     |
------------------------------------------
Eval num_timesteps=1316000, episode_reward=366.17 +/- 163.44
Episode length: 388.00 +/- 36.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 366          |
| time/                   |              |
|    total_timesteps      | 1316000      |
| train/                  |              |
|    approx_kl            | 0.0002626139 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.73        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 6420         |
|    policy_gradient_loss | -0.000189    |
|    std                  | 2.21         |
|    value_loss           | 510          |
------------------------------------------
Eval num_timesteps=1318000, episode_reward=220.14 +/- 354.63
Episode length: 353.00 +/- 31.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 1318000     |
| train/                  |             |
|    approx_kl            | 0.002171603 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.74       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.001       |
|    loss                 | 4.09e+03    |
|    n_updates            | 6430        |
|    policy_gradient_loss | -0.000949   |
|    std                  | 2.21        |
|    value_loss           | 8.82e+03    |
-----------------------------------------
Eval num_timesteps=1320000, episode_reward=351.05 +/- 279.76
Episode length: 346.40 +/- 54.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 1320000      |
| train/                  |              |
|    approx_kl            | 0.0015188727 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.74        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 88.6         |
|    n_updates            | 6440         |
|    policy_gradient_loss | -0.000552    |
|    std                  | 2.21         |
|    value_loss           | 326          |
------------------------------------------
Eval num_timesteps=1322000, episode_reward=93.37 +/- 185.49
Episode length: 393.60 +/- 19.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 394         |
|    mean_reward          | 93.4        |
| time/                   |             |
|    total_timesteps      | 1322000     |
| train/                  |             |
|    approx_kl            | 0.006010878 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.74       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 54.5        |
|    n_updates            | 6450        |
|    policy_gradient_loss | -0.0018     |
|    std                  | 2.22        |
|    value_loss           | 220         |
-----------------------------------------
Eval num_timesteps=1324000, episode_reward=65.00 +/- 65.81
Episode length: 359.00 +/- 22.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 65          |
| time/                   |             |
|    total_timesteps      | 1324000     |
| train/                  |             |
|    approx_kl            | 0.008076232 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.76       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 33.9        |
|    n_updates            | 6460        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 2.22        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=1326000, episode_reward=94.81 +/- 96.85
Episode length: 364.40 +/- 24.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 364         |
|    mean_reward          | 94.8        |
| time/                   |             |
|    total_timesteps      | 1326000     |
| train/                  |             |
|    approx_kl            | 0.004808263 |
|    clip_fraction        | 0.0712      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.76       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.001       |
|    loss                 | 21.8        |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.00336    |
|    std                  | 2.22        |
|    value_loss           | 82.5        |
-----------------------------------------
Eval num_timesteps=1328000, episode_reward=78.39 +/- 162.50
Episode length: 345.40 +/- 36.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 345         |
|    mean_reward          | 78.4        |
| time/                   |             |
|    total_timesteps      | 1328000     |
| train/                  |             |
|    approx_kl            | 0.008721411 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.76       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 22.5        |
|    n_updates            | 6480        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 2.24        |
|    value_loss           | 96.4        |
-----------------------------------------
Eval num_timesteps=1330000, episode_reward=337.81 +/- 370.99
Episode length: 384.00 +/- 10.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 338         |
| time/                   |             |
|    total_timesteps      | 1330000     |
| train/                  |             |
|    approx_kl            | 0.005876896 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.79       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 21.5        |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.00379    |
|    std                  | 2.25        |
|    value_loss           | 91.4        |
-----------------------------------------
Eval num_timesteps=1332000, episode_reward=427.68 +/- 260.23
Episode length: 386.40 +/- 36.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 428          |
| time/                   |              |
|    total_timesteps      | 1332000      |
| train/                  |              |
|    approx_kl            | 0.0025858525 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.79        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 27.1         |
|    n_updates            | 6500         |
|    policy_gradient_loss | -3.51e-05    |
|    std                  | 2.26         |
|    value_loss           | 108          |
------------------------------------------
Eval num_timesteps=1334000, episode_reward=568.56 +/- 262.80
Episode length: 388.60 +/- 42.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 569          |
| time/                   |              |
|    total_timesteps      | 1334000      |
| train/                  |              |
|    approx_kl            | 0.0021696112 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.8         |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 2.11e+03     |
|    n_updates            | 6510         |
|    policy_gradient_loss | 0.00202      |
|    std                  | 2.27         |
|    value_loss           | 5.02e+03     |
------------------------------------------
Eval num_timesteps=1336000, episode_reward=370.89 +/- 274.39
Episode length: 362.40 +/- 24.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | 371          |
| time/                   |              |
|    total_timesteps      | 1336000      |
| train/                  |              |
|    approx_kl            | 0.0008707404 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.81        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 6520         |
|    policy_gradient_loss | -0.000711    |
|    std                  | 2.27         |
|    value_loss           | 4.12e+03     |
------------------------------------------
Eval num_timesteps=1338000, episode_reward=509.36 +/- 279.51
Episode length: 395.00 +/- 50.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 509          |
| time/                   |              |
|    total_timesteps      | 1338000      |
| train/                  |              |
|    approx_kl            | 0.0026287301 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.81        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 54.2         |
|    n_updates            | 6530         |
|    policy_gradient_loss | -0.000826    |
|    std                  | 2.27         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=1340000, episode_reward=329.97 +/- 434.60
Episode length: 423.60 +/- 50.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 1340000     |
| train/                  |             |
|    approx_kl            | 0.011450847 |
|    clip_fraction        | 0.0556      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.81       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.001       |
|    loss                 | 2.3e+03     |
|    n_updates            | 6540        |
|    policy_gradient_loss | -0.00294    |
|    std                  | 2.27        |
|    value_loss           | 5.66e+03    |
-----------------------------------------
Eval num_timesteps=1342000, episode_reward=313.65 +/- 343.03
Episode length: 447.60 +/- 48.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 314          |
| time/                   |              |
|    total_timesteps      | 1342000      |
| train/                  |              |
|    approx_kl            | 0.0011104352 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.82        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.00047     |
|    std                  | 2.27         |
|    value_loss           | 4.5e+03      |
------------------------------------------
Eval num_timesteps=1344000, episode_reward=655.69 +/- 1428.08
Episode length: 473.00 +/- 117.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 656          |
| time/                   |              |
|    total_timesteps      | 1344000      |
| train/                  |              |
|    approx_kl            | 0.0005483043 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.82        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 4.42e+03     |
|    n_updates            | 6560         |
|    policy_gradient_loss | -0.00025     |
|    std                  | 2.27         |
|    value_loss           | 9.39e+03     |
------------------------------------------
Eval num_timesteps=1346000, episode_reward=482.55 +/- 491.15
Episode length: 467.60 +/- 52.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 483          |
| time/                   |              |
|    total_timesteps      | 1346000      |
| train/                  |              |
|    approx_kl            | 0.0012238265 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.82        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 107          |
|    n_updates            | 6570         |
|    policy_gradient_loss | -0.000369    |
|    std                  | 2.28         |
|    value_loss           | 450          |
------------------------------------------
Eval num_timesteps=1348000, episode_reward=190.53 +/- 435.55
Episode length: 422.60 +/- 64.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 1348000     |
| train/                  |             |
|    approx_kl            | 0.000659404 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.83       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.001       |
|    loss                 | 1.44e+03    |
|    n_updates            | 6580        |
|    policy_gradient_loss | 0.000281    |
|    std                  | 2.28        |
|    value_loss           | 3.28e+03    |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=188.55 +/- 432.77
Episode length: 428.40 +/- 65.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 428       |
|    mean_reward          | 189       |
| time/                   |           |
|    total_timesteps      | 1350000   |
| train/                  |           |
|    approx_kl            | 0.0054408 |
|    clip_fraction        | 0.00898   |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.83     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.001     |
|    loss                 | 129       |
|    n_updates            | 6590      |
|    policy_gradient_loss | -0.00123  |
|    std                  | 2.29      |
|    value_loss           | 589       |
---------------------------------------
Eval num_timesteps=1352000, episode_reward=306.75 +/- 430.09
Episode length: 453.00 +/- 56.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 307          |
| time/                   |              |
|    total_timesteps      | 1352000      |
| train/                  |              |
|    approx_kl            | 0.0007422937 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.84        |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+03     |
|    n_updates            | 6600         |
|    policy_gradient_loss | 0.000568     |
|    std                  | 2.29         |
|    value_loss           | 6.12e+03     |
------------------------------------------
Eval num_timesteps=1354000, episode_reward=248.29 +/- 462.07
Episode length: 472.80 +/- 83.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 473           |
|    mean_reward          | 248           |
| time/                   |               |
|    total_timesteps      | 1354000       |
| train/                  |               |
|    approx_kl            | 0.00013119806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.84         |
|    explained_variance   | 0.834         |
|    learning_rate        | 0.001         |
|    loss                 | 3.89e+03      |
|    n_updates            | 6610          |
|    policy_gradient_loss | 8.1e-06       |
|    std                  | 2.29          |
|    value_loss           | 8.61e+03      |
-------------------------------------------
Eval num_timesteps=1356000, episode_reward=200.66 +/- 108.51
Episode length: 471.60 +/- 60.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 472         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 1356000     |
| train/                  |             |
|    approx_kl            | 0.005065202 |
|    clip_fraction        | 0.00967     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.85       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 64.1        |
|    n_updates            | 6620        |
|    policy_gradient_loss | -0.00213    |
|    std                  | 2.29        |
|    value_loss           | 326         |
-----------------------------------------
Eval num_timesteps=1358000, episode_reward=24.04 +/- 361.41
Episode length: 427.60 +/- 27.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 24           |
| time/                   |              |
|    total_timesteps      | 1358000      |
| train/                  |              |
|    approx_kl            | 0.0019197417 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.85        |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 5.23e+03     |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 2.3          |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=1360000, episode_reward=656.61 +/- 312.20
Episode length: 470.20 +/- 36.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 657           |
| time/                   |               |
|    total_timesteps      | 1360000       |
| train/                  |               |
|    approx_kl            | 0.00038727085 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.85         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+03      |
|    n_updates            | 6640          |
|    policy_gradient_loss | -5.85e-05     |
|    std                  | 2.3           |
|    value_loss           | 3.8e+03       |
-------------------------------------------
Eval num_timesteps=1362000, episode_reward=-256.15 +/- 243.09
Episode length: 452.20 +/- 16.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 452         |
|    mean_reward          | -256        |
| time/                   |             |
|    total_timesteps      | 1362000     |
| train/                  |             |
|    approx_kl            | 0.007739222 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.86       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 56.8        |
|    n_updates            | 6650        |
|    policy_gradient_loss | -0.00249    |
|    std                  | 2.31        |
|    value_loss           | 240         |
-----------------------------------------
Eval num_timesteps=1364000, episode_reward=58.67 +/- 381.40
Episode length: 417.40 +/- 48.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 58.7         |
| time/                   |              |
|    total_timesteps      | 1364000      |
| train/                  |              |
|    approx_kl            | 0.0047742566 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 4.66e+03     |
|    n_updates            | 6660         |
|    policy_gradient_loss | 0.00054      |
|    std                  | 2.32         |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=1366000, episode_reward=243.53 +/- 509.67
Episode length: 452.20 +/- 49.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 1366000  |
---------------------------------
Eval num_timesteps=1368000, episode_reward=218.57 +/- 206.57
Episode length: 438.20 +/- 28.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 1368000      |
| train/                  |              |
|    approx_kl            | 0.0009861575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 4.57e+03     |
|    n_updates            | 6670         |
|    policy_gradient_loss | -0.000646    |
|    std                  | 2.32         |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=1370000, episode_reward=-38.64 +/- 240.04
Episode length: 412.00 +/- 87.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | -38.6        |
| time/                   |              |
|    total_timesteps      | 1370000      |
| train/                  |              |
|    approx_kl            | 0.0073769977 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 49.9         |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 2.32         |
|    value_loss           | 210          |
------------------------------------------
Eval num_timesteps=1372000, episode_reward=106.41 +/- 376.85
Episode length: 450.60 +/- 40.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 1372000      |
| train/                  |              |
|    approx_kl            | 0.0024862033 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.000626    |
|    std                  | 2.32         |
|    value_loss           | 4.7e+03      |
------------------------------------------
Eval num_timesteps=1374000, episode_reward=257.47 +/- 438.61
Episode length: 432.60 +/- 42.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 257          |
| time/                   |              |
|    total_timesteps      | 1374000      |
| train/                  |              |
|    approx_kl            | 0.0009713152 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 90.3         |
|    n_updates            | 6700         |
|    policy_gradient_loss | -0.00046     |
|    std                  | 2.32         |
|    value_loss           | 331          |
------------------------------------------
Eval num_timesteps=1376000, episode_reward=382.15 +/- 423.46
Episode length: 418.00 +/- 34.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 1376000      |
| train/                  |              |
|    approx_kl            | 0.0014572958 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 2.88e+03     |
|    n_updates            | 6710         |
|    policy_gradient_loss | -0.000427    |
|    std                  | 2.32         |
|    value_loss           | 7.24e+03     |
------------------------------------------
Eval num_timesteps=1378000, episode_reward=453.72 +/- 395.98
Episode length: 430.20 +/- 48.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 454           |
| time/                   |               |
|    total_timesteps      | 1378000       |
| train/                  |               |
|    approx_kl            | 0.00057758816 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.9          |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 118           |
|    n_updates            | 6720          |
|    policy_gradient_loss | -0.000232     |
|    std                  | 2.33          |
|    value_loss           | 544           |
-------------------------------------------
Eval num_timesteps=1380000, episode_reward=40.40 +/- 38.75
Episode length: 363.40 +/- 21.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 363          |
|    mean_reward          | 40.4         |
| time/                   |              |
|    total_timesteps      | 1380000      |
| train/                  |              |
|    approx_kl            | 0.0012884641 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.001        |
|    loss                 | 2.71e+03     |
|    n_updates            | 6730         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 2.33         |
|    value_loss           | 6.32e+03     |
------------------------------------------
Eval num_timesteps=1382000, episode_reward=-2.03 +/- 274.82
Episode length: 405.80 +/- 40.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | -2.03         |
| time/                   |               |
|    total_timesteps      | 1382000       |
| train/                  |               |
|    approx_kl            | 0.00039836357 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.92         |
|    explained_variance   | 0.769         |
|    learning_rate        | 0.001         |
|    loss                 | 4.55e+03      |
|    n_updates            | 6740          |
|    policy_gradient_loss | -3.67e-05     |
|    std                  | 2.34          |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=1384000, episode_reward=517.32 +/- 445.19
Episode length: 405.80 +/- 50.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 517          |
| time/                   |              |
|    total_timesteps      | 1384000      |
| train/                  |              |
|    approx_kl            | 0.0015433574 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 6750         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 2.34         |
|    value_loss           | 5.09e+03     |
------------------------------------------
Eval num_timesteps=1386000, episode_reward=95.75 +/- 376.44
Episode length: 404.20 +/- 30.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 95.7         |
| time/                   |              |
|    total_timesteps      | 1386000      |
| train/                  |              |
|    approx_kl            | 0.0010735474 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+03     |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.000155    |
|    std                  | 2.34         |
|    value_loss           | 4.01e+03     |
------------------------------------------
Eval num_timesteps=1388000, episode_reward=292.30 +/- 435.05
Episode length: 426.20 +/- 39.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 292           |
| time/                   |               |
|    total_timesteps      | 1388000       |
| train/                  |               |
|    approx_kl            | 0.00028036349 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.92         |
|    explained_variance   | 0.835         |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+03       |
|    n_updates            | 6770          |
|    policy_gradient_loss | -6.5e-05      |
|    std                  | 2.34          |
|    value_loss           | 7.53e+03      |
-------------------------------------------
Eval num_timesteps=1390000, episode_reward=545.90 +/- 333.57
Episode length: 400.00 +/- 23.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 546           |
| time/                   |               |
|    total_timesteps      | 1390000       |
| train/                  |               |
|    approx_kl            | 0.00015396849 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.92         |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.001         |
|    loss                 | 5.35e+03      |
|    n_updates            | 6780          |
|    policy_gradient_loss | -0.000286     |
|    std                  | 2.34          |
|    value_loss           | 1.16e+04      |
-------------------------------------------
Eval num_timesteps=1392000, episode_reward=304.17 +/- 301.94
Episode length: 400.60 +/- 19.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 1392000      |
| train/                  |              |
|    approx_kl            | 0.0055834902 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 443          |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 2.34         |
|    value_loss           | 1.11e+03     |
------------------------------------------
Eval num_timesteps=1394000, episode_reward=175.92 +/- 147.24
Episode length: 422.60 +/- 49.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 176          |
| time/                   |              |
|    total_timesteps      | 1394000      |
| train/                  |              |
|    approx_kl            | 0.0055877436 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.001        |
|    loss                 | 2.48e+03     |
|    n_updates            | 6800         |
|    policy_gradient_loss | -0.00296     |
|    std                  | 2.34         |
|    value_loss           | 5.36e+03     |
------------------------------------------
Eval num_timesteps=1396000, episode_reward=451.90 +/- 496.38
Episode length: 444.60 +/- 62.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 445         |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 1396000     |
| train/                  |             |
|    approx_kl            | 0.008693611 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.92       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 26.7        |
|    n_updates            | 6810        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 2.34        |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=1398000, episode_reward=757.41 +/- 285.41
Episode length: 475.20 +/- 40.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | 757          |
| time/                   |              |
|    total_timesteps      | 1398000      |
| train/                  |              |
|    approx_kl            | 0.0023440698 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 66.5         |
|    n_updates            | 6820         |
|    policy_gradient_loss | 4.73e-05     |
|    std                  | 2.34         |
|    value_loss           | 336          |
------------------------------------------
Eval num_timesteps=1400000, episode_reward=273.38 +/- 202.42
Episode length: 433.40 +/- 51.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 273          |
| time/                   |              |
|    total_timesteps      | 1400000      |
| train/                  |              |
|    approx_kl            | 0.0048843557 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+03     |
|    n_updates            | 6830         |
|    policy_gradient_loss | 0.00156      |
|    std                  | 2.34         |
|    value_loss           | 4.11e+03     |
------------------------------------------
Eval num_timesteps=1402000, episode_reward=396.72 +/- 549.42
Episode length: 464.20 +/- 56.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 397           |
| time/                   |               |
|    total_timesteps      | 1402000       |
| train/                  |               |
|    approx_kl            | 0.00012494109 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0.791         |
|    learning_rate        | 0.001         |
|    loss                 | 5.89e+03      |
|    n_updates            | 6840          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 2.34          |
|    value_loss           | 1.36e+04      |
-------------------------------------------
Eval num_timesteps=1404000, episode_reward=513.09 +/- 274.18
Episode length: 475.80 +/- 44.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 513           |
| time/                   |               |
|    total_timesteps      | 1404000       |
| train/                  |               |
|    approx_kl            | 0.00018390515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.94         |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.001         |
|    loss                 | 2.82e+03      |
|    n_updates            | 6850          |
|    policy_gradient_loss | -0.000309     |
|    std                  | 2.35          |
|    value_loss           | 5.47e+03      |
-------------------------------------------
Eval num_timesteps=1406000, episode_reward=308.43 +/- 637.27
Episode length: 486.40 +/- 34.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 308           |
| time/                   |               |
|    total_timesteps      | 1406000       |
| train/                  |               |
|    approx_kl            | 0.00022161458 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.94         |
|    explained_variance   | 0.82          |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 6860          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 2.35          |
|    value_loss           | 4.91e+03      |
-------------------------------------------
Eval num_timesteps=1408000, episode_reward=514.40 +/- 507.69
Episode length: 486.40 +/- 53.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 514           |
| time/                   |               |
|    total_timesteps      | 1408000       |
| train/                  |               |
|    approx_kl            | 0.00095554965 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.94         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 1.84e+03      |
|    n_updates            | 6870          |
|    policy_gradient_loss | -0.000792     |
|    std                  | 2.35          |
|    value_loss           | 4.39e+03      |
-------------------------------------------
Eval num_timesteps=1410000, episode_reward=565.69 +/- 621.29
Episode length: 499.40 +/- 16.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 499         |
|    mean_reward          | 566         |
| time/                   |             |
|    total_timesteps      | 1410000     |
| train/                  |             |
|    approx_kl            | 0.001219552 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.94       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 56.6        |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.000585   |
|    std                  | 2.35        |
|    value_loss           | 250         |
-----------------------------------------
Eval num_timesteps=1412000, episode_reward=526.09 +/- 425.40
Episode length: 458.00 +/- 35.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | 526         |
| time/                   |             |
|    total_timesteps      | 1412000     |
| train/                  |             |
|    approx_kl            | 0.005355592 |
|    clip_fraction        | 0.0151      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.96       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.001       |
|    loss                 | 4.78e+03    |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.00235    |
|    std                  | 2.36        |
|    value_loss           | 1.07e+04    |
-----------------------------------------
Eval num_timesteps=1414000, episode_reward=-73.50 +/- 363.98
Episode length: 421.20 +/- 46.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | -73.5        |
| time/                   |              |
|    total_timesteps      | 1414000      |
| train/                  |              |
|    approx_kl            | 0.0028690954 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 68.3         |
|    n_updates            | 6900         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 2.37         |
|    value_loss           | 326          |
------------------------------------------
Eval num_timesteps=1416000, episode_reward=365.03 +/- 268.26
Episode length: 447.60 +/- 41.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 365          |
| time/                   |              |
|    total_timesteps      | 1416000      |
| train/                  |              |
|    approx_kl            | 0.0067638196 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.001        |
|    loss                 | 3.23e+03     |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.000708    |
|    std                  | 2.37         |
|    value_loss           | 6.56e+03     |
------------------------------------------
Eval num_timesteps=1418000, episode_reward=396.69 +/- 281.51
Episode length: 457.00 +/- 53.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | 397          |
| time/                   |              |
|    total_timesteps      | 1418000      |
| train/                  |              |
|    approx_kl            | 0.0012124785 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+03     |
|    n_updates            | 6920         |
|    policy_gradient_loss | -0.00079     |
|    std                  | 2.37         |
|    value_loss           | 4.37e+03     |
------------------------------------------
Eval num_timesteps=1420000, episode_reward=466.92 +/- 527.31
Episode length: 475.80 +/- 9.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 467         |
| time/                   |             |
|    total_timesteps      | 1420000     |
| train/                  |             |
|    approx_kl            | 0.008821625 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.98       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 60.2        |
|    n_updates            | 6930        |
|    policy_gradient_loss | -0.00215    |
|    std                  | 2.38        |
|    value_loss           | 435         |
-----------------------------------------
Eval num_timesteps=1422000, episode_reward=85.66 +/- 511.38
Episode length: 449.00 +/- 20.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 85.7         |
| time/                   |              |
|    total_timesteps      | 1422000      |
| train/                  |              |
|    approx_kl            | 0.0044662044 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 66.3         |
|    n_updates            | 6940         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.37         |
|    value_loss           | 290          |
------------------------------------------
Eval num_timesteps=1424000, episode_reward=452.46 +/- 307.24
Episode length: 481.80 +/- 56.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 452          |
| time/                   |              |
|    total_timesteps      | 1424000      |
| train/                  |              |
|    approx_kl            | 0.0054206047 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 107          |
|    n_updates            | 6950         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 2.38         |
|    value_loss           | 506          |
------------------------------------------
Eval num_timesteps=1426000, episode_reward=60.37 +/- 523.56
Episode length: 460.00 +/- 40.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | 60.4        |
| time/                   |             |
|    total_timesteps      | 1426000     |
| train/                  |             |
|    approx_kl            | 0.008270098 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.99       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 121         |
|    n_updates            | 6960        |
|    policy_gradient_loss | -0.00286    |
|    std                  | 2.38        |
|    value_loss           | 473         |
-----------------------------------------
Eval num_timesteps=1428000, episode_reward=145.69 +/- 78.20
Episode length: 432.40 +/- 51.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 1428000      |
| train/                  |              |
|    approx_kl            | 0.0023444642 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | 0.776        |
|    learning_rate        | 0.001        |
|    loss                 | 4.85e+03     |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 2.37         |
|    value_loss           | 9.53e+03     |
------------------------------------------
Eval num_timesteps=1430000, episode_reward=112.68 +/- 185.73
Episode length: 387.80 +/- 27.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 113          |
| time/                   |              |
|    total_timesteps      | 1430000      |
| train/                  |              |
|    approx_kl            | 0.0067561604 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 134          |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.00347     |
|    std                  | 2.37         |
|    value_loss           | 324          |
------------------------------------------
Eval num_timesteps=1432000, episode_reward=-18.09 +/- 197.66
Episode length: 402.00 +/- 40.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | -18.1        |
| time/                   |              |
|    total_timesteps      | 1432000      |
| train/                  |              |
|    approx_kl            | 0.0048479307 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 84.6         |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.000989    |
|    std                  | 2.37         |
|    value_loss           | 345          |
------------------------------------------
Eval num_timesteps=1434000, episode_reward=339.73 +/- 443.65
Episode length: 451.20 +/- 33.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 340          |
| time/                   |              |
|    total_timesteps      | 1434000      |
| train/                  |              |
|    approx_kl            | 0.0077650324 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 40.9         |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 2.37         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=1436000, episode_reward=306.35 +/- 371.10
Episode length: 420.60 +/- 38.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 421         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 1436000     |
| train/                  |             |
|    approx_kl            | 0.008143548 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.96       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 24.2        |
|    n_updates            | 7010        |
|    policy_gradient_loss | -0.00449    |
|    std                  | 2.36        |
|    value_loss           | 63.7        |
-----------------------------------------
Eval num_timesteps=1438000, episode_reward=253.41 +/- 329.31
Episode length: 423.60 +/- 40.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 1438000     |
| train/                  |             |
|    approx_kl            | 0.008393581 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.95       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 26.7        |
|    n_updates            | 7020        |
|    policy_gradient_loss | -0.00237    |
|    std                  | 2.36        |
|    value_loss           | 100         |
-----------------------------------------
Eval num_timesteps=1440000, episode_reward=296.57 +/- 330.08
Episode length: 396.80 +/- 18.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.008012844 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.96       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.001       |
|    loss                 | 2.39e+03    |
|    n_updates            | 7030        |
|    policy_gradient_loss | 0.00175     |
|    std                  | 2.37        |
|    value_loss           | 6e+03       |
-----------------------------------------
Eval num_timesteps=1442000, episode_reward=328.37 +/- 399.06
Episode length: 413.00 +/- 49.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 328          |
| time/                   |              |
|    total_timesteps      | 1442000      |
| train/                  |              |
|    approx_kl            | 0.0005000697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+03     |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.000655    |
|    std                  | 2.37         |
|    value_loss           | 5.11e+03     |
------------------------------------------
Eval num_timesteps=1444000, episode_reward=358.28 +/- 185.23
Episode length: 418.40 +/- 36.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 1444000     |
| train/                  |             |
|    approx_kl            | 0.007980471 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 39.9        |
|    n_updates            | 7050        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 2.38        |
|    value_loss           | 202         |
-----------------------------------------
Eval num_timesteps=1446000, episode_reward=338.98 +/- 281.66
Episode length: 394.20 +/- 37.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 1446000      |
| train/                  |              |
|    approx_kl            | 0.0040154955 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 3.08e+03     |
|    n_updates            | 7060         |
|    policy_gradient_loss | 0.00198      |
|    std                  | 2.38         |
|    value_loss           | 7.68e+03     |
------------------------------------------
Eval num_timesteps=1448000, episode_reward=45.67 +/- 352.54
Episode length: 410.20 +/- 14.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 45.7          |
| time/                   |               |
|    total_timesteps      | 1448000       |
| train/                  |               |
|    approx_kl            | 5.1627227e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.99         |
|    explained_variance   | 0.813         |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+03      |
|    n_updates            | 7070          |
|    policy_gradient_loss | 8.04e-05      |
|    std                  | 2.38          |
|    value_loss           | 8.8e+03       |
-------------------------------------------
Eval num_timesteps=1450000, episode_reward=521.19 +/- 442.15
Episode length: 420.20 +/- 34.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 521          |
| time/                   |              |
|    total_timesteps      | 1450000      |
| train/                  |              |
|    approx_kl            | 0.0022056862 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 54.8         |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.000756    |
|    std                  | 2.38         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=1452000, episode_reward=189.36 +/- 190.87
Episode length: 412.40 +/- 23.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 1452000  |
---------------------------------
Eval num_timesteps=1454000, episode_reward=308.07 +/- 242.79
Episode length: 382.40 +/- 24.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 1454000      |
| train/                  |              |
|    approx_kl            | 0.0034054676 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+03     |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.39         |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=1456000, episode_reward=400.81 +/- 304.13
Episode length: 391.20 +/- 11.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 1456000      |
| train/                  |              |
|    approx_kl            | 0.0064781844 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 7100         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 2.39         |
|    value_loss           | 573          |
------------------------------------------
Eval num_timesteps=1458000, episode_reward=252.57 +/- 236.46
Episode length: 375.40 +/- 18.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 1458000      |
| train/                  |              |
|    approx_kl            | 0.0032343245 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 4.08e+03     |
|    n_updates            | 7110         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 2.39         |
|    value_loss           | 8.79e+03     |
------------------------------------------
Eval num_timesteps=1460000, episode_reward=270.47 +/- 211.36
Episode length: 369.00 +/- 12.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 1460000      |
| train/                  |              |
|    approx_kl            | 0.0039903047 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 47.2         |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 2.39         |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=1462000, episode_reward=410.25 +/- 386.96
Episode length: 394.40 +/- 35.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 394         |
|    mean_reward          | 410         |
| time/                   |             |
|    total_timesteps      | 1462000     |
| train/                  |             |
|    approx_kl            | 0.004485858 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9          |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 37.2        |
|    n_updates            | 7130        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 2.4         |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=1464000, episode_reward=190.65 +/- 191.29
Episode length: 372.80 +/- 25.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 1464000      |
| train/                  |              |
|    approx_kl            | 0.0020480545 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 68.7         |
|    n_updates            | 7140         |
|    policy_gradient_loss | 0.000239     |
|    std                  | 2.4          |
|    value_loss           | 266          |
------------------------------------------
Eval num_timesteps=1466000, episode_reward=255.33 +/- 313.72
Episode length: 405.60 +/- 13.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 255          |
| time/                   |              |
|    total_timesteps      | 1466000      |
| train/                  |              |
|    approx_kl            | 0.0015738806 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.01        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 55.4         |
|    n_updates            | 7150         |
|    policy_gradient_loss | -0.000281    |
|    std                  | 2.41         |
|    value_loss           | 193          |
------------------------------------------
Eval num_timesteps=1468000, episode_reward=272.31 +/- 275.51
Episode length: 364.80 +/- 23.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 1468000      |
| train/                  |              |
|    approx_kl            | 0.0024873149 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.01        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 7160         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 2.41         |
|    value_loss           | 4.41e+03     |
------------------------------------------
Eval num_timesteps=1470000, episode_reward=348.31 +/- 363.97
Episode length: 400.40 +/- 56.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 348           |
| time/                   |               |
|    total_timesteps      | 1470000       |
| train/                  |               |
|    approx_kl            | 0.00092025206 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.01         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 50.1          |
|    n_updates            | 7170          |
|    policy_gradient_loss | -0.000169     |
|    std                  | 2.41          |
|    value_loss           | 215           |
-------------------------------------------
Eval num_timesteps=1472000, episode_reward=77.60 +/- 335.65
Episode length: 418.00 +/- 30.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 418        |
|    mean_reward          | 77.6       |
| time/                   |            |
|    total_timesteps      | 1472000    |
| train/                  |            |
|    approx_kl            | 0.00582166 |
|    clip_fraction        | 0.0111     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.01      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.001      |
|    loss                 | 32.2       |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0012    |
|    std                  | 2.41       |
|    value_loss           | 133        |
----------------------------------------
Eval num_timesteps=1474000, episode_reward=452.06 +/- 391.42
Episode length: 405.80 +/- 23.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 452          |
| time/                   |              |
|    total_timesteps      | 1474000      |
| train/                  |              |
|    approx_kl            | 0.0009035014 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.02        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 4.05e+03     |
|    n_updates            | 7190         |
|    policy_gradient_loss | -2.5e-05     |
|    std                  | 2.41         |
|    value_loss           | 9.85e+03     |
------------------------------------------
Eval num_timesteps=1476000, episode_reward=177.85 +/- 367.59
Episode length: 410.40 +/- 29.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 1476000      |
| train/                  |              |
|    approx_kl            | 0.0011640931 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.02        |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+03     |
|    n_updates            | 7200         |
|    policy_gradient_loss | -0.000947    |
|    std                  | 2.41         |
|    value_loss           | 3.64e+03     |
------------------------------------------
Eval num_timesteps=1478000, episode_reward=357.01 +/- 242.34
Episode length: 396.80 +/- 5.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 357         |
| time/                   |             |
|    total_timesteps      | 1478000     |
| train/                  |             |
|    approx_kl            | 0.000841536 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.02       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 2.03e+03    |
|    n_updates            | 7210        |
|    policy_gradient_loss | 4.91e-05    |
|    std                  | 2.41        |
|    value_loss           | 5.02e+03    |
-----------------------------------------
Eval num_timesteps=1480000, episode_reward=433.57 +/- 354.32
Episode length: 449.80 +/- 46.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 434           |
| time/                   |               |
|    total_timesteps      | 1480000       |
| train/                  |               |
|    approx_kl            | 0.00017550998 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.02         |
|    explained_variance   | 0.861         |
|    learning_rate        | 0.001         |
|    loss                 | 2.36e+03      |
|    n_updates            | 7220          |
|    policy_gradient_loss | -0.000159     |
|    std                  | 2.41          |
|    value_loss           | 4.9e+03       |
-------------------------------------------
Eval num_timesteps=1482000, episode_reward=660.47 +/- 347.69
Episode length: 443.00 +/- 40.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 660           |
| time/                   |               |
|    total_timesteps      | 1482000       |
| train/                  |               |
|    approx_kl            | 3.3286837e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.02         |
|    explained_variance   | 0.8           |
|    learning_rate        | 0.001         |
|    loss                 | 6.49e+03      |
|    n_updates            | 7230          |
|    policy_gradient_loss | 4.97e-05      |
|    std                  | 2.41          |
|    value_loss           | 1.47e+04      |
-------------------------------------------
Eval num_timesteps=1484000, episode_reward=278.58 +/- 271.61
Episode length: 407.00 +/- 17.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 279           |
| time/                   |               |
|    total_timesteps      | 1484000       |
| train/                  |               |
|    approx_kl            | 0.00030686174 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.02         |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+03      |
|    n_updates            | 7240          |
|    policy_gradient_loss | -0.00059      |
|    std                  | 2.41          |
|    value_loss           | 3.16e+03      |
-------------------------------------------
Eval num_timesteps=1486000, episode_reward=192.82 +/- 482.34
Episode length: 422.20 +/- 25.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 193           |
| time/                   |               |
|    total_timesteps      | 1486000       |
| train/                  |               |
|    approx_kl            | 0.00039184064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.02         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 3.76e+03      |
|    n_updates            | 7250          |
|    policy_gradient_loss | -0.000458     |
|    std                  | 2.41          |
|    value_loss           | 8.32e+03      |
-------------------------------------------
Eval num_timesteps=1488000, episode_reward=250.85 +/- 422.81
Episode length: 413.80 +/- 17.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 251           |
| time/                   |               |
|    total_timesteps      | 1488000       |
| train/                  |               |
|    approx_kl            | 0.00025434012 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.02         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 3.75e+03      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -3.75e-05     |
|    std                  | 2.41          |
|    value_loss           | 8.41e+03      |
-------------------------------------------
Eval num_timesteps=1490000, episode_reward=454.22 +/- 329.38
Episode length: 401.40 +/- 52.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 454          |
| time/                   |              |
|    total_timesteps      | 1490000      |
| train/                  |              |
|    approx_kl            | 0.0004575115 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.03        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+03     |
|    n_updates            | 7270         |
|    policy_gradient_loss | -0.000407    |
|    std                  | 2.41         |
|    value_loss           | 4.02e+03     |
------------------------------------------
Eval num_timesteps=1492000, episode_reward=367.34 +/- 558.31
Episode length: 430.60 +/- 14.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 367           |
| time/                   |               |
|    total_timesteps      | 1492000       |
| train/                  |               |
|    approx_kl            | 0.00014373078 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.03         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 3.13e+03      |
|    n_updates            | 7280          |
|    policy_gradient_loss | 0.000124      |
|    std                  | 2.42          |
|    value_loss           | 6.51e+03      |
-------------------------------------------
Eval num_timesteps=1494000, episode_reward=252.47 +/- 324.85
Episode length: 383.00 +/- 20.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 252           |
| time/                   |               |
|    total_timesteps      | 1494000       |
| train/                  |               |
|    approx_kl            | 0.00068361685 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.03         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.000798     |
|    std                  | 2.42          |
|    value_loss           | 3.41e+03      |
-------------------------------------------
Eval num_timesteps=1496000, episode_reward=337.50 +/- 284.47
Episode length: 396.60 +/- 26.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 337          |
| time/                   |              |
|    total_timesteps      | 1496000      |
| train/                  |              |
|    approx_kl            | 0.0012662639 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.04        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+03     |
|    n_updates            | 7300         |
|    policy_gradient_loss | -0.000653    |
|    std                  | 2.42         |
|    value_loss           | 4.4e+03      |
------------------------------------------
Eval num_timesteps=1498000, episode_reward=244.34 +/- 232.91
Episode length: 408.80 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 1498000      |
| train/                  |              |
|    approx_kl            | 0.0010778862 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.05        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+03     |
|    n_updates            | 7310         |
|    policy_gradient_loss | -0.000513    |
|    std                  | 2.43         |
|    value_loss           | 6.16e+03     |
------------------------------------------
Eval num_timesteps=1500000, episode_reward=148.29 +/- 516.44
Episode length: 445.80 +/- 19.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 148          |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0003474732 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.05        |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.001        |
|    loss                 | 3.9e+03      |
|    n_updates            | 7320         |
|    policy_gradient_loss | -7.81e-05    |
|    std                  | 2.43         |
|    value_loss           | 9.08e+03     |
------------------------------------------
Eval num_timesteps=1502000, episode_reward=306.74 +/- 321.78
Episode length: 401.60 +/- 21.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 307          |
| time/                   |              |
|    total_timesteps      | 1502000      |
| train/                  |              |
|    approx_kl            | 0.0002094199 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.05        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 7330         |
|    policy_gradient_loss | -5.73e-05    |
|    std                  | 2.43         |
|    value_loss           | 4.11e+03     |
------------------------------------------
Eval num_timesteps=1504000, episode_reward=444.37 +/- 436.34
Episode length: 458.80 +/- 32.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 1504000       |
| train/                  |               |
|    approx_kl            | 0.00013370364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.06         |
|    explained_variance   | 0.84          |
|    learning_rate        | 0.001         |
|    loss                 | 5.18e+03      |
|    n_updates            | 7340          |
|    policy_gradient_loss | 0.000122      |
|    std                  | 2.43          |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=1506000, episode_reward=-51.74 +/- 202.55
Episode length: 419.00 +/- 32.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | -51.7        |
| time/                   |              |
|    total_timesteps      | 1506000      |
| train/                  |              |
|    approx_kl            | 0.0015230831 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.06        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 57.8         |
|    n_updates            | 7350         |
|    policy_gradient_loss | -0.000742    |
|    std                  | 2.44         |
|    value_loss           | 291          |
------------------------------------------
Eval num_timesteps=1508000, episode_reward=377.92 +/- 244.38
Episode length: 385.00 +/- 37.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 378          |
| time/                   |              |
|    total_timesteps      | 1508000      |
| train/                  |              |
|    approx_kl            | 0.0009998314 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 7360         |
|    policy_gradient_loss | 0.000517     |
|    std                  | 2.44         |
|    value_loss           | 2.74e+03     |
------------------------------------------
Eval num_timesteps=1510000, episode_reward=297.25 +/- 47.05
Episode length: 415.40 +/- 32.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 297          |
| time/                   |              |
|    total_timesteps      | 1510000      |
| train/                  |              |
|    approx_kl            | 0.0007025724 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 1.76e+03     |
|    n_updates            | 7370         |
|    policy_gradient_loss | -0.000423    |
|    std                  | 2.44         |
|    value_loss           | 3.72e+03     |
------------------------------------------
Eval num_timesteps=1512000, episode_reward=380.47 +/- 278.55
Episode length: 411.20 +/- 48.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 380           |
| time/                   |               |
|    total_timesteps      | 1512000       |
| train/                  |               |
|    approx_kl            | 0.00017216653 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.07         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 4.37e+03      |
|    n_updates            | 7380          |
|    policy_gradient_loss | 7.43e-05      |
|    std                  | 2.44          |
|    value_loss           | 9.06e+03      |
-------------------------------------------
Eval num_timesteps=1514000, episode_reward=300.84 +/- 331.97
Episode length: 414.20 +/- 38.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 301           |
| time/                   |               |
|    total_timesteps      | 1514000       |
| train/                  |               |
|    approx_kl            | 7.6038414e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.855         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+03      |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000129     |
|    std                  | 2.44          |
|    value_loss           | 4.28e+03      |
-------------------------------------------
Eval num_timesteps=1516000, episode_reward=687.32 +/- 202.80
Episode length: 405.00 +/- 29.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 687           |
| time/                   |               |
|    total_timesteps      | 1516000       |
| train/                  |               |
|    approx_kl            | 0.00010862766 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.895         |
|    learning_rate        | 0.001         |
|    loss                 | 1.42e+03      |
|    n_updates            | 7400          |
|    policy_gradient_loss | -0.000323     |
|    std                  | 2.44          |
|    value_loss           | 3.03e+03      |
-------------------------------------------
Eval num_timesteps=1518000, episode_reward=-230.48 +/- 115.91
Episode length: 468.00 +/- 23.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 468           |
|    mean_reward          | -230          |
| time/                   |               |
|    total_timesteps      | 1518000       |
| train/                  |               |
|    approx_kl            | 0.00088114874 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.83          |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+03      |
|    n_updates            | 7410          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 2.45          |
|    value_loss           | 4.26e+03      |
-------------------------------------------
Eval num_timesteps=1520000, episode_reward=372.27 +/- 373.31
Episode length: 386.20 +/- 20.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 372           |
| time/                   |               |
|    total_timesteps      | 1520000       |
| train/                  |               |
|    approx_kl            | 0.00057395734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 3.82e+03      |
|    n_updates            | 7420          |
|    policy_gradient_loss | 0.00022       |
|    std                  | 2.45          |
|    value_loss           | 8.54e+03      |
-------------------------------------------
Eval num_timesteps=1522000, episode_reward=71.83 +/- 171.97
Episode length: 396.00 +/- 10.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 396           |
|    mean_reward          | 71.8          |
| time/                   |               |
|    total_timesteps      | 1522000       |
| train/                  |               |
|    approx_kl            | 0.00013808449 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+03      |
|    n_updates            | 7430          |
|    policy_gradient_loss | -9.41e-05     |
|    std                  | 2.45          |
|    value_loss           | 4.49e+03      |
-------------------------------------------
Eval num_timesteps=1524000, episode_reward=200.70 +/- 305.14
Episode length: 403.80 +/- 26.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 1524000      |
| train/                  |              |
|    approx_kl            | 8.463452e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 4.96e+03     |
|    n_updates            | 7440         |
|    policy_gradient_loss | -0.000179    |
|    std                  | 2.45         |
|    value_loss           | 1.05e+04     |
------------------------------------------
Eval num_timesteps=1526000, episode_reward=445.01 +/- 278.22
Episode length: 399.40 +/- 29.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 399           |
|    mean_reward          | 445           |
| time/                   |               |
|    total_timesteps      | 1526000       |
| train/                  |               |
|    approx_kl            | 9.1152615e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 3.86e+03      |
|    n_updates            | 7450          |
|    policy_gradient_loss | -0.000182     |
|    std                  | 2.45          |
|    value_loss           | 8.8e+03       |
-------------------------------------------
Eval num_timesteps=1528000, episode_reward=315.29 +/- 314.38
Episode length: 427.40 +/- 26.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 315           |
| time/                   |               |
|    total_timesteps      | 1528000       |
| train/                  |               |
|    approx_kl            | 0.00019971776 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 2.85e+03      |
|    n_updates            | 7460          |
|    policy_gradient_loss | -0.000331     |
|    std                  | 2.45          |
|    value_loss           | 7.02e+03      |
-------------------------------------------
Eval num_timesteps=1530000, episode_reward=-213.25 +/- 249.40
Episode length: 422.00 +/- 14.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | -213          |
| time/                   |               |
|    total_timesteps      | 1530000       |
| train/                  |               |
|    approx_kl            | 0.00026477626 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0.828         |
|    learning_rate        | 0.001         |
|    loss                 | 5.2e+03       |
|    n_updates            | 7470          |
|    policy_gradient_loss | -0.000404     |
|    std                  | 2.45          |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=1532000, episode_reward=348.45 +/- 394.02
Episode length: 406.00 +/- 42.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 348           |
| time/                   |               |
|    total_timesteps      | 1532000       |
| train/                  |               |
|    approx_kl            | 9.7021286e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.001         |
|    loss                 | 2.5e+03       |
|    n_updates            | 7480          |
|    policy_gradient_loss | -6.36e-06     |
|    std                  | 2.46          |
|    value_loss           | 5.49e+03      |
-------------------------------------------
Eval num_timesteps=1534000, episode_reward=206.64 +/- 466.07
Episode length: 423.20 +/- 21.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 423           |
|    mean_reward          | 207           |
| time/                   |               |
|    total_timesteps      | 1534000       |
| train/                  |               |
|    approx_kl            | 2.8612296e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0.865         |
|    learning_rate        | 0.001         |
|    loss                 | 3.31e+03      |
|    n_updates            | 7490          |
|    policy_gradient_loss | -2.83e-05     |
|    std                  | 2.46          |
|    value_loss           | 7.53e+03      |
-------------------------------------------
Eval num_timesteps=1536000, episode_reward=158.32 +/- 490.95
Episode length: 415.80 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 416      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1538000, episode_reward=43.77 +/- 439.75
Episode length: 418.20 +/- 26.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 43.8          |
| time/                   |               |
|    total_timesteps      | 1538000       |
| train/                  |               |
|    approx_kl            | 0.00072669156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 7500          |
|    policy_gradient_loss | -0.000848     |
|    std                  | 2.46          |
|    value_loss           | 8.08e+03      |
-------------------------------------------
Eval num_timesteps=1540000, episode_reward=203.51 +/- 378.99
Episode length: 382.40 +/- 44.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 1540000      |
| train/                  |              |
|    approx_kl            | 0.0015857754 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.09        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+03     |
|    n_updates            | 7510         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 2.46         |
|    value_loss           | 3.66e+03     |
------------------------------------------
Eval num_timesteps=1542000, episode_reward=282.26 +/- 424.54
Episode length: 378.40 +/- 39.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 378           |
|    mean_reward          | 282           |
| time/                   |               |
|    total_timesteps      | 1542000       |
| train/                  |               |
|    approx_kl            | 0.00058050983 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.1          |
|    explained_variance   | 0.865         |
|    learning_rate        | 0.001         |
|    loss                 | 3.45e+03      |
|    n_updates            | 7520          |
|    policy_gradient_loss | -0.000464     |
|    std                  | 2.46          |
|    value_loss           | 7.45e+03      |
-------------------------------------------
Eval num_timesteps=1544000, episode_reward=486.80 +/- 227.37
Episode length: 377.60 +/- 21.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 487          |
| time/                   |              |
|    total_timesteps      | 1544000      |
| train/                  |              |
|    approx_kl            | 0.0001521972 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 7530         |
|    policy_gradient_loss | -9.01e-06    |
|    std                  | 2.46         |
|    value_loss           | 3.58e+03     |
------------------------------------------
Eval num_timesteps=1546000, episode_reward=338.28 +/- 358.58
Episode length: 390.80 +/- 31.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 1546000       |
| train/                  |               |
|    approx_kl            | 0.00012937305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.1          |
|    explained_variance   | 0.83          |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 7540          |
|    policy_gradient_loss | -0.000155     |
|    std                  | 2.46          |
|    value_loss           | 4.55e+03      |
-------------------------------------------
Eval num_timesteps=1548000, episode_reward=219.05 +/- 429.50
Episode length: 400.00 +/- 28.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 219           |
| time/                   |               |
|    total_timesteps      | 1548000       |
| train/                  |               |
|    approx_kl            | 0.00019113254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.1          |
|    explained_variance   | 0.813         |
|    learning_rate        | 0.001         |
|    loss                 | 6.47e+03      |
|    n_updates            | 7550          |
|    policy_gradient_loss | -0.000603     |
|    std                  | 2.46          |
|    value_loss           | 1.43e+04      |
-------------------------------------------
Eval num_timesteps=1550000, episode_reward=240.89 +/- 365.70
Episode length: 386.80 +/- 38.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 1550000      |
| train/                  |              |
|    approx_kl            | 0.0003674602 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 2.41e+03     |
|    n_updates            | 7560         |
|    policy_gradient_loss | -0.000268    |
|    std                  | 2.46         |
|    value_loss           | 5.22e+03     |
------------------------------------------
Eval num_timesteps=1552000, episode_reward=307.04 +/- 328.84
Episode length: 382.80 +/- 32.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 307           |
| time/                   |               |
|    total_timesteps      | 1552000       |
| train/                  |               |
|    approx_kl            | 0.00012422571 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.1          |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 7570          |
|    policy_gradient_loss | -4.8e-05      |
|    std                  | 2.46          |
|    value_loss           | 4.23e+03      |
-------------------------------------------
Eval num_timesteps=1554000, episode_reward=140.80 +/- 373.17
Episode length: 414.60 +/- 35.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 1554000      |
| train/                  |              |
|    approx_kl            | 9.581429e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+03     |
|    n_updates            | 7580         |
|    policy_gradient_loss | -0.000221    |
|    std                  | 2.46         |
|    value_loss           | 6.86e+03     |
------------------------------------------
Eval num_timesteps=1556000, episode_reward=284.62 +/- 168.26
Episode length: 395.20 +/- 12.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 395           |
|    mean_reward          | 285           |
| time/                   |               |
|    total_timesteps      | 1556000       |
| train/                  |               |
|    approx_kl            | 0.00031262846 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.1          |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+03      |
|    n_updates            | 7590          |
|    policy_gradient_loss | -0.000307     |
|    std                  | 2.46          |
|    value_loss           | 7.01e+03      |
-------------------------------------------
Eval num_timesteps=1558000, episode_reward=367.15 +/- 353.24
Episode length: 376.80 +/- 25.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 1558000      |
| train/                  |              |
|    approx_kl            | 0.0036638314 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 185          |
|    n_updates            | 7600         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 2.46         |
|    value_loss           | 602          |
------------------------------------------
Eval num_timesteps=1560000, episode_reward=336.61 +/- 159.75
Episode length: 360.00 +/- 14.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 360         |
|    mean_reward          | 337         |
| time/                   |             |
|    total_timesteps      | 1560000     |
| train/                  |             |
|    approx_kl            | 0.008032599 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 52          |
|    n_updates            | 7610        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 2.47        |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=1562000, episode_reward=201.45 +/- 134.38
Episode length: 354.80 +/- 47.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 1562000      |
| train/                  |              |
|    approx_kl            | 0.0022411572 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.11        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+03     |
|    n_updates            | 7620         |
|    policy_gradient_loss | 0.000636     |
|    std                  | 2.47         |
|    value_loss           | 6.02e+03     |
------------------------------------------
Eval num_timesteps=1564000, episode_reward=223.19 +/- 202.36
Episode length: 355.60 +/- 27.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 356           |
|    mean_reward          | 223           |
| time/                   |               |
|    total_timesteps      | 1564000       |
| train/                  |               |
|    approx_kl            | 9.1772934e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0.837         |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+03      |
|    n_updates            | 7630          |
|    policy_gradient_loss | 0.00026       |
|    std                  | 2.47          |
|    value_loss           | 5.41e+03      |
-------------------------------------------
Eval num_timesteps=1566000, episode_reward=265.90 +/- 148.15
Episode length: 360.20 +/- 17.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 360           |
|    mean_reward          | 266           |
| time/                   |               |
|    total_timesteps      | 1566000       |
| train/                  |               |
|    approx_kl            | 0.00039576556 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 63.6          |
|    n_updates            | 7640          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 2.47          |
|    value_loss           | 292           |
-------------------------------------------
Eval num_timesteps=1568000, episode_reward=298.81 +/- 75.45
Episode length: 405.20 +/- 39.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 299          |
| time/                   |              |
|    total_timesteps      | 1568000      |
| train/                  |              |
|    approx_kl            | 0.0007598323 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+03      |
|    n_updates            | 7650         |
|    policy_gradient_loss | 0.000438     |
|    std                  | 2.47         |
|    value_loss           | 3.57e+03     |
------------------------------------------
Eval num_timesteps=1570000, episode_reward=338.79 +/- 106.08
Episode length: 372.00 +/- 9.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | 339           |
| time/                   |               |
|    total_timesteps      | 1570000       |
| train/                  |               |
|    approx_kl            | 0.00035478588 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 78.9          |
|    n_updates            | 7660          |
|    policy_gradient_loss | -5.99e-05     |
|    std                  | 2.48          |
|    value_loss           | 377           |
-------------------------------------------
Eval num_timesteps=1572000, episode_reward=177.08 +/- 100.02
Episode length: 358.80 +/- 35.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 1572000      |
| train/                  |              |
|    approx_kl            | 0.0037775429 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.13        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 1.8e+03      |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.000124    |
|    std                  | 2.48         |
|    value_loss           | 3.93e+03     |
------------------------------------------
Eval num_timesteps=1574000, episode_reward=64.21 +/- 97.91
Episode length: 317.60 +/- 62.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 318         |
|    mean_reward          | 64.2        |
| time/                   |             |
|    total_timesteps      | 1574000     |
| train/                  |             |
|    approx_kl            | 0.008463825 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 29.4        |
|    n_updates            | 7680        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 2.48        |
|    value_loss           | 144         |
-----------------------------------------
Eval num_timesteps=1576000, episode_reward=243.33 +/- 148.86
Episode length: 325.60 +/- 29.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | 243          |
| time/                   |              |
|    total_timesteps      | 1576000      |
| train/                  |              |
|    approx_kl            | 0.0062340554 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.13        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 46.5         |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 2.48         |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=1578000, episode_reward=358.04 +/- 168.62
Episode length: 345.80 +/- 28.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 346         |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 1578000     |
| train/                  |             |
|    approx_kl            | 0.010971792 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 39.3        |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00286    |
|    std                  | 2.48        |
|    value_loss           | 137         |
-----------------------------------------
Eval num_timesteps=1580000, episode_reward=258.21 +/- 245.73
Episode length: 348.40 +/- 23.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 348          |
|    mean_reward          | 258          |
| time/                   |              |
|    total_timesteps      | 1580000      |
| train/                  |              |
|    approx_kl            | 0.0006318273 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.13        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 185          |
|    n_updates            | 7710         |
|    policy_gradient_loss | 0.000309     |
|    std                  | 2.48         |
|    value_loss           | 636          |
------------------------------------------
Eval num_timesteps=1582000, episode_reward=436.25 +/- 307.88
Episode length: 348.20 +/- 33.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 348         |
|    mean_reward          | 436         |
| time/                   |             |
|    total_timesteps      | 1582000     |
| train/                  |             |
|    approx_kl            | 0.001682393 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 278         |
|    n_updates            | 7720        |
|    policy_gradient_loss | -0.000913   |
|    std                  | 2.48        |
|    value_loss           | 816         |
-----------------------------------------
Eval num_timesteps=1584000, episode_reward=274.13 +/- 102.98
Episode length: 351.60 +/- 36.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 1584000     |
| train/                  |             |
|    approx_kl            | 0.000740318 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.001       |
|    loss                 | 2.57e+03    |
|    n_updates            | 7730        |
|    policy_gradient_loss | -0.000734   |
|    std                  | 2.48        |
|    value_loss           | 6.84e+03    |
-----------------------------------------
Eval num_timesteps=1586000, episode_reward=200.63 +/- 235.55
Episode length: 377.60 +/- 23.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 1586000      |
| train/                  |              |
|    approx_kl            | 0.0018958056 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.14        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 75.2         |
|    n_updates            | 7740         |
|    policy_gradient_loss | -0.000737    |
|    std                  | 2.49         |
|    value_loss           | 390          |
------------------------------------------
Eval num_timesteps=1588000, episode_reward=220.86 +/- 88.77
Episode length: 380.20 +/- 11.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 1588000      |
| train/                  |              |
|    approx_kl            | 0.0046493053 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.14        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 807          |
|    n_updates            | 7750         |
|    policy_gradient_loss | -0.000152    |
|    std                  | 2.49         |
|    value_loss           | 2.13e+03     |
------------------------------------------
Eval num_timesteps=1590000, episode_reward=165.92 +/- 166.77
Episode length: 392.60 +/- 8.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 166           |
| time/                   |               |
|    total_timesteps      | 1590000       |
| train/                  |               |
|    approx_kl            | 0.00087348855 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.15         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 55.7          |
|    n_updates            | 7760          |
|    policy_gradient_loss | -0.000564     |
|    std                  | 2.49          |
|    value_loss           | 233           |
-------------------------------------------
Eval num_timesteps=1592000, episode_reward=196.34 +/- 305.57
Episode length: 384.60 +/- 13.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 385         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 1592000     |
| train/                  |             |
|    approx_kl            | 0.011049863 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.15       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 56.9        |
|    n_updates            | 7770        |
|    policy_gradient_loss | -0.00169    |
|    std                  | 2.5         |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=1594000, episode_reward=157.52 +/- 259.76
Episode length: 383.00 +/- 24.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 1594000      |
| train/                  |              |
|    approx_kl            | 0.0029798704 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.15        |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 5.48e+03     |
|    n_updates            | 7780         |
|    policy_gradient_loss | -0.000735    |
|    std                  | 2.5          |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=1596000, episode_reward=268.74 +/- 321.25
Episode length: 367.60 +/- 26.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 1596000      |
| train/                  |              |
|    approx_kl            | 0.0020753166 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.15        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 71.8         |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.000869    |
|    std                  | 2.49         |
|    value_loss           | 256          |
------------------------------------------
Eval num_timesteps=1598000, episode_reward=416.04 +/- 335.21
Episode length: 360.60 +/- 17.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 361         |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 1598000     |
| train/                  |             |
|    approx_kl            | 0.010920524 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.14       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 7800        |
|    policy_gradient_loss | -0.00408    |
|    std                  | 2.48        |
|    value_loss           | 509         |
-----------------------------------------
Eval num_timesteps=1600000, episode_reward=360.72 +/- 213.69
Episode length: 354.00 +/- 17.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0052490705 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.13        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 39.2         |
|    n_updates            | 7810         |
|    policy_gradient_loss | -0.00533     |
|    std                  | 2.48         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=1602000, episode_reward=357.80 +/- 130.31
Episode length: 349.00 +/- 17.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 349         |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 1602000     |
| train/                  |             |
|    approx_kl            | 0.013200452 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.12       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.001       |
|    loss                 | 1.47e+03    |
|    n_updates            | 7820        |
|    policy_gradient_loss | -0.0022     |
|    std                  | 2.47        |
|    value_loss           | 3.8e+03     |
-----------------------------------------
Eval num_timesteps=1604000, episode_reward=584.17 +/- 177.62
Episode length: 374.60 +/- 26.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 584          |
| time/                   |              |
|    total_timesteps      | 1604000      |
| train/                  |              |
|    approx_kl            | 0.0038873712 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.11        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 7830         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 2.47         |
|    value_loss           | 389          |
------------------------------------------
Eval num_timesteps=1606000, episode_reward=99.21 +/- 290.41
Episode length: 408.80 +/- 43.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 99.2        |
| time/                   |             |
|    total_timesteps      | 1606000     |
| train/                  |             |
|    approx_kl            | 0.008443603 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.001       |
|    loss                 | 2.36e+03    |
|    n_updates            | 7840        |
|    policy_gradient_loss | -5.19e-05   |
|    std                  | 2.47        |
|    value_loss           | 5.06e+03    |
-----------------------------------------
Eval num_timesteps=1608000, episode_reward=616.67 +/- 143.59
Episode length: 389.00 +/- 27.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 389           |
|    mean_reward          | 617           |
| time/                   |               |
|    total_timesteps      | 1608000       |
| train/                  |               |
|    approx_kl            | 0.00065382663 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 1.84e+03      |
|    n_updates            | 7850          |
|    policy_gradient_loss | 0.000101      |
|    std                  | 2.47          |
|    value_loss           | 4.13e+03      |
-------------------------------------------
Eval num_timesteps=1610000, episode_reward=225.92 +/- 187.47
Episode length: 370.60 +/- 15.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 1610000     |
| train/                  |             |
|    approx_kl            | 8.04672e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.001       |
|    loss                 | 3.54e+03    |
|    n_updates            | 7860        |
|    policy_gradient_loss | -0.000158   |
|    std                  | 2.47        |
|    value_loss           | 8.95e+03    |
-----------------------------------------
Eval num_timesteps=1612000, episode_reward=263.06 +/- 419.21
Episode length: 420.20 +/- 34.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 263          |
| time/                   |              |
|    total_timesteps      | 1612000      |
| train/                  |              |
|    approx_kl            | 8.922609e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.11        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 2.39e+03     |
|    n_updates            | 7870         |
|    policy_gradient_loss | -8.89e-05    |
|    std                  | 2.47         |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=1614000, episode_reward=469.07 +/- 487.18
Episode length: 398.80 +/- 32.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 469         |
| time/                   |             |
|    total_timesteps      | 1614000     |
| train/                  |             |
|    approx_kl            | 0.002799246 |
|    clip_fraction        | 0.00137     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 1.31e+03    |
|    n_updates            | 7880        |
|    policy_gradient_loss | -0.00218    |
|    std                  | 2.47        |
|    value_loss           | 2.79e+03    |
-----------------------------------------
Eval num_timesteps=1616000, episode_reward=-68.35 +/- 203.55
Episode length: 405.20 +/- 52.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | -68.4        |
| time/                   |              |
|    total_timesteps      | 1616000      |
| train/                  |              |
|    approx_kl            | 0.0022120657 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.11        |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.001        |
|    loss                 | 4.41e+03     |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.000292    |
|    std                  | 2.47         |
|    value_loss           | 9.28e+03     |
------------------------------------------
Eval num_timesteps=1618000, episode_reward=575.28 +/- 257.63
Episode length: 410.40 +/- 36.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 575           |
| time/                   |               |
|    total_timesteps      | 1618000       |
| train/                  |               |
|    approx_kl            | 0.00036095947 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0.864         |
|    learning_rate        | 0.001         |
|    loss                 | 4.7e+03       |
|    n_updates            | 7900          |
|    policy_gradient_loss | -0.000368     |
|    std                  | 2.47          |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=1620000, episode_reward=370.43 +/- 436.12
Episode length: 391.60 +/- 19.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 370           |
| time/                   |               |
|    total_timesteps      | 1620000       |
| train/                  |               |
|    approx_kl            | 0.00022217305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+03      |
|    n_updates            | 7910          |
|    policy_gradient_loss | -0.000222     |
|    std                  | 2.47          |
|    value_loss           | 5.37e+03      |
-------------------------------------------
Eval num_timesteps=1622000, episode_reward=373.75 +/- 481.21
Episode length: 423.60 +/- 33.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 424      |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 1622000  |
---------------------------------
Eval num_timesteps=1624000, episode_reward=178.49 +/- 515.18
Episode length: 417.00 +/- 31.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 178           |
| time/                   |               |
|    total_timesteps      | 1624000       |
| train/                  |               |
|    approx_kl            | 0.00025049344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.864         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+03      |
|    n_updates            | 7920          |
|    policy_gradient_loss | -0.000306     |
|    std                  | 2.47          |
|    value_loss           | 5.23e+03      |
-------------------------------------------
Eval num_timesteps=1626000, episode_reward=-1.12 +/- 329.19
Episode length: 411.60 +/- 34.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 412           |
|    mean_reward          | -1.12         |
| time/                   |               |
|    total_timesteps      | 1626000       |
| train/                  |               |
|    approx_kl            | 0.00012202165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+03      |
|    n_updates            | 7930          |
|    policy_gradient_loss | -1.43e-05     |
|    std                  | 2.47          |
|    value_loss           | 5.23e+03      |
-------------------------------------------
Eval num_timesteps=1628000, episode_reward=26.89 +/- 219.71
Episode length: 395.60 +/- 44.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 396           |
|    mean_reward          | 26.9          |
| time/                   |               |
|    total_timesteps      | 1628000       |
| train/                  |               |
|    approx_kl            | 0.00020120206 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 7940          |
|    policy_gradient_loss | -0.000342     |
|    std                  | 2.48          |
|    value_loss           | 3.64e+03      |
-------------------------------------------
Eval num_timesteps=1630000, episode_reward=294.63 +/- 256.39
Episode length: 425.20 +/- 25.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 295           |
| time/                   |               |
|    total_timesteps      | 1630000       |
| train/                  |               |
|    approx_kl            | 3.7175632e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+03      |
|    n_updates            | 7950          |
|    policy_gradient_loss | -0.000102     |
|    std                  | 2.48          |
|    value_loss           | 2.26e+03      |
-------------------------------------------
Eval num_timesteps=1632000, episode_reward=146.31 +/- 438.23
Episode length: 409.20 +/- 42.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 1632000     |
| train/                  |             |
|    approx_kl            | 0.001511972 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.001       |
|    loss                 | 1.35e+03    |
|    n_updates            | 7960        |
|    policy_gradient_loss | -0.00143    |
|    std                  | 2.48        |
|    value_loss           | 3.15e+03    |
-----------------------------------------
Eval num_timesteps=1634000, episode_reward=203.73 +/- 433.80
Episode length: 410.00 +/- 18.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 1634000      |
| train/                  |              |
|    approx_kl            | 0.0013355974 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.13        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 323          |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.000338    |
|    std                  | 2.47         |
|    value_loss           | 944          |
------------------------------------------
Eval num_timesteps=1636000, episode_reward=85.99 +/- 438.55
Episode length: 430.20 +/- 53.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 86           |
| time/                   |              |
|    total_timesteps      | 1636000      |
| train/                  |              |
|    approx_kl            | 0.0067559564 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 161          |
|    n_updates            | 7980         |
|    policy_gradient_loss | -0.00323     |
|    std                  | 2.47         |
|    value_loss           | 662          |
------------------------------------------
Eval num_timesteps=1638000, episode_reward=320.43 +/- 585.94
Episode length: 464.60 +/- 31.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 465         |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 1638000     |
| train/                  |             |
|    approx_kl            | 0.006257192 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.001       |
|    loss                 | 2.88e+03    |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0022     |
|    std                  | 2.46        |
|    value_loss           | 7.06e+03    |
-----------------------------------------
Eval num_timesteps=1640000, episode_reward=157.18 +/- 327.14
Episode length: 465.00 +/- 22.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 157          |
| time/                   |              |
|    total_timesteps      | 1640000      |
| train/                  |              |
|    approx_kl            | 0.0066145007 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.11        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 37.9         |
|    n_updates            | 8000         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 2.47         |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=1642000, episode_reward=98.23 +/- 336.61
Episode length: 459.40 +/- 25.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 98.2          |
| time/                   |               |
|    total_timesteps      | 1642000       |
| train/                  |               |
|    approx_kl            | 0.00077881373 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.001         |
|    loss                 | 4.05e+03      |
|    n_updates            | 8010          |
|    policy_gradient_loss | -1.84e-05     |
|    std                  | 2.47          |
|    value_loss           | 9.69e+03      |
-------------------------------------------
Eval num_timesteps=1644000, episode_reward=124.06 +/- 388.76
Episode length: 422.80 +/- 35.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 423           |
|    mean_reward          | 124           |
| time/                   |               |
|    total_timesteps      | 1644000       |
| train/                  |               |
|    approx_kl            | 0.00023729002 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.891         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+03      |
|    n_updates            | 8020          |
|    policy_gradient_loss | -0.000403     |
|    std                  | 2.47          |
|    value_loss           | 3.37e+03      |
-------------------------------------------
Eval num_timesteps=1646000, episode_reward=483.53 +/- 557.79
Episode length: 486.00 +/- 41.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 484           |
| time/                   |               |
|    total_timesteps      | 1646000       |
| train/                  |               |
|    approx_kl            | 0.00027408038 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.12         |
|    explained_variance   | 0.822         |
|    learning_rate        | 0.001         |
|    loss                 | 3.55e+03      |
|    n_updates            | 8030          |
|    policy_gradient_loss | -0.00045      |
|    std                  | 2.47          |
|    value_loss           | 8.41e+03      |
-------------------------------------------
Eval num_timesteps=1648000, episode_reward=209.27 +/- 418.97
Episode length: 460.00 +/- 32.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 209          |
| time/                   |              |
|    total_timesteps      | 1648000      |
| train/                  |              |
|    approx_kl            | 0.0016289154 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 84.9         |
|    n_updates            | 8040         |
|    policy_gradient_loss | -0.000841    |
|    std                  | 2.47         |
|    value_loss           | 424          |
------------------------------------------
Eval num_timesteps=1650000, episode_reward=767.45 +/- 453.35
Episode length: 486.20 +/- 47.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 767         |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.002498249 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.12       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 100         |
|    n_updates            | 8050        |
|    policy_gradient_loss | -0.000493   |
|    std                  | 2.48        |
|    value_loss           | 347         |
-----------------------------------------
Eval num_timesteps=1652000, episode_reward=234.53 +/- 291.21
Episode length: 487.60 +/- 33.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 488           |
|    mean_reward          | 235           |
| time/                   |               |
|    total_timesteps      | 1652000       |
| train/                  |               |
|    approx_kl            | 0.00053855963 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.13         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 47            |
|    n_updates            | 8060          |
|    policy_gradient_loss | 0.000181      |
|    std                  | 2.49          |
|    value_loss           | 320           |
-------------------------------------------
Eval num_timesteps=1654000, episode_reward=162.32 +/- 330.26
Episode length: 480.80 +/- 39.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 1654000     |
| train/                  |             |
|    approx_kl            | 0.008348759 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.15       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.001       |
|    loss                 | 1.65e+03    |
|    n_updates            | 8070        |
|    policy_gradient_loss | -0.00153    |
|    std                  | 2.5         |
|    value_loss           | 4.77e+03    |
-----------------------------------------
Eval num_timesteps=1656000, episode_reward=172.88 +/- 285.35
Episode length: 452.80 +/- 33.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 173           |
| time/                   |               |
|    total_timesteps      | 1656000       |
| train/                  |               |
|    approx_kl            | 0.00029582233 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.17         |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.001         |
|    loss                 | 218           |
|    n_updates            | 8080          |
|    policy_gradient_loss | 8.59e-05      |
|    std                  | 2.51          |
|    value_loss           | 850           |
-------------------------------------------
Eval num_timesteps=1658000, episode_reward=430.43 +/- 440.68
Episode length: 490.40 +/- 38.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 1658000       |
| train/                  |               |
|    approx_kl            | 0.00054188515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.18         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+03      |
|    n_updates            | 8090          |
|    policy_gradient_loss | -0.000312     |
|    std                  | 2.51          |
|    value_loss           | 5.02e+03      |
-------------------------------------------
Eval num_timesteps=1660000, episode_reward=586.52 +/- 396.98
Episode length: 478.80 +/- 34.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 479           |
|    mean_reward          | 587           |
| time/                   |               |
|    total_timesteps      | 1660000       |
| train/                  |               |
|    approx_kl            | 0.00011997396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.18         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 3.15e+03      |
|    n_updates            | 8100          |
|    policy_gradient_loss | 1.49e-05      |
|    std                  | 2.51          |
|    value_loss           | 7.1e+03       |
-------------------------------------------
Eval num_timesteps=1662000, episode_reward=505.88 +/- 220.54
Episode length: 487.40 +/- 32.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 1662000      |
| train/                  |              |
|    approx_kl            | 0.0024013096 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.18        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+03     |
|    n_updates            | 8110         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 2.51         |
|    value_loss           | 3.47e+03     |
------------------------------------------
Eval num_timesteps=1664000, episode_reward=326.08 +/- 130.83
Episode length: 496.40 +/- 38.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 496        |
|    mean_reward          | 326        |
| time/                   |            |
|    total_timesteps      | 1664000    |
| train/                  |            |
|    approx_kl            | 0.00308331 |
|    clip_fraction        | 0.00488    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.17      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.001      |
|    loss                 | 3.18e+03   |
|    n_updates            | 8120       |
|    policy_gradient_loss | 0.00179    |
|    std                  | 2.51       |
|    value_loss           | 7.32e+03   |
----------------------------------------
Eval num_timesteps=1666000, episode_reward=603.47 +/- 287.64
Episode length: 491.40 +/- 22.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 491           |
|    mean_reward          | 603           |
| time/                   |               |
|    total_timesteps      | 1666000       |
| train/                  |               |
|    approx_kl            | 0.00015676039 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.17         |
|    explained_variance   | 0.819         |
|    learning_rate        | 0.001         |
|    loss                 | 4.1e+03       |
|    n_updates            | 8130          |
|    policy_gradient_loss | -0.000122     |
|    std                  | 2.51          |
|    value_loss           | 9.89e+03      |
-------------------------------------------
Eval num_timesteps=1668000, episode_reward=566.20 +/- 205.38
Episode length: 504.00 +/- 38.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 504           |
|    mean_reward          | 566           |
| time/                   |               |
|    total_timesteps      | 1668000       |
| train/                  |               |
|    approx_kl            | 2.7074973e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.17         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 3.15e+03      |
|    n_updates            | 8140          |
|    policy_gradient_loss | -0.000123     |
|    std                  | 2.51          |
|    value_loss           | 6.91e+03      |
-------------------------------------------
Eval num_timesteps=1670000, episode_reward=433.17 +/- 242.39
Episode length: 487.00 +/- 23.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 433           |
| time/                   |               |
|    total_timesteps      | 1670000       |
| train/                  |               |
|    approx_kl            | 0.00052679225 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.17         |
|    explained_variance   | 0.878         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 8150          |
|    policy_gradient_loss | -0.000685     |
|    std                  | 2.51          |
|    value_loss           | 4.49e+03      |
-------------------------------------------
Eval num_timesteps=1672000, episode_reward=231.09 +/- 364.42
Episode length: 520.20 +/- 16.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | 231          |
| time/                   |              |
|    total_timesteps      | 1672000      |
| train/                  |              |
|    approx_kl            | 0.0020189798 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.17        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 8160         |
|    policy_gradient_loss | -0.000643    |
|    std                  | 2.51         |
|    value_loss           | 582          |
------------------------------------------
Eval num_timesteps=1674000, episode_reward=296.59 +/- 417.95
Episode length: 518.00 +/- 26.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | 297          |
| time/                   |              |
|    total_timesteps      | 1674000      |
| train/                  |              |
|    approx_kl            | 0.0019331858 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.18        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 1.55e+03     |
|    n_updates            | 8170         |
|    policy_gradient_loss | 0.00013      |
|    std                  | 2.51         |
|    value_loss           | 4.07e+03     |
------------------------------------------
Eval num_timesteps=1676000, episode_reward=223.24 +/- 161.22
Episode length: 504.00 +/- 31.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 1676000      |
| train/                  |              |
|    approx_kl            | 0.0052362774 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.18        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 60.6         |
|    n_updates            | 8180         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 2.51         |
|    value_loss           | 260          |
------------------------------------------
Eval num_timesteps=1678000, episode_reward=330.19 +/- 264.47
Episode length: 507.20 +/- 31.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 1678000     |
| train/                  |             |
|    approx_kl            | 0.003561717 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.19       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.000248   |
|    std                  | 2.52        |
|    value_loss           | 392         |
-----------------------------------------
Eval num_timesteps=1680000, episode_reward=184.66 +/- 99.48
Episode length: 520.80 +/- 33.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 521          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0035334835 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.21        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 30.4         |
|    n_updates            | 8200         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 2.53         |
|    value_loss           | 143          |
------------------------------------------
Eval num_timesteps=1682000, episode_reward=169.37 +/- 229.21
Episode length: 464.40 +/- 57.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 1682000     |
| train/                  |             |
|    approx_kl            | 0.004892757 |
|    clip_fraction        | 0.0113      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.23       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 81.7        |
|    n_updates            | 8210        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 2.55        |
|    value_loss           | 360         |
-----------------------------------------
Eval num_timesteps=1684000, episode_reward=195.36 +/- 184.88
Episode length: 488.00 +/- 30.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 1684000      |
| train/                  |              |
|    approx_kl            | 0.0043921648 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.24        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 58.2         |
|    n_updates            | 8220         |
|    policy_gradient_loss | -0.00522     |
|    std                  | 2.55         |
|    value_loss           | 185          |
------------------------------------------
Eval num_timesteps=1686000, episode_reward=177.14 +/- 63.67
Episode length: 533.60 +/- 39.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 534         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 1686000     |
| train/                  |             |
|    approx_kl            | 0.004505477 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.25       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.001       |
|    loss                 | 433         |
|    n_updates            | 8230        |
|    policy_gradient_loss | 0.000182    |
|    std                  | 2.56        |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=1688000, episode_reward=307.01 +/- 189.99
Episode length: 506.00 +/- 28.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 1688000     |
| train/                  |             |
|    approx_kl            | 0.005877515 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.26       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 41.5        |
|    n_updates            | 8240        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 2.57        |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=1690000, episode_reward=368.72 +/- 294.36
Episode length: 500.20 +/- 23.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 1690000     |
| train/                  |             |
|    approx_kl            | 0.007913401 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.27       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 38.6        |
|    n_updates            | 8250        |
|    policy_gradient_loss | -0.00237    |
|    std                  | 2.57        |
|    value_loss           | 132         |
-----------------------------------------
Eval num_timesteps=1692000, episode_reward=297.73 +/- 271.21
Episode length: 456.20 +/- 55.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 298          |
| time/                   |              |
|    total_timesteps      | 1692000      |
| train/                  |              |
|    approx_kl            | 0.0048190993 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+03     |
|    n_updates            | 8260         |
|    policy_gradient_loss | -0.00273     |
|    std                  | 2.57         |
|    value_loss           | 5.75e+03     |
------------------------------------------
Eval num_timesteps=1694000, episode_reward=307.06 +/- 103.60
Episode length: 489.40 +/- 43.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 307           |
| time/                   |               |
|    total_timesteps      | 1694000       |
| train/                  |               |
|    approx_kl            | 0.00084726675 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.27         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 76.2          |
|    n_updates            | 8270          |
|    policy_gradient_loss | -0.000217     |
|    std                  | 2.57          |
|    value_loss           | 308           |
-------------------------------------------
Eval num_timesteps=1696000, episode_reward=292.33 +/- 223.15
Episode length: 444.80 +/- 38.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 1696000      |
| train/                  |              |
|    approx_kl            | 0.0025731619 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 177          |
|    n_updates            | 8280         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.57         |
|    value_loss           | 692          |
------------------------------------------
Eval num_timesteps=1698000, episode_reward=-47.34 +/- 212.89
Episode length: 457.60 +/- 56.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | -47.3       |
| time/                   |             |
|    total_timesteps      | 1698000     |
| train/                  |             |
|    approx_kl            | 0.005669903 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.27       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 44.8        |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 2.56        |
|    value_loss           | 246         |
-----------------------------------------
Eval num_timesteps=1700000, episode_reward=423.26 +/- 267.60
Episode length: 459.20 +/- 48.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 423          |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0071644373 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.26        |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 2.12e+03     |
|    n_updates            | 8300         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 2.56         |
|    value_loss           | 5.25e+03     |
------------------------------------------
Eval num_timesteps=1702000, episode_reward=401.42 +/- 388.99
Episode length: 458.60 +/- 43.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 1702000      |
| train/                  |              |
|    approx_kl            | 0.0009409763 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.26        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 50.2         |
|    n_updates            | 8310         |
|    policy_gradient_loss | -0.000379    |
|    std                  | 2.57         |
|    value_loss           | 176          |
------------------------------------------
Eval num_timesteps=1704000, episode_reward=133.13 +/- 443.30
Episode length: 438.40 +/- 32.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 1704000      |
| train/                  |              |
|    approx_kl            | 0.0020530114 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 21.7         |
|    n_updates            | 8320         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.57         |
|    value_loss           | 91.3         |
------------------------------------------
Eval num_timesteps=1706000, episode_reward=153.71 +/- 363.75
Episode length: 466.20 +/- 69.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 154          |
| time/                   |              |
|    total_timesteps      | 1706000      |
| train/                  |              |
|    approx_kl            | 0.0032450724 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 8330         |
|    policy_gradient_loss | 0.000772     |
|    std                  | 2.57         |
|    value_loss           | 6.21e+03     |
------------------------------------------
Eval num_timesteps=1708000, episode_reward=410.76 +/- 500.87
Episode length: 496.60 +/- 30.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 1708000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=347.17 +/- 339.94
Episode length: 425.60 +/- 27.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 347           |
| time/                   |               |
|    total_timesteps      | 1710000       |
| train/                  |               |
|    approx_kl            | 0.00048063463 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.27         |
|    explained_variance   | 0.854         |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+03      |
|    n_updates            | 8340          |
|    policy_gradient_loss | -0.000372     |
|    std                  | 2.57          |
|    value_loss           | 4.96e+03      |
-------------------------------------------
Eval num_timesteps=1712000, episode_reward=223.79 +/- 363.84
Episode length: 490.00 +/- 36.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 224          |
| time/                   |              |
|    total_timesteps      | 1712000      |
| train/                  |              |
|    approx_kl            | 0.0049311146 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 76.6         |
|    n_updates            | 8350         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 2.57         |
|    value_loss           | 504          |
------------------------------------------
Eval num_timesteps=1714000, episode_reward=261.41 +/- 465.73
Episode length: 499.20 +/- 51.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 499          |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 1714000      |
| train/                  |              |
|    approx_kl            | 0.0060610767 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 179          |
|    n_updates            | 8360         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 2.58         |
|    value_loss           | 716          |
------------------------------------------
Eval num_timesteps=1716000, episode_reward=427.72 +/- 248.43
Episode length: 527.60 +/- 10.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 528         |
|    mean_reward          | 428         |
| time/                   |             |
|    total_timesteps      | 1716000     |
| train/                  |             |
|    approx_kl            | 0.005263136 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.28       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 52.3        |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 2.59        |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=1718000, episode_reward=516.06 +/- 167.17
Episode length: 507.80 +/- 50.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 516          |
| time/                   |              |
|    total_timesteps      | 1718000      |
| train/                  |              |
|    approx_kl            | 0.0024751895 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.29        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+03     |
|    n_updates            | 8380         |
|    policy_gradient_loss | 0.000351     |
|    std                  | 2.59         |
|    value_loss           | 5.21e+03     |
------------------------------------------
Eval num_timesteps=1720000, episode_reward=467.34 +/- 527.45
Episode length: 485.20 +/- 22.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 467           |
| time/                   |               |
|    total_timesteps      | 1720000       |
| train/                  |               |
|    approx_kl            | 0.00027154677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.3          |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+03      |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000278     |
|    std                  | 2.6           |
|    value_loss           | 4.31e+03      |
-------------------------------------------
Eval num_timesteps=1722000, episode_reward=492.11 +/- 306.15
Episode length: 499.60 +/- 26.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 492         |
| time/                   |             |
|    total_timesteps      | 1722000     |
| train/                  |             |
|    approx_kl            | 0.000850352 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.31       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.001       |
|    loss                 | 3.28e+03    |
|    n_updates            | 8400        |
|    policy_gradient_loss | -0.000708   |
|    std                  | 2.6         |
|    value_loss           | 7.34e+03    |
-----------------------------------------
Eval num_timesteps=1724000, episode_reward=210.57 +/- 263.42
Episode length: 467.80 +/- 48.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 211          |
| time/                   |              |
|    total_timesteps      | 1724000      |
| train/                  |              |
|    approx_kl            | 0.0020147231 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.31        |
|    explained_variance   | 0.92         |
|    learning_rate        | 0.001        |
|    loss                 | 453          |
|    n_updates            | 8410         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 2.6          |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=1726000, episode_reward=149.33 +/- 257.77
Episode length: 495.00 +/- 42.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 1726000      |
| train/                  |              |
|    approx_kl            | 0.0016975434 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.31        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.001        |
|    loss                 | 77.4         |
|    n_updates            | 8420         |
|    policy_gradient_loss | -0.000673    |
|    std                  | 2.61         |
|    value_loss           | 535          |
------------------------------------------
Eval num_timesteps=1728000, episode_reward=278.70 +/- 161.58
Episode length: 499.60 +/- 31.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 1728000     |
| train/                  |             |
|    approx_kl            | 0.011041289 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.32       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 88.2        |
|    n_updates            | 8430        |
|    policy_gradient_loss | -0.00429    |
|    std                  | 2.61        |
|    value_loss           | 343         |
-----------------------------------------
Eval num_timesteps=1730000, episode_reward=181.67 +/- 253.83
Episode length: 509.20 +/- 24.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 1730000     |
| train/                  |             |
|    approx_kl            | 0.004462174 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.33       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 29.6        |
|    n_updates            | 8440        |
|    policy_gradient_loss | -0.00149    |
|    std                  | 2.62        |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=1732000, episode_reward=372.87 +/- 579.68
Episode length: 493.00 +/- 31.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | 373         |
| time/                   |             |
|    total_timesteps      | 1732000     |
| train/                  |             |
|    approx_kl            | 0.008732354 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.35       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 2.65e+03    |
|    n_updates            | 8450        |
|    policy_gradient_loss | -0.00213    |
|    std                  | 2.63        |
|    value_loss           | 7.2e+03     |
-----------------------------------------
Eval num_timesteps=1734000, episode_reward=216.23 +/- 339.05
Episode length: 443.80 +/- 40.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 1734000      |
| train/                  |              |
|    approx_kl            | 0.0032839659 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.37        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 8460         |
|    policy_gradient_loss | -0.000923    |
|    std                  | 2.64         |
|    value_loss           | 538          |
------------------------------------------
Eval num_timesteps=1736000, episode_reward=498.56 +/- 334.40
Episode length: 459.00 +/- 20.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 499          |
| time/                   |              |
|    total_timesteps      | 1736000      |
| train/                  |              |
|    approx_kl            | 0.0005942347 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.38        |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 4.21e+03     |
|    n_updates            | 8470         |
|    policy_gradient_loss | -9.53e-05    |
|    std                  | 2.65         |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=1738000, episode_reward=283.28 +/- 194.84
Episode length: 417.80 +/- 32.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 283           |
| time/                   |               |
|    total_timesteps      | 1738000       |
| train/                  |               |
|    approx_kl            | 0.00029983473 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.864         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+03      |
|    n_updates            | 8480          |
|    policy_gradient_loss | -0.000378     |
|    std                  | 2.65          |
|    value_loss           | 4.5e+03       |
-------------------------------------------
Eval num_timesteps=1740000, episode_reward=308.75 +/- 350.99
Episode length: 417.00 +/- 50.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 1740000      |
| train/                  |              |
|    approx_kl            | 0.0010072769 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+03     |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00088     |
|    std                  | 2.65         |
|    value_loss           | 1.05e+04     |
------------------------------------------
Eval num_timesteps=1742000, episode_reward=355.00 +/- 230.86
Episode length: 439.00 +/- 41.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 355           |
| time/                   |               |
|    total_timesteps      | 1742000       |
| train/                  |               |
|    approx_kl            | 0.00048412243 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+03      |
|    n_updates            | 8500          |
|    policy_gradient_loss | -0.000227     |
|    std                  | 2.65          |
|    value_loss           | 2.48e+03      |
-------------------------------------------
Eval num_timesteps=1744000, episode_reward=181.11 +/- 431.10
Episode length: 430.20 +/- 42.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 1744000      |
| train/                  |              |
|    approx_kl            | 7.751724e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.001        |
|    loss                 | 6.1e+03      |
|    n_updates            | 8510         |
|    policy_gradient_loss | 3.38e-05     |
|    std                  | 2.65         |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=1746000, episode_reward=359.05 +/- 334.83
Episode length: 379.40 +/- 26.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 379           |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 1746000       |
| train/                  |               |
|    approx_kl            | 0.00011778911 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+03      |
|    n_updates            | 8520          |
|    policy_gradient_loss | -0.000154     |
|    std                  | 2.65          |
|    value_loss           | 3.63e+03      |
-------------------------------------------
Eval num_timesteps=1748000, episode_reward=278.73 +/- 451.25
Episode length: 385.00 +/- 26.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 279          |
| time/                   |              |
|    total_timesteps      | 1748000      |
| train/                  |              |
|    approx_kl            | 0.0026528486 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 258          |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 2.65         |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=1750000, episode_reward=522.63 +/- 456.01
Episode length: 418.20 +/- 33.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 523         |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.003071369 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.39       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 3.5e+03     |
|    n_updates            | 8540        |
|    policy_gradient_loss | 9.35e-05    |
|    std                  | 2.65        |
|    value_loss           | 8.2e+03     |
-----------------------------------------
Eval num_timesteps=1752000, episode_reward=265.54 +/- 554.71
Episode length: 434.20 +/- 26.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 266           |
| time/                   |               |
|    total_timesteps      | 1752000       |
| train/                  |               |
|    approx_kl            | 0.00015577002 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.001         |
|    loss                 | 7.2e+03       |
|    n_updates            | 8550          |
|    policy_gradient_loss | 0.000371      |
|    std                  | 2.66          |
|    value_loss           | 1.45e+04      |
-------------------------------------------
Eval num_timesteps=1754000, episode_reward=479.36 +/- 517.18
Episode length: 427.60 +/- 46.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 479         |
| time/                   |             |
|    total_timesteps      | 1754000     |
| train/                  |             |
|    approx_kl            | 4.16736e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.39       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 4.03e+03    |
|    n_updates            | 8560        |
|    policy_gradient_loss | -0.000118   |
|    std                  | 2.66        |
|    value_loss           | 8.71e+03    |
-----------------------------------------
Eval num_timesteps=1756000, episode_reward=-58.41 +/- 263.56
Episode length: 393.60 +/- 33.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | -58.4         |
| time/                   |               |
|    total_timesteps      | 1756000       |
| train/                  |               |
|    approx_kl            | 5.6141173e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.817         |
|    learning_rate        | 0.001         |
|    loss                 | 4.42e+03      |
|    n_updates            | 8570          |
|    policy_gradient_loss | -0.000147     |
|    std                  | 2.66          |
|    value_loss           | 9.82e+03      |
-------------------------------------------
Eval num_timesteps=1758000, episode_reward=-93.91 +/- 259.63
Episode length: 431.00 +/- 15.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | -93.9         |
| time/                   |               |
|    total_timesteps      | 1758000       |
| train/                  |               |
|    approx_kl            | 0.00010467414 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.001         |
|    loss                 | 2.23e+03      |
|    n_updates            | 8580          |
|    policy_gradient_loss | -0.000178     |
|    std                  | 2.65          |
|    value_loss           | 4.57e+03      |
-------------------------------------------
Eval num_timesteps=1760000, episode_reward=-23.20 +/- 346.99
Episode length: 416.00 +/- 30.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | -23.2        |
| time/                   |              |
|    total_timesteps      | 1760000      |
| train/                  |              |
|    approx_kl            | 0.0004686798 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 2.39e+03     |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.000524    |
|    std                  | 2.65         |
|    value_loss           | 5.28e+03     |
------------------------------------------
Eval num_timesteps=1762000, episode_reward=95.64 +/- 518.09
Episode length: 438.00 +/- 42.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 95.6         |
| time/                   |              |
|    total_timesteps      | 1762000      |
| train/                  |              |
|    approx_kl            | 0.0007821531 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+03     |
|    n_updates            | 8600         |
|    policy_gradient_loss | -0.000945    |
|    std                  | 2.65         |
|    value_loss           | 8e+03        |
------------------------------------------
Eval num_timesteps=1764000, episode_reward=328.58 +/- 496.37
Episode length: 429.40 +/- 39.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 1764000      |
| train/                  |              |
|    approx_kl            | 0.0002205086 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 3.39e+03     |
|    n_updates            | 8610         |
|    policy_gradient_loss | -0.00012     |
|    std                  | 2.66         |
|    value_loss           | 6.76e+03     |
------------------------------------------
Eval num_timesteps=1766000, episode_reward=136.96 +/- 464.75
Episode length: 430.20 +/- 37.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 137           |
| time/                   |               |
|    total_timesteps      | 1766000       |
| train/                  |               |
|    approx_kl            | 3.1002506e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 5.58e+03      |
|    n_updates            | 8620          |
|    policy_gradient_loss | -1.56e-05     |
|    std                  | 2.66          |
|    value_loss           | 1.22e+04      |
-------------------------------------------
Eval num_timesteps=1768000, episode_reward=309.13 +/- 403.84
Episode length: 409.60 +/- 27.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 1768000      |
| train/                  |              |
|    approx_kl            | 6.293747e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.39        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 6.03e+03     |
|    n_updates            | 8630         |
|    policy_gradient_loss | -0.000156    |
|    std                  | 2.66         |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=1770000, episode_reward=202.74 +/- 392.05
Episode length: 424.00 +/- 31.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 203           |
| time/                   |               |
|    total_timesteps      | 1770000       |
| train/                  |               |
|    approx_kl            | 0.00015502074 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.877         |
|    learning_rate        | 0.001         |
|    loss                 | 2.93e+03      |
|    n_updates            | 8640          |
|    policy_gradient_loss | -0.00051      |
|    std                  | 2.66          |
|    value_loss           | 6.04e+03      |
-------------------------------------------
Eval num_timesteps=1772000, episode_reward=205.32 +/- 465.76
Episode length: 401.60 +/- 19.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | 205           |
| time/                   |               |
|    total_timesteps      | 1772000       |
| train/                  |               |
|    approx_kl            | 7.4808544e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.856         |
|    learning_rate        | 0.001         |
|    loss                 | 5.22e+03      |
|    n_updates            | 8650          |
|    policy_gradient_loss | 2.81e-05      |
|    std                  | 2.66          |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=1774000, episode_reward=305.85 +/- 488.70
Episode length: 459.40 +/- 32.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 306           |
| time/                   |               |
|    total_timesteps      | 1774000       |
| train/                  |               |
|    approx_kl            | 4.3534936e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.39         |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.001         |
|    loss                 | 2.12e+03      |
|    n_updates            | 8660          |
|    policy_gradient_loss | -0.000162     |
|    std                  | 2.66          |
|    value_loss           | 5.1e+03       |
-------------------------------------------
Eval num_timesteps=1776000, episode_reward=74.18 +/- 464.23
Episode length: 430.40 +/- 20.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 74.2         |
| time/                   |              |
|    total_timesteps      | 1776000      |
| train/                  |              |
|    approx_kl            | 0.0004304881 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.4         |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 1.93e+03     |
|    n_updates            | 8670         |
|    policy_gradient_loss | -0.000428    |
|    std                  | 2.66         |
|    value_loss           | 4.3e+03      |
------------------------------------------
Eval num_timesteps=1778000, episode_reward=198.56 +/- 473.59
Episode length: 440.80 +/- 45.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 199          |
| time/                   |              |
|    total_timesteps      | 1778000      |
| train/                  |              |
|    approx_kl            | 0.0003447627 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.4         |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 8680         |
|    policy_gradient_loss | 0.000171     |
|    std                  | 2.66         |
|    value_loss           | 3.96e+03     |
------------------------------------------
Eval num_timesteps=1780000, episode_reward=68.08 +/- 422.02
Episode length: 446.80 +/- 32.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 68.1          |
| time/                   |               |
|    total_timesteps      | 1780000       |
| train/                  |               |
|    approx_kl            | 0.00020049984 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.4          |
|    explained_variance   | 0.877         |
|    learning_rate        | 0.001         |
|    loss                 | 1.96e+03      |
|    n_updates            | 8690          |
|    policy_gradient_loss | -0.000278     |
|    std                  | 2.66          |
|    value_loss           | 4.74e+03      |
-------------------------------------------
Eval num_timesteps=1782000, episode_reward=651.07 +/- 273.94
Episode length: 413.60 +/- 38.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 651         |
| time/                   |             |
|    total_timesteps      | 1782000     |
| train/                  |             |
|    approx_kl            | 0.003791808 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.41       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 30.3        |
|    n_updates            | 8700        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 2.67        |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=1784000, episode_reward=441.99 +/- 543.70
Episode length: 464.00 +/- 11.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 442         |
| time/                   |             |
|    total_timesteps      | 1784000     |
| train/                  |             |
|    approx_kl            | 0.014984304 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 2.07e+03    |
|    n_updates            | 8710        |
|    policy_gradient_loss | -0.00291    |
|    std                  | 2.68        |
|    value_loss           | 5e+03       |
-----------------------------------------
Eval num_timesteps=1786000, episode_reward=110.87 +/- 622.71
Episode length: 443.00 +/- 26.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 111          |
| time/                   |              |
|    total_timesteps      | 1786000      |
| train/                  |              |
|    approx_kl            | 0.0015561164 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.43        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 139          |
|    n_updates            | 8720         |
|    policy_gradient_loss | -0.000854    |
|    std                  | 2.69         |
|    value_loss           | 600          |
------------------------------------------
Eval num_timesteps=1788000, episode_reward=442.68 +/- 291.21
Episode length: 448.60 +/- 38.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 443          |
| time/                   |              |
|    total_timesteps      | 1788000      |
| train/                  |              |
|    approx_kl            | 0.0044081258 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.44        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 5.13e+03     |
|    n_updates            | 8730         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 2.7          |
|    value_loss           | 1.22e+04     |
------------------------------------------
Eval num_timesteps=1790000, episode_reward=202.44 +/- 361.59
Episode length: 426.20 +/- 48.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | 202          |
| time/                   |              |
|    total_timesteps      | 1790000      |
| train/                  |              |
|    approx_kl            | 0.0005204474 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.44        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 247          |
|    n_updates            | 8740         |
|    policy_gradient_loss | -7e-05       |
|    std                  | 2.7          |
|    value_loss           | 1.03e+03     |
------------------------------------------
Eval num_timesteps=1792000, episode_reward=204.91 +/- 361.80
Episode length: 415.80 +/- 39.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 416      |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 1792000  |
---------------------------------
Eval num_timesteps=1794000, episode_reward=188.44 +/- 385.19
Episode length: 416.00 +/- 52.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 188           |
| time/                   |               |
|    total_timesteps      | 1794000       |
| train/                  |               |
|    approx_kl            | 0.00076412316 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.44         |
|    explained_variance   | 0.834         |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+03      |
|    n_updates            | 8750          |
|    policy_gradient_loss | 2.66e-05      |
|    std                  | 2.7           |
|    value_loss           | 7.98e+03      |
-------------------------------------------
Eval num_timesteps=1796000, episode_reward=534.05 +/- 391.50
Episode length: 425.80 +/- 46.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 534           |
| time/                   |               |
|    total_timesteps      | 1796000       |
| train/                  |               |
|    approx_kl            | 0.00080511137 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.44         |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 5.03e+03      |
|    n_updates            | 8760          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 2.7           |
|    value_loss           | 1.08e+04      |
-------------------------------------------
Eval num_timesteps=1798000, episode_reward=387.93 +/- 292.07
Episode length: 398.20 +/- 25.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 398           |
|    mean_reward          | 388           |
| time/                   |               |
|    total_timesteps      | 1798000       |
| train/                  |               |
|    approx_kl            | 0.00076897943 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.44         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+03      |
|    n_updates            | 8770          |
|    policy_gradient_loss | -0.000574     |
|    std                  | 2.7           |
|    value_loss           | 4.74e+03      |
-------------------------------------------
Eval num_timesteps=1800000, episode_reward=446.70 +/- 375.11
Episode length: 412.60 +/- 33.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 447          |
| time/                   |              |
|    total_timesteps      | 1800000      |
| train/                  |              |
|    approx_kl            | 0.0009353787 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.44        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+03     |
|    n_updates            | 8780         |
|    policy_gradient_loss | -0.000632    |
|    std                  | 2.7          |
|    value_loss           | 6.06e+03     |
------------------------------------------
Eval num_timesteps=1802000, episode_reward=453.56 +/- 335.57
Episode length: 408.40 +/- 36.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 454           |
| time/                   |               |
|    total_timesteps      | 1802000       |
| train/                  |               |
|    approx_kl            | 0.00028766462 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.44         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+03      |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000323     |
|    std                  | 2.7           |
|    value_loss           | 9.52e+03      |
-------------------------------------------
Eval num_timesteps=1804000, episode_reward=164.40 +/- 405.92
Episode length: 406.40 +/- 28.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 164           |
| time/                   |               |
|    total_timesteps      | 1804000       |
| train/                  |               |
|    approx_kl            | 0.00019744752 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.45         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 2.95e+03      |
|    n_updates            | 8800          |
|    policy_gradient_loss | -0.000254     |
|    std                  | 2.7           |
|    value_loss           | 6.78e+03      |
-------------------------------------------
Eval num_timesteps=1806000, episode_reward=443.76 +/- 141.96
Episode length: 365.40 +/- 42.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 444          |
| time/                   |              |
|    total_timesteps      | 1806000      |
| train/                  |              |
|    approx_kl            | 0.0010570807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.001        |
|    loss                 | 4.35e+03     |
|    n_updates            | 8810         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 2.7          |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=1808000, episode_reward=79.36 +/- 389.78
Episode length: 410.40 +/- 32.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 79.4         |
| time/                   |              |
|    total_timesteps      | 1808000      |
| train/                  |              |
|    approx_kl            | 0.0007317604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 258          |
|    n_updates            | 8820         |
|    policy_gradient_loss | -0.000307    |
|    std                  | 2.7          |
|    value_loss           | 808          |
------------------------------------------
Eval num_timesteps=1810000, episode_reward=424.05 +/- 357.40
Episode length: 419.40 +/- 26.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | 424         |
| time/                   |             |
|    total_timesteps      | 1810000     |
| train/                  |             |
|    approx_kl            | 0.002480553 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.45       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.001       |
|    loss                 | 3.6e+03     |
|    n_updates            | 8830        |
|    policy_gradient_loss | 0.00153     |
|    std                  | 2.71        |
|    value_loss           | 8e+03       |
-----------------------------------------
Eval num_timesteps=1812000, episode_reward=212.50 +/- 266.72
Episode length: 398.60 +/- 23.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 1812000      |
| train/                  |              |
|    approx_kl            | 5.518162e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 2.78e+03     |
|    n_updates            | 8840         |
|    policy_gradient_loss | -2.52e-06    |
|    std                  | 2.71         |
|    value_loss           | 6.23e+03     |
------------------------------------------
Eval num_timesteps=1814000, episode_reward=233.28 +/- 363.55
Episode length: 388.80 +/- 18.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 389           |
|    mean_reward          | 233           |
| time/                   |               |
|    total_timesteps      | 1814000       |
| train/                  |               |
|    approx_kl            | 0.00033014422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.46         |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+03      |
|    n_updates            | 8850          |
|    policy_gradient_loss | -0.000465     |
|    std                  | 2.71          |
|    value_loss           | 9.39e+03      |
-------------------------------------------
Eval num_timesteps=1816000, episode_reward=162.28 +/- 416.26
Episode length: 392.00 +/- 32.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 162           |
| time/                   |               |
|    total_timesteps      | 1816000       |
| train/                  |               |
|    approx_kl            | 0.00015971877 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.46         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 111           |
|    n_updates            | 8860          |
|    policy_gradient_loss | -6.78e-05     |
|    std                  | 2.71          |
|    value_loss           | 446           |
-------------------------------------------
Eval num_timesteps=1818000, episode_reward=149.23 +/- 280.93
Episode length: 426.40 +/- 60.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 149           |
| time/                   |               |
|    total_timesteps      | 1818000       |
| train/                  |               |
|    approx_kl            | 0.00027868323 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.47         |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 8870          |
|    policy_gradient_loss | 0.000455      |
|    std                  | 2.72          |
|    value_loss           | 3.57e+03      |
-------------------------------------------
Eval num_timesteps=1820000, episode_reward=443.88 +/- 472.21
Episode length: 434.40 +/- 46.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 1820000       |
| train/                  |               |
|    approx_kl            | 0.00013368271 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.48         |
|    explained_variance   | 0.821         |
|    learning_rate        | 0.001         |
|    loss                 | 3.23e+03      |
|    n_updates            | 8880          |
|    policy_gradient_loss | -0.000506     |
|    std                  | 2.72          |
|    value_loss           | 7.96e+03      |
-------------------------------------------
Eval num_timesteps=1822000, episode_reward=93.77 +/- 313.10
Episode length: 383.20 +/- 17.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 93.8         |
| time/                   |              |
|    total_timesteps      | 1822000      |
| train/                  |              |
|    approx_kl            | 0.0019562086 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.48        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.000551    |
|    std                  | 2.73         |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=1824000, episode_reward=-104.17 +/- 184.57
Episode length: 450.80 +/- 28.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | -104         |
| time/                   |              |
|    total_timesteps      | 1824000      |
| train/                  |              |
|    approx_kl            | 0.0010819279 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.49        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 2.05e+03     |
|    n_updates            | 8900         |
|    policy_gradient_loss | -0.000294    |
|    std                  | 2.73         |
|    value_loss           | 4.74e+03     |
------------------------------------------
Eval num_timesteps=1826000, episode_reward=201.76 +/- 286.67
Episode length: 410.40 +/- 28.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 202           |
| time/                   |               |
|    total_timesteps      | 1826000       |
| train/                  |               |
|    approx_kl            | 0.00014595818 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.001         |
|    loss                 | 3.78e+03      |
|    n_updates            | 8910          |
|    policy_gradient_loss | 0.000135      |
|    std                  | 2.73          |
|    value_loss           | 8.5e+03       |
-------------------------------------------
Eval num_timesteps=1828000, episode_reward=219.86 +/- 373.37
Episode length: 407.80 +/- 18.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 408            |
|    mean_reward          | 220            |
| time/                   |                |
|    total_timesteps      | 1828000        |
| train/                  |                |
|    approx_kl            | 0.000109642366 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.49          |
|    explained_variance   | 0.924          |
|    learning_rate        | 0.001          |
|    loss                 | 1.31e+03       |
|    n_updates            | 8920           |
|    policy_gradient_loss | -0.00013       |
|    std                  | 2.73           |
|    value_loss           | 3.01e+03       |
--------------------------------------------
Eval num_timesteps=1830000, episode_reward=173.49 +/- 300.88
Episode length: 401.40 +/- 30.78
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 401            |
|    mean_reward          | 173            |
| time/                   |                |
|    total_timesteps      | 1830000        |
| train/                  |                |
|    approx_kl            | 0.000111069414 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.49          |
|    explained_variance   | 0.818          |
|    learning_rate        | 0.001          |
|    loss                 | 3.27e+03       |
|    n_updates            | 8930           |
|    policy_gradient_loss | -0.00023       |
|    std                  | 2.73           |
|    value_loss           | 7.45e+03       |
--------------------------------------------
Eval num_timesteps=1832000, episode_reward=522.07 +/- 423.37
Episode length: 411.00 +/- 33.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 522           |
| time/                   |               |
|    total_timesteps      | 1832000       |
| train/                  |               |
|    approx_kl            | 0.00063012005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 5.24e+03      |
|    n_updates            | 8940          |
|    policy_gradient_loss | -0.000897     |
|    std                  | 2.73          |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=1834000, episode_reward=251.46 +/- 386.17
Episode length: 401.00 +/- 28.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 251          |
| time/                   |              |
|    total_timesteps      | 1834000      |
| train/                  |              |
|    approx_kl            | 0.0005746492 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.49        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 8950         |
|    policy_gradient_loss | -0.000337    |
|    std                  | 2.73         |
|    value_loss           | 4.05e+03     |
------------------------------------------
Eval num_timesteps=1836000, episode_reward=130.84 +/- 345.44
Episode length: 403.00 +/- 34.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 403           |
|    mean_reward          | 131           |
| time/                   |               |
|    total_timesteps      | 1836000       |
| train/                  |               |
|    approx_kl            | 0.00023842795 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 4.04e+03      |
|    n_updates            | 8960          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.73          |
|    value_loss           | 9.14e+03      |
-------------------------------------------
Eval num_timesteps=1838000, episode_reward=321.41 +/- 393.84
Episode length: 403.60 +/- 37.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 321           |
| time/                   |               |
|    total_timesteps      | 1838000       |
| train/                  |               |
|    approx_kl            | 0.00020722629 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.856         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 8970          |
|    policy_gradient_loss | -0.000251     |
|    std                  | 2.73          |
|    value_loss           | 4.36e+03      |
-------------------------------------------
Eval num_timesteps=1840000, episode_reward=415.12 +/- 223.70
Episode length: 373.20 +/- 7.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 415          |
| time/                   |              |
|    total_timesteps      | 1840000      |
| train/                  |              |
|    approx_kl            | 0.0026615884 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 94.3         |
|    n_updates            | 8980         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.73         |
|    value_loss           | 388          |
------------------------------------------
Eval num_timesteps=1842000, episode_reward=328.99 +/- 328.47
Episode length: 393.40 +/- 34.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 1842000      |
| train/                  |              |
|    approx_kl            | 0.0030454206 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.49        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 984          |
|    n_updates            | 8990         |
|    policy_gradient_loss | 0.00142      |
|    std                  | 2.73         |
|    value_loss           | 2.22e+03     |
------------------------------------------
Eval num_timesteps=1844000, episode_reward=240.79 +/- 384.27
Episode length: 464.60 +/- 32.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 1844000      |
| train/                  |              |
|    approx_kl            | 0.0023180859 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 233          |
|    n_updates            | 9000         |
|    policy_gradient_loss | -0.000353    |
|    std                  | 2.73         |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=1846000, episode_reward=98.94 +/- 319.30
Episode length: 429.60 +/- 49.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 98.9          |
| time/                   |               |
|    total_timesteps      | 1846000       |
| train/                  |               |
|    approx_kl            | 0.00045889927 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.768         |
|    learning_rate        | 0.001         |
|    loss                 | 7.1e+03       |
|    n_updates            | 9010          |
|    policy_gradient_loss | 0.000389      |
|    std                  | 2.73          |
|    value_loss           | 1.57e+04      |
-------------------------------------------
Eval num_timesteps=1848000, episode_reward=417.69 +/- 160.65
Episode length: 409.80 +/- 27.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 1848000      |
| train/                  |              |
|    approx_kl            | 7.330498e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.49        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 3.42e+03     |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.000126    |
|    std                  | 2.73         |
|    value_loss           | 7.33e+03     |
------------------------------------------
Eval num_timesteps=1850000, episode_reward=-140.05 +/- 217.30
Episode length: 401.40 +/- 38.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 1850000      |
| train/                  |              |
|    approx_kl            | 5.813013e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.49        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+03     |
|    n_updates            | 9030         |
|    policy_gradient_loss | 2.1e-05      |
|    std                  | 2.73         |
|    value_loss           | 4.69e+03     |
------------------------------------------
Eval num_timesteps=1852000, episode_reward=312.47 +/- 481.22
Episode length: 449.60 +/- 38.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 312           |
| time/                   |               |
|    total_timesteps      | 1852000       |
| train/                  |               |
|    approx_kl            | 8.1033795e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.001         |
|    loss                 | 4.02e+03      |
|    n_updates            | 9040          |
|    policy_gradient_loss | -0.000172     |
|    std                  | 2.73          |
|    value_loss           | 9.03e+03      |
-------------------------------------------
Eval num_timesteps=1854000, episode_reward=270.11 +/- 491.28
Episode length: 418.60 +/- 30.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 270           |
| time/                   |               |
|    total_timesteps      | 1854000       |
| train/                  |               |
|    approx_kl            | 7.0202426e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.49         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 4.49e+03      |
|    n_updates            | 9050          |
|    policy_gradient_loss | -5.15e-05     |
|    std                  | 2.73          |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=1856000, episode_reward=434.57 +/- 433.29
Episode length: 439.40 +/- 20.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 1856000       |
| train/                  |               |
|    approx_kl            | 7.2885625e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.877         |
|    learning_rate        | 0.001         |
|    loss                 | 3.99e+03      |
|    n_updates            | 9060          |
|    policy_gradient_loss | -0.000204     |
|    std                  | 2.74          |
|    value_loss           | 9.2e+03       |
-------------------------------------------
Eval num_timesteps=1858000, episode_reward=371.22 +/- 439.87
Episode length: 430.00 +/- 21.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 371           |
| time/                   |               |
|    total_timesteps      | 1858000       |
| train/                  |               |
|    approx_kl            | 0.00014317437 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.851         |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+03      |
|    n_updates            | 9070          |
|    policy_gradient_loss | -0.000413     |
|    std                  | 2.74          |
|    value_loss           | 6.28e+03      |
-------------------------------------------
Eval num_timesteps=1860000, episode_reward=427.25 +/- 396.77
Episode length: 482.00 +/- 27.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 427           |
| time/                   |               |
|    total_timesteps      | 1860000       |
| train/                  |               |
|    approx_kl            | 0.00086644175 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 231           |
|    n_updates            | 9080          |
|    policy_gradient_loss | -0.000876     |
|    std                  | 2.74          |
|    value_loss           | 1.03e+03      |
-------------------------------------------
Eval num_timesteps=1862000, episode_reward=494.78 +/- 365.60
Episode length: 453.80 +/- 39.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 495          |
| time/                   |              |
|    total_timesteps      | 1862000      |
| train/                  |              |
|    approx_kl            | 0.0008054302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 3.25e+03     |
|    n_updates            | 9090         |
|    policy_gradient_loss | 2.73e-06     |
|    std                  | 2.74         |
|    value_loss           | 7.29e+03     |
------------------------------------------
Eval num_timesteps=1864000, episode_reward=253.32 +/- 300.11
Episode length: 399.40 +/- 39.73
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 399            |
|    mean_reward          | 253            |
| time/                   |                |
|    total_timesteps      | 1864000        |
| train/                  |                |
|    approx_kl            | 0.000114881404 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.5           |
|    explained_variance   | 0.842          |
|    learning_rate        | 0.001          |
|    loss                 | 4.93e+03       |
|    n_updates            | 9100           |
|    policy_gradient_loss | -0.000342      |
|    std                  | 2.74           |
|    value_loss           | 1.16e+04       |
--------------------------------------------
Eval num_timesteps=1866000, episode_reward=227.08 +/- 537.91
Episode length: 428.80 +/- 30.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 227           |
| time/                   |               |
|    total_timesteps      | 1866000       |
| train/                  |               |
|    approx_kl            | 0.00036754788 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 9110          |
|    policy_gradient_loss | -0.000288     |
|    std                  | 2.74          |
|    value_loss           | 6.02e+03      |
-------------------------------------------
Eval num_timesteps=1868000, episode_reward=572.08 +/- 339.10
Episode length: 423.60 +/- 51.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 572           |
| time/                   |               |
|    total_timesteps      | 1868000       |
| train/                  |               |
|    approx_kl            | 0.00041400857 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 2.28e+03      |
|    n_updates            | 9120          |
|    policy_gradient_loss | -0.000389     |
|    std                  | 2.74          |
|    value_loss           | 5.42e+03      |
-------------------------------------------
Eval num_timesteps=1870000, episode_reward=192.09 +/- 417.60
Episode length: 425.40 +/- 40.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 192           |
| time/                   |               |
|    total_timesteps      | 1870000       |
| train/                  |               |
|    approx_kl            | 0.00084909145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 3.01e+03      |
|    n_updates            | 9130          |
|    policy_gradient_loss | 0.00015       |
|    std                  | 2.74          |
|    value_loss           | 7.13e+03      |
-------------------------------------------
Eval num_timesteps=1872000, episode_reward=397.62 +/- 309.09
Episode length: 386.20 +/- 33.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 398          |
| time/                   |              |
|    total_timesteps      | 1872000      |
| train/                  |              |
|    approx_kl            | 0.0010582102 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+03     |
|    n_updates            | 9140         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 2.74         |
|    value_loss           | 4.25e+03     |
------------------------------------------
Eval num_timesteps=1874000, episode_reward=96.25 +/- 282.22
Episode length: 405.80 +/- 35.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 96.3         |
| time/                   |              |
|    total_timesteps      | 1874000      |
| train/                  |              |
|    approx_kl            | 0.0028614001 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.51        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 4.61e+03     |
|    n_updates            | 9150         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 2.74         |
|    value_loss           | 1.08e+04     |
------------------------------------------
Eval num_timesteps=1876000, episode_reward=399.76 +/- 637.07
Episode length: 419.40 +/- 45.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 1876000      |
| train/                  |              |
|    approx_kl            | 0.0023087994 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.51        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 9160         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.74         |
|    value_loss           | 925          |
------------------------------------------
Eval num_timesteps=1878000, episode_reward=40.27 +/- 505.55
Episode length: 419.80 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 40.3     |
| time/              |          |
|    total_timesteps | 1878000  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=-97.18 +/- 169.85
Episode length: 412.80 +/- 33.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | -97.2        |
| time/                   |              |
|    total_timesteps      | 1880000      |
| train/                  |              |
|    approx_kl            | 0.0024216494 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.51        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+03     |
|    n_updates            | 9170         |
|    policy_gradient_loss | 1.58e-05     |
|    std                  | 2.74         |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=1882000, episode_reward=151.05 +/- 208.20
Episode length: 383.00 +/- 24.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 151          |
| time/                   |              |
|    total_timesteps      | 1882000      |
| train/                  |              |
|    approx_kl            | 0.0005851388 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.51        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 4.28e+03     |
|    n_updates            | 9180         |
|    policy_gradient_loss | -0.000453    |
|    std                  | 2.74         |
|    value_loss           | 9.44e+03     |
------------------------------------------
Eval num_timesteps=1884000, episode_reward=-230.13 +/- 147.70
Episode length: 400.60 +/- 19.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | -230          |
| time/                   |               |
|    total_timesteps      | 1884000       |
| train/                  |               |
|    approx_kl            | 0.00080407935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.51         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+03       |
|    n_updates            | 9190          |
|    policy_gradient_loss | -0.00107      |
|    std                  | 2.74          |
|    value_loss           | 3.46e+03      |
-------------------------------------------
Eval num_timesteps=1886000, episode_reward=198.11 +/- 328.81
Episode length: 377.60 +/- 18.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 1886000      |
| train/                  |              |
|    approx_kl            | 0.0008515002 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.51        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 9200         |
|    policy_gradient_loss | -0.000278    |
|    std                  | 2.74         |
|    value_loss           | 668          |
------------------------------------------
Eval num_timesteps=1888000, episode_reward=-31.63 +/- 385.36
Episode length: 408.80 +/- 19.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | -31.6         |
| time/                   |               |
|    total_timesteps      | 1888000       |
| train/                  |               |
|    approx_kl            | 0.00032309725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.51         |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 9210          |
|    policy_gradient_loss | 0.000897      |
|    std                  | 2.74          |
|    value_loss           | 3.8e+03       |
-------------------------------------------
Eval num_timesteps=1890000, episode_reward=445.37 +/- 351.67
Episode length: 385.20 +/- 18.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 385           |
|    mean_reward          | 445           |
| time/                   |               |
|    total_timesteps      | 1890000       |
| train/                  |               |
|    approx_kl            | 0.00043490576 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.5          |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 2.9e+03       |
|    n_updates            | 9220          |
|    policy_gradient_loss | -0.000486     |
|    std                  | 2.74          |
|    value_loss           | 6.94e+03      |
-------------------------------------------
Eval num_timesteps=1892000, episode_reward=484.50 +/- 319.74
Episode length: 405.40 +/- 27.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 484           |
| time/                   |               |
|    total_timesteps      | 1892000       |
| train/                  |               |
|    approx_kl            | 0.00017579205 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.51         |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+03      |
|    n_updates            | 9230          |
|    policy_gradient_loss | -7.12e-05     |
|    std                  | 2.74          |
|    value_loss           | 3.87e+03      |
-------------------------------------------
Eval num_timesteps=1894000, episode_reward=167.50 +/- 374.54
Episode length: 383.20 +/- 22.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 1894000     |
| train/                  |             |
|    approx_kl            | 0.001750011 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.51       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 2.69e+03    |
|    n_updates            | 9240        |
|    policy_gradient_loss | -0.00135    |
|    std                  | 2.75        |
|    value_loss           | 5.96e+03    |
-----------------------------------------
Eval num_timesteps=1896000, episode_reward=205.62 +/- 240.29
Episode length: 352.60 +/- 20.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 353           |
|    mean_reward          | 206           |
| time/                   |               |
|    total_timesteps      | 1896000       |
| train/                  |               |
|    approx_kl            | 0.00067566906 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.52         |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.001         |
|    loss                 | 3.78e+03      |
|    n_updates            | 9250          |
|    policy_gradient_loss | 0.000837      |
|    std                  | 2.75          |
|    value_loss           | 9.15e+03      |
-------------------------------------------
Eval num_timesteps=1898000, episode_reward=315.86 +/- 202.63
Episode length: 325.20 +/- 22.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 1898000     |
| train/                  |             |
|    approx_kl            | 0.008454197 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.52       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 89.6        |
|    n_updates            | 9260        |
|    policy_gradient_loss | -0.00224    |
|    std                  | 2.76        |
|    value_loss           | 395         |
-----------------------------------------
Eval num_timesteps=1900000, episode_reward=203.09 +/- 337.73
Episode length: 386.00 +/- 30.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 1900000      |
| train/                  |              |
|    approx_kl            | 0.0010940402 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+03     |
|    n_updates            | 9270         |
|    policy_gradient_loss | -0.000476    |
|    std                  | 2.76         |
|    value_loss           | 4.38e+03     |
------------------------------------------
Eval num_timesteps=1902000, episode_reward=64.35 +/- 234.42
Episode length: 350.00 +/- 33.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 350         |
|    mean_reward          | 64.4        |
| time/                   |             |
|    total_timesteps      | 1902000     |
| train/                  |             |
|    approx_kl            | 0.002745311 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.54       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.001       |
|    loss                 | 1.67e+03    |
|    n_updates            | 9280        |
|    policy_gradient_loss | -0.000573   |
|    std                  | 2.76        |
|    value_loss           | 3.37e+03    |
-----------------------------------------
Eval num_timesteps=1904000, episode_reward=119.04 +/- 242.95
Episode length: 380.60 +/- 38.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 119         |
| time/                   |             |
|    total_timesteps      | 1904000     |
| train/                  |             |
|    approx_kl            | 0.006032481 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.54       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 151         |
|    n_updates            | 9290        |
|    policy_gradient_loss | -0.00206    |
|    std                  | 2.76        |
|    value_loss           | 527         |
-----------------------------------------
Eval num_timesteps=1906000, episode_reward=294.73 +/- 367.13
Episode length: 395.40 +/- 27.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 295          |
| time/                   |              |
|    total_timesteps      | 1906000      |
| train/                  |              |
|    approx_kl            | 0.0036821533 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+03     |
|    n_updates            | 9300         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 2.76         |
|    value_loss           | 4.33e+03     |
------------------------------------------
Eval num_timesteps=1908000, episode_reward=126.13 +/- 88.76
Episode length: 352.20 +/- 11.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 1908000      |
| train/                  |              |
|    approx_kl            | 0.0046460857 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.54        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 54.4         |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 2.77         |
|    value_loss           | 172          |
------------------------------------------
Eval num_timesteps=1910000, episode_reward=469.81 +/- 266.54
Episode length: 369.80 +/- 26.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 370        |
|    mean_reward          | 470        |
| time/                   |            |
|    total_timesteps      | 1910000    |
| train/                  |            |
|    approx_kl            | 0.00572877 |
|    clip_fraction        | 0.0167     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.56      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.001      |
|    loss                 | 23.2       |
|    n_updates            | 9320       |
|    policy_gradient_loss | -0.00158   |
|    std                  | 2.78       |
|    value_loss           | 88.4       |
----------------------------------------
Eval num_timesteps=1912000, episode_reward=299.80 +/- 314.38
Episode length: 372.20 +/- 43.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 372         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 1912000     |
| train/                  |             |
|    approx_kl            | 0.003934949 |
|    clip_fraction        | 0.00708     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.57       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+03     |
|    n_updates            | 9330        |
|    policy_gradient_loss | 6.13e-05    |
|    std                  | 2.78        |
|    value_loss           | 2.98e+03    |
-----------------------------------------
Eval num_timesteps=1914000, episode_reward=5.81 +/- 176.94
Episode length: 389.20 +/- 24.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 389           |
|    mean_reward          | 5.81          |
| time/                   |               |
|    total_timesteps      | 1914000       |
| train/                  |               |
|    approx_kl            | 0.00055806513 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.57         |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 2.58e+03      |
|    n_updates            | 9340          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 2.78          |
|    value_loss           | 5.58e+03      |
-------------------------------------------
Eval num_timesteps=1916000, episode_reward=-116.15 +/- 213.39
Episode length: 379.40 +/- 30.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 379           |
|    mean_reward          | -116          |
| time/                   |               |
|    total_timesteps      | 1916000       |
| train/                  |               |
|    approx_kl            | 0.00021543677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.57         |
|    explained_variance   | 0.827         |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+03      |
|    n_updates            | 9350          |
|    policy_gradient_loss | -2.05e-05     |
|    std                  | 2.78          |
|    value_loss           | 7.4e+03       |
-------------------------------------------
Eval num_timesteps=1918000, episode_reward=271.20 +/- 218.83
Episode length: 352.60 +/- 16.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 353           |
|    mean_reward          | 271           |
| time/                   |               |
|    total_timesteps      | 1918000       |
| train/                  |               |
|    approx_kl            | 6.9075235e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.58         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 3.86e+03      |
|    n_updates            | 9360          |
|    policy_gradient_loss | -0.000199     |
|    std                  | 2.78          |
|    value_loss           | 8.61e+03      |
-------------------------------------------
Eval num_timesteps=1920000, episode_reward=100.82 +/- 407.31
Episode length: 391.00 +/- 29.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 101           |
| time/                   |               |
|    total_timesteps      | 1920000       |
| train/                  |               |
|    approx_kl            | 0.00032842983 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.58         |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+03      |
|    n_updates            | 9370          |
|    policy_gradient_loss | -0.000598     |
|    std                  | 2.78          |
|    value_loss           | 3.95e+03      |
-------------------------------------------
Eval num_timesteps=1922000, episode_reward=103.93 +/- 114.90
Episode length: 337.80 +/- 25.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 1922000      |
| train/                  |              |
|    approx_kl            | 0.0027580934 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+03     |
|    n_updates            | 9380         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 2.78         |
|    value_loss           | 3.92e+03     |
------------------------------------------
Eval num_timesteps=1924000, episode_reward=285.17 +/- 254.37
Episode length: 355.20 +/- 21.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | 285          |
| time/                   |              |
|    total_timesteps      | 1924000      |
| train/                  |              |
|    approx_kl            | 0.0022123756 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.57        |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.001        |
|    loss                 | 1.8e+03      |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.000407    |
|    std                  | 2.78         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=1926000, episode_reward=233.19 +/- 371.64
Episode length: 359.20 +/- 35.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 233          |
| time/                   |              |
|    total_timesteps      | 1926000      |
| train/                  |              |
|    approx_kl            | 0.0032520152 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.57        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+03     |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 2.77         |
|    value_loss           | 4.09e+03     |
------------------------------------------
Eval num_timesteps=1928000, episode_reward=223.46 +/- 436.08
Episode length: 385.40 +/- 31.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 1928000      |
| train/                  |              |
|    approx_kl            | 0.0005582571 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.56        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 52.2         |
|    n_updates            | 9410         |
|    policy_gradient_loss | 8.14e-05     |
|    std                  | 2.77         |
|    value_loss           | 305          |
------------------------------------------
Eval num_timesteps=1930000, episode_reward=509.74 +/- 322.26
Episode length: 371.40 +/- 8.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 510         |
| time/                   |             |
|    total_timesteps      | 1930000     |
| train/                  |             |
|    approx_kl            | 0.003300495 |
|    clip_fraction        | 0.00444     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.56       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.001       |
|    loss                 | 1.4e+03     |
|    n_updates            | 9420        |
|    policy_gradient_loss | 0.00102     |
|    std                  | 2.77        |
|    value_loss           | 3.38e+03    |
-----------------------------------------
Eval num_timesteps=1932000, episode_reward=322.13 +/- 278.16
Episode length: 364.40 +/- 23.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | 322           |
| time/                   |               |
|    total_timesteps      | 1932000       |
| train/                  |               |
|    approx_kl            | 0.00022617492 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.56         |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 9430          |
|    policy_gradient_loss | 0.000259      |
|    std                  | 2.78          |
|    value_loss           | 4.18e+03      |
-------------------------------------------
Eval num_timesteps=1934000, episode_reward=388.18 +/- 174.80
Episode length: 347.80 +/- 12.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 348          |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 1934000      |
| train/                  |              |
|    approx_kl            | 0.0003467162 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.57        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 9440         |
|    policy_gradient_loss | -0.000622    |
|    std                  | 2.78         |
|    value_loss           | 3.47e+03     |
------------------------------------------
Eval num_timesteps=1936000, episode_reward=58.50 +/- 304.91
Episode length: 357.60 +/- 26.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 358           |
|    mean_reward          | 58.5          |
| time/                   |               |
|    total_timesteps      | 1936000       |
| train/                  |               |
|    approx_kl            | 0.00014228551 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.57         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+03      |
|    n_updates            | 9450          |
|    policy_gradient_loss | 1.39e-05      |
|    std                  | 2.78          |
|    value_loss           | 3.7e+03       |
-------------------------------------------
Eval num_timesteps=1938000, episode_reward=296.34 +/- 395.19
Episode length: 385.00 +/- 26.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 296          |
| time/                   |              |
|    total_timesteps      | 1938000      |
| train/                  |              |
|    approx_kl            | 3.745404e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 3.55e+03     |
|    n_updates            | 9460         |
|    policy_gradient_loss | 7.43e-05     |
|    std                  | 2.78         |
|    value_loss           | 8.84e+03     |
------------------------------------------
Eval num_timesteps=1940000, episode_reward=351.42 +/- 340.61
Episode length: 379.20 +/- 42.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 1940000      |
| train/                  |              |
|    approx_kl            | 0.0007952477 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+03     |
|    n_updates            | 9470         |
|    policy_gradient_loss | -0.000747    |
|    std                  | 2.79         |
|    value_loss           | 9.07e+03     |
------------------------------------------
Eval num_timesteps=1942000, episode_reward=308.87 +/- 156.10
Episode length: 356.80 +/- 32.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 1942000      |
| train/                  |              |
|    approx_kl            | 0.0016005911 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 3.41e+03     |
|    n_updates            | 9480         |
|    policy_gradient_loss | -0.000845    |
|    std                  | 2.79         |
|    value_loss           | 8.45e+03     |
------------------------------------------
Eval num_timesteps=1944000, episode_reward=343.25 +/- 128.17
Episode length: 341.00 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 1944000      |
| train/                  |              |
|    approx_kl            | 6.702711e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 2.96e+03     |
|    n_updates            | 9490         |
|    policy_gradient_loss | 0.000403     |
|    std                  | 2.79         |
|    value_loss           | 6.49e+03     |
------------------------------------------
Eval num_timesteps=1946000, episode_reward=285.63 +/- 128.91
Episode length: 350.40 +/- 25.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 1946000      |
| train/                  |              |
|    approx_kl            | 0.0037318375 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.59        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 60.7         |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.79         |
|    value_loss           | 304          |
------------------------------------------
Eval num_timesteps=1948000, episode_reward=457.03 +/- 205.08
Episode length: 379.00 +/- 18.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 457          |
| time/                   |              |
|    total_timesteps      | 1948000      |
| train/                  |              |
|    approx_kl            | 0.0037566712 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 24.5         |
|    n_updates            | 9510         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 2.81         |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=1950000, episode_reward=200.08 +/- 244.53
Episode length: 373.80 +/- 7.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | 200          |
| time/                   |              |
|    total_timesteps      | 1950000      |
| train/                  |              |
|    approx_kl            | 0.0048582605 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.62        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 35.3         |
|    n_updates            | 9520         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 2.83         |
|    value_loss           | 132          |
------------------------------------------
Eval num_timesteps=1952000, episode_reward=104.00 +/- 281.93
Episode length: 392.40 +/- 48.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 1952000      |
| train/                  |              |
|    approx_kl            | 0.0026704872 |
|    clip_fraction        | 0.0818       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 30.8         |
|    n_updates            | 9530         |
|    policy_gradient_loss | -0.00371     |
|    std                  | 2.83         |
|    value_loss           | 94.2         |
------------------------------------------
Eval num_timesteps=1954000, episode_reward=549.37 +/- 371.72
Episode length: 397.20 +/- 34.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 549          |
| time/                   |              |
|    total_timesteps      | 1954000      |
| train/                  |              |
|    approx_kl            | 0.0032591838 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.63        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+03     |
|    n_updates            | 9540         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 2.83         |
|    value_loss           | 7.53e+03     |
------------------------------------------
Eval num_timesteps=1956000, episode_reward=483.31 +/- 274.48
Episode length: 402.20 +/- 51.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 483          |
| time/                   |              |
|    total_timesteps      | 1956000      |
| train/                  |              |
|    approx_kl            | 0.0006299383 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.63        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 9550         |
|    policy_gradient_loss | -0.000349    |
|    std                  | 2.82         |
|    value_loss           | 3.84e+03     |
------------------------------------------
Eval num_timesteps=1958000, episode_reward=211.16 +/- 393.06
Episode length: 414.00 +/- 36.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 211           |
| time/                   |               |
|    total_timesteps      | 1958000       |
| train/                  |               |
|    approx_kl            | 0.00019432616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.63         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+03      |
|    n_updates            | 9560          |
|    policy_gradient_loss | 6.67e-05      |
|    std                  | 2.82          |
|    value_loss           | 5.14e+03      |
-------------------------------------------
Eval num_timesteps=1960000, episode_reward=154.10 +/- 422.43
Episode length: 404.80 +/- 36.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 154          |
| time/                   |              |
|    total_timesteps      | 1960000      |
| train/                  |              |
|    approx_kl            | 0.0010558632 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.63        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 154          |
|    n_updates            | 9570         |
|    policy_gradient_loss | -0.000717    |
|    std                  | 2.83         |
|    value_loss           | 874          |
------------------------------------------
Eval num_timesteps=1962000, episode_reward=193.05 +/- 470.59
Episode length: 424.20 +/- 21.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 1962000      |
| train/                  |              |
|    approx_kl            | 0.0021865119 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 3.23e+03     |
|    n_updates            | 9580         |
|    policy_gradient_loss | 1.92e-05     |
|    std                  | 2.83         |
|    value_loss           | 9.31e+03     |
------------------------------------------
Eval num_timesteps=1964000, episode_reward=420.01 +/- 491.87
Episode length: 424.80 +/- 40.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 1964000  |
---------------------------------
Eval num_timesteps=1966000, episode_reward=294.31 +/- 411.15
Episode length: 424.60 +/- 31.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 294           |
| time/                   |               |
|    total_timesteps      | 1966000       |
| train/                  |               |
|    approx_kl            | 0.00030941365 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.64         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+03      |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000101     |
|    std                  | 2.83          |
|    value_loss           | 6.54e+03      |
-------------------------------------------
Eval num_timesteps=1968000, episode_reward=119.16 +/- 371.30
Episode length: 425.20 +/- 16.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 1968000      |
| train/                  |              |
|    approx_kl            | 0.0001399706 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 6.1e+03      |
|    n_updates            | 9600         |
|    policy_gradient_loss | -0.000253    |
|    std                  | 2.83         |
|    value_loss           | 1.51e+04     |
------------------------------------------
Eval num_timesteps=1970000, episode_reward=-81.60 +/- 371.56
Episode length: 435.20 +/- 15.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | -81.6        |
| time/                   |              |
|    total_timesteps      | 1970000      |
| train/                  |              |
|    approx_kl            | 9.668525e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 7.17e+03     |
|    n_updates            | 9610         |
|    policy_gradient_loss | 0.000107     |
|    std                  | 2.83         |
|    value_loss           | 1.61e+04     |
------------------------------------------
Eval num_timesteps=1972000, episode_reward=157.33 +/- 267.44
Episode length: 417.60 +/- 11.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 157           |
| time/                   |               |
|    total_timesteps      | 1972000       |
| train/                  |               |
|    approx_kl            | 0.00011325546 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.64         |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.001         |
|    loss                 | 3.08e+03      |
|    n_updates            | 9620          |
|    policy_gradient_loss | -0.000385     |
|    std                  | 2.83          |
|    value_loss           | 6.68e+03      |
-------------------------------------------
Eval num_timesteps=1974000, episode_reward=-155.36 +/- 327.18
Episode length: 449.20 +/- 39.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 449        |
|    mean_reward          | -155       |
| time/                   |            |
|    total_timesteps      | 1974000    |
| train/                  |            |
|    approx_kl            | 8.5476e-05 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.64      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.001      |
|    loss                 | 1.61e+03   |
|    n_updates            | 9630       |
|    policy_gradient_loss | -9.16e-05  |
|    std                  | 2.84       |
|    value_loss           | 3.85e+03   |
----------------------------------------
Eval num_timesteps=1976000, episode_reward=38.13 +/- 285.79
Episode length: 414.60 +/- 28.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 415           |
|    mean_reward          | 38.1          |
| time/                   |               |
|    total_timesteps      | 1976000       |
| train/                  |               |
|    approx_kl            | 0.00066895515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.65         |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.001         |
|    loss                 | 413           |
|    n_updates            | 9640          |
|    policy_gradient_loss | -0.000587     |
|    std                  | 2.84          |
|    value_loss           | 1.24e+03      |
-------------------------------------------
Eval num_timesteps=1978000, episode_reward=270.05 +/- 537.33
Episode length: 438.00 +/- 23.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 1978000      |
| train/                  |              |
|    approx_kl            | 0.0012179622 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.65        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+03      |
|    n_updates            | 9650         |
|    policy_gradient_loss | -0.000751    |
|    std                  | 2.84         |
|    value_loss           | 3.61e+03     |
------------------------------------------
Eval num_timesteps=1980000, episode_reward=332.49 +/- 240.48
Episode length: 377.80 +/- 25.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 1980000      |
| train/                  |              |
|    approx_kl            | 0.0005169689 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.65        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 9660         |
|    policy_gradient_loss | -0.00043     |
|    std                  | 2.84         |
|    value_loss           | 3.52e+03     |
------------------------------------------
Eval num_timesteps=1982000, episode_reward=496.13 +/- 238.67
Episode length: 403.60 +/- 24.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 1982000       |
| train/                  |               |
|    approx_kl            | 0.00040356905 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 9670          |
|    policy_gradient_loss | -0.000446     |
|    std                  | 2.85          |
|    value_loss           | 3.41e+03      |
-------------------------------------------
Eval num_timesteps=1984000, episode_reward=-155.50 +/- 236.24
Episode length: 429.40 +/- 13.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | -156          |
| time/                   |               |
|    total_timesteps      | 1984000       |
| train/                  |               |
|    approx_kl            | 0.00096176926 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.837         |
|    learning_rate        | 0.001         |
|    loss                 | 1.84e+03      |
|    n_updates            | 9680          |
|    policy_gradient_loss | -0.000257     |
|    std                  | 2.85          |
|    value_loss           | 5.05e+03      |
-------------------------------------------
Eval num_timesteps=1986000, episode_reward=240.12 +/- 501.04
Episode length: 435.80 +/- 34.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 240          |
| time/                   |              |
|    total_timesteps      | 1986000      |
| train/                  |              |
|    approx_kl            | 0.0006320532 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.66        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 3e+03        |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00121     |
|    std                  | 2.85         |
|    value_loss           | 7.14e+03     |
------------------------------------------
Eval num_timesteps=1988000, episode_reward=84.69 +/- 476.35
Episode length: 452.80 +/- 53.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 84.7         |
| time/                   |              |
|    total_timesteps      | 1988000      |
| train/                  |              |
|    approx_kl            | 0.0004371364 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 5.31e+03     |
|    n_updates            | 9700         |
|    policy_gradient_loss | -0.000607    |
|    std                  | 2.85         |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=1990000, episode_reward=136.29 +/- 433.14
Episode length: 464.00 +/- 44.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 136           |
| time/                   |               |
|    total_timesteps      | 1990000       |
| train/                  |               |
|    approx_kl            | 0.00017267634 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.001         |
|    loss                 | 3.96e+03      |
|    n_updates            | 9710          |
|    policy_gradient_loss | -0.000134     |
|    std                  | 2.85          |
|    value_loss           | 8.1e+03       |
-------------------------------------------
Eval num_timesteps=1992000, episode_reward=475.78 +/- 520.39
Episode length: 483.60 +/- 36.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | 476         |
| time/                   |             |
|    total_timesteps      | 1992000     |
| train/                  |             |
|    approx_kl            | 0.001036647 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.67       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 442         |
|    n_updates            | 9720        |
|    policy_gradient_loss | -0.000592   |
|    std                  | 2.85        |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=1994000, episode_reward=316.19 +/- 569.65
Episode length: 444.20 +/- 26.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 316           |
| time/                   |               |
|    total_timesteps      | 1994000       |
| train/                  |               |
|    approx_kl            | 0.00029214777 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 138           |
|    n_updates            | 9730          |
|    policy_gradient_loss | 0.000372      |
|    std                  | 2.85          |
|    value_loss           | 736           |
-------------------------------------------
Eval num_timesteps=1996000, episode_reward=481.28 +/- 523.43
Episode length: 431.00 +/- 22.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 481           |
| time/                   |               |
|    total_timesteps      | 1996000       |
| train/                  |               |
|    approx_kl            | 0.00082671485 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+03      |
|    n_updates            | 9740          |
|    policy_gradient_loss | 0.000374      |
|    std                  | 2.85          |
|    value_loss           | 2.89e+03      |
-------------------------------------------
Eval num_timesteps=1998000, episode_reward=192.15 +/- 404.94
Episode length: 487.80 +/- 56.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 1998000      |
| train/                  |              |
|    approx_kl            | 0.0013194816 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.001        |
|    loss                 | 1.82e+03     |
|    n_updates            | 9750         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 2.85         |
|    value_loss           | 4.99e+03     |
------------------------------------------
Eval num_timesteps=2000000, episode_reward=-128.93 +/- 365.26
Episode length: 471.80 +/- 43.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 2000000      |
| train/                  |              |
|    approx_kl            | 0.0026290151 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.001        |
|    loss                 | 4.64e+03     |
|    n_updates            | 9760         |
|    policy_gradient_loss | -0.000596    |
|    std                  | 2.85         |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=2002000, episode_reward=223.47 +/- 620.04
Episode length: 495.20 +/- 34.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 2002000      |
| train/                  |              |
|    approx_kl            | 0.0028977552 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 841          |
|    n_updates            | 9770         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 2.86         |
|    value_loss           | 2.38e+03     |
------------------------------------------
Eval num_timesteps=2004000, episode_reward=639.38 +/- 491.31
Episode length: 475.60 +/- 56.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 639         |
| time/                   |             |
|    total_timesteps      | 2004000     |
| train/                  |             |
|    approx_kl            | 0.004546092 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.67       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 43.8        |
|    n_updates            | 9780        |
|    policy_gradient_loss | -0.00154    |
|    std                  | 2.86        |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=2006000, episode_reward=-98.57 +/- 367.19
Episode length: 480.20 +/- 38.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | -98.6        |
| time/                   |              |
|    total_timesteps      | 2006000      |
| train/                  |              |
|    approx_kl            | 0.0016003377 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 6.7e+03      |
|    n_updates            | 9790         |
|    policy_gradient_loss | -8.83e-05    |
|    std                  | 2.86         |
|    value_loss           | 1.58e+04     |
------------------------------------------
Eval num_timesteps=2008000, episode_reward=248.91 +/- 460.27
Episode length: 476.80 +/- 42.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 2008000      |
| train/                  |              |
|    approx_kl            | 0.0011173832 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 74.5         |
|    n_updates            | 9800         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 2.86         |
|    value_loss           | 382          |
------------------------------------------
Eval num_timesteps=2010000, episode_reward=-189.72 +/- 289.58
Episode length: 437.20 +/- 63.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | -190         |
| time/                   |              |
|    total_timesteps      | 2010000      |
| train/                  |              |
|    approx_kl            | 0.0018895122 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 2.8e+03      |
|    n_updates            | 9810         |
|    policy_gradient_loss | 0.000803     |
|    std                  | 2.86         |
|    value_loss           | 7.27e+03     |
------------------------------------------
Eval num_timesteps=2012000, episode_reward=63.75 +/- 518.57
Episode length: 423.20 +/- 26.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 423        |
|    mean_reward          | 63.8       |
| time/                   |            |
|    total_timesteps      | 2012000    |
| train/                  |            |
|    approx_kl            | 0.00373031 |
|    clip_fraction        | 0.00308    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.67      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.001      |
|    loss                 | 153        |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.00165   |
|    std                  | 2.86       |
|    value_loss           | 801        |
----------------------------------------
Eval num_timesteps=2014000, episode_reward=358.17 +/- 332.93
Episode length: 479.80 +/- 43.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 358          |
| time/                   |              |
|    total_timesteps      | 2014000      |
| train/                  |              |
|    approx_kl            | 0.0039331415 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+03     |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.000723    |
|    std                  | 2.87         |
|    value_loss           | 7.08e+03     |
------------------------------------------
Eval num_timesteps=2016000, episode_reward=36.82 +/- 537.38
Episode length: 441.60 +/- 28.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 36.8         |
| time/                   |              |
|    total_timesteps      | 2016000      |
| train/                  |              |
|    approx_kl            | 0.0002736792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+03     |
|    n_updates            | 9840         |
|    policy_gradient_loss | 0.00044      |
|    std                  | 2.87         |
|    value_loss           | 4.33e+03     |
------------------------------------------
Eval num_timesteps=2018000, episode_reward=573.26 +/- 504.51
Episode length: 461.00 +/- 13.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 573          |
| time/                   |              |
|    total_timesteps      | 2018000      |
| train/                  |              |
|    approx_kl            | 3.740919e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 2.06e+03     |
|    n_updates            | 9850         |
|    policy_gradient_loss | 5.83e-07     |
|    std                  | 2.87         |
|    value_loss           | 5.08e+03     |
------------------------------------------
Eval num_timesteps=2020000, episode_reward=191.40 +/- 349.16
Episode length: 439.60 +/- 40.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | 191           |
| time/                   |               |
|    total_timesteps      | 2020000       |
| train/                  |               |
|    approx_kl            | 3.1458272e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.874         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+03      |
|    n_updates            | 9860          |
|    policy_gradient_loss | 3.42e-05      |
|    std                  | 2.87          |
|    value_loss           | 6.54e+03      |
-------------------------------------------
Eval num_timesteps=2022000, episode_reward=475.60 +/- 462.25
Episode length: 418.40 +/- 23.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 476           |
| time/                   |               |
|    total_timesteps      | 2022000       |
| train/                  |               |
|    approx_kl            | 2.3558357e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.872         |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+03       |
|    n_updates            | 9870          |
|    policy_gradient_loss | -0.000129     |
|    std                  | 2.87          |
|    value_loss           | 7.48e+03      |
-------------------------------------------
Eval num_timesteps=2024000, episode_reward=439.60 +/- 516.97
Episode length: 435.40 +/- 34.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 435           |
|    mean_reward          | 440           |
| time/                   |               |
|    total_timesteps      | 2024000       |
| train/                  |               |
|    approx_kl            | 6.9969974e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 6.16e+03      |
|    n_updates            | 9880          |
|    policy_gradient_loss | 0.000162      |
|    std                  | 2.87          |
|    value_loss           | 1.66e+04      |
-------------------------------------------
Eval num_timesteps=2026000, episode_reward=443.97 +/- 521.12
Episode length: 450.00 +/- 29.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 2026000       |
| train/                  |               |
|    approx_kl            | 5.2638294e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.000171     |
|    std                  | 2.87          |
|    value_loss           | 3.92e+03      |
-------------------------------------------
Eval num_timesteps=2028000, episode_reward=256.06 +/- 379.54
Episode length: 399.40 +/- 28.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 2028000     |
| train/                  |             |
|    approx_kl            | 0.007849661 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.69       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 23.6        |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.00366    |
|    std                  | 2.89        |
|    value_loss           | 75          |
-----------------------------------------
Eval num_timesteps=2030000, episode_reward=287.10 +/- 429.79
Episode length: 415.00 +/- 46.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 2030000      |
| train/                  |              |
|    approx_kl            | 0.0031609181 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.72        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 6.13e+03     |
|    n_updates            | 9910         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 2.91         |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=2032000, episode_reward=-126.92 +/- 331.58
Episode length: 451.00 +/- 30.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 451           |
|    mean_reward          | -127          |
| time/                   |               |
|    total_timesteps      | 2032000       |
| train/                  |               |
|    approx_kl            | 0.00034702264 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.73         |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 9920          |
|    policy_gradient_loss | -0.000275     |
|    std                  | 2.92          |
|    value_loss           | 3.52e+03      |
-------------------------------------------
Eval num_timesteps=2034000, episode_reward=485.55 +/- 269.85
Episode length: 396.60 +/- 37.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 397           |
|    mean_reward          | 486           |
| time/                   |               |
|    total_timesteps      | 2034000       |
| train/                  |               |
|    approx_kl            | 0.00012900087 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.73         |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 3.79e+03      |
|    n_updates            | 9930          |
|    policy_gradient_loss | -8.2e-05      |
|    std                  | 2.92          |
|    value_loss           | 8.51e+03      |
-------------------------------------------
Eval num_timesteps=2036000, episode_reward=560.52 +/- 598.46
Episode length: 449.80 +/- 48.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 561          |
| time/                   |              |
|    total_timesteps      | 2036000      |
| train/                  |              |
|    approx_kl            | 6.414103e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.73        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.000107    |
|    std                  | 2.92         |
|    value_loss           | 3.73e+03     |
------------------------------------------
Eval num_timesteps=2038000, episode_reward=168.92 +/- 321.99
Episode length: 435.40 +/- 55.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 2038000      |
| train/                  |              |
|    approx_kl            | 8.014671e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.001        |
|    loss                 | 2.16e+03     |
|    n_updates            | 9950         |
|    policy_gradient_loss | -0.000139    |
|    std                  | 2.92         |
|    value_loss           | 4.14e+03     |
------------------------------------------
Eval num_timesteps=2040000, episode_reward=-187.71 +/- 223.78
Episode length: 394.60 +/- 39.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | -188         |
| time/                   |              |
|    total_timesteps      | 2040000      |
| train/                  |              |
|    approx_kl            | 0.0074701374 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 46.5         |
|    n_updates            | 9960         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.93         |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=2042000, episode_reward=198.83 +/- 547.61
Episode length: 443.80 +/- 23.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 199          |
| time/                   |              |
|    total_timesteps      | 2042000      |
| train/                  |              |
|    approx_kl            | 0.0032815859 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.75        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 714          |
|    n_updates            | 9970         |
|    policy_gradient_loss | -0.000104    |
|    std                  | 2.93         |
|    value_loss           | 1.89e+03     |
------------------------------------------
Eval num_timesteps=2044000, episode_reward=555.51 +/- 448.36
Episode length: 466.40 +/- 42.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 2044000      |
| train/                  |              |
|    approx_kl            | 0.0005037086 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.75        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 4.62e+03     |
|    n_updates            | 9980         |
|    policy_gradient_loss | -0.000255    |
|    std                  | 2.94         |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=2046000, episode_reward=371.44 +/- 550.47
Episode length: 435.20 +/- 67.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 435           |
|    mean_reward          | 371           |
| time/                   |               |
|    total_timesteps      | 2046000       |
| train/                  |               |
|    approx_kl            | 0.00024746454 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.76         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+03      |
|    n_updates            | 9990          |
|    policy_gradient_loss | 0.00031       |
|    std                  | 2.94          |
|    value_loss           | 9.14e+03      |
-------------------------------------------
Eval num_timesteps=2048000, episode_reward=453.36 +/- 485.20
Episode length: 471.00 +/- 43.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 2048000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 447      |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 199      |
|    iterations      | 1000     |
|    time_elapsed    | 10243    |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=68.42 +/- 438.43
Episode length: 432.40 +/- 20.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 432           |
|    mean_reward          | 68.4          |
| time/                   |               |
|    total_timesteps      | 2050000       |
| train/                  |               |
|    approx_kl            | 0.00022962433 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.76         |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+03      |
|    n_updates            | 10000         |
|    policy_gradient_loss | -0.000536     |
|    std                  | 2.94          |
|    value_loss           | 3.96e+03      |
-------------------------------------------
Eval num_timesteps=2052000, episode_reward=71.64 +/- 265.12
Episode length: 407.20 +/- 51.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 71.6         |
| time/                   |              |
|    total_timesteps      | 2052000      |
| train/                  |              |
|    approx_kl            | 0.0003375497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.76        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 10010        |
|    policy_gradient_loss | -3.1e-05     |
|    std                  | 2.94         |
|    value_loss           | 3.39e+03     |
------------------------------------------
Eval num_timesteps=2054000, episode_reward=430.84 +/- 297.33
Episode length: 436.40 +/- 53.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 431          |
| time/                   |              |
|    total_timesteps      | 2054000      |
| train/                  |              |
|    approx_kl            | 0.0009219644 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.76        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 10020        |
|    policy_gradient_loss | -0.000515    |
|    std                  | 2.94         |
|    value_loss           | 860          |
------------------------------------------
Eval num_timesteps=2056000, episode_reward=132.71 +/- 418.49
Episode length: 444.60 +/- 22.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 445         |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 2056000     |
| train/                  |             |
|    approx_kl            | 0.002829732 |
|    clip_fraction        | 0.0019      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.76       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 2.34e+03    |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.000915   |
|    std                  | 2.94        |
|    value_loss           | 6.27e+03    |
-----------------------------------------
Eval num_timesteps=2058000, episode_reward=239.48 +/- 244.63
Episode length: 435.60 +/- 39.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 2058000      |
| train/                  |              |
|    approx_kl            | 0.0010635692 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.76        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+03     |
|    n_updates            | 10040        |
|    policy_gradient_loss | -0.000972    |
|    std                  | 2.95         |
|    value_loss           | 7.2e+03      |
------------------------------------------
Eval num_timesteps=2060000, episode_reward=337.41 +/- 486.33
Episode length: 416.80 +/- 39.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 337           |
| time/                   |               |
|    total_timesteps      | 2060000       |
| train/                  |               |
|    approx_kl            | 0.00023182941 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.76         |
|    explained_variance   | 0.856         |
|    learning_rate        | 0.001         |
|    loss                 | 1.98e+03      |
|    n_updates            | 10050         |
|    policy_gradient_loss | 9.67e-05      |
|    std                  | 2.95          |
|    value_loss           | 4.63e+03      |
-------------------------------------------
Eval num_timesteps=2062000, episode_reward=-21.12 +/- 343.03
Episode length: 443.60 +/- 27.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 444         |
|    mean_reward          | -21.1       |
| time/                   |             |
|    total_timesteps      | 2062000     |
| train/                  |             |
|    approx_kl            | 0.008087186 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.76       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 42.2        |
|    n_updates            | 10060       |
|    policy_gradient_loss | -0.00271    |
|    std                  | 2.94        |
|    value_loss           | 193         |
-----------------------------------------
Eval num_timesteps=2064000, episode_reward=-3.74 +/- 596.06
Episode length: 461.80 +/- 37.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 462        |
|    mean_reward          | -3.74      |
| time/                   |            |
|    total_timesteps      | 2064000    |
| train/                  |            |
|    approx_kl            | 0.00485498 |
|    clip_fraction        | 0.00781    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.75      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.001      |
|    loss                 | 34.5       |
|    n_updates            | 10070      |
|    policy_gradient_loss | -0.000695  |
|    std                  | 2.93       |
|    value_loss           | 172        |
----------------------------------------
Eval num_timesteps=2066000, episode_reward=201.40 +/- 562.64
Episode length: 459.60 +/- 32.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 460       |
|    mean_reward          | 201       |
| time/                   |           |
|    total_timesteps      | 2066000   |
| train/                  |           |
|    approx_kl            | 0.0051836 |
|    clip_fraction        | 0.0122    |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.74     |
|    explained_variance   | 0.836     |
|    learning_rate        | 0.001     |
|    loss                 | 2.47e+03  |
|    n_updates            | 10080     |
|    policy_gradient_loss | -0.00299  |
|    std                  | 2.92      |
|    value_loss           | 6.5e+03   |
---------------------------------------
Eval num_timesteps=2068000, episode_reward=-71.23 +/- 293.57
Episode length: 426.80 +/- 24.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | -71.2         |
| time/                   |               |
|    total_timesteps      | 2068000       |
| train/                  |               |
|    approx_kl            | 0.00045971194 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.74         |
|    explained_variance   | 0.836         |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+03      |
|    n_updates            | 10090         |
|    policy_gradient_loss | 0.000589      |
|    std                  | 2.92          |
|    value_loss           | 1.05e+04      |
-------------------------------------------
Eval num_timesteps=2070000, episode_reward=420.68 +/- 402.43
Episode length: 395.00 +/- 23.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 395           |
|    mean_reward          | 421           |
| time/                   |               |
|    total_timesteps      | 2070000       |
| train/                  |               |
|    approx_kl            | 2.8977374e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.74         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 3.31e+03      |
|    n_updates            | 10100         |
|    policy_gradient_loss | -8.24e-05     |
|    std                  | 2.92          |
|    value_loss           | 9.04e+03      |
-------------------------------------------
Eval num_timesteps=2072000, episode_reward=343.58 +/- 412.09
Episode length: 424.40 +/- 34.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 344           |
| time/                   |               |
|    total_timesteps      | 2072000       |
| train/                  |               |
|    approx_kl            | 0.00013251696 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.74         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+03      |
|    n_updates            | 10110         |
|    policy_gradient_loss | -0.000249     |
|    std                  | 2.92          |
|    value_loss           | 4.27e+03      |
-------------------------------------------
Eval num_timesteps=2074000, episode_reward=174.96 +/- 255.06
Episode length: 386.20 +/- 59.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 2074000      |
| train/                  |              |
|    approx_kl            | 0.0002289796 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.75        |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.001        |
|    loss                 | 5.52e+03     |
|    n_updates            | 10120        |
|    policy_gradient_loss | -4.91e-05    |
|    std                  | 2.92         |
|    value_loss           | 1.4e+04      |
------------------------------------------
Eval num_timesteps=2076000, episode_reward=-3.78 +/- 193.26
Episode length: 441.20 +/- 35.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | -3.78        |
| time/                   |              |
|    total_timesteps      | 2076000      |
| train/                  |              |
|    approx_kl            | 0.0074268007 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 42.7         |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.00202     |
|    std                  | 2.92         |
|    value_loss           | 167          |
------------------------------------------
Eval num_timesteps=2078000, episode_reward=352.03 +/- 354.29
Episode length: 423.00 +/- 69.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 2078000      |
| train/                  |              |
|    approx_kl            | 0.0004851087 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+03     |
|    n_updates            | 10140        |
|    policy_gradient_loss | 6.9e-05      |
|    std                  | 2.92         |
|    value_loss           | 2.25e+03     |
------------------------------------------
Eval num_timesteps=2080000, episode_reward=201.93 +/- 330.14
Episode length: 380.60 +/- 18.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 202           |
| time/                   |               |
|    total_timesteps      | 2080000       |
| train/                  |               |
|    approx_kl            | 0.00015922228 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.73         |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 10150         |
|    policy_gradient_loss | -8.06e-06     |
|    std                  | 2.92          |
|    value_loss           | 3.65e+03      |
-------------------------------------------
Eval num_timesteps=2082000, episode_reward=82.87 +/- 57.99
Episode length: 393.20 +/- 25.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 82.9          |
| time/                   |               |
|    total_timesteps      | 2082000       |
| train/                  |               |
|    approx_kl            | 3.0336756e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.73         |
|    explained_variance   | 0.861         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 10160         |
|    policy_gradient_loss | -4.08e-05     |
|    std                  | 2.92          |
|    value_loss           | 4.08e+03      |
-------------------------------------------
Eval num_timesteps=2084000, episode_reward=329.37 +/- 281.53
Episode length: 447.20 +/- 46.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 2084000      |
| train/                  |              |
|    approx_kl            | 5.210933e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 499          |
|    n_updates            | 10170        |
|    policy_gradient_loss | -9.76e-05    |
|    std                  | 2.92         |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=2086000, episode_reward=499.16 +/- 184.27
Episode length: 463.00 +/- 39.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 463         |
|    mean_reward          | 499         |
| time/                   |             |
|    total_timesteps      | 2086000     |
| train/                  |             |
|    approx_kl            | 0.007170345 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.74       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 704         |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.00145    |
|    std                  | 2.93        |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=2088000, episode_reward=219.14 +/- 480.37
Episode length: 479.20 +/- 43.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 2088000      |
| train/                  |              |
|    approx_kl            | 0.0036868677 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.76        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 67.1         |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.000353    |
|    std                  | 2.95         |
|    value_loss           | 398          |
------------------------------------------
Eval num_timesteps=2090000, episode_reward=391.94 +/- 493.39
Episode length: 539.80 +/- 117.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | 392          |
| time/                   |              |
|    total_timesteps      | 2090000      |
| train/                  |              |
|    approx_kl            | 0.0037546004 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.79        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 34.1         |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 2.97         |
|    value_loss           | 140          |
------------------------------------------
Eval num_timesteps=2092000, episode_reward=682.37 +/- 374.30
Episode length: 542.80 +/- 87.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 543          |
|    mean_reward          | 682          |
| time/                   |              |
|    total_timesteps      | 2092000      |
| train/                  |              |
|    approx_kl            | 0.0021465863 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.81        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 49.6         |
|    n_updates            | 10210        |
|    policy_gradient_loss | -0.000665    |
|    std                  | 2.98         |
|    value_loss           | 378          |
------------------------------------------
Eval num_timesteps=2094000, episode_reward=250.84 +/- 350.47
Episode length: 536.60 +/- 45.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 537        |
|    mean_reward          | 251        |
| time/                   |            |
|    total_timesteps      | 2094000    |
| train/                  |            |
|    approx_kl            | 0.00202824 |
|    clip_fraction        | 0.00205    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.82      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.001      |
|    loss                 | 79.8       |
|    n_updates            | 10220      |
|    policy_gradient_loss | 0.000314   |
|    std                  | 2.98       |
|    value_loss           | 459        |
----------------------------------------
Eval num_timesteps=2096000, episode_reward=19.99 +/- 375.27
Episode length: 506.40 +/- 64.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 20           |
| time/                   |              |
|    total_timesteps      | 2096000      |
| train/                  |              |
|    approx_kl            | 0.0003882225 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 811          |
|    n_updates            | 10230        |
|    policy_gradient_loss | 0.00114      |
|    std                  | 2.99         |
|    value_loss           | 3.45e+03     |
------------------------------------------
Eval num_timesteps=2098000, episode_reward=143.54 +/- 466.70
Episode length: 565.20 +/- 44.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 565          |
|    mean_reward          | 144          |
| time/                   |              |
|    total_timesteps      | 2098000      |
| train/                  |              |
|    approx_kl            | 0.0014560259 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 140          |
|    n_updates            | 10240        |
|    policy_gradient_loss | -0.000646    |
|    std                  | 2.99         |
|    value_loss           | 904          |
------------------------------------------
Eval num_timesteps=2100000, episode_reward=393.19 +/- 392.55
Episode length: 587.80 +/- 60.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 588          |
|    mean_reward          | 393          |
| time/                   |              |
|    total_timesteps      | 2100000      |
| train/                  |              |
|    approx_kl            | 0.0033741714 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | 157          |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 2.99         |
|    value_loss           | 801          |
------------------------------------------
Eval num_timesteps=2102000, episode_reward=252.39 +/- 198.79
Episode length: 613.80 +/- 50.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 614         |
|    mean_reward          | 252         |
| time/                   |             |
|    total_timesteps      | 2102000     |
| train/                  |             |
|    approx_kl            | 0.004043207 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 10260       |
|    policy_gradient_loss | -0.00178    |
|    std                  | 3           |
|    value_loss           | 458         |
-----------------------------------------
Eval num_timesteps=2104000, episode_reward=148.69 +/- 170.54
Episode length: 592.80 +/- 83.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 593          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 2104000      |
| train/                  |              |
|    approx_kl            | 0.0015210262 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.84        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 192          |
|    n_updates            | 10270        |
|    policy_gradient_loss | -0.000297    |
|    std                  | 3            |
|    value_loss           | 836          |
------------------------------------------
Eval num_timesteps=2106000, episode_reward=253.12 +/- 216.05
Episode length: 595.00 +/- 78.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 595           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 2106000       |
| train/                  |               |
|    approx_kl            | 0.00025094266 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.84         |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.001         |
|    loss                 | 119           |
|    n_updates            | 10280         |
|    policy_gradient_loss | -0.000109     |
|    std                  | 3             |
|    value_loss           | 492           |
-------------------------------------------
Eval num_timesteps=2108000, episode_reward=150.22 +/- 465.74
Episode length: 581.20 +/- 54.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 581        |
|    mean_reward          | 150        |
| time/                   |            |
|    total_timesteps      | 2108000    |
| train/                  |            |
|    approx_kl            | 0.00806042 |
|    clip_fraction        | 0.0258     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.83      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.001      |
|    loss                 | 33.4       |
|    n_updates            | 10290      |
|    policy_gradient_loss | -0.00478   |
|    std                  | 2.99       |
|    value_loss           | 168        |
----------------------------------------
Eval num_timesteps=2110000, episode_reward=456.00 +/- 129.96
Episode length: 587.60 +/- 18.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 588         |
|    mean_reward          | 456         |
| time/                   |             |
|    total_timesteps      | 2110000     |
| train/                  |             |
|    approx_kl            | 0.006787301 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.81       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 93.7        |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 2.97        |
|    value_loss           | 589         |
-----------------------------------------
Eval num_timesteps=2112000, episode_reward=355.62 +/- 241.99
Episode length: 532.40 +/- 83.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 356         |
| time/                   |             |
|    total_timesteps      | 2112000     |
| train/                  |             |
|    approx_kl            | 0.001492283 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.8        |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.001       |
|    loss                 | 2.18e+03    |
|    n_updates            | 10310       |
|    policy_gradient_loss | 0.000156    |
|    std                  | 2.96        |
|    value_loss           | 5.75e+03    |
-----------------------------------------
Eval num_timesteps=2114000, episode_reward=534.11 +/- 368.47
Episode length: 483.60 +/- 74.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 534          |
| time/                   |              |
|    total_timesteps      | 2114000      |
| train/                  |              |
|    approx_kl            | 0.0036601105 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 75.2         |
|    n_updates            | 10320        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 2.97         |
|    value_loss           | 418          |
------------------------------------------
Eval num_timesteps=2116000, episode_reward=291.28 +/- 592.97
Episode length: 572.20 +/- 112.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 572          |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 2116000      |
| train/                  |              |
|    approx_kl            | 0.0011536127 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.715        |
|    learning_rate        | 0.001        |
|    loss                 | 531          |
|    n_updates            | 10330        |
|    policy_gradient_loss | -4.54e-05    |
|    std                  | 2.97         |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=2118000, episode_reward=343.41 +/- 217.82
Episode length: 532.80 +/- 45.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 533          |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 2118000      |
| train/                  |              |
|    approx_kl            | 0.0072576012 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.001        |
|    loss                 | 398          |
|    n_updates            | 10340        |
|    policy_gradient_loss | -0.00287     |
|    std                  | 2.97         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=2120000, episode_reward=689.89 +/- 1243.01
Episode length: 546.80 +/- 85.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 547          |
|    mean_reward          | 690          |
| time/                   |              |
|    total_timesteps      | 2120000      |
| train/                  |              |
|    approx_kl            | 0.0028318404 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 236          |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 2.97         |
|    value_loss           | 791          |
------------------------------------------
Eval num_timesteps=2122000, episode_reward=419.54 +/- 306.41
Episode length: 541.00 +/- 58.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 541          |
|    mean_reward          | 420          |
| time/                   |              |
|    total_timesteps      | 2122000      |
| train/                  |              |
|    approx_kl            | 0.0054038833 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 247          |
|    n_updates            | 10360        |
|    policy_gradient_loss | -0.00202     |
|    std                  | 2.98         |
|    value_loss           | 779          |
------------------------------------------
Eval num_timesteps=2124000, episode_reward=196.12 +/- 209.08
Episode length: 572.00 +/- 110.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 572         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 2124000     |
| train/                  |             |
|    approx_kl            | 0.003757777 |
|    clip_fraction        | 0.00405     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.82       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.001       |
|    loss                 | 296         |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.0021     |
|    std                  | 2.99        |
|    value_loss           | 956         |
-----------------------------------------
Eval num_timesteps=2126000, episode_reward=33.38 +/- 105.12
Episode length: 510.80 +/- 140.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 511        |
|    mean_reward          | 33.4       |
| time/                   |            |
|    total_timesteps      | 2126000    |
| train/                  |            |
|    approx_kl            | 0.00644816 |
|    clip_fraction        | 0.0228     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.84      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.001      |
|    loss                 | 126        |
|    n_updates            | 10380      |
|    policy_gradient_loss | -0.00199   |
|    std                  | 3          |
|    value_loss           | 414        |
----------------------------------------
Eval num_timesteps=2128000, episode_reward=-32.14 +/- 49.91
Episode length: 410.40 +/- 133.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | -32.1       |
| time/                   |             |
|    total_timesteps      | 2128000     |
| train/                  |             |
|    approx_kl            | 0.002080257 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.84       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 210         |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00122    |
|    std                  | 3           |
|    value_loss           | 759         |
-----------------------------------------
Eval num_timesteps=2130000, episode_reward=-52.67 +/- 51.24
Episode length: 418.00 +/- 84.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | -52.7        |
| time/                   |              |
|    total_timesteps      | 2130000      |
| train/                  |              |
|    approx_kl            | 0.0038822184 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.84        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 81.6         |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.000668    |
|    std                  | 3            |
|    value_loss           | 315          |
------------------------------------------
Eval num_timesteps=2132000, episode_reward=-80.21 +/- 15.46
Episode length: 405.60 +/- 42.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | -80.2        |
| time/                   |              |
|    total_timesteps      | 2132000      |
| train/                  |              |
|    approx_kl            | 0.0057121376 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 68.1         |
|    n_updates            | 10410        |
|    policy_gradient_loss | -0.000851    |
|    std                  | 3            |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=2134000, episode_reward=-69.43 +/- 38.82
Episode length: 380.20 +/- 55.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | -69.4    |
| time/              |          |
|    total_timesteps | 2134000  |
---------------------------------
Eval num_timesteps=2136000, episode_reward=-14.77 +/- 96.85
Episode length: 461.80 +/- 121.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | -14.8        |
| time/                   |              |
|    total_timesteps      | 2136000      |
| train/                  |              |
|    approx_kl            | 0.0028887768 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 111          |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.000706    |
|    std                  | 3            |
|    value_loss           | 311          |
------------------------------------------
Eval num_timesteps=2138000, episode_reward=-59.03 +/- 27.80
Episode length: 525.80 +/- 77.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 526         |
|    mean_reward          | -59         |
| time/                   |             |
|    total_timesteps      | 2138000     |
| train/                  |             |
|    approx_kl            | 0.004573977 |
|    clip_fraction        | 0.00869     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 97.9        |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.00171    |
|    std                  | 3           |
|    value_loss           | 294         |
-----------------------------------------
Eval num_timesteps=2140000, episode_reward=-72.02 +/- 42.15
Episode length: 396.20 +/- 12.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | -72         |
| time/                   |             |
|    total_timesteps      | 2140000     |
| train/                  |             |
|    approx_kl            | 0.006029686 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 67          |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.000889   |
|    std                  | 3           |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=2142000, episode_reward=-88.51 +/- 35.28
Episode length: 488.20 +/- 38.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | -88.5       |
| time/                   |             |
|    total_timesteps      | 2142000     |
| train/                  |             |
|    approx_kl            | 0.005707442 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.85       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 42.8        |
|    n_updates            | 10450       |
|    policy_gradient_loss | -0.00157    |
|    std                  | 3.01        |
|    value_loss           | 165         |
-----------------------------------------
Eval num_timesteps=2144000, episode_reward=-17.79 +/- 43.89
Episode length: 586.60 +/- 90.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 587          |
|    mean_reward          | -17.8        |
| time/                   |              |
|    total_timesteps      | 2144000      |
| train/                  |              |
|    approx_kl            | 0.0108654145 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.86        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 120          |
|    n_updates            | 10460        |
|    policy_gradient_loss | -0.00262     |
|    std                  | 3.02         |
|    value_loss           | 363          |
------------------------------------------
Eval num_timesteps=2146000, episode_reward=-24.97 +/- 85.59
Episode length: 512.80 +/- 73.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 513          |
|    mean_reward          | -25          |
| time/                   |              |
|    total_timesteps      | 2146000      |
| train/                  |              |
|    approx_kl            | 0.0019690418 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.87        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.00119     |
|    std                  | 3.02         |
|    value_loss           | 801          |
------------------------------------------
Eval num_timesteps=2148000, episode_reward=-24.09 +/- 50.75
Episode length: 525.00 +/- 64.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -24.1       |
| time/                   |             |
|    total_timesteps      | 2148000     |
| train/                  |             |
|    approx_kl            | 0.008897485 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.87       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.001       |
|    loss                 | 131         |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.00265    |
|    std                  | 3.02        |
|    value_loss           | 425         |
-----------------------------------------
Eval num_timesteps=2150000, episode_reward=104.14 +/- 140.51
Episode length: 616.40 +/- 123.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 616         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 2150000     |
| train/                  |             |
|    approx_kl            | 0.002602229 |
|    clip_fraction        | 0.00249     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.00162    |
|    std                  | 3.02        |
|    value_loss           | 430         |
-----------------------------------------
Eval num_timesteps=2152000, episode_reward=25.08 +/- 86.64
Episode length: 512.60 +/- 38.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | 25.1        |
| time/                   |             |
|    total_timesteps      | 2152000     |
| train/                  |             |
|    approx_kl            | 0.007592293 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 165         |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.00335    |
|    std                  | 3.02        |
|    value_loss           | 479         |
-----------------------------------------
Eval num_timesteps=2154000, episode_reward=93.10 +/- 176.54
Episode length: 588.80 +/- 94.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 589         |
|    mean_reward          | 93.1        |
| time/                   |             |
|    total_timesteps      | 2154000     |
| train/                  |             |
|    approx_kl            | 0.007946982 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.89       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 50.7        |
|    n_updates            | 10510       |
|    policy_gradient_loss | -0.00235    |
|    std                  | 3.02        |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=2156000, episode_reward=3.69 +/- 22.41
Episode length: 570.40 +/- 50.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 570         |
|    mean_reward          | 3.69        |
| time/                   |             |
|    total_timesteps      | 2156000     |
| train/                  |             |
|    approx_kl            | 0.010963871 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | 78          |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.00435    |
|    std                  | 3.02        |
|    value_loss           | 299         |
-----------------------------------------
Eval num_timesteps=2158000, episode_reward=-28.81 +/- 46.22
Episode length: 543.60 +/- 115.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | -28.8        |
| time/                   |              |
|    total_timesteps      | 2158000      |
| train/                  |              |
|    approx_kl            | 0.0026894147 |
|    clip_fraction        | 0.0493       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.001        |
|    loss                 | 82.2         |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.0027      |
|    std                  | 3.03         |
|    value_loss           | 306          |
------------------------------------------
Eval num_timesteps=2160000, episode_reward=-11.96 +/- 73.08
Episode length: 506.00 +/- 84.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | -12         |
| time/                   |             |
|    total_timesteps      | 2160000     |
| train/                  |             |
|    approx_kl            | 0.002587771 |
|    clip_fraction        | 0.00459     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.9        |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 84.4        |
|    n_updates            | 10540       |
|    policy_gradient_loss | -0.00198    |
|    std                  | 3.04        |
|    value_loss           | 442         |
-----------------------------------------
Eval num_timesteps=2162000, episode_reward=-21.55 +/- 14.37
Episode length: 528.00 +/- 41.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 528          |
|    mean_reward          | -21.5        |
| time/                   |              |
|    total_timesteps      | 2162000      |
| train/                  |              |
|    approx_kl            | 0.0065232934 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.9         |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 104          |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.00385     |
|    std                  | 3.04         |
|    value_loss           | 314          |
------------------------------------------
Eval num_timesteps=2164000, episode_reward=-52.46 +/- 61.78
Episode length: 478.00 +/- 69.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 478        |
|    mean_reward          | -52.5      |
| time/                   |            |
|    total_timesteps      | 2164000    |
| train/                  |            |
|    approx_kl            | 0.00478272 |
|    clip_fraction        | 0.00801    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.9       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.001      |
|    loss                 | 99.2       |
|    n_updates            | 10560      |
|    policy_gradient_loss | -0.00119   |
|    std                  | 3.04       |
|    value_loss           | 358        |
----------------------------------------
Eval num_timesteps=2166000, episode_reward=-56.08 +/- 40.52
Episode length: 466.60 +/- 19.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | -56.1        |
| time/                   |              |
|    total_timesteps      | 2166000      |
| train/                  |              |
|    approx_kl            | 0.0013176982 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.9         |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 94.3         |
|    n_updates            | 10570        |
|    policy_gradient_loss | -0.00078     |
|    std                  | 3.03         |
|    value_loss           | 298          |
------------------------------------------
Eval num_timesteps=2168000, episode_reward=-63.14 +/- 68.96
Episode length: 490.00 +/- 93.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 490         |
|    mean_reward          | -63.1       |
| time/                   |             |
|    total_timesteps      | 2168000     |
| train/                  |             |
|    approx_kl            | 0.000603102 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.89       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.001       |
|    loss                 | 76          |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.00052    |
|    std                  | 3.02        |
|    value_loss           | 364         |
-----------------------------------------
Eval num_timesteps=2170000, episode_reward=-56.69 +/- 46.97
Episode length: 475.60 +/- 61.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | -56.7        |
| time/                   |              |
|    total_timesteps      | 2170000      |
| train/                  |              |
|    approx_kl            | 0.0047452725 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 75.3         |
|    n_updates            | 10590        |
|    policy_gradient_loss | -0.00202     |
|    std                  | 3.02         |
|    value_loss           | 336          |
------------------------------------------
Eval num_timesteps=2172000, episode_reward=-76.07 +/- 42.36
Episode length: 455.00 +/- 51.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 455         |
|    mean_reward          | -76.1       |
| time/                   |             |
|    total_timesteps      | 2172000     |
| train/                  |             |
|    approx_kl            | 0.005036112 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.89       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 42.7        |
|    n_updates            | 10600       |
|    policy_gradient_loss | -0.00309    |
|    std                  | 3.03        |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=2174000, episode_reward=-10.70 +/- 62.21
Episode length: 538.60 +/- 91.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 539          |
|    mean_reward          | -10.7        |
| time/                   |              |
|    total_timesteps      | 2174000      |
| train/                  |              |
|    approx_kl            | 0.0055732983 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 62.4         |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.00229     |
|    std                  | 3.04         |
|    value_loss           | 226          |
------------------------------------------
Eval num_timesteps=2176000, episode_reward=-49.60 +/- 80.86
Episode length: 482.00 +/- 69.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | -49.6        |
| time/                   |              |
|    total_timesteps      | 2176000      |
| train/                  |              |
|    approx_kl            | 0.0077031776 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 80.6         |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.000342    |
|    std                  | 3.04         |
|    value_loss           | 329          |
------------------------------------------
Eval num_timesteps=2178000, episode_reward=244.42 +/- 370.66
Episode length: 556.60 +/- 27.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 557          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 2178000      |
| train/                  |              |
|    approx_kl            | 0.0077130613 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 60.4         |
|    n_updates            | 10630        |
|    policy_gradient_loss | -0.00311     |
|    std                  | 3.05         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=2180000, episode_reward=221.23 +/- 303.42
Episode length: 554.40 +/- 16.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 554         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 2180000     |
| train/                  |             |
|    approx_kl            | 0.004397218 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.94       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 41.6        |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.00097    |
|    std                  | 3.06        |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=2182000, episode_reward=-22.75 +/- 39.92
Episode length: 489.00 +/- 38.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 489         |
|    mean_reward          | -22.7       |
| time/                   |             |
|    total_timesteps      | 2182000     |
| train/                  |             |
|    approx_kl            | 0.010544945 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.95       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.001       |
|    loss                 | 169         |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.0023     |
|    std                  | 3.06        |
|    value_loss           | 622         |
-----------------------------------------
Eval num_timesteps=2184000, episode_reward=153.81 +/- 225.77
Episode length: 550.00 +/- 55.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 550         |
|    mean_reward          | 154         |
| time/                   |             |
|    total_timesteps      | 2184000     |
| train/                  |             |
|    approx_kl            | 0.010391694 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.95       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.00307    |
|    std                  | 3.06        |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=2186000, episode_reward=136.10 +/- 150.44
Episode length: 560.40 +/- 106.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 560          |
|    mean_reward          | 136          |
| time/                   |              |
|    total_timesteps      | 2186000      |
| train/                  |              |
|    approx_kl            | 0.0058820006 |
|    clip_fraction        | 0.0629       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 60.6         |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.00236     |
|    std                  | 3.05         |
|    value_loss           | 242          |
------------------------------------------
Eval num_timesteps=2188000, episode_reward=-190.35 +/- 457.67
Episode length: 513.00 +/- 57.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 2188000     |
| train/                  |             |
|    approx_kl            | 0.007574495 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.94       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.001       |
|    loss                 | 181         |
|    n_updates            | 10680       |
|    policy_gradient_loss | 0.000213    |
|    std                  | 3.04        |
|    value_loss           | 678         |
-----------------------------------------
Eval num_timesteps=2190000, episode_reward=69.14 +/- 243.72
Episode length: 541.00 +/- 131.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 541          |
|    mean_reward          | 69.1         |
| time/                   |              |
|    total_timesteps      | 2190000      |
| train/                  |              |
|    approx_kl            | 0.0040020477 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.94        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 167          |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.000828    |
|    std                  | 3.04         |
|    value_loss           | 514          |
------------------------------------------
Eval num_timesteps=2192000, episode_reward=66.13 +/- 835.90
Episode length: 535.60 +/- 59.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 536          |
|    mean_reward          | 66.1         |
| time/                   |              |
|    total_timesteps      | 2192000      |
| train/                  |              |
|    approx_kl            | 0.0045269616 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.93        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 235          |
|    n_updates            | 10700        |
|    policy_gradient_loss | -0.000388    |
|    std                  | 3.03         |
|    value_loss           | 745          |
------------------------------------------
Eval num_timesteps=2194000, episode_reward=775.72 +/- 321.38
Episode length: 571.60 +/- 47.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 572          |
|    mean_reward          | 776          |
| time/                   |              |
|    total_timesteps      | 2194000      |
| train/                  |              |
|    approx_kl            | 0.0029394366 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.93        |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.001        |
|    loss                 | 2.22e+03     |
|    n_updates            | 10710        |
|    policy_gradient_loss | 4.65e-05     |
|    std                  | 3.03         |
|    value_loss           | 6.59e+03     |
------------------------------------------
Eval num_timesteps=2196000, episode_reward=153.44 +/- 286.55
Episode length: 558.40 +/- 58.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 558          |
|    mean_reward          | 153          |
| time/                   |              |
|    total_timesteps      | 2196000      |
| train/                  |              |
|    approx_kl            | 0.0043814564 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.93        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 166          |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.0015      |
|    std                  | 3.04         |
|    value_loss           | 695          |
------------------------------------------
Eval num_timesteps=2198000, episode_reward=401.94 +/- 262.18
Episode length: 536.60 +/- 57.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 402          |
| time/                   |              |
|    total_timesteps      | 2198000      |
| train/                  |              |
|    approx_kl            | 0.0023644227 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.94        |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+03      |
|    n_updates            | 10730        |
|    policy_gradient_loss | -0.00141     |
|    std                  | 3.04         |
|    value_loss           | 4.61e+03     |
------------------------------------------
Eval num_timesteps=2200000, episode_reward=226.95 +/- 193.83
Episode length: 555.60 +/- 20.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 556         |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 2200000     |
| train/                  |             |
|    approx_kl            | 0.003131052 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.94       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.001       |
|    loss                 | 1.93e+03    |
|    n_updates            | 10740       |
|    policy_gradient_loss | -0.00172    |
|    std                  | 3.04        |
|    value_loss           | 4.88e+03    |
-----------------------------------------
Eval num_timesteps=2202000, episode_reward=30.74 +/- 48.98
Episode length: 540.00 +/- 64.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | 30.7         |
| time/                   |              |
|    total_timesteps      | 2202000      |
| train/                  |              |
|    approx_kl            | 0.0018768477 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.94        |
|    explained_variance   | 0.789        |
|    learning_rate        | 0.001        |
|    loss                 | 327          |
|    n_updates            | 10750        |
|    policy_gradient_loss | -0.000855    |
|    std                  | 3.05         |
|    value_loss           | 923          |
------------------------------------------
Eval num_timesteps=2204000, episode_reward=128.31 +/- 171.40
Episode length: 520.20 +/- 49.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 2204000      |
| train/                  |              |
|    approx_kl            | 0.0031665359 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.94        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 87.2         |
|    n_updates            | 10760        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 3.05         |
|    value_loss           | 357          |
------------------------------------------
Eval num_timesteps=2206000, episode_reward=49.26 +/- 205.41
Episode length: 510.60 +/- 111.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 49.3        |
| time/                   |             |
|    total_timesteps      | 2206000     |
| train/                  |             |
|    approx_kl            | 0.005141696 |
|    clip_fraction        | 0.00991     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.95       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 42.9        |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.00147    |
|    std                  | 3.05        |
|    value_loss           | 159         |
-----------------------------------------
Eval num_timesteps=2208000, episode_reward=20.91 +/- 85.41
Episode length: 541.80 +/- 52.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | 20.9         |
| time/                   |              |
|    total_timesteps      | 2208000      |
| train/                  |              |
|    approx_kl            | 0.0042829206 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 46.5         |
|    n_updates            | 10780        |
|    policy_gradient_loss | -0.000942    |
|    std                  | 3.05         |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=2210000, episode_reward=103.72 +/- 227.27
Episode length: 528.80 +/- 58.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 2210000      |
| train/                  |              |
|    approx_kl            | 0.0027796107 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 55.5         |
|    n_updates            | 10790        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 3.06         |
|    value_loss           | 214          |
------------------------------------------
Eval num_timesteps=2212000, episode_reward=-9.41 +/- 119.42
Episode length: 563.60 +/- 73.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 564         |
|    mean_reward          | -9.41       |
| time/                   |             |
|    total_timesteps      | 2212000     |
| train/                  |             |
|    approx_kl            | 0.005383512 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.96       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 76.9        |
|    n_updates            | 10800       |
|    policy_gradient_loss | -0.00217    |
|    std                  | 3.08        |
|    value_loss           | 309         |
-----------------------------------------
Eval num_timesteps=2214000, episode_reward=74.70 +/- 125.91
Episode length: 542.80 +/- 62.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 543         |
|    mean_reward          | 74.7        |
| time/                   |             |
|    total_timesteps      | 2214000     |
| train/                  |             |
|    approx_kl            | 0.006283197 |
|    clip_fraction        | 0.0343      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.99       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 56.5        |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.00205    |
|    std                  | 3.1         |
|    value_loss           | 227         |
-----------------------------------------
Eval num_timesteps=2216000, episode_reward=-31.94 +/- 81.08
Episode length: 515.20 +/- 88.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -31.9       |
| time/                   |             |
|    total_timesteps      | 2216000     |
| train/                  |             |
|    approx_kl            | 0.004742144 |
|    clip_fraction        | 0.00581     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 27.4        |
|    n_updates            | 10820       |
|    policy_gradient_loss | -0.00046    |
|    std                  | 3.11        |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=2218000, episode_reward=-69.94 +/- 37.57
Episode length: 427.40 +/- 36.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | -69.9       |
| time/                   |             |
|    total_timesteps      | 2218000     |
| train/                  |             |
|    approx_kl            | 0.008206533 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10         |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 42.6        |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.00365    |
|    std                  | 3.12        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=2220000, episode_reward=-58.21 +/- 34.26
Episode length: 454.60 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 455      |
|    mean_reward     | -58.2    |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2222000, episode_reward=-23.01 +/- 55.20
Episode length: 524.80 +/- 85.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -23         |
| time/                   |             |
|    total_timesteps      | 2222000     |
| train/                  |             |
|    approx_kl            | 0.006873087 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10         |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 66.8        |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.00139    |
|    std                  | 3.14        |
|    value_loss           | 309         |
-----------------------------------------
Eval num_timesteps=2224000, episode_reward=-72.99 +/- 15.16
Episode length: 434.40 +/- 24.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | -73          |
| time/                   |              |
|    total_timesteps      | 2224000      |
| train/                  |              |
|    approx_kl            | 0.0061609647 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 33.2         |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 3.16         |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=2226000, episode_reward=-20.88 +/- 27.67
Episode length: 525.80 +/- 32.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 526         |
|    mean_reward          | -20.9       |
| time/                   |             |
|    total_timesteps      | 2226000     |
| train/                  |             |
|    approx_kl            | 0.013568452 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 49.9        |
|    n_updates            | 10860       |
|    policy_gradient_loss | -0.00219    |
|    std                  | 3.17        |
|    value_loss           | 219         |
-----------------------------------------
Eval num_timesteps=2228000, episode_reward=184.62 +/- 501.09
Episode length: 494.80 +/- 57.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 2228000      |
| train/                  |              |
|    approx_kl            | 0.0053914706 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 77.1         |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.0018      |
|    std                  | 3.18         |
|    value_loss           | 257          |
------------------------------------------
Eval num_timesteps=2230000, episode_reward=-5.60 +/- 65.92
Episode length: 494.80 +/- 69.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -5.6        |
| time/                   |             |
|    total_timesteps      | 2230000     |
| train/                  |             |
|    approx_kl            | 0.014925398 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 77.1        |
|    n_updates            | 10880       |
|    policy_gradient_loss | -0.00362    |
|    std                  | 3.19        |
|    value_loss           | 303         |
-----------------------------------------
Eval num_timesteps=2232000, episode_reward=98.84 +/- 206.24
Episode length: 524.60 +/- 85.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 98.8         |
| time/                   |              |
|    total_timesteps      | 2232000      |
| train/                  |              |
|    approx_kl            | 0.0058364696 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 61.7         |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00457     |
|    std                  | 3.22         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=2234000, episode_reward=127.46 +/- 129.33
Episode length: 630.00 +/- 139.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 630          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 2234000      |
| train/                  |              |
|    approx_kl            | 0.0019011945 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 47.9         |
|    n_updates            | 10900        |
|    policy_gradient_loss | 0.000508     |
|    std                  | 3.24         |
|    value_loss           | 161          |
------------------------------------------
Eval num_timesteps=2236000, episode_reward=21.67 +/- 76.24
Episode length: 501.40 +/- 34.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 501          |
|    mean_reward          | 21.7         |
| time/                   |              |
|    total_timesteps      | 2236000      |
| train/                  |              |
|    approx_kl            | 0.0030716732 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 24.8         |
|    n_updates            | 10910        |
|    policy_gradient_loss | -0.000535    |
|    std                  | 3.26         |
|    value_loss           | 91.1         |
------------------------------------------
Eval num_timesteps=2238000, episode_reward=73.73 +/- 133.87
Episode length: 547.00 +/- 63.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 547          |
|    mean_reward          | 73.7         |
| time/                   |              |
|    total_timesteps      | 2238000      |
| train/                  |              |
|    approx_kl            | 0.0011982836 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 35.7         |
|    n_updates            | 10920        |
|    policy_gradient_loss | 7.55e-05     |
|    std                  | 3.28         |
|    value_loss           | 137          |
------------------------------------------
Eval num_timesteps=2240000, episode_reward=161.22 +/- 75.92
Episode length: 566.20 +/- 24.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 566         |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 2240000     |
| train/                  |             |
|    approx_kl            | 0.008898446 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 24.7        |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.00289    |
|    std                  | 3.29        |
|    value_loss           | 96.3        |
-----------------------------------------
Eval num_timesteps=2242000, episode_reward=487.01 +/- 444.46
Episode length: 522.40 +/- 58.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 522         |
|    mean_reward          | 487         |
| time/                   |             |
|    total_timesteps      | 2242000     |
| train/                  |             |
|    approx_kl            | 0.017527912 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+03    |
|    n_updates            | 10940       |
|    policy_gradient_loss | 0.00258     |
|    std                  | 3.3         |
|    value_loss           | 3.15e+03    |
-----------------------------------------
Eval num_timesteps=2244000, episode_reward=391.48 +/- 261.81
Episode length: 613.00 +/- 63.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 613          |
|    mean_reward          | 391          |
| time/                   |              |
|    total_timesteps      | 2244000      |
| train/                  |              |
|    approx_kl            | 0.0006986846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 111          |
|    n_updates            | 10950        |
|    policy_gradient_loss | -0.000777    |
|    std                  | 3.31         |
|    value_loss           | 417          |
------------------------------------------
Eval num_timesteps=2246000, episode_reward=508.24 +/- 358.41
Episode length: 548.60 +/- 26.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 549          |
|    mean_reward          | 508          |
| time/                   |              |
|    total_timesteps      | 2246000      |
| train/                  |              |
|    approx_kl            | 0.0006772049 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 1.86e+03     |
|    n_updates            | 10960        |
|    policy_gradient_loss | -0.000128    |
|    std                  | 3.32         |
|    value_loss           | 6.16e+03     |
------------------------------------------
Eval num_timesteps=2248000, episode_reward=160.85 +/- 60.16
Episode length: 583.20 +/- 46.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 583          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 2248000      |
| train/                  |              |
|    approx_kl            | 0.0032747078 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 10970        |
|    policy_gradient_loss | -0.00227     |
|    std                  | 3.32         |
|    value_loss           | 579          |
------------------------------------------
Eval num_timesteps=2250000, episode_reward=189.58 +/- 161.58
Episode length: 515.60 +/- 44.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0019954545 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 211          |
|    n_updates            | 10980        |
|    policy_gradient_loss | -0.000641    |
|    std                  | 3.32         |
|    value_loss           | 648          |
------------------------------------------
Eval num_timesteps=2252000, episode_reward=98.31 +/- 119.52
Episode length: 472.20 +/- 57.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 98.3         |
| time/                   |              |
|    total_timesteps      | 2252000      |
| train/                  |              |
|    approx_kl            | 0.0008298597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.000417    |
|    std                  | 3.32         |
|    value_loss           | 457          |
------------------------------------------
Eval num_timesteps=2254000, episode_reward=399.80 +/- 201.04
Episode length: 542.60 +/- 10.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 543          |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 2254000      |
| train/                  |              |
|    approx_kl            | 0.0016381626 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 66           |
|    n_updates            | 11000        |
|    policy_gradient_loss | -0.000429    |
|    std                  | 3.31         |
|    value_loss           | 263          |
------------------------------------------
Eval num_timesteps=2256000, episode_reward=553.56 +/- 459.07
Episode length: 467.00 +/- 40.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | 554         |
| time/                   |             |
|    total_timesteps      | 2256000     |
| train/                  |             |
|    approx_kl            | 0.004291719 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 74.9        |
|    n_updates            | 11010       |
|    policy_gradient_loss | -0.000795   |
|    std                  | 3.31        |
|    value_loss           | 290         |
-----------------------------------------
Eval num_timesteps=2258000, episode_reward=248.70 +/- 241.00
Episode length: 432.20 +/- 47.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 2258000     |
| train/                  |             |
|    approx_kl            | 0.003469033 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 11020       |
|    policy_gradient_loss | 4.74e-05    |
|    std                  | 3.31        |
|    value_loss           | 3.45e+03    |
-----------------------------------------
Eval num_timesteps=2260000, episode_reward=464.72 +/- 191.10
Episode length: 454.60 +/- 40.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 2260000      |
| train/                  |              |
|    approx_kl            | 0.0006871545 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 136          |
|    n_updates            | 11030        |
|    policy_gradient_loss | -0.000566    |
|    std                  | 3.31         |
|    value_loss           | 536          |
------------------------------------------
Eval num_timesteps=2262000, episode_reward=427.34 +/- 267.73
Episode length: 484.00 +/- 39.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 427          |
| time/                   |              |
|    total_timesteps      | 2262000      |
| train/                  |              |
|    approx_kl            | 0.0056182835 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 32.6         |
|    n_updates            | 11040        |
|    policy_gradient_loss | -0.00201     |
|    std                  | 3.32         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=2264000, episode_reward=483.52 +/- 497.96
Episode length: 599.00 +/- 76.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 599         |
|    mean_reward          | 484         |
| time/                   |             |
|    total_timesteps      | 2264000     |
| train/                  |             |
|    approx_kl            | 0.004097227 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.001       |
|    loss                 | 3.58e+03    |
|    n_updates            | 11050       |
|    policy_gradient_loss | -0.000238   |
|    std                  | 3.32        |
|    value_loss           | 9.37e+03    |
-----------------------------------------
Eval num_timesteps=2266000, episode_reward=499.62 +/- 371.27
Episode length: 558.60 +/- 83.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 559          |
|    mean_reward          | 500          |
| time/                   |              |
|    total_timesteps      | 2266000      |
| train/                  |              |
|    approx_kl            | 0.0004252887 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 11060        |
|    policy_gradient_loss | 0.000243     |
|    std                  | 3.33         |
|    value_loss           | 3.23e+03     |
------------------------------------------
Eval num_timesteps=2268000, episode_reward=364.92 +/- 400.43
Episode length: 509.60 +/- 81.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 510           |
|    mean_reward          | 365           |
| time/                   |               |
|    total_timesteps      | 2268000       |
| train/                  |               |
|    approx_kl            | 0.00074295927 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.001         |
|    loss                 | 128           |
|    n_updates            | 11070         |
|    policy_gradient_loss | -0.000734     |
|    std                  | 3.33          |
|    value_loss           | 752           |
-------------------------------------------
Eval num_timesteps=2270000, episode_reward=522.26 +/- 570.29
Episode length: 516.60 +/- 93.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 522          |
| time/                   |              |
|    total_timesteps      | 2270000      |
| train/                  |              |
|    approx_kl            | 0.0017192902 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 3.37e+03     |
|    n_updates            | 11080        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 3.34         |
|    value_loss           | 8.04e+03     |
------------------------------------------
Eval num_timesteps=2272000, episode_reward=470.21 +/- 519.34
Episode length: 597.00 +/- 63.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 597           |
|    mean_reward          | 470           |
| time/                   |               |
|    total_timesteps      | 2272000       |
| train/                  |               |
|    approx_kl            | 0.00037789217 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+03      |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000144     |
|    std                  | 3.34          |
|    value_loss           | 4.56e+03      |
-------------------------------------------
Eval num_timesteps=2274000, episode_reward=430.42 +/- 260.32
Episode length: 486.40 +/- 73.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 2274000       |
| train/                  |               |
|    approx_kl            | 0.00035251642 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 11100         |
|    policy_gradient_loss | -0.000611     |
|    std                  | 3.35          |
|    value_loss           | 3.84e+03      |
-------------------------------------------
Eval num_timesteps=2276000, episode_reward=873.64 +/- 506.64
Episode length: 553.40 +/- 98.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 874          |
| time/                   |              |
|    total_timesteps      | 2276000      |
| train/                  |              |
|    approx_kl            | 0.0018257734 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 91.8         |
|    n_updates            | 11110        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 3.35         |
|    value_loss           | 335          |
------------------------------------------
Eval num_timesteps=2278000, episode_reward=484.73 +/- 312.58
Episode length: 549.00 +/- 17.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 549          |
|    mean_reward          | 485          |
| time/                   |              |
|    total_timesteps      | 2278000      |
| train/                  |              |
|    approx_kl            | 0.0071715694 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 86.6         |
|    n_updates            | 11120        |
|    policy_gradient_loss | -0.00217     |
|    std                  | 3.36         |
|    value_loss           | 297          |
------------------------------------------
Eval num_timesteps=2280000, episode_reward=770.10 +/- 699.61
Episode length: 606.00 +/- 112.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 606         |
|    mean_reward          | 770         |
| time/                   |             |
|    total_timesteps      | 2280000     |
| train/                  |             |
|    approx_kl            | 0.004729315 |
|    clip_fraction        | 0.00767     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 51.8        |
|    n_updates            | 11130       |
|    policy_gradient_loss | -0.00254    |
|    std                  | 3.36        |
|    value_loss           | 187         |
-----------------------------------------
Eval num_timesteps=2282000, episode_reward=664.31 +/- 542.22
Episode length: 515.00 +/- 84.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 664          |
| time/                   |              |
|    total_timesteps      | 2282000      |
| train/                  |              |
|    approx_kl            | 0.0023681975 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.001        |
|    loss                 | 2.13e+03     |
|    n_updates            | 11140        |
|    policy_gradient_loss | 0.00038      |
|    std                  | 3.35         |
|    value_loss           | 6.55e+03     |
------------------------------------------
Eval num_timesteps=2284000, episode_reward=339.36 +/- 323.94
Episode length: 477.40 +/- 81.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 2284000      |
| train/                  |              |
|    approx_kl            | 0.0013700561 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 133          |
|    n_updates            | 11150        |
|    policy_gradient_loss | -0.00087     |
|    std                  | 3.35         |
|    value_loss           | 639          |
------------------------------------------
Eval num_timesteps=2286000, episode_reward=50.51 +/- 498.51
Episode length: 470.60 +/- 64.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 50.5         |
| time/                   |              |
|    total_timesteps      | 2286000      |
| train/                  |              |
|    approx_kl            | 0.0014805004 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.799        |
|    learning_rate        | 0.001        |
|    loss                 | 2.67e+03     |
|    n_updates            | 11160        |
|    policy_gradient_loss | -0.000397    |
|    std                  | 3.36         |
|    value_loss           | 6.4e+03      |
------------------------------------------
Eval num_timesteps=2288000, episode_reward=517.47 +/- 695.37
Episode length: 475.60 +/- 71.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 517          |
| time/                   |              |
|    total_timesteps      | 2288000      |
| train/                  |              |
|    approx_kl            | 0.0014487999 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 331          |
|    n_updates            | 11170        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 3.36         |
|    value_loss           | 1.35e+03     |
------------------------------------------
Eval num_timesteps=2290000, episode_reward=355.99 +/- 562.28
Episode length: 445.00 +/- 70.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 356          |
| time/                   |              |
|    total_timesteps      | 2290000      |
| train/                  |              |
|    approx_kl            | 0.0026279134 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 3.66e+03     |
|    n_updates            | 11180        |
|    policy_gradient_loss | -0.00165     |
|    std                  | 3.37         |
|    value_loss           | 9.32e+03     |
------------------------------------------
Eval num_timesteps=2292000, episode_reward=661.07 +/- 483.60
Episode length: 525.60 +/- 56.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 661          |
| time/                   |              |
|    total_timesteps      | 2292000      |
| train/                  |              |
|    approx_kl            | 0.0029845838 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 182          |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 3.37         |
|    value_loss           | 656          |
------------------------------------------
Eval num_timesteps=2294000, episode_reward=299.38 +/- 421.65
Episode length: 465.80 +/- 59.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 299          |
| time/                   |              |
|    total_timesteps      | 2294000      |
| train/                  |              |
|    approx_kl            | 0.0006652626 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 242          |
|    n_updates            | 11200        |
|    policy_gradient_loss | 0.000276     |
|    std                  | 3.36         |
|    value_loss           | 877          |
------------------------------------------
Eval num_timesteps=2296000, episode_reward=372.95 +/- 561.63
Episode length: 481.80 +/- 50.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 373           |
| time/                   |               |
|    total_timesteps      | 2296000       |
| train/                  |               |
|    approx_kl            | 0.00043561807 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.802         |
|    learning_rate        | 0.001         |
|    loss                 | 3.03e+03      |
|    n_updates            | 11210         |
|    policy_gradient_loss | 0.00011       |
|    std                  | 3.36          |
|    value_loss           | 6.39e+03      |
-------------------------------------------
Eval num_timesteps=2298000, episode_reward=450.13 +/- 135.94
Episode length: 554.20 +/- 72.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 554           |
|    mean_reward          | 450           |
| time/                   |               |
|    total_timesteps      | 2298000       |
| train/                  |               |
|    approx_kl            | 0.00010405792 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.001         |
|    loss                 | 1.88e+03      |
|    n_updates            | 11220         |
|    policy_gradient_loss | -0.000196     |
|    std                  | 3.36          |
|    value_loss           | 5.1e+03       |
-------------------------------------------
Eval num_timesteps=2300000, episode_reward=243.15 +/- 206.19
Episode length: 463.80 +/- 48.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.000110694 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.001       |
|    loss                 | 5.52e+03    |
|    n_updates            | 11230       |
|    policy_gradient_loss | -0.000147   |
|    std                  | 3.36        |
|    value_loss           | 1.27e+04    |
-----------------------------------------
Eval num_timesteps=2302000, episode_reward=157.53 +/- 86.96
Episode length: 478.00 +/- 54.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 2302000      |
| train/                  |              |
|    approx_kl            | 0.0010451889 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.001        |
|    loss                 | 693          |
|    n_updates            | 11240        |
|    policy_gradient_loss | -0.000821    |
|    std                  | 3.36         |
|    value_loss           | 1.66e+03     |
------------------------------------------
Eval num_timesteps=2304000, episode_reward=75.11 +/- 664.53
Episode length: 501.00 +/- 46.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 75.1     |
| time/              |          |
|    total_timesteps | 2304000  |
---------------------------------
Eval num_timesteps=2306000, episode_reward=151.14 +/- 252.32
Episode length: 480.40 +/- 37.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 480           |
|    mean_reward          | 151           |
| time/                   |               |
|    total_timesteps      | 2306000       |
| train/                  |               |
|    approx_kl            | 0.00089692126 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 137           |
|    n_updates            | 11250         |
|    policy_gradient_loss | 4.4e-05       |
|    std                  | 3.37          |
|    value_loss           | 423           |
-------------------------------------------
Eval num_timesteps=2308000, episode_reward=496.00 +/- 248.31
Episode length: 451.00 +/- 47.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 451           |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 2308000       |
| train/                  |               |
|    approx_kl            | 0.00037402756 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 137           |
|    n_updates            | 11260         |
|    policy_gradient_loss | -5.66e-05     |
|    std                  | 3.37          |
|    value_loss           | 450           |
-------------------------------------------
Eval num_timesteps=2310000, episode_reward=383.43 +/- 183.38
Episode length: 449.00 +/- 71.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 383           |
| time/                   |               |
|    total_timesteps      | 2310000       |
| train/                  |               |
|    approx_kl            | 0.00028497123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 11270         |
|    policy_gradient_loss | -2.74e-05     |
|    std                  | 3.38          |
|    value_loss           | 4.21e+03      |
-------------------------------------------
Eval num_timesteps=2312000, episode_reward=340.65 +/- 719.70
Episode length: 471.60 +/- 90.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 2312000      |
| train/                  |              |
|    approx_kl            | 0.0003286026 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 136          |
|    n_updates            | 11280        |
|    policy_gradient_loss | -0.000527    |
|    std                  | 3.39         |
|    value_loss           | 446          |
------------------------------------------
Eval num_timesteps=2314000, episode_reward=323.63 +/- 322.37
Episode length: 458.60 +/- 44.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 324          |
| time/                   |              |
|    total_timesteps      | 2314000      |
| train/                  |              |
|    approx_kl            | 0.0012400049 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+03     |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.000266    |
|    std                  | 3.4          |
|    value_loss           | 1.22e+04     |
------------------------------------------
Eval num_timesteps=2316000, episode_reward=4.94 +/- 219.38
Episode length: 548.20 +/- 45.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 548           |
|    mean_reward          | 4.94          |
| time/                   |               |
|    total_timesteps      | 2316000       |
| train/                  |               |
|    approx_kl            | 0.00083653483 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.001         |
|    loss                 | 275           |
|    n_updates            | 11300         |
|    policy_gradient_loss | -0.000928     |
|    std                  | 3.4           |
|    value_loss           | 857           |
-------------------------------------------
Eval num_timesteps=2318000, episode_reward=453.03 +/- 582.34
Episode length: 580.80 +/- 130.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 581         |
|    mean_reward          | 453         |
| time/                   |             |
|    total_timesteps      | 2318000     |
| train/                  |             |
|    approx_kl            | 0.002107713 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.001       |
|    loss                 | 1.68e+03    |
|    n_updates            | 11310       |
|    policy_gradient_loss | -0.0015     |
|    std                  | 3.41        |
|    value_loss           | 4.14e+03    |
-----------------------------------------
Eval num_timesteps=2320000, episode_reward=123.87 +/- 296.52
Episode length: 499.80 +/- 67.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 124          |
| time/                   |              |
|    total_timesteps      | 2320000      |
| train/                  |              |
|    approx_kl            | 0.0005837039 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 415          |
|    n_updates            | 11320        |
|    policy_gradient_loss | 6.15e-05     |
|    std                  | 3.41         |
|    value_loss           | 2e+03        |
------------------------------------------
Eval num_timesteps=2322000, episode_reward=502.39 +/- 620.01
Episode length: 530.40 +/- 37.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 530           |
|    mean_reward          | 502           |
| time/                   |               |
|    total_timesteps      | 2322000       |
| train/                  |               |
|    approx_kl            | 0.00012107837 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.878         |
|    learning_rate        | 0.001         |
|    loss                 | 267           |
|    n_updates            | 11330         |
|    policy_gradient_loss | -0.00033      |
|    std                  | 3.41          |
|    value_loss           | 907           |
-------------------------------------------
Eval num_timesteps=2324000, episode_reward=199.14 +/- 237.36
Episode length: 531.60 +/- 92.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 2324000     |
| train/                  |             |
|    approx_kl            | 0.002568399 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 125         |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.00113    |
|    std                  | 3.41        |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=2326000, episode_reward=370.71 +/- 299.81
Episode length: 559.20 +/- 108.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 559         |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 2326000     |
| train/                  |             |
|    approx_kl            | 0.002305086 |
|    clip_fraction        | 0.00229     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 98.5        |
|    n_updates            | 11350       |
|    policy_gradient_loss | -0.00131    |
|    std                  | 3.41        |
|    value_loss           | 431         |
-----------------------------------------
Eval num_timesteps=2328000, episode_reward=520.06 +/- 438.89
Episode length: 550.80 +/- 30.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 551          |
|    mean_reward          | 520          |
| time/                   |              |
|    total_timesteps      | 2328000      |
| train/                  |              |
|    approx_kl            | 0.0029040994 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.647        |
|    learning_rate        | 0.001        |
|    loss                 | 5.1e+03      |
|    n_updates            | 11360        |
|    policy_gradient_loss | -0.000834    |
|    std                  | 3.41         |
|    value_loss           | 1.15e+04     |
------------------------------------------
Eval num_timesteps=2330000, episode_reward=171.88 +/- 209.41
Episode length: 460.60 +/- 79.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 2330000      |
| train/                  |              |
|    approx_kl            | 0.0033867296 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | 161          |
|    n_updates            | 11370        |
|    policy_gradient_loss | -0.00165     |
|    std                  | 3.42         |
|    value_loss           | 555          |
------------------------------------------
Eval num_timesteps=2332000, episode_reward=200.99 +/- 334.70
Episode length: 387.80 +/- 44.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 2332000     |
| train/                  |             |
|    approx_kl            | 0.005550404 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.001       |
|    loss                 | 1.52e+03    |
|    n_updates            | 11380       |
|    policy_gradient_loss | -0.000172   |
|    std                  | 3.42        |
|    value_loss           | 4.16e+03    |
-----------------------------------------
Eval num_timesteps=2334000, episode_reward=341.86 +/- 514.13
Episode length: 400.40 +/- 50.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 2334000     |
| train/                  |             |
|    approx_kl            | 0.002893149 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 183         |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.000891   |
|    std                  | 3.43        |
|    value_loss           | 532         |
-----------------------------------------
Eval num_timesteps=2336000, episode_reward=182.54 +/- 485.03
Episode length: 406.80 +/- 64.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 2336000      |
| train/                  |              |
|    approx_kl            | 0.0014649348 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 11400        |
|    policy_gradient_loss | -0.000734    |
|    std                  | 3.42         |
|    value_loss           | 496          |
------------------------------------------
Eval num_timesteps=2338000, episode_reward=414.47 +/- 172.49
Episode length: 370.60 +/- 23.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 414          |
| time/                   |              |
|    total_timesteps      | 2338000      |
| train/                  |              |
|    approx_kl            | 0.0050247945 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 128          |
|    n_updates            | 11410        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 3.42         |
|    value_loss           | 429          |
------------------------------------------
Eval num_timesteps=2340000, episode_reward=106.66 +/- 333.73
Episode length: 383.80 +/- 41.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 107          |
| time/                   |              |
|    total_timesteps      | 2340000      |
| train/                  |              |
|    approx_kl            | 0.0041037938 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 3.8e+03      |
|    n_updates            | 11420        |
|    policy_gradient_loss | -0.0012      |
|    std                  | 3.43         |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=2342000, episode_reward=125.85 +/- 278.72
Episode length: 376.60 +/- 45.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 2342000      |
| train/                  |              |
|    approx_kl            | 0.0006442664 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 11430        |
|    policy_gradient_loss | 0.000118     |
|    std                  | 3.43         |
|    value_loss           | 5.73e+03     |
------------------------------------------
Eval num_timesteps=2344000, episode_reward=269.98 +/- 403.37
Episode length: 428.40 +/- 98.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 428           |
|    mean_reward          | 270           |
| time/                   |               |
|    total_timesteps      | 2344000       |
| train/                  |               |
|    approx_kl            | 0.00031364046 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 195           |
|    n_updates            | 11440         |
|    policy_gradient_loss | -0.000284     |
|    std                  | 3.43          |
|    value_loss           | 640           |
-------------------------------------------
Eval num_timesteps=2346000, episode_reward=486.19 +/- 69.10
Episode length: 374.40 +/- 47.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | 486          |
| time/                   |              |
|    total_timesteps      | 2346000      |
| train/                  |              |
|    approx_kl            | 0.0016774554 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 11450        |
|    policy_gradient_loss | -0.000721    |
|    std                  | 3.43         |
|    value_loss           | 603          |
------------------------------------------
Eval num_timesteps=2348000, episode_reward=299.74 +/- 50.81
Episode length: 367.60 +/- 27.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 300          |
| time/                   |              |
|    total_timesteps      | 2348000      |
| train/                  |              |
|    approx_kl            | 0.0024080349 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.001        |
|    loss                 | 2.39e+03     |
|    n_updates            | 11460        |
|    policy_gradient_loss | -0.000995    |
|    std                  | 3.43         |
|    value_loss           | 5.71e+03     |
------------------------------------------
Eval num_timesteps=2350000, episode_reward=27.46 +/- 330.30
Episode length: 376.20 +/- 53.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 27.5         |
| time/                   |              |
|    total_timesteps      | 2350000      |
| train/                  |              |
|    approx_kl            | 0.0014043233 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 11470        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 3.43         |
|    value_loss           | 441          |
------------------------------------------
Eval num_timesteps=2352000, episode_reward=988.15 +/- 965.99
Episode length: 532.80 +/- 120.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 533          |
|    mean_reward          | 988          |
| time/                   |              |
|    total_timesteps      | 2352000      |
| train/                  |              |
|    approx_kl            | 0.0017783663 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.749        |
|    learning_rate        | 0.001        |
|    loss                 | 4.21e+03     |
|    n_updates            | 11480        |
|    policy_gradient_loss | -0.000212    |
|    std                  | 3.43         |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=2354000, episode_reward=-190.78 +/- 306.28
Episode length: 426.20 +/- 55.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | -191          |
| time/                   |               |
|    total_timesteps      | 2354000       |
| train/                  |               |
|    approx_kl            | 0.00039523345 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.78          |
|    learning_rate        | 0.001         |
|    loss                 | 2.08e+03      |
|    n_updates            | 11490         |
|    policy_gradient_loss | -0.000172     |
|    std                  | 3.43          |
|    value_loss           | 5.31e+03      |
-------------------------------------------
Eval num_timesteps=2356000, episode_reward=71.49 +/- 389.47
Episode length: 403.80 +/- 43.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 71.5          |
| time/                   |               |
|    total_timesteps      | 2356000       |
| train/                  |               |
|    approx_kl            | 0.00011244285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.78          |
|    learning_rate        | 0.001         |
|    loss                 | 3.2e+03       |
|    n_updates            | 11500         |
|    policy_gradient_loss | -0.000124     |
|    std                  | 3.44          |
|    value_loss           | 8.1e+03       |
-------------------------------------------
Eval num_timesteps=2358000, episode_reward=316.89 +/- 277.29
Episode length: 385.60 +/- 14.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 2358000      |
| train/                  |              |
|    approx_kl            | 0.0001934855 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 2.8e+03      |
|    n_updates            | 11510        |
|    policy_gradient_loss | -0.000451    |
|    std                  | 3.44         |
|    value_loss           | 6.36e+03     |
------------------------------------------
Eval num_timesteps=2360000, episode_reward=552.77 +/- 340.33
Episode length: 381.00 +/- 45.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 553           |
| time/                   |               |
|    total_timesteps      | 2360000       |
| train/                  |               |
|    approx_kl            | 0.00037485335 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.87          |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+03      |
|    n_updates            | 11520         |
|    policy_gradient_loss | -0.000821     |
|    std                  | 3.44          |
|    value_loss           | 3.75e+03      |
-------------------------------------------
Eval num_timesteps=2362000, episode_reward=261.05 +/- 186.95
Episode length: 342.00 +/- 29.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 2362000     |
| train/                  |             |
|    approx_kl            | 0.001397567 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 125         |
|    n_updates            | 11530       |
|    policy_gradient_loss | -0.000857   |
|    std                  | 3.44        |
|    value_loss           | 615         |
-----------------------------------------
Eval num_timesteps=2364000, episode_reward=201.66 +/- 320.94
Episode length: 355.40 +/- 34.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 202         |
| time/                   |             |
|    total_timesteps      | 2364000     |
| train/                  |             |
|    approx_kl            | 0.004940982 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 121         |
|    n_updates            | 11540       |
|    policy_gradient_loss | -0.00161    |
|    std                  | 3.43        |
|    value_loss           | 495         |
-----------------------------------------
Eval num_timesteps=2366000, episode_reward=311.14 +/- 166.29
Episode length: 340.40 +/- 17.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 2366000      |
| train/                  |              |
|    approx_kl            | 0.0055415556 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 71           |
|    n_updates            | 11550        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 3.43         |
|    value_loss           | 286          |
------------------------------------------
Eval num_timesteps=2368000, episode_reward=220.13 +/- 137.38
Episode length: 317.60 +/- 20.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 2368000      |
| train/                  |              |
|    approx_kl            | 0.0006620049 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 11560        |
|    policy_gradient_loss | -0.000246    |
|    std                  | 3.44         |
|    value_loss           | 508          |
------------------------------------------
Eval num_timesteps=2370000, episode_reward=198.50 +/- 104.53
Episode length: 320.60 +/- 12.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 2370000     |
| train/                  |             |
|    approx_kl            | 0.007076228 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 147         |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.00229    |
|    std                  | 3.45        |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=2372000, episode_reward=354.94 +/- 143.04
Episode length: 339.00 +/- 17.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 2372000      |
| train/                  |              |
|    approx_kl            | 0.0035341945 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.618        |
|    learning_rate        | 0.001        |
|    loss                 | 5.37e+03     |
|    n_updates            | 11580        |
|    policy_gradient_loss | 0.00028      |
|    std                  | 3.45         |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=2374000, episode_reward=144.89 +/- 78.47
Episode length: 320.20 +/- 7.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | 145          |
| time/                   |              |
|    total_timesteps      | 2374000      |
| train/                  |              |
|    approx_kl            | 0.0006963612 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.000369    |
|    std                  | 3.46         |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=2376000, episode_reward=175.45 +/- 108.05
Episode length: 307.80 +/- 23.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 2376000      |
| train/                  |              |
|    approx_kl            | 0.0016595154 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 11600        |
|    policy_gradient_loss | -0.000985    |
|    std                  | 3.46         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=2378000, episode_reward=69.79 +/- 89.80
Episode length: 295.20 +/- 34.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | 69.8         |
| time/                   |              |
|    total_timesteps      | 2378000      |
| train/                  |              |
|    approx_kl            | 0.0006673774 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 106          |
|    n_updates            | 11610        |
|    policy_gradient_loss | -5.8e-05     |
|    std                  | 3.46         |
|    value_loss           | 332          |
------------------------------------------
Eval num_timesteps=2380000, episode_reward=113.10 +/- 62.49
Episode length: 297.40 +/- 15.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 113          |
| time/                   |              |
|    total_timesteps      | 2380000      |
| train/                  |              |
|    approx_kl            | 0.0019439535 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 48.6         |
|    n_updates            | 11620        |
|    policy_gradient_loss | -0.00085     |
|    std                  | 3.46         |
|    value_loss           | 173          |
------------------------------------------
Eval num_timesteps=2382000, episode_reward=70.46 +/- 40.03
Episode length: 286.80 +/- 14.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 70.5         |
| time/                   |              |
|    total_timesteps      | 2382000      |
| train/                  |              |
|    approx_kl            | 0.0033023027 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 42.6         |
|    n_updates            | 11630        |
|    policy_gradient_loss | -0.000271    |
|    std                  | 3.44         |
|    value_loss           | 150          |
------------------------------------------
Eval num_timesteps=2384000, episode_reward=62.12 +/- 31.48
Episode length: 282.40 +/- 8.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 62.1         |
| time/                   |              |
|    total_timesteps      | 2384000      |
| train/                  |              |
|    approx_kl            | 0.0015698595 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 98.7         |
|    n_updates            | 11640        |
|    policy_gradient_loss | -0.000633    |
|    std                  | 3.43         |
|    value_loss           | 333          |
------------------------------------------
Eval num_timesteps=2386000, episode_reward=190.39 +/- 139.83
Episode length: 322.40 +/- 36.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 2386000     |
| train/                  |             |
|    approx_kl            | 0.007882851 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 70.1        |
|    n_updates            | 11650       |
|    policy_gradient_loss | -0.00222    |
|    std                  | 3.44        |
|    value_loss           | 206         |
-----------------------------------------
Eval num_timesteps=2388000, episode_reward=179.84 +/- 288.63
Episode length: 318.20 +/- 35.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 2388000      |
| train/                  |              |
|    approx_kl            | 0.0034259558 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+03     |
|    n_updates            | 11660        |
|    policy_gradient_loss | -0.00208     |
|    std                  | 3.45         |
|    value_loss           | 3.57e+03     |
------------------------------------------
Eval num_timesteps=2390000, episode_reward=295.08 +/- 105.61
Episode length: 330.40 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 2390000  |
---------------------------------
Eval num_timesteps=2392000, episode_reward=285.32 +/- 178.56
Episode length: 359.40 +/- 34.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 2392000     |
| train/                  |             |
|    approx_kl            | 0.007653973 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 32.9        |
|    n_updates            | 11670       |
|    policy_gradient_loss | -0.00226    |
|    std                  | 3.48        |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=2394000, episode_reward=380.49 +/- 171.55
Episode length: 341.60 +/- 20.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | 380         |
| time/                   |             |
|    total_timesteps      | 2394000     |
| train/                  |             |
|    approx_kl            | 0.005020182 |
|    clip_fraction        | 0.0063      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 31.1        |
|    n_updates            | 11680       |
|    policy_gradient_loss | -0.00104    |
|    std                  | 3.5         |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=2396000, episode_reward=157.95 +/- 265.51
Episode length: 409.40 +/- 65.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 2396000     |
| train/                  |             |
|    approx_kl            | 0.003928786 |
|    clip_fraction        | 0.00527     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.001       |
|    loss                 | 4.41e+03    |
|    n_updates            | 11690       |
|    policy_gradient_loss | 0.00166     |
|    std                  | 3.51        |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=2398000, episode_reward=249.60 +/- 489.14
Episode length: 402.20 +/- 29.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | 250           |
| time/                   |               |
|    total_timesteps      | 2398000       |
| train/                  |               |
|    approx_kl            | 0.00023747914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.837         |
|    learning_rate        | 0.001         |
|    loss                 | 2.45e+03      |
|    n_updates            | 11700         |
|    policy_gradient_loss | 0.000343      |
|    std                  | 3.51          |
|    value_loss           | 5.64e+03      |
-------------------------------------------
Eval num_timesteps=2400000, episode_reward=151.43 +/- 348.57
Episode length: 412.40 +/- 29.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 412           |
|    mean_reward          | 151           |
| time/                   |               |
|    total_timesteps      | 2400000       |
| train/                  |               |
|    approx_kl            | 2.7859933e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.807         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+03      |
|    n_updates            | 11710         |
|    policy_gradient_loss | -0.000129     |
|    std                  | 3.51          |
|    value_loss           | 7.08e+03      |
-------------------------------------------
Eval num_timesteps=2402000, episode_reward=-415.41 +/- 72.56
Episode length: 449.40 +/- 22.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | -415          |
| time/                   |               |
|    total_timesteps      | 2402000       |
| train/                  |               |
|    approx_kl            | 1.1023309e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.801         |
|    learning_rate        | 0.001         |
|    loss                 | 6.83e+03      |
|    n_updates            | 11720         |
|    policy_gradient_loss | 4.54e-05      |
|    std                  | 3.51          |
|    value_loss           | 1.53e+04      |
-------------------------------------------
Eval num_timesteps=2404000, episode_reward=280.01 +/- 241.18
Episode length: 412.80 +/- 70.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 413           |
|    mean_reward          | 280           |
| time/                   |               |
|    total_timesteps      | 2404000       |
| train/                  |               |
|    approx_kl            | 1.0947231e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+03      |
|    n_updates            | 11730         |
|    policy_gradient_loss | -3.97e-05     |
|    std                  | 3.52          |
|    value_loss           | 3.44e+03      |
-------------------------------------------
Eval num_timesteps=2406000, episode_reward=201.72 +/- 509.86
Episode length: 444.40 +/- 18.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 202           |
| time/                   |               |
|    total_timesteps      | 2406000       |
| train/                  |               |
|    approx_kl            | 5.9826736e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.001         |
|    loss                 | 5.3e+03       |
|    n_updates            | 11740         |
|    policy_gradient_loss | -0.00022      |
|    std                  | 3.52          |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=2408000, episode_reward=426.65 +/- 273.12
Episode length: 451.20 +/- 101.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 427          |
| time/                   |              |
|    total_timesteps      | 2408000      |
| train/                  |              |
|    approx_kl            | 8.880667e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 3.11e+03     |
|    n_updates            | 11750        |
|    policy_gradient_loss | -0.000121    |
|    std                  | 3.52         |
|    value_loss           | 7.68e+03     |
------------------------------------------
Eval num_timesteps=2410000, episode_reward=99.94 +/- 260.18
Episode length: 386.20 +/- 45.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 99.9          |
| time/                   |               |
|    total_timesteps      | 2410000       |
| train/                  |               |
|    approx_kl            | 5.5136683e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.001         |
|    loss                 | 4.58e+03      |
|    n_updates            | 11760         |
|    policy_gradient_loss | -2.01e-05     |
|    std                  | 3.52          |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=2412000, episode_reward=513.73 +/- 252.19
Episode length: 388.20 +/- 24.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 514          |
| time/                   |              |
|    total_timesteps      | 2412000      |
| train/                  |              |
|    approx_kl            | 0.0022395165 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 96.5         |
|    n_updates            | 11770        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 3.52         |
|    value_loss           | 495          |
------------------------------------------
Eval num_timesteps=2414000, episode_reward=340.82 +/- 181.95
Episode length: 402.60 +/- 30.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 403          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 2414000      |
| train/                  |              |
|    approx_kl            | 0.0031938232 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 79.8         |
|    n_updates            | 11780        |
|    policy_gradient_loss | 0.000102     |
|    std                  | 3.52         |
|    value_loss           | 342          |
------------------------------------------
Eval num_timesteps=2416000, episode_reward=-55.08 +/- 275.47
Episode length: 416.00 +/- 42.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | -55.1        |
| time/                   |              |
|    total_timesteps      | 2416000      |
| train/                  |              |
|    approx_kl            | 0.0003250484 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+03     |
|    n_updates            | 11790        |
|    policy_gradient_loss | 1.51e-06     |
|    std                  | 3.52         |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=2418000, episode_reward=432.53 +/- 161.13
Episode length: 374.60 +/- 33.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 375           |
|    mean_reward          | 433           |
| time/                   |               |
|    total_timesteps      | 2418000       |
| train/                  |               |
|    approx_kl            | 0.00047569067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.5         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 207           |
|    n_updates            | 11800         |
|    policy_gradient_loss | -0.000492     |
|    std                  | 3.52          |
|    value_loss           | 952           |
-------------------------------------------
Eval num_timesteps=2420000, episode_reward=100.84 +/- 468.98
Episode length: 404.80 +/- 46.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 101          |
| time/                   |              |
|    total_timesteps      | 2420000      |
| train/                  |              |
|    approx_kl            | 0.0014655216 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+03     |
|    n_updates            | 11810        |
|    policy_gradient_loss | -0.000831    |
|    std                  | 3.52         |
|    value_loss           | 5.1e+03      |
------------------------------------------
Eval num_timesteps=2422000, episode_reward=343.82 +/- 346.52
Episode length: 391.40 +/- 62.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 344         |
| time/                   |             |
|    total_timesteps      | 2422000     |
| train/                  |             |
|    approx_kl            | 0.001171068 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 64.6        |
|    n_updates            | 11820       |
|    policy_gradient_loss | -0.000369   |
|    std                  | 3.52        |
|    value_loss           | 268         |
-----------------------------------------
Eval num_timesteps=2424000, episode_reward=445.07 +/- 274.70
Episode length: 443.80 +/- 46.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 445          |
| time/                   |              |
|    total_timesteps      | 2424000      |
| train/                  |              |
|    approx_kl            | 0.0005755581 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.001        |
|    loss                 | 3.12e+03     |
|    n_updates            | 11830        |
|    policy_gradient_loss | 0.000239     |
|    std                  | 3.53         |
|    value_loss           | 7.96e+03     |
------------------------------------------
Eval num_timesteps=2426000, episode_reward=54.42 +/- 249.03
Episode length: 383.40 +/- 36.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 54.4         |
| time/                   |              |
|    total_timesteps      | 2426000      |
| train/                  |              |
|    approx_kl            | 0.0003722886 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.767        |
|    learning_rate        | 0.001        |
|    loss                 | 3.93e+03     |
|    n_updates            | 11840        |
|    policy_gradient_loss | 6.73e-07     |
|    std                  | 3.53         |
|    value_loss           | 8.95e+03     |
------------------------------------------
Eval num_timesteps=2428000, episode_reward=221.69 +/- 470.75
Episode length: 397.60 +/- 46.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 2428000     |
| train/                  |             |
|    approx_kl            | 0.005325985 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 59.8        |
|    n_updates            | 11850       |
|    policy_gradient_loss | -0.00191    |
|    std                  | 3.53        |
|    value_loss           | 263         |
-----------------------------------------
Eval num_timesteps=2430000, episode_reward=420.56 +/- 238.19
Episode length: 409.40 +/- 79.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 2430000     |
| train/                  |             |
|    approx_kl            | 0.004312291 |
|    clip_fraction        | 0.00747     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.001       |
|    loss                 | 2.17e+03    |
|    n_updates            | 11860       |
|    policy_gradient_loss | -0.00105    |
|    std                  | 3.53        |
|    value_loss           | 5e+03       |
-----------------------------------------
Eval num_timesteps=2432000, episode_reward=148.80 +/- 494.62
Episode length: 458.40 +/- 67.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 149           |
| time/                   |               |
|    total_timesteps      | 2432000       |
| train/                  |               |
|    approx_kl            | 0.00060400064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 3.05e+03      |
|    n_updates            | 11870         |
|    policy_gradient_loss | -0.000266     |
|    std                  | 3.53          |
|    value_loss           | 7.58e+03      |
-------------------------------------------
Eval num_timesteps=2434000, episode_reward=391.22 +/- 489.14
Episode length: 477.00 +/- 45.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 477           |
|    mean_reward          | 391           |
| time/                   |               |
|    total_timesteps      | 2434000       |
| train/                  |               |
|    approx_kl            | 3.7608756e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+03      |
|    n_updates            | 11880         |
|    policy_gradient_loss | 2.76e-05      |
|    std                  | 3.53          |
|    value_loss           | 3.75e+03      |
-------------------------------------------
Eval num_timesteps=2436000, episode_reward=287.66 +/- 132.12
Episode length: 478.40 +/- 44.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 288          |
| time/                   |              |
|    total_timesteps      | 2436000      |
| train/                  |              |
|    approx_kl            | 0.0055581965 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 33.9         |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00208     |
|    std                  | 3.54         |
|    value_loss           | 139          |
------------------------------------------
Eval num_timesteps=2438000, episode_reward=876.80 +/- 358.74
Episode length: 501.80 +/- 30.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | 877         |
| time/                   |             |
|    total_timesteps      | 2438000     |
| train/                  |             |
|    approx_kl            | 0.002792579 |
|    clip_fraction        | 0.00278     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.001       |
|    loss                 | 1.86e+03    |
|    n_updates            | 11900       |
|    policy_gradient_loss | 0.000373    |
|    std                  | 3.55        |
|    value_loss           | 4.11e+03    |
-----------------------------------------
Eval num_timesteps=2440000, episode_reward=169.99 +/- 266.40
Episode length: 415.40 +/- 63.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 2440000      |
| train/                  |              |
|    approx_kl            | 0.0030173468 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 224          |
|    n_updates            | 11910        |
|    policy_gradient_loss | -0.00226     |
|    std                  | 3.56         |
|    value_loss           | 989          |
------------------------------------------
Eval num_timesteps=2442000, episode_reward=41.86 +/- 209.83
Episode length: 430.20 +/- 40.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 41.9         |
| time/                   |              |
|    total_timesteps      | 2442000      |
| train/                  |              |
|    approx_kl            | 0.0012673106 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 71           |
|    n_updates            | 11920        |
|    policy_gradient_loss | 0.000151     |
|    std                  | 3.57         |
|    value_loss           | 294          |
------------------------------------------
Eval num_timesteps=2444000, episode_reward=331.13 +/- 214.78
Episode length: 475.00 +/- 51.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 2444000     |
| train/                  |             |
|    approx_kl            | 0.005614636 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 11930       |
|    policy_gradient_loss | -0.00168    |
|    std                  | 3.58        |
|    value_loss           | 537         |
-----------------------------------------
Eval num_timesteps=2446000, episode_reward=-33.11 +/- 385.87
Episode length: 456.00 +/- 61.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | -33.1        |
| time/                   |              |
|    total_timesteps      | 2446000      |
| train/                  |              |
|    approx_kl            | 0.0011687294 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+03     |
|    n_updates            | 11940        |
|    policy_gradient_loss | 0.000202     |
|    std                  | 3.58         |
|    value_loss           | 5.54e+03     |
------------------------------------------
Eval num_timesteps=2448000, episode_reward=347.10 +/- 280.63
Episode length: 430.20 +/- 57.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | 347         |
| time/                   |             |
|    total_timesteps      | 2448000     |
| train/                  |             |
|    approx_kl            | 0.005999416 |
|    clip_fraction        | 0.00952     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 60.1        |
|    n_updates            | 11950       |
|    policy_gradient_loss | -0.00231    |
|    std                  | 3.58        |
|    value_loss           | 258         |
-----------------------------------------
Eval num_timesteps=2450000, episode_reward=181.08 +/- 479.05
Episode length: 410.60 +/- 40.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 2450000      |
| train/                  |              |
|    approx_kl            | 0.0026738602 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 128          |
|    n_updates            | 11960        |
|    policy_gradient_loss | 0.00122      |
|    std                  | 3.58         |
|    value_loss           | 841          |
------------------------------------------
Eval num_timesteps=2452000, episode_reward=449.90 +/- 274.03
Episode length: 422.00 +/- 54.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 450          |
| time/                   |              |
|    total_timesteps      | 2452000      |
| train/                  |              |
|    approx_kl            | 0.0015205484 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 1.86e+03     |
|    n_updates            | 11970        |
|    policy_gradient_loss | -0.000473    |
|    std                  | 3.58         |
|    value_loss           | 4.29e+03     |
------------------------------------------
Eval num_timesteps=2454000, episode_reward=580.64 +/- 307.89
Episode length: 428.60 +/- 55.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 581          |
| time/                   |              |
|    total_timesteps      | 2454000      |
| train/                  |              |
|    approx_kl            | 8.939361e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 1.9e+03      |
|    n_updates            | 11980        |
|    policy_gradient_loss | 0.00042      |
|    std                  | 3.58         |
|    value_loss           | 4.29e+03     |
------------------------------------------
Eval num_timesteps=2456000, episode_reward=442.60 +/- 479.32
Episode length: 476.60 +/- 79.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 443          |
| time/                   |              |
|    total_timesteps      | 2456000      |
| train/                  |              |
|    approx_kl            | 0.0023106989 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.000827    |
|    std                  | 3.58         |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=2458000, episode_reward=151.10 +/- 384.44
Episode length: 505.00 +/- 94.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 505        |
|    mean_reward          | 151        |
| time/                   |            |
|    total_timesteps      | 2458000    |
| train/                  |            |
|    approx_kl            | 0.00400747 |
|    clip_fraction        | 0.0062     |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.001      |
|    loss                 | 3.02e+03   |
|    n_updates            | 12000      |
|    policy_gradient_loss | -0.0014    |
|    std                  | 3.59       |
|    value_loss           | 6.35e+03   |
----------------------------------------
Eval num_timesteps=2460000, episode_reward=780.94 +/- 466.38
Episode length: 458.00 +/- 64.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 781           |
| time/                   |               |
|    total_timesteps      | 2460000       |
| train/                  |               |
|    approx_kl            | 0.00064696145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.784         |
|    learning_rate        | 0.001         |
|    loss                 | 5.79e+03      |
|    n_updates            | 12010         |
|    policy_gradient_loss | -0.000485     |
|    std                  | 3.59          |
|    value_loss           | 1.39e+04      |
-------------------------------------------
Eval num_timesteps=2462000, episode_reward=4.51 +/- 315.03
Episode length: 465.40 +/- 52.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 4.51         |
| time/                   |              |
|    total_timesteps      | 2462000      |
| train/                  |              |
|    approx_kl            | 0.0003009853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+03     |
|    n_updates            | 12020        |
|    policy_gradient_loss | -0.000902    |
|    std                  | 3.59         |
|    value_loss           | 5.67e+03     |
------------------------------------------
Eval num_timesteps=2464000, episode_reward=488.86 +/- 269.60
Episode length: 475.60 +/- 74.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 489           |
| time/                   |               |
|    total_timesteps      | 2464000       |
| train/                  |               |
|    approx_kl            | 0.00042390442 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.803         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+03      |
|    n_updates            | 12030         |
|    policy_gradient_loss | -0.000732     |
|    std                  | 3.59          |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=2466000, episode_reward=388.76 +/- 249.55
Episode length: 398.80 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 389          |
| time/                   |              |
|    total_timesteps      | 2466000      |
| train/                  |              |
|    approx_kl            | 0.0007435569 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 12040        |
|    policy_gradient_loss | -0.000541    |
|    std                  | 3.59         |
|    value_loss           | 332          |
------------------------------------------
Eval num_timesteps=2468000, episode_reward=40.06 +/- 378.80
Episode length: 444.80 +/- 23.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 445           |
|    mean_reward          | 40.1          |
| time/                   |               |
|    total_timesteps      | 2468000       |
| train/                  |               |
|    approx_kl            | 0.00089942926 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.001         |
|    loss                 | 4.37e+03      |
|    n_updates            | 12050         |
|    policy_gradient_loss | -0.000166     |
|    std                  | 3.6           |
|    value_loss           | 9.95e+03      |
-------------------------------------------
Eval num_timesteps=2470000, episode_reward=43.76 +/- 397.24
Episode length: 448.60 +/- 79.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 43.8          |
| time/                   |               |
|    total_timesteps      | 2470000       |
| train/                  |               |
|    approx_kl            | 0.00011603683 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+03      |
|    n_updates            | 12060         |
|    policy_gradient_loss | -6.06e-05     |
|    std                  | 3.6           |
|    value_loss           | 4.82e+03      |
-------------------------------------------
Eval num_timesteps=2472000, episode_reward=-15.53 +/- 225.14
Episode length: 449.80 +/- 42.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | -15.5         |
| time/                   |               |
|    total_timesteps      | 2472000       |
| train/                  |               |
|    approx_kl            | 0.00044817766 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.832         |
|    learning_rate        | 0.001         |
|    loss                 | 3.15e+03      |
|    n_updates            | 12070         |
|    policy_gradient_loss | -0.000416     |
|    std                  | 3.6           |
|    value_loss           | 7.39e+03      |
-------------------------------------------
Eval num_timesteps=2474000, episode_reward=-70.69 +/- 397.24
Episode length: 442.60 +/- 30.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | -70.7         |
| time/                   |               |
|    total_timesteps      | 2474000       |
| train/                  |               |
|    approx_kl            | 0.00043065718 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 164           |
|    n_updates            | 12080         |
|    policy_gradient_loss | -0.000379     |
|    std                  | 3.61          |
|    value_loss           | 700           |
-------------------------------------------
Eval num_timesteps=2476000, episode_reward=683.36 +/- 191.76
Episode length: 555.40 +/- 78.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 555      |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 2476000  |
---------------------------------
Eval num_timesteps=2478000, episode_reward=623.08 +/- 483.46
Episode length: 468.80 +/- 72.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | 623           |
| time/                   |               |
|    total_timesteps      | 2478000       |
| train/                  |               |
|    approx_kl            | 0.00021943325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.6         |
|    explained_variance   | 0.797         |
|    learning_rate        | 0.001         |
|    loss                 | 3.81e+03      |
|    n_updates            | 12090         |
|    policy_gradient_loss | 0.000207      |
|    std                  | 3.61          |
|    value_loss           | 9.54e+03      |
-------------------------------------------
Eval num_timesteps=2480000, episode_reward=46.14 +/- 245.29
Episode length: 453.60 +/- 37.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 46.1          |
| time/                   |               |
|    total_timesteps      | 2480000       |
| train/                  |               |
|    approx_kl            | 0.00010158919 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+03      |
|    n_updates            | 12100         |
|    policy_gradient_loss | -0.000259     |
|    std                  | 3.62          |
|    value_loss           | 4.89e+03      |
-------------------------------------------
Eval num_timesteps=2482000, episode_reward=479.45 +/- 286.20
Episode length: 504.00 +/- 26.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 504           |
|    mean_reward          | 479           |
| time/                   |               |
|    total_timesteps      | 2482000       |
| train/                  |               |
|    approx_kl            | 0.00042037677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 178           |
|    n_updates            | 12110         |
|    policy_gradient_loss | -0.000327     |
|    std                  | 3.63          |
|    value_loss           | 642           |
-------------------------------------------
Eval num_timesteps=2484000, episode_reward=442.61 +/- 153.92
Episode length: 416.40 +/- 42.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 443           |
| time/                   |               |
|    total_timesteps      | 2484000       |
| train/                  |               |
|    approx_kl            | 0.00084782386 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.001         |
|    loss                 | 882           |
|    n_updates            | 12120         |
|    policy_gradient_loss | -0.000222     |
|    std                  | 3.64          |
|    value_loss           | 2.37e+03      |
-------------------------------------------
Eval num_timesteps=2486000, episode_reward=577.35 +/- 503.98
Episode length: 455.20 +/- 65.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 577           |
| time/                   |               |
|    total_timesteps      | 2486000       |
| train/                  |               |
|    approx_kl            | 0.00033732582 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 4.06e+03      |
|    n_updates            | 12130         |
|    policy_gradient_loss | -8.59e-05     |
|    std                  | 3.64          |
|    value_loss           | 9.08e+03      |
-------------------------------------------
Eval num_timesteps=2488000, episode_reward=131.26 +/- 388.30
Episode length: 439.80 +/- 59.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | 131           |
| time/                   |               |
|    total_timesteps      | 2488000       |
| train/                  |               |
|    approx_kl            | 4.7253096e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.862         |
|    learning_rate        | 0.001         |
|    loss                 | 938           |
|    n_updates            | 12140         |
|    policy_gradient_loss | -9.45e-05     |
|    std                  | 3.64          |
|    value_loss           | 3.48e+03      |
-------------------------------------------
Eval num_timesteps=2490000, episode_reward=508.44 +/- 577.07
Episode length: 507.80 +/- 35.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 508           |
| time/                   |               |
|    total_timesteps      | 2490000       |
| train/                  |               |
|    approx_kl            | 0.00014301509 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.855         |
|    learning_rate        | 0.001         |
|    loss                 | 836           |
|    n_updates            | 12150         |
|    policy_gradient_loss | -0.000137     |
|    std                  | 3.65          |
|    value_loss           | 2.26e+03      |
-------------------------------------------
Eval num_timesteps=2492000, episode_reward=377.23 +/- 285.50
Episode length: 534.00 +/- 78.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 534          |
|    mean_reward          | 377          |
| time/                   |              |
|    total_timesteps      | 2492000      |
| train/                  |              |
|    approx_kl            | 0.0018122906 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.001        |
|    loss                 | 3e+03        |
|    n_updates            | 12160        |
|    policy_gradient_loss | -0.00168     |
|    std                  | 3.65         |
|    value_loss           | 7.63e+03     |
------------------------------------------
Eval num_timesteps=2494000, episode_reward=125.56 +/- 704.20
Episode length: 479.00 +/- 73.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 2494000      |
| train/                  |              |
|    approx_kl            | 0.0014765938 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 131          |
|    n_updates            | 12170        |
|    policy_gradient_loss | -0.000941    |
|    std                  | 3.66         |
|    value_loss           | 517          |
------------------------------------------
Eval num_timesteps=2496000, episode_reward=322.00 +/- 217.54
Episode length: 406.60 +/- 59.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 407         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 2496000     |
| train/                  |             |
|    approx_kl            | 0.006628093 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.001       |
|    loss                 | 3.24e+03    |
|    n_updates            | 12180       |
|    policy_gradient_loss | 3.46e-05    |
|    std                  | 3.67        |
|    value_loss           | 8.4e+03     |
-----------------------------------------
Eval num_timesteps=2498000, episode_reward=18.58 +/- 384.45
Episode length: 425.40 +/- 65.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 18.6          |
| time/                   |               |
|    total_timesteps      | 2498000       |
| train/                  |               |
|    approx_kl            | 0.00058152305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 4.48e+03      |
|    n_updates            | 12190         |
|    policy_gradient_loss | 0.00061       |
|    std                  | 3.67          |
|    value_loss           | 1.06e+04      |
-------------------------------------------
Eval num_timesteps=2500000, episode_reward=497.36 +/- 295.56
Episode length: 440.80 +/- 43.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 497          |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 8.775681e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+03     |
|    n_updates            | 12200        |
|    policy_gradient_loss | -0.000124    |
|    std                  | 3.67         |
|    value_loss           | 4.63e+03     |
------------------------------------------
Eval num_timesteps=2502000, episode_reward=-10.00 +/- 523.73
Episode length: 406.00 +/- 40.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | -10          |
| time/                   |              |
|    total_timesteps      | 2502000      |
| train/                  |              |
|    approx_kl            | 0.0001238246 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 391          |
|    n_updates            | 12210        |
|    policy_gradient_loss | -6.13e-05    |
|    std                  | 3.67         |
|    value_loss           | 1.73e+03     |
------------------------------------------
Eval num_timesteps=2504000, episode_reward=262.68 +/- 645.07
Episode length: 450.60 +/- 44.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 451         |
|    mean_reward          | 263         |
| time/                   |             |
|    total_timesteps      | 2504000     |
| train/                  |             |
|    approx_kl            | 0.002727234 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 49.5        |
|    n_updates            | 12220       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 3.68        |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=2506000, episode_reward=55.66 +/- 421.98
Episode length: 411.80 +/- 17.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 55.7         |
| time/                   |              |
|    total_timesteps      | 2506000      |
| train/                  |              |
|    approx_kl            | 0.0055121174 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.759        |
|    learning_rate        | 0.001        |
|    loss                 | 5.83e+03     |
|    n_updates            | 12230        |
|    policy_gradient_loss | -0.00172     |
|    std                  | 3.68         |
|    value_loss           | 1.45e+04     |
------------------------------------------
Eval num_timesteps=2508000, episode_reward=570.73 +/- 296.34
Episode length: 449.60 +/- 50.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 571           |
| time/                   |               |
|    total_timesteps      | 2508000       |
| train/                  |               |
|    approx_kl            | 0.00055531925 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.82          |
|    learning_rate        | 0.001         |
|    loss                 | 5.7e+03       |
|    n_updates            | 12240         |
|    policy_gradient_loss | 0.000291      |
|    std                  | 3.69          |
|    value_loss           | 1.29e+04      |
-------------------------------------------
Eval num_timesteps=2510000, episode_reward=21.76 +/- 447.01
Episode length: 430.00 +/- 34.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 21.8          |
| time/                   |               |
|    total_timesteps      | 2510000       |
| train/                  |               |
|    approx_kl            | 0.00046227712 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 80.4          |
|    n_updates            | 12250         |
|    policy_gradient_loss | -0.000521     |
|    std                  | 3.69          |
|    value_loss           | 562           |
-------------------------------------------
Eval num_timesteps=2512000, episode_reward=312.91 +/- 270.90
Episode length: 459.80 +/- 52.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 313           |
| time/                   |               |
|    total_timesteps      | 2512000       |
| train/                  |               |
|    approx_kl            | 0.00031165028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.001         |
|    loss                 | 866           |
|    n_updates            | 12260         |
|    policy_gradient_loss | 0.000502      |
|    std                  | 3.7           |
|    value_loss           | 2.62e+03      |
-------------------------------------------
Eval num_timesteps=2514000, episode_reward=174.63 +/- 118.85
Episode length: 439.20 +/- 72.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 439         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 2514000     |
| train/                  |             |
|    approx_kl            | 0.002623938 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 116         |
|    n_updates            | 12270       |
|    policy_gradient_loss | -0.00169    |
|    std                  | 3.7         |
|    value_loss           | 562         |
-----------------------------------------
Eval num_timesteps=2516000, episode_reward=2.79 +/- 630.06
Episode length: 431.00 +/- 41.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 2.79         |
| time/                   |              |
|    total_timesteps      | 2516000      |
| train/                  |              |
|    approx_kl            | 0.0036195447 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+03     |
|    n_updates            | 12280        |
|    policy_gradient_loss | 0.001        |
|    std                  | 3.71         |
|    value_loss           | 8.07e+03     |
------------------------------------------
Eval num_timesteps=2518000, episode_reward=-39.51 +/- 359.55
Episode length: 427.40 +/- 32.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | -39.5         |
| time/                   |               |
|    total_timesteps      | 2518000       |
| train/                  |               |
|    approx_kl            | 0.00034256006 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.806         |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+03       |
|    n_updates            | 12290         |
|    policy_gradient_loss | -0.000155     |
|    std                  | 3.71          |
|    value_loss           | 8.28e+03      |
-------------------------------------------
Eval num_timesteps=2520000, episode_reward=399.38 +/- 519.00
Episode length: 430.40 +/- 58.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 399           |
| time/                   |               |
|    total_timesteps      | 2520000       |
| train/                  |               |
|    approx_kl            | 0.00021363483 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.7         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 360           |
|    n_updates            | 12300         |
|    policy_gradient_loss | -0.000266     |
|    std                  | 3.71          |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=2522000, episode_reward=109.54 +/- 555.94
Episode length: 408.80 +/- 37.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 110          |
| time/                   |              |
|    total_timesteps      | 2522000      |
| train/                  |              |
|    approx_kl            | 0.0016896462 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 107          |
|    n_updates            | 12310        |
|    policy_gradient_loss | -0.000951    |
|    std                  | 3.71         |
|    value_loss           | 436          |
------------------------------------------
Eval num_timesteps=2524000, episode_reward=119.63 +/- 348.09
Episode length: 417.00 +/- 49.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 2524000      |
| train/                  |              |
|    approx_kl            | 0.0058568045 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 85.7         |
|    n_updates            | 12320        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 3.72         |
|    value_loss           | 332          |
------------------------------------------
Eval num_timesteps=2526000, episode_reward=129.93 +/- 356.98
Episode length: 464.20 +/- 56.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 130          |
| time/                   |              |
|    total_timesteps      | 2526000      |
| train/                  |              |
|    approx_kl            | 0.0012616044 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 4.17e+03     |
|    n_updates            | 12330        |
|    policy_gradient_loss | 0.000248     |
|    std                  | 3.73         |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=2528000, episode_reward=191.22 +/- 507.44
Episode length: 449.40 +/- 33.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 2528000      |
| train/                  |              |
|    approx_kl            | 0.0006757736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+03     |
|    n_updates            | 12340        |
|    policy_gradient_loss | -0.000773    |
|    std                  | 3.74         |
|    value_loss           | 4.86e+03     |
------------------------------------------
Eval num_timesteps=2530000, episode_reward=17.64 +/- 401.35
Episode length: 466.60 +/- 42.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 2530000     |
| train/                  |             |
|    approx_kl            | 0.001565424 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 116         |
|    n_updates            | 12350       |
|    policy_gradient_loss | -0.000688   |
|    std                  | 3.75        |
|    value_loss           | 368         |
-----------------------------------------
Eval num_timesteps=2532000, episode_reward=1002.07 +/- 834.90
Episode length: 535.40 +/- 17.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 535          |
|    mean_reward          | 1e+03        |
| time/                   |              |
|    total_timesteps      | 2532000      |
| train/                  |              |
|    approx_kl            | 0.0061425334 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 95.9         |
|    n_updates            | 12360        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 3.77         |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=2534000, episode_reward=613.71 +/- 354.13
Episode length: 516.40 +/- 5.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 614          |
| time/                   |              |
|    total_timesteps      | 2534000      |
| train/                  |              |
|    approx_kl            | 0.0016149648 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+03     |
|    n_updates            | 12370        |
|    policy_gradient_loss | 4.81e-06     |
|    std                  | 3.78         |
|    value_loss           | 3.65e+03     |
------------------------------------------
Eval num_timesteps=2536000, episode_reward=467.80 +/- 267.66
Episode length: 492.80 +/- 89.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 493          |
|    mean_reward          | 468          |
| time/                   |              |
|    total_timesteps      | 2536000      |
| train/                  |              |
|    approx_kl            | 0.0006581543 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.315        |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+03     |
|    n_updates            | 12380        |
|    policy_gradient_loss | -0.000847    |
|    std                  | 3.79         |
|    value_loss           | 5.94e+03     |
------------------------------------------
Eval num_timesteps=2538000, episode_reward=253.68 +/- 199.16
Episode length: 466.40 +/- 63.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 254           |
| time/                   |               |
|    total_timesteps      | 2538000       |
| train/                  |               |
|    approx_kl            | 0.00080688926 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.8         |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.001         |
|    loss                 | 495           |
|    n_updates            | 12390         |
|    policy_gradient_loss | 6.34e-05      |
|    std                  | 3.79          |
|    value_loss           | 1.33e+03      |
-------------------------------------------
Eval num_timesteps=2540000, episode_reward=468.68 +/- 508.88
Episode length: 507.60 +/- 45.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 469           |
| time/                   |               |
|    total_timesteps      | 2540000       |
| train/                  |               |
|    approx_kl            | 0.00064243143 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.8         |
|    explained_variance   | 0.735         |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+03      |
|    n_updates            | 12400         |
|    policy_gradient_loss | 0.000375      |
|    std                  | 3.79          |
|    value_loss           | 9.78e+03      |
-------------------------------------------
Eval num_timesteps=2542000, episode_reward=389.94 +/- 470.03
Episode length: 465.80 +/- 62.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 2542000      |
| train/                  |              |
|    approx_kl            | 0.0014126031 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 602          |
|    n_updates            | 12410        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 3.8          |
|    value_loss           | 1.75e+03     |
------------------------------------------
Eval num_timesteps=2544000, episode_reward=525.78 +/- 487.22
Episode length: 521.00 +/- 88.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 521          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 2544000      |
| train/                  |              |
|    approx_kl            | 0.0026961335 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.001        |
|    loss                 | 451          |
|    n_updates            | 12420        |
|    policy_gradient_loss | -0.000768    |
|    std                  | 3.81         |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=2546000, episode_reward=392.48 +/- 298.22
Episode length: 476.40 +/- 57.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 392          |
| time/                   |              |
|    total_timesteps      | 2546000      |
| train/                  |              |
|    approx_kl            | 0.0011694868 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 447          |
|    n_updates            | 12430        |
|    policy_gradient_loss | 0.000151     |
|    std                  | 3.82         |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=2548000, episode_reward=141.46 +/- 438.78
Episode length: 449.60 +/- 53.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 141         |
| time/                   |             |
|    total_timesteps      | 2548000     |
| train/                  |             |
|    approx_kl            | 0.002510973 |
|    clip_fraction        | 0.00239     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.001       |
|    loss                 | 243         |
|    n_updates            | 12440       |
|    policy_gradient_loss | -0.000733   |
|    std                  | 3.83        |
|    value_loss           | 842         |
-----------------------------------------
Eval num_timesteps=2550000, episode_reward=12.50 +/- 449.45
Episode length: 528.80 +/- 73.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 529        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 2550000    |
| train/                  |            |
|    approx_kl            | 0.00526123 |
|    clip_fraction        | 0.0105     |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.001      |
|    loss                 | 127        |
|    n_updates            | 12450      |
|    policy_gradient_loss | -0.00139   |
|    std                  | 3.84       |
|    value_loss           | 401        |
----------------------------------------
Eval num_timesteps=2552000, episode_reward=444.10 +/- 308.65
Episode length: 450.20 +/- 52.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 444         |
| time/                   |             |
|    total_timesteps      | 2552000     |
| train/                  |             |
|    approx_kl            | 0.003163607 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 53.1        |
|    n_updates            | 12460       |
|    policy_gradient_loss | -0.00106    |
|    std                  | 3.87        |
|    value_loss           | 217         |
-----------------------------------------
Eval num_timesteps=2554000, episode_reward=92.44 +/- 35.18
Episode length: 502.20 +/- 111.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 502          |
|    mean_reward          | 92.4         |
| time/                   |              |
|    total_timesteps      | 2554000      |
| train/                  |              |
|    approx_kl            | 0.0021831808 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 12470        |
|    policy_gradient_loss | 0.000774     |
|    std                  | 3.88         |
|    value_loss           | 340          |
------------------------------------------
Eval num_timesteps=2556000, episode_reward=210.29 +/- 134.40
Episode length: 511.40 +/- 43.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 2556000     |
| train/                  |             |
|    approx_kl            | 0.007027127 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 89.3        |
|    n_updates            | 12480       |
|    policy_gradient_loss | -0.00327    |
|    std                  | 3.89        |
|    value_loss           | 337         |
-----------------------------------------
Eval num_timesteps=2558000, episode_reward=390.75 +/- 314.57
Episode length: 495.60 +/- 39.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 391          |
| time/                   |              |
|    total_timesteps      | 2558000      |
| train/                  |              |
|    approx_kl            | 0.0029876044 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+03     |
|    n_updates            | 12490        |
|    policy_gradient_loss | -0.00246     |
|    std                  | 3.89         |
|    value_loss           | 6.27e+03     |
------------------------------------------
Eval num_timesteps=2560000, episode_reward=64.89 +/- 559.48
Episode length: 565.20 +/- 77.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 565      |
|    mean_reward     | 64.9     |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2562000, episode_reward=210.18 +/- 482.53
Episode length: 515.40 +/- 40.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 210          |
| time/                   |              |
|    total_timesteps      | 2562000      |
| train/                  |              |
|    approx_kl            | 0.0027906455 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 126          |
|    n_updates            | 12500        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 3.9          |
|    value_loss           | 439          |
------------------------------------------
Eval num_timesteps=2564000, episode_reward=176.51 +/- 335.17
Episode length: 485.60 +/- 74.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 2564000      |
| train/                  |              |
|    approx_kl            | 0.0014311871 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 345          |
|    n_updates            | 12510        |
|    policy_gradient_loss | -0.000508    |
|    std                  | 3.9          |
|    value_loss           | 1.29e+03     |
------------------------------------------
Eval num_timesteps=2566000, episode_reward=399.89 +/- 333.64
Episode length: 465.60 +/- 45.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 2566000      |
| train/                  |              |
|    approx_kl            | 0.0009427528 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 12520        |
|    policy_gradient_loss | -0.000341    |
|    std                  | 3.91         |
|    value_loss           | 3.41e+03     |
------------------------------------------
Eval num_timesteps=2568000, episode_reward=475.16 +/- 385.95
Episode length: 519.40 +/- 82.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 519           |
|    mean_reward          | 475           |
| time/                   |               |
|    total_timesteps      | 2568000       |
| train/                  |               |
|    approx_kl            | 0.00015969048 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.9         |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.001         |
|    loss                 | 184           |
|    n_updates            | 12530         |
|    policy_gradient_loss | -0.000113     |
|    std                  | 3.92          |
|    value_loss           | 839           |
-------------------------------------------
Eval num_timesteps=2570000, episode_reward=326.00 +/- 489.47
Episode length: 513.00 +/- 56.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 513           |
|    mean_reward          | 326           |
| time/                   |               |
|    total_timesteps      | 2570000       |
| train/                  |               |
|    approx_kl            | 0.00026858185 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.83          |
|    learning_rate        | 0.001         |
|    loss                 | 2.25e+03      |
|    n_updates            | 12540         |
|    policy_gradient_loss | 5.56e-06      |
|    std                  | 3.93          |
|    value_loss           | 5.64e+03      |
-------------------------------------------
Eval num_timesteps=2572000, episode_reward=401.58 +/- 325.93
Episode length: 472.20 +/- 106.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 402          |
| time/                   |              |
|    total_timesteps      | 2572000      |
| train/                  |              |
|    approx_kl            | 0.0024379888 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 135          |
|    n_updates            | 12550        |
|    policy_gradient_loss | -0.00139     |
|    std                  | 3.94         |
|    value_loss           | 515          |
------------------------------------------
Eval num_timesteps=2574000, episode_reward=184.30 +/- 243.58
Episode length: 419.60 +/- 86.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 2574000      |
| train/                  |              |
|    approx_kl            | 0.0044454243 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 95.1         |
|    n_updates            | 12560        |
|    policy_gradient_loss | -0.000693    |
|    std                  | 3.95         |
|    value_loss           | 374          |
------------------------------------------
Eval num_timesteps=2576000, episode_reward=289.39 +/- 165.89
Episode length: 397.60 +/- 82.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 2576000     |
| train/                  |             |
|    approx_kl            | 0.002196724 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 48.6        |
|    n_updates            | 12570       |
|    policy_gradient_loss | -0.000913   |
|    std                  | 3.96        |
|    value_loss           | 191         |
-----------------------------------------
Eval num_timesteps=2578000, episode_reward=423.54 +/- 510.73
Episode length: 376.80 +/- 102.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 2578000      |
| train/                  |              |
|    approx_kl            | 0.0024123262 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 2.64e+03     |
|    n_updates            | 12580        |
|    policy_gradient_loss | 5.29e-05     |
|    std                  | 3.96         |
|    value_loss           | 6.07e+03     |
------------------------------------------
Eval num_timesteps=2580000, episode_reward=345.92 +/- 177.99
Episode length: 361.00 +/- 16.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 361           |
|    mean_reward          | 346           |
| time/                   |               |
|    total_timesteps      | 2580000       |
| train/                  |               |
|    approx_kl            | 0.00033697754 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.785         |
|    learning_rate        | 0.001         |
|    loss                 | 3.32e+03      |
|    n_updates            | 12590         |
|    policy_gradient_loss | 6.16e-06      |
|    std                  | 3.96          |
|    value_loss           | 8.74e+03      |
-------------------------------------------
Eval num_timesteps=2582000, episode_reward=284.20 +/- 164.48
Episode length: 396.60 +/- 103.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 2582000      |
| train/                  |              |
|    approx_kl            | 0.0017405802 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 242          |
|    n_updates            | 12600        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 3.96         |
|    value_loss           | 683          |
------------------------------------------
Eval num_timesteps=2584000, episode_reward=467.20 +/- 99.94
Episode length: 359.80 +/- 26.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 360         |
|    mean_reward          | 467         |
| time/                   |             |
|    total_timesteps      | 2584000     |
| train/                  |             |
|    approx_kl            | 0.002762096 |
|    clip_fraction        | 0.00171     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.001       |
|    loss                 | 1.66e+03    |
|    n_updates            | 12610       |
|    policy_gradient_loss | 0.00108     |
|    std                  | 3.96        |
|    value_loss           | 4.11e+03    |
-----------------------------------------
Eval num_timesteps=2586000, episode_reward=73.77 +/- 123.58
Episode length: 349.20 +/- 33.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 349           |
|    mean_reward          | 73.8          |
| time/                   |               |
|    total_timesteps      | 2586000       |
| train/                  |               |
|    approx_kl            | 0.00028735082 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 58            |
|    n_updates            | 12620         |
|    policy_gradient_loss | -9.93e-05     |
|    std                  | 3.96          |
|    value_loss           | 258           |
-------------------------------------------
Eval num_timesteps=2588000, episode_reward=249.25 +/- 210.23
Episode length: 333.80 +/- 41.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 334           |
|    mean_reward          | 249           |
| time/                   |               |
|    total_timesteps      | 2588000       |
| train/                  |               |
|    approx_kl            | 0.00035413096 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 65.9          |
|    n_updates            | 12630         |
|    policy_gradient_loss | -0.000266     |
|    std                  | 3.95          |
|    value_loss           | 286           |
-------------------------------------------
Eval num_timesteps=2590000, episode_reward=289.18 +/- 108.22
Episode length: 388.40 +/- 54.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 289           |
| time/                   |               |
|    total_timesteps      | 2590000       |
| train/                  |               |
|    approx_kl            | 0.00021058862 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+03      |
|    n_updates            | 12640         |
|    policy_gradient_loss | 0.000414      |
|    std                  | 3.95          |
|    value_loss           | 4.28e+03      |
-------------------------------------------
Eval num_timesteps=2592000, episode_reward=121.91 +/- 78.68
Episode length: 336.40 +/- 40.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 336           |
|    mean_reward          | 122           |
| time/                   |               |
|    total_timesteps      | 2592000       |
| train/                  |               |
|    approx_kl            | 0.00015806308 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 78.5          |
|    n_updates            | 12650         |
|    policy_gradient_loss | -0.000162     |
|    std                  | 3.96          |
|    value_loss           | 320           |
-------------------------------------------
Eval num_timesteps=2594000, episode_reward=281.32 +/- 56.47
Episode length: 379.00 +/- 34.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 281          |
| time/                   |              |
|    total_timesteps      | 2594000      |
| train/                  |              |
|    approx_kl            | 0.0025335993 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 45           |
|    n_updates            | 12660        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 3.97         |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=2596000, episode_reward=302.41 +/- 680.49
Episode length: 410.20 +/- 60.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 302          |
| time/                   |              |
|    total_timesteps      | 2596000      |
| train/                  |              |
|    approx_kl            | 0.0038918098 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 79.2         |
|    n_updates            | 12670        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 3.97         |
|    value_loss           | 287          |
------------------------------------------
Eval num_timesteps=2598000, episode_reward=387.95 +/- 455.35
Episode length: 403.80 +/- 31.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 2598000      |
| train/                  |              |
|    approx_kl            | 0.0011831888 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.771        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 12680        |
|    policy_gradient_loss | 0.000131     |
|    std                  | 3.95         |
|    value_loss           | 5.39e+03     |
------------------------------------------
Eval num_timesteps=2600000, episode_reward=499.15 +/- 290.80
Episode length: 516.40 +/- 77.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 499         |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.001475987 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 35.6        |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.000842   |
|    std                  | 3.95        |
|    value_loss           | 152         |
-----------------------------------------
Eval num_timesteps=2602000, episode_reward=571.68 +/- 347.43
Episode length: 481.00 +/- 50.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 572          |
| time/                   |              |
|    total_timesteps      | 2602000      |
| train/                  |              |
|    approx_kl            | 0.0019002222 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 152          |
|    n_updates            | 12700        |
|    policy_gradient_loss | 0.000337     |
|    std                  | 3.95         |
|    value_loss           | 687          |
------------------------------------------
Eval num_timesteps=2604000, episode_reward=805.38 +/- 698.20
Episode length: 554.00 +/- 90.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 554        |
|    mean_reward          | 805        |
| time/                   |            |
|    total_timesteps      | 2604000    |
| train/                  |            |
|    approx_kl            | 0.00217995 |
|    clip_fraction        | 0.000781   |
|    clip_range           | 0.2        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.001      |
|    loss                 | 204        |
|    n_updates            | 12710      |
|    policy_gradient_loss | -0.000911  |
|    std                  | 3.96       |
|    value_loss           | 589        |
----------------------------------------
Eval num_timesteps=2606000, episode_reward=347.93 +/- 305.23
Episode length: 493.00 +/- 43.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 493          |
|    mean_reward          | 348          |
| time/                   |              |
|    total_timesteps      | 2606000      |
| train/                  |              |
|    approx_kl            | 0.0017625254 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 3.74e+03     |
|    n_updates            | 12720        |
|    policy_gradient_loss | -0.000158    |
|    std                  | 3.96         |
|    value_loss           | 8.61e+03     |
------------------------------------------
Eval num_timesteps=2608000, episode_reward=247.30 +/- 192.80
Episode length: 558.40 +/- 40.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 558           |
|    mean_reward          | 247           |
| time/                   |               |
|    total_timesteps      | 2608000       |
| train/                  |               |
|    approx_kl            | 0.00029939023 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 114           |
|    n_updates            | 12730         |
|    policy_gradient_loss | -0.000403     |
|    std                  | 3.97          |
|    value_loss           | 570           |
-------------------------------------------
Eval num_timesteps=2610000, episode_reward=747.38 +/- 375.36
Episode length: 497.40 +/- 141.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 747          |
| time/                   |              |
|    total_timesteps      | 2610000      |
| train/                  |              |
|    approx_kl            | 0.0014270198 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 62.1         |
|    n_updates            | 12740        |
|    policy_gradient_loss | -0.000539    |
|    std                  | 3.98         |
|    value_loss           | 267          |
------------------------------------------
Eval num_timesteps=2612000, episode_reward=306.15 +/- 691.74
Episode length: 485.80 +/- 47.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 306           |
| time/                   |               |
|    total_timesteps      | 2612000       |
| train/                  |               |
|    approx_kl            | 0.00063091144 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.611         |
|    learning_rate        | 0.001         |
|    loss                 | 8.22e+03      |
|    n_updates            | 12750         |
|    policy_gradient_loss | -0.000287     |
|    std                  | 3.99          |
|    value_loss           | 1.96e+04      |
-------------------------------------------
Eval num_timesteps=2614000, episode_reward=389.78 +/- 224.03
Episode length: 451.20 +/- 42.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 2614000      |
| train/                  |              |
|    approx_kl            | 0.0015970708 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 123          |
|    n_updates            | 12760        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 4            |
|    value_loss           | 523          |
------------------------------------------
Eval num_timesteps=2616000, episode_reward=163.60 +/- 266.13
Episode length: 433.60 +/- 61.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 434         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 2616000     |
| train/                  |             |
|    approx_kl            | 0.002145485 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 3.4e+03     |
|    n_updates            | 12770       |
|    policy_gradient_loss | -0.000484   |
|    std                  | 4           |
|    value_loss           | 8.29e+03    |
-----------------------------------------
Eval num_timesteps=2618000, episode_reward=427.75 +/- 334.94
Episode length: 424.40 +/- 52.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 428           |
| time/                   |               |
|    total_timesteps      | 2618000       |
| train/                  |               |
|    approx_kl            | 0.00036413767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.001         |
|    loss                 | 181           |
|    n_updates            | 12780         |
|    policy_gradient_loss | -0.000139     |
|    std                  | 4             |
|    value_loss           | 1.2e+03       |
-------------------------------------------
Eval num_timesteps=2620000, episode_reward=198.74 +/- 389.30
Episode length: 469.60 +/- 93.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 199           |
| time/                   |               |
|    total_timesteps      | 2620000       |
| train/                  |               |
|    approx_kl            | 0.00020356377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11           |
|    explained_variance   | 0.872         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+03      |
|    n_updates            | 12790         |
|    policy_gradient_loss | -0.000436     |
|    std                  | 4             |
|    value_loss           | 4.31e+03      |
-------------------------------------------
Eval num_timesteps=2622000, episode_reward=412.49 +/- 398.50
Episode length: 475.20 +/- 40.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | 412          |
| time/                   |              |
|    total_timesteps      | 2622000      |
| train/                  |              |
|    approx_kl            | 0.0027426323 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 43.4         |
|    n_updates            | 12800        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 4.02         |
|    value_loss           | 167          |
------------------------------------------
Eval num_timesteps=2624000, episode_reward=677.92 +/- 340.44
Episode length: 497.20 +/- 63.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 678          |
| time/                   |              |
|    total_timesteps      | 2624000      |
| train/                  |              |
|    approx_kl            | 0.0036179556 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+03     |
|    n_updates            | 12810        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 4.03         |
|    value_loss           | 3.28e+03     |
------------------------------------------
Eval num_timesteps=2626000, episode_reward=404.91 +/- 303.31
Episode length: 481.60 +/- 64.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 405          |
| time/                   |              |
|    total_timesteps      | 2626000      |
| train/                  |              |
|    approx_kl            | 0.0008229292 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 241          |
|    n_updates            | 12820        |
|    policy_gradient_loss | 0.000285     |
|    std                  | 4.04         |
|    value_loss           | 751          |
------------------------------------------
Eval num_timesteps=2628000, episode_reward=310.39 +/- 376.82
Episode length: 492.20 +/- 85.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 492           |
|    mean_reward          | 310           |
| time/                   |               |
|    total_timesteps      | 2628000       |
| train/                  |               |
|    approx_kl            | 4.7068053e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.001         |
|    loss                 | 426           |
|    n_updates            | 12830         |
|    policy_gradient_loss | 1.65e-05      |
|    std                  | 4.04          |
|    value_loss           | 1.47e+03      |
-------------------------------------------
Eval num_timesteps=2630000, episode_reward=427.94 +/- 337.78
Episode length: 471.60 +/- 91.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 472           |
|    mean_reward          | 428           |
| time/                   |               |
|    total_timesteps      | 2630000       |
| train/                  |               |
|    approx_kl            | 6.9644244e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.001         |
|    loss                 | 238           |
|    n_updates            | 12840         |
|    policy_gradient_loss | -3.39e-05     |
|    std                  | 4.05          |
|    value_loss           | 704           |
-------------------------------------------
Eval num_timesteps=2632000, episode_reward=148.57 +/- 400.61
Episode length: 424.60 +/- 78.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 2632000      |
| train/                  |              |
|    approx_kl            | 0.0024566848 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 222          |
|    n_updates            | 12850        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 4.06         |
|    value_loss           | 681          |
------------------------------------------
Eval num_timesteps=2634000, episode_reward=326.19 +/- 337.71
Episode length: 535.80 +/- 30.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 536         |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 2634000     |
| train/                  |             |
|    approx_kl            | 0.003787056 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 166         |
|    n_updates            | 12860       |
|    policy_gradient_loss | -0.00117    |
|    std                  | 4.06        |
|    value_loss           | 591         |
-----------------------------------------
Eval num_timesteps=2636000, episode_reward=227.74 +/- 477.01
Episode length: 487.00 +/- 64.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 228           |
| time/                   |               |
|    total_timesteps      | 2636000       |
| train/                  |               |
|    approx_kl            | 0.00015004259 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 280           |
|    n_updates            | 12870         |
|    policy_gradient_loss | 0.000194      |
|    std                  | 4.05          |
|    value_loss           | 892           |
-------------------------------------------
Eval num_timesteps=2638000, episode_reward=163.20 +/- 121.59
Episode length: 447.40 +/- 113.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 163          |
| time/                   |              |
|    total_timesteps      | 2638000      |
| train/                  |              |
|    approx_kl            | 0.0019841925 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.001        |
|    loss                 | 454          |
|    n_updates            | 12880        |
|    policy_gradient_loss | -0.000458    |
|    std                  | 4.03         |
|    value_loss           | 1.81e+03     |
------------------------------------------
Eval num_timesteps=2640000, episode_reward=-76.15 +/- 576.59
Episode length: 490.60 +/- 57.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 491          |
|    mean_reward          | -76.1        |
| time/                   |              |
|    total_timesteps      | 2640000      |
| train/                  |              |
|    approx_kl            | 0.0013268242 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 222          |
|    n_updates            | 12890        |
|    policy_gradient_loss | -0.000659    |
|    std                  | 4.03         |
|    value_loss           | 751          |
------------------------------------------
Eval num_timesteps=2642000, episode_reward=91.88 +/- 399.67
Episode length: 466.00 +/- 97.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 91.9        |
| time/                   |             |
|    total_timesteps      | 2642000     |
| train/                  |             |
|    approx_kl            | 0.002752848 |
|    clip_fraction        | 0.00142     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 90.3        |
|    n_updates            | 12900       |
|    policy_gradient_loss | -0.00145    |
|    std                  | 4.04        |
|    value_loss           | 323         |
-----------------------------------------
Eval num_timesteps=2644000, episode_reward=829.68 +/- 409.68
Episode length: 441.20 +/- 55.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 830          |
| time/                   |              |
|    total_timesteps      | 2644000      |
| train/                  |              |
|    approx_kl            | 0.0019956725 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.612        |
|    learning_rate        | 0.001        |
|    loss                 | 4.22e+03     |
|    n_updates            | 12910        |
|    policy_gradient_loss | 0.000758     |
|    std                  | 4.05         |
|    value_loss           | 1.06e+04     |
------------------------------------------
Eval num_timesteps=2646000, episode_reward=263.87 +/- 380.63
Episode length: 448.40 +/- 43.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 2646000  |
---------------------------------
Eval num_timesteps=2648000, episode_reward=558.16 +/- 621.17
Episode length: 418.60 +/- 80.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 558          |
| time/                   |              |
|    total_timesteps      | 2648000      |
| train/                  |              |
|    approx_kl            | 0.0016755154 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 168          |
|    n_updates            | 12920        |
|    policy_gradient_loss | -0.00223     |
|    std                  | 4.06         |
|    value_loss           | 742          |
------------------------------------------
Eval num_timesteps=2650000, episode_reward=243.67 +/- 209.84
Episode length: 387.60 +/- 48.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 244           |
| time/                   |               |
|    total_timesteps      | 2650000       |
| train/                  |               |
|    approx_kl            | 0.00034311123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.692         |
|    learning_rate        | 0.001         |
|    loss                 | 2.87e+03      |
|    n_updates            | 12930         |
|    policy_gradient_loss | 0.000271      |
|    std                  | 4.06          |
|    value_loss           | 8.58e+03      |
-------------------------------------------
Eval num_timesteps=2652000, episode_reward=-25.21 +/- 265.91
Episode length: 411.40 +/- 36.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | -25.2         |
| time/                   |               |
|    total_timesteps      | 2652000       |
| train/                  |               |
|    approx_kl            | 0.00017011559 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.001         |
|    loss                 | 2.99e+03      |
|    n_updates            | 12940         |
|    policy_gradient_loss | -0.0003       |
|    std                  | 4.06          |
|    value_loss           | 7.58e+03      |
-------------------------------------------
Eval num_timesteps=2654000, episode_reward=215.81 +/- 561.03
Episode length: 482.20 +/- 104.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 216           |
| time/                   |               |
|    total_timesteps      | 2654000       |
| train/                  |               |
|    approx_kl            | 0.00031897772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.813         |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+03      |
|    n_updates            | 12950         |
|    policy_gradient_loss | -0.000576     |
|    std                  | 4.06          |
|    value_loss           | 6.41e+03      |
-------------------------------------------
Eval num_timesteps=2656000, episode_reward=454.05 +/- 225.72
Episode length: 418.80 +/- 89.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 454          |
| time/                   |              |
|    total_timesteps      | 2656000      |
| train/                  |              |
|    approx_kl            | 0.0003427036 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+03     |
|    n_updates            | 12960        |
|    policy_gradient_loss | -8.21e-05    |
|    std                  | 4.06         |
|    value_loss           | 4.6e+03      |
------------------------------------------
Eval num_timesteps=2658000, episode_reward=36.88 +/- 317.74
Episode length: 408.20 +/- 74.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 36.9         |
| time/                   |              |
|    total_timesteps      | 2658000      |
| train/                  |              |
|    approx_kl            | 7.755068e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+03     |
|    n_updates            | 12970        |
|    policy_gradient_loss | -9.02e-05    |
|    std                  | 4.07         |
|    value_loss           | 4.99e+03     |
------------------------------------------
Eval num_timesteps=2660000, episode_reward=193.06 +/- 336.37
Episode length: 398.40 +/- 41.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 2660000      |
| train/                  |              |
|    approx_kl            | 0.0001590256 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 12980        |
|    policy_gradient_loss | -0.000294    |
|    std                  | 4.07         |
|    value_loss           | 4.09e+03     |
------------------------------------------
Eval num_timesteps=2662000, episode_reward=-233.78 +/- 387.53
Episode length: 513.00 +/- 108.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 513          |
|    mean_reward          | -234         |
| time/                   |              |
|    total_timesteps      | 2662000      |
| train/                  |              |
|    approx_kl            | 0.0010057878 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 1.55e+03     |
|    n_updates            | 12990        |
|    policy_gradient_loss | -0.000624    |
|    std                  | 4.07         |
|    value_loss           | 3.87e+03     |
------------------------------------------
Eval num_timesteps=2664000, episode_reward=409.85 +/- 211.96
Episode length: 442.60 +/- 57.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 410           |
| time/                   |               |
|    total_timesteps      | 2664000       |
| train/                  |               |
|    approx_kl            | 0.00016919512 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.799         |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+03      |
|    n_updates            | 13000         |
|    policy_gradient_loss | 0.000103      |
|    std                  | 4.07          |
|    value_loss           | 6.62e+03      |
-------------------------------------------
Eval num_timesteps=2666000, episode_reward=380.52 +/- 476.74
Episode length: 487.20 +/- 47.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 381         |
| time/                   |             |
|    total_timesteps      | 2666000     |
| train/                  |             |
|    approx_kl            | 7.31608e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.001       |
|    loss                 | 1.03e+03    |
|    n_updates            | 13010       |
|    policy_gradient_loss | -0.000149   |
|    std                  | 4.07        |
|    value_loss           | 2.92e+03    |
-----------------------------------------
Eval num_timesteps=2668000, episode_reward=425.62 +/- 409.16
Episode length: 475.20 +/- 85.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | 426          |
| time/                   |              |
|    total_timesteps      | 2668000      |
| train/                  |              |
|    approx_kl            | 0.0030405757 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 158          |
|    n_updates            | 13020        |
|    policy_gradient_loss | -0.00217     |
|    std                  | 4.08         |
|    value_loss           | 855          |
------------------------------------------
Eval num_timesteps=2670000, episode_reward=664.35 +/- 524.30
Episode length: 432.40 +/- 81.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 664          |
| time/                   |              |
|    total_timesteps      | 2670000      |
| train/                  |              |
|    approx_kl            | 0.0032764615 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+03     |
|    n_updates            | 13030        |
|    policy_gradient_loss | 0.00178      |
|    std                  | 4.08         |
|    value_loss           | 3.74e+03     |
------------------------------------------
Eval num_timesteps=2672000, episode_reward=359.12 +/- 857.66
Episode length: 498.40 +/- 88.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 359          |
| time/                   |              |
|    total_timesteps      | 2672000      |
| train/                  |              |
|    approx_kl            | 0.0014995966 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 873          |
|    n_updates            | 13040        |
|    policy_gradient_loss | -0.00123     |
|    std                  | 4.09         |
|    value_loss           | 2.33e+03     |
------------------------------------------
Eval num_timesteps=2674000, episode_reward=386.04 +/- 318.71
Episode length: 573.80 +/- 61.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 574          |
|    mean_reward          | 386          |
| time/                   |              |
|    total_timesteps      | 2674000      |
| train/                  |              |
|    approx_kl            | 0.0027168004 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 3e+03        |
|    n_updates            | 13050        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 4.1          |
|    value_loss           | 7.07e+03     |
------------------------------------------
Eval num_timesteps=2676000, episode_reward=747.24 +/- 696.80
Episode length: 523.80 +/- 95.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 524           |
|    mean_reward          | 747           |
| time/                   |               |
|    total_timesteps      | 2676000       |
| train/                  |               |
|    approx_kl            | 0.00043514866 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+03      |
|    n_updates            | 13060         |
|    policy_gradient_loss | 5.56e-05      |
|    std                  | 4.11          |
|    value_loss           | 4.3e+03       |
-------------------------------------------
Eval num_timesteps=2678000, episode_reward=633.85 +/- 431.36
Episode length: 562.00 +/- 16.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 562           |
|    mean_reward          | 634           |
| time/                   |               |
|    total_timesteps      | 2678000       |
| train/                  |               |
|    approx_kl            | 0.00019895015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.001         |
|    loss                 | 268           |
|    n_updates            | 13070         |
|    policy_gradient_loss | -0.000159     |
|    std                  | 4.11          |
|    value_loss           | 799           |
-------------------------------------------
Eval num_timesteps=2680000, episode_reward=339.62 +/- 374.69
Episode length: 542.00 +/- 28.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | 340          |
| time/                   |              |
|    total_timesteps      | 2680000      |
| train/                  |              |
|    approx_kl            | 0.0009452256 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 101          |
|    n_updates            | 13080        |
|    policy_gradient_loss | -0.000561    |
|    std                  | 4.11         |
|    value_loss           | 492          |
------------------------------------------
Eval num_timesteps=2682000, episode_reward=45.21 +/- 264.90
Episode length: 539.20 +/- 76.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 539         |
|    mean_reward          | 45.2        |
| time/                   |             |
|    total_timesteps      | 2682000     |
| train/                  |             |
|    approx_kl            | 0.006024642 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 13090       |
|    policy_gradient_loss | -0.00187    |
|    std                  | 4.11        |
|    value_loss           | 409         |
-----------------------------------------
Eval num_timesteps=2684000, episode_reward=66.58 +/- 213.94
Episode length: 536.60 +/- 76.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 66.6         |
| time/                   |              |
|    total_timesteps      | 2684000      |
| train/                  |              |
|    approx_kl            | 0.0026454618 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 13100        |
|    policy_gradient_loss | -0.000117    |
|    std                  | 4.12         |
|    value_loss           | 562          |
------------------------------------------
Eval num_timesteps=2686000, episode_reward=244.15 +/- 169.05
Episode length: 597.80 +/- 8.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 598          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 2686000      |
| train/                  |              |
|    approx_kl            | 0.0010870113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.001        |
|    loss                 | 116          |
|    n_updates            | 13110        |
|    policy_gradient_loss | -9.83e-05    |
|    std                  | 4.13         |
|    value_loss           | 571          |
------------------------------------------
Eval num_timesteps=2688000, episode_reward=140.76 +/- 202.78
Episode length: 535.00 +/- 85.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 535           |
|    mean_reward          | 141           |
| time/                   |               |
|    total_timesteps      | 2688000       |
| train/                  |               |
|    approx_kl            | 0.00057268253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.75          |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 13120         |
|    policy_gradient_loss | -0.000302     |
|    std                  | 4.14          |
|    value_loss           | 4.96e+03      |
-------------------------------------------
Eval num_timesteps=2690000, episode_reward=299.61 +/- 327.50
Episode length: 581.60 +/- 83.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 582           |
|    mean_reward          | 300           |
| time/                   |               |
|    total_timesteps      | 2690000       |
| train/                  |               |
|    approx_kl            | 0.00016818417 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.624         |
|    learning_rate        | 0.001         |
|    loss                 | 2.85e+03      |
|    n_updates            | 13130         |
|    policy_gradient_loss | 6.29e-06      |
|    std                  | 4.14          |
|    value_loss           | 6.49e+03      |
-------------------------------------------
Eval num_timesteps=2692000, episode_reward=525.03 +/- 537.04
Episode length: 581.00 +/- 51.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 581          |
|    mean_reward          | 525          |
| time/                   |              |
|    total_timesteps      | 2692000      |
| train/                  |              |
|    approx_kl            | 0.0022388338 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 13140        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 4.15         |
|    value_loss           | 480          |
------------------------------------------
Eval num_timesteps=2694000, episode_reward=-21.81 +/- 225.91
Episode length: 512.60 +/- 43.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 513          |
|    mean_reward          | -21.8        |
| time/                   |              |
|    total_timesteps      | 2694000      |
| train/                  |              |
|    approx_kl            | 0.0033549676 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 2.16e+03     |
|    n_updates            | 13150        |
|    policy_gradient_loss | 0.000145     |
|    std                  | 4.16         |
|    value_loss           | 5.5e+03      |
------------------------------------------
Eval num_timesteps=2696000, episode_reward=266.41 +/- 208.91
Episode length: 524.80 +/- 46.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 2696000      |
| train/                  |              |
|    approx_kl            | 0.0010437795 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 13160        |
|    policy_gradient_loss | -0.000333    |
|    std                  | 4.17         |
|    value_loss           | 427          |
------------------------------------------
Eval num_timesteps=2698000, episode_reward=534.95 +/- 478.02
Episode length: 518.40 +/- 60.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | 535          |
| time/                   |              |
|    total_timesteps      | 2698000      |
| train/                  |              |
|    approx_kl            | 0.0027591097 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 174          |
|    n_updates            | 13170        |
|    policy_gradient_loss | -0.000908    |
|    std                  | 4.17         |
|    value_loss           | 497          |
------------------------------------------
Eval num_timesteps=2700000, episode_reward=234.22 +/- 594.11
Episode length: 488.40 +/- 56.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 234          |
| time/                   |              |
|    total_timesteps      | 2700000      |
| train/                  |              |
|    approx_kl            | 0.0010511691 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+03      |
|    n_updates            | 13180        |
|    policy_gradient_loss | -2.53e-05    |
|    std                  | 4.18         |
|    value_loss           | 4.01e+03     |
------------------------------------------
Eval num_timesteps=2702000, episode_reward=221.51 +/- 236.27
Episode length: 477.00 +/- 46.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 2702000      |
| train/                  |              |
|    approx_kl            | 0.0009679828 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 239          |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.000747    |
|    std                  | 4.18         |
|    value_loss           | 739          |
------------------------------------------
Eval num_timesteps=2704000, episode_reward=233.30 +/- 254.48
Episode length: 450.40 +/- 65.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 233          |
| time/                   |              |
|    total_timesteps      | 2704000      |
| train/                  |              |
|    approx_kl            | 0.0020633577 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 2.58e+03     |
|    n_updates            | 13200        |
|    policy_gradient_loss | -0.000945    |
|    std                  | 4.19         |
|    value_loss           | 6.24e+03     |
------------------------------------------
Eval num_timesteps=2706000, episode_reward=274.23 +/- 148.85
Episode length: 461.20 +/- 65.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 274          |
| time/                   |              |
|    total_timesteps      | 2706000      |
| train/                  |              |
|    approx_kl            | 0.0011879548 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.853        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 13210        |
|    policy_gradient_loss | -0.000313    |
|    std                  | 4.19         |
|    value_loss           | 3.49e+03     |
------------------------------------------
Eval num_timesteps=2708000, episode_reward=555.52 +/- 431.71
Episode length: 484.00 +/- 68.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 2708000      |
| train/                  |              |
|    approx_kl            | 8.386094e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 13220        |
|    policy_gradient_loss | 0.000442     |
|    std                  | 4.19         |
|    value_loss           | 4.4e+03      |
------------------------------------------
Eval num_timesteps=2710000, episode_reward=242.50 +/- 435.19
Episode length: 476.00 +/- 63.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 2710000     |
| train/                  |             |
|    approx_kl            | 0.002961681 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 13230       |
|    policy_gradient_loss | -0.00156    |
|    std                  | 4.2         |
|    value_loss           | 569         |
-----------------------------------------
Eval num_timesteps=2712000, episode_reward=530.31 +/- 507.25
Episode length: 537.60 +/- 18.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 538         |
|    mean_reward          | 530         |
| time/                   |             |
|    total_timesteps      | 2712000     |
| train/                  |             |
|    approx_kl            | 0.002888859 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 291         |
|    n_updates            | 13240       |
|    policy_gradient_loss | 0.000432    |
|    std                  | 4.21        |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=2714000, episode_reward=226.62 +/- 147.29
Episode length: 542.00 +/- 78.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 2714000      |
| train/                  |              |
|    approx_kl            | 0.0021417497 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 79.9         |
|    n_updates            | 13250        |
|    policy_gradient_loss | -0.00157     |
|    std                  | 4.22         |
|    value_loss           | 312          |
------------------------------------------
Eval num_timesteps=2716000, episode_reward=384.96 +/- 374.45
Episode length: 533.60 +/- 31.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 534         |
|    mean_reward          | 385         |
| time/                   |             |
|    total_timesteps      | 2716000     |
| train/                  |             |
|    approx_kl            | 0.002609356 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.001       |
|    loss                 | 412         |
|    n_updates            | 13260       |
|    policy_gradient_loss | -8.9e-05    |
|    std                  | 4.25        |
|    value_loss           | 1.13e+03    |
-----------------------------------------
Eval num_timesteps=2718000, episode_reward=137.41 +/- 82.74
Episode length: 517.40 +/- 34.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 2718000      |
| train/                  |              |
|    approx_kl            | 0.0026564049 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+03     |
|    n_updates            | 13270        |
|    policy_gradient_loss | -0.000947    |
|    std                  | 4.26         |
|    value_loss           | 3.9e+03      |
------------------------------------------
Eval num_timesteps=2720000, episode_reward=195.24 +/- 207.33
Episode length: 483.80 +/- 92.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 2720000      |
| train/                  |              |
|    approx_kl            | 0.0017889561 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 58.5         |
|    n_updates            | 13280        |
|    policy_gradient_loss | -0.000759    |
|    std                  | 4.27         |
|    value_loss           | 232          |
------------------------------------------
Eval num_timesteps=2722000, episode_reward=591.12 +/- 487.06
Episode length: 537.60 +/- 38.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 591          |
| time/                   |              |
|    total_timesteps      | 2722000      |
| train/                  |              |
|    approx_kl            | 0.0027021808 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 1.82e+03     |
|    n_updates            | 13290        |
|    policy_gradient_loss | 0.00114      |
|    std                  | 4.28         |
|    value_loss           | 4.84e+03     |
------------------------------------------
Eval num_timesteps=2724000, episode_reward=116.49 +/- 508.81
Episode length: 526.60 +/- 67.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 527           |
|    mean_reward          | 116           |
| time/                   |               |
|    total_timesteps      | 2724000       |
| train/                  |               |
|    approx_kl            | 0.00013716632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.737         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+03      |
|    n_updates            | 13300         |
|    policy_gradient_loss | -0.000104     |
|    std                  | 4.29          |
|    value_loss           | 6.04e+03      |
-------------------------------------------
Eval num_timesteps=2726000, episode_reward=12.38 +/- 392.48
Episode length: 495.00 +/- 75.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 2726000     |
| train/                  |             |
|    approx_kl            | 6.90766e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.001       |
|    loss                 | 1.79e+03    |
|    n_updates            | 13310       |
|    policy_gradient_loss | -0.000123   |
|    std                  | 4.29        |
|    value_loss           | 4.36e+03    |
-----------------------------------------
Eval num_timesteps=2728000, episode_reward=306.92 +/- 444.59
Episode length: 488.80 +/- 67.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 307           |
| time/                   |               |
|    total_timesteps      | 2728000       |
| train/                  |               |
|    approx_kl            | 0.00069357525 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 121           |
|    n_updates            | 13320         |
|    policy_gradient_loss | -0.000692     |
|    std                  | 4.29          |
|    value_loss           | 432           |
-------------------------------------------
Eval num_timesteps=2730000, episode_reward=187.36 +/- 243.51
Episode length: 407.20 +/- 79.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 2730000      |
| train/                  |              |
|    approx_kl            | 0.0027950753 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 13330        |
|    policy_gradient_loss | -0.000845    |
|    std                  | 4.28         |
|    value_loss           | 857          |
------------------------------------------
Eval num_timesteps=2732000, episode_reward=-75.03 +/- 298.46
Episode length: 441.60 +/- 76.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 442      |
|    mean_reward     | -75      |
| time/              |          |
|    total_timesteps | 2732000  |
---------------------------------
Eval num_timesteps=2734000, episode_reward=581.50 +/- 391.72
Episode length: 461.60 +/- 53.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 582          |
| time/                   |              |
|    total_timesteps      | 2734000      |
| train/                  |              |
|    approx_kl            | 0.0011161417 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 98.1         |
|    n_updates            | 13340        |
|    policy_gradient_loss | -0.000691    |
|    std                  | 4.29         |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=2736000, episode_reward=318.04 +/- 544.86
Episode length: 484.80 +/- 129.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 485          |
|    mean_reward          | 318          |
| time/                   |              |
|    total_timesteps      | 2736000      |
| train/                  |              |
|    approx_kl            | 0.0011524262 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 3.51e+03     |
|    n_updates            | 13350        |
|    policy_gradient_loss | -0.000441    |
|    std                  | 4.3          |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=2738000, episode_reward=245.56 +/- 245.85
Episode length: 448.80 +/- 57.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 246          |
| time/                   |              |
|    total_timesteps      | 2738000      |
| train/                  |              |
|    approx_kl            | 0.0002617741 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 864          |
|    n_updates            | 13360        |
|    policy_gradient_loss | -5.51e-05    |
|    std                  | 4.3          |
|    value_loss           | 3.21e+03     |
------------------------------------------
Eval num_timesteps=2740000, episode_reward=452.04 +/- 271.13
Episode length: 374.80 +/- 32.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 375           |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 2740000       |
| train/                  |               |
|    approx_kl            | 0.00020262311 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 107           |
|    n_updates            | 13370         |
|    policy_gradient_loss | -0.000252     |
|    std                  | 4.31          |
|    value_loss           | 398           |
-------------------------------------------
Eval num_timesteps=2742000, episode_reward=414.00 +/- 281.95
Episode length: 448.60 +/- 53.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 449        |
|    mean_reward          | 414        |
| time/                   |            |
|    total_timesteps      | 2742000    |
| train/                  |            |
|    approx_kl            | 0.00253563 |
|    clip_fraction        | 0.000732   |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.3      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.001      |
|    loss                 | 121        |
|    n_updates            | 13380      |
|    policy_gradient_loss | -0.00116   |
|    std                  | 4.31       |
|    value_loss           | 446        |
----------------------------------------
Eval num_timesteps=2744000, episode_reward=353.92 +/- 164.91
Episode length: 523.20 +/- 81.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 523         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 2744000     |
| train/                  |             |
|    approx_kl            | 0.009816244 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 74          |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.00287    |
|    std                  | 4.31        |
|    value_loss           | 304         |
-----------------------------------------
Eval num_timesteps=2746000, episode_reward=111.34 +/- 489.80
Episode length: 440.00 +/- 70.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 111         |
| time/                   |             |
|    total_timesteps      | 2746000     |
| train/                  |             |
|    approx_kl            | 0.006494497 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.001       |
|    loss                 | 1.82e+03    |
|    n_updates            | 13400       |
|    policy_gradient_loss | -0.00113    |
|    std                  | 4.31        |
|    value_loss           | 4.37e+03    |
-----------------------------------------
Eval num_timesteps=2748000, episode_reward=375.32 +/- 300.96
Episode length: 416.00 +/- 95.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 2748000      |
| train/                  |              |
|    approx_kl            | 0.0013958015 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 64.9         |
|    n_updates            | 13410        |
|    policy_gradient_loss | -0.00144     |
|    std                  | 4.32         |
|    value_loss           | 304          |
------------------------------------------
Eval num_timesteps=2750000, episode_reward=237.28 +/- 263.09
Episode length: 353.40 +/- 31.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 353          |
|    mean_reward          | 237          |
| time/                   |              |
|    total_timesteps      | 2750000      |
| train/                  |              |
|    approx_kl            | 0.0039897626 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 13420        |
|    policy_gradient_loss | -0.0013      |
|    std                  | 4.33         |
|    value_loss           | 494          |
------------------------------------------
Eval num_timesteps=2752000, episode_reward=141.52 +/- 104.89
Episode length: 354.60 +/- 20.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | 142          |
| time/                   |              |
|    total_timesteps      | 2752000      |
| train/                  |              |
|    approx_kl            | 0.0012757124 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 13430        |
|    policy_gradient_loss | -0.000129    |
|    std                  | 4.33         |
|    value_loss           | 5.35e+03     |
------------------------------------------
Eval num_timesteps=2754000, episode_reward=306.79 +/- 160.80
Episode length: 364.60 +/- 41.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 365           |
|    mean_reward          | 307           |
| time/                   |               |
|    total_timesteps      | 2754000       |
| train/                  |               |
|    approx_kl            | 0.00013309301 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 13440         |
|    policy_gradient_loss | 5.81e-05      |
|    std                  | 4.33          |
|    value_loss           | 3.61e+03      |
-------------------------------------------
Eval num_timesteps=2756000, episode_reward=396.88 +/- 191.34
Episode length: 365.80 +/- 25.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 397          |
| time/                   |              |
|    total_timesteps      | 2756000      |
| train/                  |              |
|    approx_kl            | 0.0054371813 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 63.4         |
|    n_updates            | 13450        |
|    policy_gradient_loss | -0.00217     |
|    std                  | 4.33         |
|    value_loss           | 248          |
------------------------------------------
Eval num_timesteps=2758000, episode_reward=288.80 +/- 195.47
Episode length: 336.40 +/- 29.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 336          |
|    mean_reward          | 289          |
| time/                   |              |
|    total_timesteps      | 2758000      |
| train/                  |              |
|    approx_kl            | 0.0064420113 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 325          |
|    n_updates            | 13460        |
|    policy_gradient_loss | -0.0025      |
|    std                  | 4.32         |
|    value_loss           | 849          |
------------------------------------------
Eval num_timesteps=2760000, episode_reward=178.25 +/- 191.04
Episode length: 355.40 +/- 46.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 2760000      |
| train/                  |              |
|    approx_kl            | 0.0024741734 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 146          |
|    n_updates            | 13470        |
|    policy_gradient_loss | 6.03e-05     |
|    std                  | 4.32         |
|    value_loss           | 533          |
------------------------------------------
Eval num_timesteps=2762000, episode_reward=416.65 +/- 256.59
Episode length: 369.80 +/- 47.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 2762000      |
| train/                  |              |
|    approx_kl            | 0.0050501684 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 68.1         |
|    n_updates            | 13480        |
|    policy_gradient_loss | -0.00301     |
|    std                  | 4.31         |
|    value_loss           | 263          |
------------------------------------------
Eval num_timesteps=2764000, episode_reward=222.56 +/- 109.46
Episode length: 395.00 +/- 107.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 2764000      |
| train/                  |              |
|    approx_kl            | 0.0027647053 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+03     |
|    n_updates            | 13490        |
|    policy_gradient_loss | -0.000117    |
|    std                  | 4.31         |
|    value_loss           | 6.08e+03     |
------------------------------------------
Eval num_timesteps=2766000, episode_reward=352.10 +/- 81.61
Episode length: 505.40 +/- 82.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 2766000      |
| train/                  |              |
|    approx_kl            | 0.0007649706 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 116          |
|    n_updates            | 13500        |
|    policy_gradient_loss | -0.000267    |
|    std                  | 4.31         |
|    value_loss           | 400          |
------------------------------------------
Eval num_timesteps=2768000, episode_reward=272.12 +/- 372.41
Episode length: 508.40 +/- 66.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 2768000      |
| train/                  |              |
|    approx_kl            | 0.0025879377 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 123          |
|    n_updates            | 13510        |
|    policy_gradient_loss | -0.0015      |
|    std                  | 4.31         |
|    value_loss           | 547          |
------------------------------------------
Eval num_timesteps=2770000, episode_reward=472.18 +/- 403.63
Episode length: 533.00 +/- 129.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 533           |
|    mean_reward          | 472           |
| time/                   |               |
|    total_timesteps      | 2770000       |
| train/                  |               |
|    approx_kl            | 0.00081720954 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+03      |
|    n_updates            | 13520         |
|    policy_gradient_loss | 0.000689      |
|    std                  | 4.31          |
|    value_loss           | 3.75e+03      |
-------------------------------------------
Eval num_timesteps=2772000, episode_reward=580.36 +/- 430.32
Episode length: 556.40 +/- 86.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 556           |
|    mean_reward          | 580           |
| time/                   |               |
|    total_timesteps      | 2772000       |
| train/                  |               |
|    approx_kl            | 7.3229225e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 745           |
|    n_updates            | 13530         |
|    policy_gradient_loss | -9.94e-05     |
|    std                  | 4.31          |
|    value_loss           | 2.47e+03      |
-------------------------------------------
Eval num_timesteps=2774000, episode_reward=375.95 +/- 135.50
Episode length: 428.20 +/- 100.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 2774000      |
| train/                  |              |
|    approx_kl            | 0.0003930666 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.781        |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 13540        |
|    policy_gradient_loss | -0.000594    |
|    std                  | 4.32         |
|    value_loss           | 6e+03        |
------------------------------------------
Eval num_timesteps=2776000, episode_reward=141.36 +/- 458.64
Episode length: 434.20 +/- 95.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 2776000      |
| train/                  |              |
|    approx_kl            | 0.0025557608 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 13550        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 4.33         |
|    value_loss           | 499          |
------------------------------------------
Eval num_timesteps=2778000, episode_reward=253.22 +/- 517.08
Episode length: 537.00 +/- 71.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 2778000      |
| train/                  |              |
|    approx_kl            | 0.0016308513 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+03     |
|    n_updates            | 13560        |
|    policy_gradient_loss | -4.99e-05    |
|    std                  | 4.33         |
|    value_loss           | 8.26e+03     |
------------------------------------------
Eval num_timesteps=2780000, episode_reward=472.85 +/- 248.31
Episode length: 429.40 +/- 80.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 2780000      |
| train/                  |              |
|    approx_kl            | 0.0011988659 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 1.61e+03     |
|    n_updates            | 13570        |
|    policy_gradient_loss | -0.0011      |
|    std                  | 4.34         |
|    value_loss           | 3.86e+03     |
------------------------------------------
Eval num_timesteps=2782000, episode_reward=546.27 +/- 245.12
Episode length: 504.40 +/- 71.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 546          |
| time/                   |              |
|    total_timesteps      | 2782000      |
| train/                  |              |
|    approx_kl            | 6.542326e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+03     |
|    n_updates            | 13580        |
|    policy_gradient_loss | 1.91e-05     |
|    std                  | 4.35         |
|    value_loss           | 3.93e+03     |
------------------------------------------
Eval num_timesteps=2784000, episode_reward=545.44 +/- 895.56
Episode length: 452.60 +/- 108.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 545           |
| time/                   |               |
|    total_timesteps      | 2784000       |
| train/                  |               |
|    approx_kl            | 0.00023885773 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 13590         |
|    policy_gradient_loss | -0.000417     |
|    std                  | 4.35          |
|    value_loss           | 4.25e+03      |
-------------------------------------------
Eval num_timesteps=2786000, episode_reward=261.00 +/- 224.02
Episode length: 477.80 +/- 110.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 2786000      |
| train/                  |              |
|    approx_kl            | 0.0024772475 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 235          |
|    n_updates            | 13600        |
|    policy_gradient_loss | -0.0015      |
|    std                  | 4.35         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=2788000, episode_reward=55.48 +/- 385.59
Episode length: 460.20 +/- 62.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 55.5         |
| time/                   |              |
|    total_timesteps      | 2788000      |
| train/                  |              |
|    approx_kl            | 0.0027749306 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 3.25e+03     |
|    n_updates            | 13610        |
|    policy_gradient_loss | 0.000338     |
|    std                  | 4.35         |
|    value_loss           | 8.02e+03     |
------------------------------------------
Eval num_timesteps=2790000, episode_reward=239.68 +/- 119.94
Episode length: 469.60 +/- 89.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | 240          |
| time/                   |              |
|    total_timesteps      | 2790000      |
| train/                  |              |
|    approx_kl            | 7.669415e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 2.9e+03      |
|    n_updates            | 13620        |
|    policy_gradient_loss | 0.000141     |
|    std                  | 4.35         |
|    value_loss           | 6.65e+03     |
------------------------------------------
Eval num_timesteps=2792000, episode_reward=516.76 +/- 777.91
Episode length: 498.80 +/- 121.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 499           |
|    mean_reward          | 517           |
| time/                   |               |
|    total_timesteps      | 2792000       |
| train/                  |               |
|    approx_kl            | 0.00014632236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.001         |
|    loss                 | 442           |
|    n_updates            | 13630         |
|    policy_gradient_loss | -0.000279     |
|    std                  | 4.36          |
|    value_loss           | 1.52e+03      |
-------------------------------------------
Eval num_timesteps=2794000, episode_reward=243.77 +/- 384.49
Episode length: 399.80 +/- 49.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 244           |
| time/                   |               |
|    total_timesteps      | 2794000       |
| train/                  |               |
|    approx_kl            | 0.00046278865 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.001         |
|    loss                 | 285           |
|    n_updates            | 13640         |
|    policy_gradient_loss | -0.000202     |
|    std                  | 4.35          |
|    value_loss           | 958           |
-------------------------------------------
Eval num_timesteps=2796000, episode_reward=282.17 +/- 87.64
Episode length: 454.40 +/- 54.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 282          |
| time/                   |              |
|    total_timesteps      | 2796000      |
| train/                  |              |
|    approx_kl            | 0.0007121044 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 141          |
|    n_updates            | 13650        |
|    policy_gradient_loss | -0.000515    |
|    std                  | 4.35         |
|    value_loss           | 419          |
------------------------------------------
Eval num_timesteps=2798000, episode_reward=193.63 +/- 94.87
Episode length: 420.00 +/- 125.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 2798000      |
| train/                  |              |
|    approx_kl            | 0.0005095935 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+03     |
|    n_updates            | 13660        |
|    policy_gradient_loss | 0.000474     |
|    std                  | 4.36         |
|    value_loss           | 5.81e+03     |
------------------------------------------
Eval num_timesteps=2800000, episode_reward=294.85 +/- 348.75
Episode length: 429.80 +/- 79.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 295           |
| time/                   |               |
|    total_timesteps      | 2800000       |
| train/                  |               |
|    approx_kl            | 0.00053239043 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 53.5          |
|    n_updates            | 13670         |
|    policy_gradient_loss | -0.000467     |
|    std                  | 4.37          |
|    value_loss           | 225           |
-------------------------------------------
Eval num_timesteps=2802000, episode_reward=187.11 +/- 103.40
Episode length: 424.60 +/- 54.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 187           |
| time/                   |               |
|    total_timesteps      | 2802000       |
| train/                  |               |
|    approx_kl            | 0.00092303916 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.826         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+03      |
|    n_updates            | 13680         |
|    policy_gradient_loss | 0.00044       |
|    std                  | 4.39          |
|    value_loss           | 3.96e+03      |
-------------------------------------------
Eval num_timesteps=2804000, episode_reward=271.35 +/- 166.88
Episode length: 409.40 +/- 61.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 2804000      |
| train/                  |              |
|    approx_kl            | 7.779585e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 2.25e+03     |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.000312    |
|    std                  | 4.39         |
|    value_loss           | 6.08e+03     |
------------------------------------------
Eval num_timesteps=2806000, episode_reward=256.69 +/- 361.27
Episode length: 474.60 +/- 97.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 257           |
| time/                   |               |
|    total_timesteps      | 2806000       |
| train/                  |               |
|    approx_kl            | 0.00079567835 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 116           |
|    n_updates            | 13700         |
|    policy_gradient_loss | -0.000782     |
|    std                  | 4.4           |
|    value_loss           | 497           |
-------------------------------------------
Eval num_timesteps=2808000, episode_reward=414.45 +/- 437.90
Episode length: 438.40 +/- 86.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 438        |
|    mean_reward          | 414        |
| time/                   |            |
|    total_timesteps      | 2808000    |
| train/                  |            |
|    approx_kl            | 0.00057347 |
|    clip_fraction        | 9.77e-05   |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.4      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.001      |
|    loss                 | 150        |
|    n_updates            | 13710      |
|    policy_gradient_loss | 3.99e-05   |
|    std                  | 4.4        |
|    value_loss           | 702        |
----------------------------------------
Eval num_timesteps=2810000, episode_reward=241.49 +/- 154.78
Episode length: 394.80 +/- 77.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 2810000      |
| train/                  |              |
|    approx_kl            | 0.0022801468 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 166          |
|    n_updates            | 13720        |
|    policy_gradient_loss | -0.00142     |
|    std                  | 4.42         |
|    value_loss           | 667          |
------------------------------------------
Eval num_timesteps=2812000, episode_reward=102.30 +/- 336.60
Episode length: 397.20 +/- 60.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 102          |
| time/                   |              |
|    total_timesteps      | 2812000      |
| train/                  |              |
|    approx_kl            | 0.0039289845 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+03     |
|    n_updates            | 13730        |
|    policy_gradient_loss | 0.000168     |
|    std                  | 4.42         |
|    value_loss           | 4.34e+03     |
------------------------------------------
Eval num_timesteps=2814000, episode_reward=218.68 +/- 136.97
Episode length: 452.00 +/- 78.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 2814000      |
| train/                  |              |
|    approx_kl            | 0.0001577597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 13740        |
|    policy_gradient_loss | -5.81e-05    |
|    std                  | 4.42         |
|    value_loss           | 3.1e+03      |
------------------------------------------
Eval num_timesteps=2816000, episode_reward=257.76 +/- 440.92
Episode length: 424.00 +/- 93.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 424      |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 2816000  |
---------------------------------
Eval num_timesteps=2818000, episode_reward=262.39 +/- 140.45
Episode length: 345.00 +/- 23.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 345          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 2818000      |
| train/                  |              |
|    approx_kl            | 0.0005341928 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 633          |
|    n_updates            | 13750        |
|    policy_gradient_loss | -0.000584    |
|    std                  | 4.44         |
|    value_loss           | 1.51e+03     |
------------------------------------------
Eval num_timesteps=2820000, episode_reward=381.58 +/- 655.84
Episode length: 443.60 +/- 78.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 0.0010953927 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 13760        |
|    policy_gradient_loss | -0.000797    |
|    std                  | 4.45         |
|    value_loss           | 3.76e+03     |
------------------------------------------
Eval num_timesteps=2822000, episode_reward=633.19 +/- 458.02
Episode length: 495.20 +/- 87.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 633          |
| time/                   |              |
|    total_timesteps      | 2822000      |
| train/                  |              |
|    approx_kl            | 0.0039830506 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 182          |
|    n_updates            | 13770        |
|    policy_gradient_loss | -0.00156     |
|    std                  | 4.45         |
|    value_loss           | 606          |
------------------------------------------
Eval num_timesteps=2824000, episode_reward=320.25 +/- 206.34
Episode length: 388.80 +/- 55.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 320          |
| time/                   |              |
|    total_timesteps      | 2824000      |
| train/                  |              |
|    approx_kl            | 0.0022393118 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 28.7         |
|    n_updates            | 13780        |
|    policy_gradient_loss | -0.000212    |
|    std                  | 4.45         |
|    value_loss           | 131          |
------------------------------------------
Eval num_timesteps=2826000, episode_reward=176.96 +/- 306.56
Episode length: 336.20 +/- 73.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 336         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 2826000     |
| train/                  |             |
|    approx_kl            | 0.003917205 |
|    clip_fraction        | 0.00635     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.001       |
|    loss                 | 1.47e+03    |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.000795   |
|    std                  | 4.44        |
|    value_loss           | 3.96e+03    |
-----------------------------------------
Eval num_timesteps=2828000, episode_reward=187.78 +/- 119.29
Episode length: 356.00 +/- 47.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 2828000     |
| train/                  |             |
|    approx_kl            | 0.004326975 |
|    clip_fraction        | 0.0042      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 69.7        |
|    n_updates            | 13800       |
|    policy_gradient_loss | -0.00176    |
|    std                  | 4.45        |
|    value_loss           | 329         |
-----------------------------------------
Eval num_timesteps=2830000, episode_reward=164.45 +/- 166.69
Episode length: 330.60 +/- 37.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 331          |
|    mean_reward          | 164          |
| time/                   |              |
|    total_timesteps      | 2830000      |
| train/                  |              |
|    approx_kl            | 0.0017582688 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 57.2         |
|    n_updates            | 13810        |
|    policy_gradient_loss | 0.000141     |
|    std                  | 4.45         |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=2832000, episode_reward=155.11 +/- 108.48
Episode length: 351.60 +/- 34.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 155          |
| time/                   |              |
|    total_timesteps      | 2832000      |
| train/                  |              |
|    approx_kl            | 0.0041694045 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 47           |
|    n_updates            | 13820        |
|    policy_gradient_loss | -0.00177     |
|    std                  | 4.46         |
|    value_loss           | 220          |
------------------------------------------
Eval num_timesteps=2834000, episode_reward=197.39 +/- 123.50
Episode length: 354.20 +/- 41.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 354           |
|    mean_reward          | 197           |
| time/                   |               |
|    total_timesteps      | 2834000       |
| train/                  |               |
|    approx_kl            | 0.00024127739 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 55.7          |
|    n_updates            | 13830         |
|    policy_gradient_loss | 8.55e-05      |
|    std                  | 4.47          |
|    value_loss           | 216           |
-------------------------------------------
Eval num_timesteps=2836000, episode_reward=79.84 +/- 145.93
Episode length: 306.80 +/- 37.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | 79.8         |
| time/                   |              |
|    total_timesteps      | 2836000      |
| train/                  |              |
|    approx_kl            | 0.0027669668 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 43           |
|    n_updates            | 13840        |
|    policy_gradient_loss | -0.000592    |
|    std                  | 4.47         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=2838000, episode_reward=16.11 +/- 119.96
Episode length: 275.80 +/- 42.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 2838000      |
| train/                  |              |
|    approx_kl            | 0.0063794665 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 36.4         |
|    n_updates            | 13850        |
|    policy_gradient_loss | -0.00079     |
|    std                  | 4.5          |
|    value_loss           | 143          |
------------------------------------------
Eval num_timesteps=2840000, episode_reward=287.88 +/- 308.91
Episode length: 476.00 +/- 122.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 2840000     |
| train/                  |             |
|    approx_kl            | 0.009056538 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 13860       |
|    policy_gradient_loss | -0.00245    |
|    std                  | 4.51        |
|    value_loss           | 396         |
-----------------------------------------
Eval num_timesteps=2842000, episode_reward=410.56 +/- 345.54
Episode length: 387.80 +/- 42.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | 411         |
| time/                   |             |
|    total_timesteps      | 2842000     |
| train/                  |             |
|    approx_kl            | 0.003953879 |
|    clip_fraction        | 0.004       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 77.6        |
|    n_updates            | 13870       |
|    policy_gradient_loss | -0.00107    |
|    std                  | 4.51        |
|    value_loss           | 293         |
-----------------------------------------
Eval num_timesteps=2844000, episode_reward=301.12 +/- 284.44
Episode length: 507.80 +/- 104.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 2844000      |
| train/                  |              |
|    approx_kl            | 0.0035632781 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 82.9         |
|    n_updates            | 13880        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 4.5          |
|    value_loss           | 419          |
------------------------------------------
Eval num_timesteps=2846000, episode_reward=174.68 +/- 150.43
Episode length: 390.40 +/- 65.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 2846000      |
| train/                  |              |
|    approx_kl            | 0.0012731231 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.001        |
|    loss                 | 2.29e+03     |
|    n_updates            | 13890        |
|    policy_gradient_loss | 0.000865     |
|    std                  | 4.5          |
|    value_loss           | 5.73e+03     |
------------------------------------------
Eval num_timesteps=2848000, episode_reward=137.36 +/- 125.12
Episode length: 397.20 +/- 53.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 2848000     |
| train/                  |             |
|    approx_kl            | 0.002497072 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 118         |
|    n_updates            | 13900       |
|    policy_gradient_loss | -0.0012     |
|    std                  | 4.5         |
|    value_loss           | 417         |
-----------------------------------------
Eval num_timesteps=2850000, episode_reward=115.01 +/- 254.26
Episode length: 365.00 +/- 29.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 365         |
|    mean_reward          | 115         |
| time/                   |             |
|    total_timesteps      | 2850000     |
| train/                  |             |
|    approx_kl            | 0.006095702 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 63          |
|    n_updates            | 13910       |
|    policy_gradient_loss | -0.00104    |
|    std                  | 4.51        |
|    value_loss           | 193         |
-----------------------------------------
Eval num_timesteps=2852000, episode_reward=215.81 +/- 299.97
Episode length: 334.00 +/- 69.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 334          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 2852000      |
| train/                  |              |
|    approx_kl            | 0.0003981339 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+03     |
|    n_updates            | 13920        |
|    policy_gradient_loss | -0.000602    |
|    std                  | 4.52         |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=2854000, episode_reward=149.11 +/- 119.17
Episode length: 390.60 +/- 107.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 149           |
| time/                   |               |
|    total_timesteps      | 2854000       |
| train/                  |               |
|    approx_kl            | 0.00092614256 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 59.1          |
|    n_updates            | 13930         |
|    policy_gradient_loss | -0.000388     |
|    std                  | 4.53          |
|    value_loss           | 244           |
-------------------------------------------
Eval num_timesteps=2856000, episode_reward=220.33 +/- 186.74
Episode length: 455.60 +/- 108.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 2856000      |
| train/                  |              |
|    approx_kl            | 0.0049128365 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 73.7         |
|    n_updates            | 13940        |
|    policy_gradient_loss | -0.00149     |
|    std                  | 4.53         |
|    value_loss           | 234          |
------------------------------------------
Eval num_timesteps=2858000, episode_reward=19.88 +/- 399.34
Episode length: 507.80 +/- 143.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 2858000      |
| train/                  |              |
|    approx_kl            | 0.0039607906 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 37.5         |
|    n_updates            | 13950        |
|    policy_gradient_loss | -0.00044     |
|    std                  | 4.54         |
|    value_loss           | 121          |
------------------------------------------
Eval num_timesteps=2860000, episode_reward=270.38 +/- 236.90
Episode length: 378.40 +/- 44.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 2860000      |
| train/                  |              |
|    approx_kl            | 0.0040155123 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 88.8         |
|    n_updates            | 13960        |
|    policy_gradient_loss | -0.000667    |
|    std                  | 4.55         |
|    value_loss           | 304          |
------------------------------------------
Eval num_timesteps=2862000, episode_reward=167.34 +/- 207.44
Episode length: 478.00 +/- 108.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 167          |
| time/                   |              |
|    total_timesteps      | 2862000      |
| train/                  |              |
|    approx_kl            | 0.0030590591 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 90.7         |
|    n_updates            | 13970        |
|    policy_gradient_loss | -0.000937    |
|    std                  | 4.55         |
|    value_loss           | 347          |
------------------------------------------
Eval num_timesteps=2864000, episode_reward=96.19 +/- 128.57
Episode length: 339.20 +/- 44.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 339         |
|    mean_reward          | 96.2        |
| time/                   |             |
|    total_timesteps      | 2864000     |
| train/                  |             |
|    approx_kl            | 0.004594491 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 135         |
|    n_updates            | 13980       |
|    policy_gradient_loss | -0.00173    |
|    std                  | 4.54        |
|    value_loss           | 485         |
-----------------------------------------
Eval num_timesteps=2866000, episode_reward=10.64 +/- 49.47
Episode length: 286.00 +/- 38.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 286           |
|    mean_reward          | 10.6          |
| time/                   |               |
|    total_timesteps      | 2866000       |
| train/                  |               |
|    approx_kl            | 0.00096841564 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 72.2          |
|    n_updates            | 13990         |
|    policy_gradient_loss | -1.5e-05      |
|    std                  | 4.54          |
|    value_loss           | 204           |
-------------------------------------------
Eval num_timesteps=2868000, episode_reward=71.19 +/- 84.80
Episode length: 322.20 +/- 31.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 71.2         |
| time/                   |              |
|    total_timesteps      | 2868000      |
| train/                  |              |
|    approx_kl            | 0.0018206949 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 14000        |
|    policy_gradient_loss | -0.00091     |
|    std                  | 4.55         |
|    value_loss           | 498          |
------------------------------------------
Eval num_timesteps=2870000, episode_reward=154.21 +/- 232.90
Episode length: 325.20 +/- 72.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | 154          |
| time/                   |              |
|    total_timesteps      | 2870000      |
| train/                  |              |
|    approx_kl            | 0.0033012596 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 73.5         |
|    n_updates            | 14010        |
|    policy_gradient_loss | -0.00109     |
|    std                  | 4.57         |
|    value_loss           | 281          |
------------------------------------------
Eval num_timesteps=2872000, episode_reward=42.86 +/- 133.83
Episode length: 279.40 +/- 62.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 42.9         |
| time/                   |              |
|    total_timesteps      | 2872000      |
| train/                  |              |
|    approx_kl            | 0.0044528954 |
|    clip_fraction        | 0.00815      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 81.2         |
|    n_updates            | 14020        |
|    policy_gradient_loss | -0.00179     |
|    std                  | 4.6          |
|    value_loss           | 260          |
------------------------------------------
Eval num_timesteps=2874000, episode_reward=15.00 +/- 87.84
Episode length: 275.80 +/- 42.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 2874000       |
| train/                  |               |
|    approx_kl            | 0.00086126954 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 51.4          |
|    n_updates            | 14030         |
|    policy_gradient_loss | -0.000547     |
|    std                  | 4.61          |
|    value_loss           | 189           |
-------------------------------------------
Eval num_timesteps=2876000, episode_reward=137.03 +/- 235.34
Episode length: 325.20 +/- 59.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 2876000     |
| train/                  |             |
|    approx_kl            | 0.000823052 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 92.4        |
|    n_updates            | 14040       |
|    policy_gradient_loss | -0.000477   |
|    std                  | 4.62        |
|    value_loss           | 370         |
-----------------------------------------
Eval num_timesteps=2878000, episode_reward=106.66 +/- 70.47
Episode length: 351.40 +/- 45.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | 107          |
| time/                   |              |
|    total_timesteps      | 2878000      |
| train/                  |              |
|    approx_kl            | 0.0067195073 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 36.3         |
|    n_updates            | 14050        |
|    policy_gradient_loss | -0.00309     |
|    std                  | 4.62         |
|    value_loss           | 129          |
------------------------------------------
Eval num_timesteps=2880000, episode_reward=237.50 +/- 175.29
Episode length: 426.60 +/- 114.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 2880000     |
| train/                  |             |
|    approx_kl            | 0.004974561 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 63.1        |
|    n_updates            | 14060       |
|    policy_gradient_loss | 0.000533    |
|    std                  | 4.63        |
|    value_loss           | 266         |
-----------------------------------------
Eval num_timesteps=2882000, episode_reward=582.70 +/- 299.60
Episode length: 509.60 +/- 95.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 583         |
| time/                   |             |
|    total_timesteps      | 2882000     |
| train/                  |             |
|    approx_kl            | 0.005681089 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 82.9        |
|    n_updates            | 14070       |
|    policy_gradient_loss | -0.00132    |
|    std                  | 4.65        |
|    value_loss           | 311         |
-----------------------------------------
Eval num_timesteps=2884000, episode_reward=193.81 +/- 112.38
Episode length: 412.20 +/- 102.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 2884000      |
| train/                  |              |
|    approx_kl            | 0.0035428181 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 94.7         |
|    n_updates            | 14080        |
|    policy_gradient_loss | -0.000837    |
|    std                  | 4.66         |
|    value_loss           | 412          |
------------------------------------------
Eval num_timesteps=2886000, episode_reward=340.91 +/- 344.09
Episode length: 423.60 +/- 98.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | 341         |
| time/                   |             |
|    total_timesteps      | 2886000     |
| train/                  |             |
|    approx_kl            | 0.003558312 |
|    clip_fraction        | 0.00273     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 99.5        |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.001      |
|    std                  | 4.68        |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=2888000, episode_reward=249.75 +/- 134.39
Episode length: 398.60 +/- 67.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 2888000     |
| train/                  |             |
|    approx_kl            | 0.003933876 |
|    clip_fraction        | 0.00493     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 42.8        |
|    n_updates            | 14100       |
|    policy_gradient_loss | -0.00151    |
|    std                  | 4.68        |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=2890000, episode_reward=238.85 +/- 314.46
Episode length: 451.80 +/- 52.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 2890000      |
| train/                  |              |
|    approx_kl            | 0.0017002377 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 14110        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 4.67         |
|    value_loss           | 4.09e+03     |
------------------------------------------
Eval num_timesteps=2892000, episode_reward=272.60 +/- 577.94
Episode length: 527.20 +/- 55.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 527          |
|    mean_reward          | 273          |
| time/                   |              |
|    total_timesteps      | 2892000      |
| train/                  |              |
|    approx_kl            | 0.0016889793 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 81.5         |
|    n_updates            | 14120        |
|    policy_gradient_loss | -0.00159     |
|    std                  | 4.67         |
|    value_loss           | 316          |
------------------------------------------
Eval num_timesteps=2894000, episode_reward=-28.14 +/- 545.12
Episode length: 509.80 +/- 44.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | -28.1       |
| time/                   |             |
|    total_timesteps      | 2894000     |
| train/                  |             |
|    approx_kl            | 0.001497661 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 1.45e+03    |
|    n_updates            | 14130       |
|    policy_gradient_loss | -0.000805   |
|    std                  | 4.67        |
|    value_loss           | 4.68e+03    |
-----------------------------------------
Eval num_timesteps=2896000, episode_reward=95.53 +/- 301.28
Episode length: 541.60 +/- 37.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 542           |
|    mean_reward          | 95.5          |
| time/                   |               |
|    total_timesteps      | 2896000       |
| train/                  |               |
|    approx_kl            | 0.00032588062 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 110           |
|    n_updates            | 14140         |
|    policy_gradient_loss | 1.3e-05       |
|    std                  | 4.68          |
|    value_loss           | 633           |
-------------------------------------------
Eval num_timesteps=2898000, episode_reward=50.12 +/- 227.47
Episode length: 466.40 +/- 47.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 50.1          |
| time/                   |               |
|    total_timesteps      | 2898000       |
| train/                  |               |
|    approx_kl            | 0.00024576296 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+03       |
|    n_updates            | 14150         |
|    policy_gradient_loss | -0.000312     |
|    std                  | 4.68          |
|    value_loss           | 5.18e+03      |
-------------------------------------------
Eval num_timesteps=2900000, episode_reward=-86.41 +/- 196.84
Episode length: 510.60 +/- 47.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | -86.4        |
| time/                   |              |
|    total_timesteps      | 2900000      |
| train/                  |              |
|    approx_kl            | 0.0040414454 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 74.7         |
|    n_updates            | 14160        |
|    policy_gradient_loss | -0.00174     |
|    std                  | 4.69         |
|    value_loss           | 425          |
------------------------------------------
Eval num_timesteps=2902000, episode_reward=157.72 +/- 440.97
Episode length: 540.00 +/- 67.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 2902000  |
---------------------------------
Eval num_timesteps=2904000, episode_reward=-74.01 +/- 54.26
Episode length: 469.20 +/- 57.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | -74          |
| time/                   |              |
|    total_timesteps      | 2904000      |
| train/                  |              |
|    approx_kl            | 0.0009932077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 160          |
|    n_updates            | 14170        |
|    policy_gradient_loss | 0.000664     |
|    std                  | 4.7          |
|    value_loss           | 676          |
------------------------------------------
Eval num_timesteps=2906000, episode_reward=250.81 +/- 358.82
Episode length: 543.80 +/- 34.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 251          |
| time/                   |              |
|    total_timesteps      | 2906000      |
| train/                  |              |
|    approx_kl            | 0.0015540705 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 14180        |
|    policy_gradient_loss | 0.000107     |
|    std                  | 4.71         |
|    value_loss           | 3.36e+03     |
------------------------------------------
Eval num_timesteps=2908000, episode_reward=-65.09 +/- 95.07
Episode length: 496.20 +/- 78.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -65.1        |
| time/                   |              |
|    total_timesteps      | 2908000      |
| train/                  |              |
|    approx_kl            | 0.0063040527 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 4.72         |
|    value_loss           | 395          |
------------------------------------------
Eval num_timesteps=2910000, episode_reward=-129.46 +/- 37.66
Episode length: 457.60 +/- 89.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 2910000      |
| train/                  |              |
|    approx_kl            | 0.0011071817 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 14200        |
|    policy_gradient_loss | 0.000247     |
|    std                  | 4.74         |
|    value_loss           | 496          |
------------------------------------------
Eval num_timesteps=2912000, episode_reward=29.16 +/- 240.55
Episode length: 515.60 +/- 56.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 29.2         |
| time/                   |              |
|    total_timesteps      | 2912000      |
| train/                  |              |
|    approx_kl            | 0.0013919263 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 157          |
|    n_updates            | 14210        |
|    policy_gradient_loss | -0.000462    |
|    std                  | 4.75         |
|    value_loss           | 595          |
------------------------------------------
Eval num_timesteps=2914000, episode_reward=-10.08 +/- 134.06
Episode length: 527.40 +/- 49.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 527           |
|    mean_reward          | -10.1         |
| time/                   |               |
|    total_timesteps      | 2914000       |
| train/                  |               |
|    approx_kl            | 0.00090879004 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.7         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 88.3          |
|    n_updates            | 14220         |
|    policy_gradient_loss | -0.000724     |
|    std                  | 4.74          |
|    value_loss           | 294           |
-------------------------------------------
Eval num_timesteps=2916000, episode_reward=30.01 +/- 104.45
Episode length: 510.00 +/- 45.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 510          |
|    mean_reward          | 30           |
| time/                   |              |
|    total_timesteps      | 2916000      |
| train/                  |              |
|    approx_kl            | 0.0013108874 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 206          |
|    n_updates            | 14230        |
|    policy_gradient_loss | -0.000796    |
|    std                  | 4.73         |
|    value_loss           | 805          |
------------------------------------------
Eval num_timesteps=2918000, episode_reward=202.73 +/- 256.87
Episode length: 515.80 +/- 35.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 2918000      |
| train/                  |              |
|    approx_kl            | 0.0028200129 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 88.2         |
|    n_updates            | 14240        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 4.74         |
|    value_loss           | 307          |
------------------------------------------
Eval num_timesteps=2920000, episode_reward=409.46 +/- 352.82
Episode length: 460.20 +/- 81.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 409          |
| time/                   |              |
|    total_timesteps      | 2920000      |
| train/                  |              |
|    approx_kl            | 0.0006949208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 89.3         |
|    n_updates            | 14250        |
|    policy_gradient_loss | 0.00027      |
|    std                  | 4.76         |
|    value_loss           | 534          |
------------------------------------------
Eval num_timesteps=2922000, episode_reward=435.24 +/- 383.92
Episode length: 466.60 +/- 89.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 2922000       |
| train/                  |               |
|    approx_kl            | 0.00088855147 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.7         |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.001         |
|    loss                 | 111           |
|    n_updates            | 14260         |
|    policy_gradient_loss | -0.000724     |
|    std                  | 4.75          |
|    value_loss           | 590           |
-------------------------------------------
Eval num_timesteps=2924000, episode_reward=303.98 +/- 193.70
Episode length: 416.60 +/- 75.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 2924000      |
| train/                  |              |
|    approx_kl            | 0.0051740827 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 51.7         |
|    n_updates            | 14270        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 4.75         |
|    value_loss           | 257          |
------------------------------------------
Eval num_timesteps=2926000, episode_reward=217.39 +/- 170.05
Episode length: 428.80 +/- 63.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 217          |
| time/                   |              |
|    total_timesteps      | 2926000      |
| train/                  |              |
|    approx_kl            | 0.0056452276 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.001        |
|    loss                 | 2.1e+03      |
|    n_updates            | 14280        |
|    policy_gradient_loss | 0.000577     |
|    std                  | 4.76         |
|    value_loss           | 5.49e+03     |
------------------------------------------
Eval num_timesteps=2928000, episode_reward=148.81 +/- 100.45
Episode length: 379.60 +/- 28.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 2928000      |
| train/                  |              |
|    approx_kl            | 0.0006334913 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 14290        |
|    policy_gradient_loss | -0.000344    |
|    std                  | 4.76         |
|    value_loss           | 514          |
------------------------------------------
Eval num_timesteps=2930000, episode_reward=502.46 +/- 572.58
Episode length: 425.80 +/- 85.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 502           |
| time/                   |               |
|    total_timesteps      | 2930000       |
| train/                  |               |
|    approx_kl            | 0.00097321894 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.7         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 88.5          |
|    n_updates            | 14300         |
|    policy_gradient_loss | -0.000691     |
|    std                  | 4.78          |
|    value_loss           | 512           |
-------------------------------------------
Eval num_timesteps=2932000, episode_reward=84.91 +/- 43.64
Episode length: 340.20 +/- 23.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | 84.9         |
| time/                   |              |
|    total_timesteps      | 2932000      |
| train/                  |              |
|    approx_kl            | 0.0041720564 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 61.2         |
|    n_updates            | 14310        |
|    policy_gradient_loss | -0.00235     |
|    std                  | 4.78         |
|    value_loss           | 280          |
------------------------------------------
Eval num_timesteps=2934000, episode_reward=91.52 +/- 107.35
Episode length: 321.80 +/- 52.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 91.5         |
| time/                   |              |
|    total_timesteps      | 2934000      |
| train/                  |              |
|    approx_kl            | 0.0031177667 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 87.7         |
|    n_updates            | 14320        |
|    policy_gradient_loss | 0.000328     |
|    std                  | 4.79         |
|    value_loss           | 339          |
------------------------------------------
Eval num_timesteps=2936000, episode_reward=63.53 +/- 80.33
Episode length: 332.80 +/- 31.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 63.5        |
| time/                   |             |
|    total_timesteps      | 2936000     |
| train/                  |             |
|    approx_kl            | 0.005435073 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 29.5        |
|    n_updates            | 14330       |
|    policy_gradient_loss | -0.00168    |
|    std                  | 4.8         |
|    value_loss           | 117         |
-----------------------------------------
Eval num_timesteps=2938000, episode_reward=253.06 +/- 522.14
Episode length: 503.60 +/- 151.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 2938000      |
| train/                  |              |
|    approx_kl            | 0.0031376865 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 14340        |
|    policy_gradient_loss | -0.000323    |
|    std                  | 4.8          |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=2940000, episode_reward=290.35 +/- 331.25
Episode length: 482.00 +/- 60.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 482         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 2940000     |
| train/                  |             |
|    approx_kl            | 0.008989586 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.7       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 31          |
|    n_updates            | 14350       |
|    policy_gradient_loss | -0.00282    |
|    std                  | 4.82        |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=2942000, episode_reward=47.45 +/- 320.70
Episode length: 485.60 +/- 92.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 47.4         |
| time/                   |              |
|    total_timesteps      | 2942000      |
| train/                  |              |
|    approx_kl            | 0.0010920431 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 104          |
|    n_updates            | 14360        |
|    policy_gradient_loss | 0.000103     |
|    std                  | 4.84         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=2944000, episode_reward=223.67 +/- 519.37
Episode length: 489.00 +/- 88.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 224           |
| time/                   |               |
|    total_timesteps      | 2944000       |
| train/                  |               |
|    approx_kl            | 0.00038870572 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.67          |
|    learning_rate        | 0.001         |
|    loss                 | 2.44e+03      |
|    n_updates            | 14370         |
|    policy_gradient_loss | -0.000137     |
|    std                  | 4.84          |
|    value_loss           | 6.56e+03      |
-------------------------------------------
Eval num_timesteps=2946000, episode_reward=183.03 +/- 207.47
Episode length: 433.60 +/- 74.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 2946000      |
| train/                  |              |
|    approx_kl            | 0.0010090689 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 51.7         |
|    n_updates            | 14380        |
|    policy_gradient_loss | -0.000522    |
|    std                  | 4.85         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=2948000, episode_reward=111.18 +/- 451.68
Episode length: 431.60 +/- 92.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 111          |
| time/                   |              |
|    total_timesteps      | 2948000      |
| train/                  |              |
|    approx_kl            | 0.0030005956 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 48.5         |
|    n_updates            | 14390        |
|    policy_gradient_loss | -0.000964    |
|    std                  | 4.86         |
|    value_loss           | 208          |
------------------------------------------
Eval num_timesteps=2950000, episode_reward=85.78 +/- 140.04
Episode length: 442.80 +/- 52.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 443         |
|    mean_reward          | 85.8        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.005644246 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 85.7        |
|    n_updates            | 14400       |
|    policy_gradient_loss | -0.000616   |
|    std                  | 4.87        |
|    value_loss           | 278         |
-----------------------------------------
Eval num_timesteps=2952000, episode_reward=305.99 +/- 229.96
Episode length: 546.60 +/- 68.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 547          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 2952000      |
| train/                  |              |
|    approx_kl            | 0.0028760266 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 61.9         |
|    n_updates            | 14410        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 4.88         |
|    value_loss           | 229          |
------------------------------------------
Eval num_timesteps=2954000, episode_reward=182.64 +/- 335.01
Episode length: 558.40 +/- 66.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 558          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 2954000      |
| train/                  |              |
|    approx_kl            | 0.0013454139 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 60.6         |
|    n_updates            | 14420        |
|    policy_gradient_loss | 0.000536     |
|    std                  | 4.9          |
|    value_loss           | 205          |
------------------------------------------
Eval num_timesteps=2956000, episode_reward=-2.21 +/- 196.07
Episode length: 432.40 +/- 79.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | -2.21        |
| time/                   |              |
|    total_timesteps      | 2956000      |
| train/                  |              |
|    approx_kl            | 0.0013287554 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 63.8         |
|    n_updates            | 14430        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 4.9          |
|    value_loss           | 240          |
------------------------------------------
Eval num_timesteps=2958000, episode_reward=361.82 +/- 427.05
Episode length: 546.40 +/- 24.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 362          |
| time/                   |              |
|    total_timesteps      | 2958000      |
| train/                  |              |
|    approx_kl            | 0.0012640405 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.726        |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+03     |
|    n_updates            | 14440        |
|    policy_gradient_loss | 4.62e-05     |
|    std                  | 4.91         |
|    value_loss           | 5.13e+03     |
------------------------------------------
Eval num_timesteps=2960000, episode_reward=156.83 +/- 188.12
Episode length: 508.40 +/- 51.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 157          |
| time/                   |              |
|    total_timesteps      | 2960000      |
| train/                  |              |
|    approx_kl            | 0.0010192834 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.815        |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+03     |
|    n_updates            | 14450        |
|    policy_gradient_loss | -0.000919    |
|    std                  | 4.91         |
|    value_loss           | 3.9e+03      |
------------------------------------------
Eval num_timesteps=2962000, episode_reward=322.13 +/- 282.87
Episode length: 495.80 +/- 59.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 496           |
|    mean_reward          | 322           |
| time/                   |               |
|    total_timesteps      | 2962000       |
| train/                  |               |
|    approx_kl            | 0.00067205826 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.847         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+03      |
|    n_updates            | 14460         |
|    policy_gradient_loss | -0.000165     |
|    std                  | 4.91          |
|    value_loss           | 4.63e+03      |
-------------------------------------------
Eval num_timesteps=2964000, episode_reward=591.10 +/- 437.15
Episode length: 545.20 +/- 71.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 545           |
|    mean_reward          | 591           |
| time/                   |               |
|    total_timesteps      | 2964000       |
| train/                  |               |
|    approx_kl            | 0.00015821401 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+03      |
|    n_updates            | 14470         |
|    policy_gradient_loss | 2.48e-05      |
|    std                  | 4.91          |
|    value_loss           | 3.38e+03      |
-------------------------------------------
Eval num_timesteps=2966000, episode_reward=316.51 +/- 297.93
Episode length: 479.00 +/- 52.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 479           |
|    mean_reward          | 317           |
| time/                   |               |
|    total_timesteps      | 2966000       |
| train/                  |               |
|    approx_kl            | 0.00035677353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.001         |
|    loss                 | 196           |
|    n_updates            | 14480         |
|    policy_gradient_loss | -0.0004       |
|    std                  | 4.91          |
|    value_loss           | 780           |
-------------------------------------------
Eval num_timesteps=2968000, episode_reward=456.03 +/- 324.92
Episode length: 508.80 +/- 53.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 509        |
|    mean_reward          | 456        |
| time/                   |            |
|    total_timesteps      | 2968000    |
| train/                  |            |
|    approx_kl            | 0.00796097 |
|    clip_fraction        | 0.0219     |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 56         |
|    n_updates            | 14490      |
|    policy_gradient_loss | -0.00218   |
|    std                  | 4.91       |
|    value_loss           | 238        |
----------------------------------------
Eval num_timesteps=2970000, episode_reward=162.71 +/- 434.60
Episode length: 507.20 +/- 57.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 2970000     |
| train/                  |             |
|    approx_kl            | 0.001410039 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.001       |
|    loss                 | 2.73e+03    |
|    n_updates            | 14500       |
|    policy_gradient_loss | 0.000117    |
|    std                  | 4.91        |
|    value_loss           | 7.06e+03    |
-----------------------------------------
Eval num_timesteps=2972000, episode_reward=224.60 +/- 346.48
Episode length: 446.20 +/- 97.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 2972000      |
| train/                  |              |
|    approx_kl            | 7.525252e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 76.6         |
|    n_updates            | 14510        |
|    policy_gradient_loss | -8.05e-05    |
|    std                  | 4.91         |
|    value_loss           | 313          |
------------------------------------------
Eval num_timesteps=2974000, episode_reward=495.92 +/- 560.16
Episode length: 545.40 +/- 46.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 545           |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 2974000       |
| train/                  |               |
|    approx_kl            | 0.00030085852 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 226           |
|    n_updates            | 14520         |
|    policy_gradient_loss | -0.000444     |
|    std                  | 4.9           |
|    value_loss           | 751           |
-------------------------------------------
Eval num_timesteps=2976000, episode_reward=99.99 +/- 516.84
Episode length: 438.20 +/- 75.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 100           |
| time/                   |               |
|    total_timesteps      | 2976000       |
| train/                  |               |
|    approx_kl            | 0.00030780042 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.761         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 14530         |
|    policy_gradient_loss | -0.000181     |
|    std                  | 4.9           |
|    value_loss           | 4.87e+03      |
-------------------------------------------
Eval num_timesteps=2978000, episode_reward=288.29 +/- 235.38
Episode length: 434.00 +/- 69.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 288           |
| time/                   |               |
|    total_timesteps      | 2978000       |
| train/                  |               |
|    approx_kl            | 0.00077942247 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.001         |
|    loss                 | 64.4          |
|    n_updates            | 14540         |
|    policy_gradient_loss | -0.000388     |
|    std                  | 4.9           |
|    value_loss           | 269           |
-------------------------------------------
Eval num_timesteps=2980000, episode_reward=295.16 +/- 496.85
Episode length: 455.40 +/- 69.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 295           |
| time/                   |               |
|    total_timesteps      | 2980000       |
| train/                  |               |
|    approx_kl            | 0.00038972762 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.858         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 14550         |
|    policy_gradient_loss | 0.000479      |
|    std                  | 4.9           |
|    value_loss           | 4.36e+03      |
-------------------------------------------
Eval num_timesteps=2982000, episode_reward=446.66 +/- 486.07
Episode length: 474.00 +/- 44.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | 447         |
| time/                   |             |
|    total_timesteps      | 2982000     |
| train/                  |             |
|    approx_kl            | 0.000207682 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 81.3        |
|    n_updates            | 14560       |
|    policy_gradient_loss | -0.00036    |
|    std                  | 4.91        |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=2984000, episode_reward=276.46 +/- 268.93
Episode length: 483.00 +/- 81.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 276           |
| time/                   |               |
|    total_timesteps      | 2984000       |
| train/                  |               |
|    approx_kl            | 0.00039720588 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.798         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+03      |
|    n_updates            | 14570         |
|    policy_gradient_loss | -0.000738     |
|    std                  | 4.92          |
|    value_loss           | 4.7e+03       |
-------------------------------------------
Eval num_timesteps=2986000, episode_reward=412.69 +/- 486.17
Episode length: 428.80 +/- 57.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 413           |
| time/                   |               |
|    total_timesteps      | 2986000       |
| train/                  |               |
|    approx_kl            | 0.00033076698 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+03      |
|    n_updates            | 14580         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 4.93          |
|    value_loss           | 3.42e+03      |
-------------------------------------------
Eval num_timesteps=2988000, episode_reward=251.21 +/- 281.75
Episode length: 424.40 +/- 119.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 424      |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=-55.28 +/- 386.59
Episode length: 483.20 +/- 55.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | -55.3         |
| time/                   |               |
|    total_timesteps      | 2990000       |
| train/                  |               |
|    approx_kl            | 0.00026190587 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 14590         |
|    policy_gradient_loss | -0.000503     |
|    std                  | 4.94          |
|    value_loss           | 3.94e+03      |
-------------------------------------------
Eval num_timesteps=2992000, episode_reward=224.10 +/- 158.27
Episode length: 423.60 +/- 91.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 224           |
| time/                   |               |
|    total_timesteps      | 2992000       |
| train/                  |               |
|    approx_kl            | 0.00055919157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 167           |
|    n_updates            | 14600         |
|    policy_gradient_loss | -0.000425     |
|    std                  | 4.94          |
|    value_loss           | 734           |
-------------------------------------------
Eval num_timesteps=2994000, episode_reward=368.49 +/- 595.61
Episode length: 431.20 +/- 33.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 368          |
| time/                   |              |
|    total_timesteps      | 2994000      |
| train/                  |              |
|    approx_kl            | 0.0003348397 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+03     |
|    n_updates            | 14610        |
|    policy_gradient_loss | 0.000104     |
|    std                  | 4.95         |
|    value_loss           | 3.5e+03      |
------------------------------------------
Eval num_timesteps=2996000, episode_reward=237.45 +/- 380.22
Episode length: 443.60 +/- 40.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 237          |
| time/                   |              |
|    total_timesteps      | 2996000      |
| train/                  |              |
|    approx_kl            | 0.0001200269 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+03     |
|    n_updates            | 14620        |
|    policy_gradient_loss | -0.000235    |
|    std                  | 4.95         |
|    value_loss           | 4.52e+03     |
------------------------------------------
Eval num_timesteps=2998000, episode_reward=371.15 +/- 175.06
Episode length: 438.80 +/- 62.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 371           |
| time/                   |               |
|    total_timesteps      | 2998000       |
| train/                  |               |
|    approx_kl            | 0.00028492627 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 79.9          |
|    n_updates            | 14630         |
|    policy_gradient_loss | -0.0004       |
|    std                  | 4.94          |
|    value_loss           | 311           |
-------------------------------------------
Eval num_timesteps=3000000, episode_reward=118.33 +/- 244.04
Episode length: 466.80 +/- 87.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 118           |
| time/                   |               |
|    total_timesteps      | 3000000       |
| train/                  |               |
|    approx_kl            | 0.00025668697 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 38            |
|    n_updates            | 14640         |
|    policy_gradient_loss | -0.00018      |
|    std                  | 4.94          |
|    value_loss           | 218           |
-------------------------------------------
Eval num_timesteps=3002000, episode_reward=382.84 +/- 168.37
Episode length: 455.60 +/- 81.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 383          |
| time/                   |              |
|    total_timesteps      | 3002000      |
| train/                  |              |
|    approx_kl            | 0.0017422537 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 64.4         |
|    n_updates            | 14650        |
|    policy_gradient_loss | -0.000536    |
|    std                  | 4.96         |
|    value_loss           | 254          |
------------------------------------------
Eval num_timesteps=3004000, episode_reward=523.73 +/- 374.08
Episode length: 541.00 +/- 26.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 541         |
|    mean_reward          | 524         |
| time/                   |             |
|    total_timesteps      | 3004000     |
| train/                  |             |
|    approx_kl            | 0.000674309 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 73.9        |
|    n_updates            | 14660       |
|    policy_gradient_loss | -0.000643   |
|    std                  | 4.99        |
|    value_loss           | 270         |
-----------------------------------------
Eval num_timesteps=3006000, episode_reward=172.65 +/- 294.42
Episode length: 513.20 +/- 53.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 513          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 3006000      |
| train/                  |              |
|    approx_kl            | 0.0005458241 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.001        |
|    loss                 | 3.04e+03     |
|    n_updates            | 14670        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 5.01         |
|    value_loss           | 8.58e+03     |
------------------------------------------
Eval num_timesteps=3008000, episode_reward=228.95 +/- 120.45
Episode length: 463.20 +/- 78.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 229          |
| time/                   |              |
|    total_timesteps      | 3008000      |
| train/                  |              |
|    approx_kl            | 0.0033539142 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 32.6         |
|    n_updates            | 14680        |
|    policy_gradient_loss | -0.00186     |
|    std                  | 5.01         |
|    value_loss           | 164          |
------------------------------------------
Eval num_timesteps=3010000, episode_reward=409.17 +/- 527.65
Episode length: 494.80 +/- 98.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 409          |
| time/                   |              |
|    total_timesteps      | 3010000      |
| train/                  |              |
|    approx_kl            | 0.0033803887 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 125          |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.000587    |
|    std                  | 5.01         |
|    value_loss           | 821          |
------------------------------------------
Eval num_timesteps=3012000, episode_reward=302.52 +/- 210.32
Episode length: 449.20 +/- 85.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 303           |
| time/                   |               |
|    total_timesteps      | 3012000       |
| train/                  |               |
|    approx_kl            | 0.00052205025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.9         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 64.8          |
|    n_updates            | 14700         |
|    policy_gradient_loss | 0.000397      |
|    std                  | 5.01          |
|    value_loss           | 373           |
-------------------------------------------
Eval num_timesteps=3014000, episode_reward=150.49 +/- 174.59
Episode length: 424.60 +/- 96.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 150          |
| time/                   |              |
|    total_timesteps      | 3014000      |
| train/                  |              |
|    approx_kl            | 0.0018732903 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 55.4         |
|    n_updates            | 14710        |
|    policy_gradient_loss | -0.00205     |
|    std                  | 4.99         |
|    value_loss           | 240          |
------------------------------------------
Eval num_timesteps=3016000, episode_reward=270.73 +/- 311.62
Episode length: 397.80 +/- 107.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 3016000      |
| train/                  |              |
|    approx_kl            | 0.0061598346 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 37.3         |
|    n_updates            | 14720        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 4.98         |
|    value_loss           | 193          |
------------------------------------------
Eval num_timesteps=3018000, episode_reward=398.35 +/- 271.84
Episode length: 457.60 +/- 103.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | 398         |
| time/                   |             |
|    total_timesteps      | 3018000     |
| train/                  |             |
|    approx_kl            | 0.002060493 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 48          |
|    n_updates            | 14730       |
|    policy_gradient_loss | -0.000201   |
|    std                  | 4.98        |
|    value_loss           | 247         |
-----------------------------------------
Eval num_timesteps=3020000, episode_reward=262.32 +/- 321.13
Episode length: 382.00 +/- 35.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 3020000      |
| train/                  |              |
|    approx_kl            | 0.0014938428 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 45.5         |
|    n_updates            | 14740        |
|    policy_gradient_loss | -0.000938    |
|    std                  | 4.99         |
|    value_loss           | 151          |
------------------------------------------
Eval num_timesteps=3022000, episode_reward=85.46 +/- 100.65
Episode length: 382.20 +/- 31.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 382         |
|    mean_reward          | 85.5        |
| time/                   |             |
|    total_timesteps      | 3022000     |
| train/                  |             |
|    approx_kl            | 0.005820644 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 36.1        |
|    n_updates            | 14750       |
|    policy_gradient_loss | -0.00182    |
|    std                  | 5           |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=3024000, episode_reward=56.09 +/- 68.35
Episode length: 332.40 +/- 23.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 332         |
|    mean_reward          | 56.1        |
| time/                   |             |
|    total_timesteps      | 3024000     |
| train/                  |             |
|    approx_kl            | 0.005450157 |
|    clip_fraction        | 0.00815     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 42.8        |
|    n_updates            | 14760       |
|    policy_gradient_loss | -0.00224    |
|    std                  | 5.01        |
|    value_loss           | 179         |
-----------------------------------------
Eval num_timesteps=3026000, episode_reward=142.74 +/- 202.65
Episode length: 345.00 +/- 33.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 345          |
|    mean_reward          | 143          |
| time/                   |              |
|    total_timesteps      | 3026000      |
| train/                  |              |
|    approx_kl            | 0.0022204462 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 14770        |
|    policy_gradient_loss | 0.000237     |
|    std                  | 5.02         |
|    value_loss           | 487          |
------------------------------------------
Eval num_timesteps=3028000, episode_reward=218.83 +/- 250.06
Episode length: 502.40 +/- 143.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 502          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 3028000      |
| train/                  |              |
|    approx_kl            | 0.0020063114 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 63.3         |
|    n_updates            | 14780        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 5.04         |
|    value_loss           | 190          |
------------------------------------------
Eval num_timesteps=3030000, episode_reward=648.61 +/- 722.12
Episode length: 437.00 +/- 111.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | 649         |
| time/                   |             |
|    total_timesteps      | 3030000     |
| train/                  |             |
|    approx_kl            | 0.008028761 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 34.6        |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.00256    |
|    std                  | 5.06        |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=3032000, episode_reward=127.07 +/- 191.67
Episode length: 379.40 +/- 13.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 127         |
| time/                   |             |
|    total_timesteps      | 3032000     |
| train/                  |             |
|    approx_kl            | 0.004342995 |
|    clip_fraction        | 0.00464     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 31.8        |
|    n_updates            | 14800       |
|    policy_gradient_loss | -0.00183    |
|    std                  | 5.08        |
|    value_loss           | 130         |
-----------------------------------------
Eval num_timesteps=3034000, episode_reward=197.29 +/- 269.40
Episode length: 477.60 +/- 61.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 3034000     |
| train/                  |             |
|    approx_kl            | 0.009484185 |
|    clip_fraction        | 0.0537      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 25.6        |
|    n_updates            | 14810       |
|    policy_gradient_loss | -0.00351    |
|    std                  | 5.11        |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=3036000, episode_reward=14.32 +/- 108.34
Episode length: 464.00 +/- 70.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 3036000      |
| train/                  |              |
|    approx_kl            | 0.0034577153 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 95.6         |
|    n_updates            | 14820        |
|    policy_gradient_loss | -0.000704    |
|    std                  | 5.14         |
|    value_loss           | 449          |
------------------------------------------
Eval num_timesteps=3038000, episode_reward=58.79 +/- 89.62
Episode length: 524.80 +/- 84.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 58.8       |
| time/                   |            |
|    total_timesteps      | 3038000    |
| train/                  |            |
|    approx_kl            | 0.00642136 |
|    clip_fraction        | 0.0136     |
|    clip_range           | 0.2        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.001      |
|    loss                 | 86.7       |
|    n_updates            | 14830      |
|    policy_gradient_loss | -0.00221   |
|    std                  | 5.16       |
|    value_loss           | 319        |
----------------------------------------
Eval num_timesteps=3040000, episode_reward=-65.46 +/- 157.94
Episode length: 467.00 +/- 53.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | -65.5       |
| time/                   |             |
|    total_timesteps      | 3040000     |
| train/                  |             |
|    approx_kl            | 0.003512418 |
|    clip_fraction        | 0.00361     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 2.58e+03    |
|    n_updates            | 14840       |
|    policy_gradient_loss | -0.000642   |
|    std                  | 5.17        |
|    value_loss           | 6.75e+03    |
-----------------------------------------
Eval num_timesteps=3042000, episode_reward=209.62 +/- 293.73
Episode length: 527.00 +/- 98.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 527          |
|    mean_reward          | 210          |
| time/                   |              |
|    total_timesteps      | 3042000      |
| train/                  |              |
|    approx_kl            | 0.0022471328 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 58           |
|    n_updates            | 14850        |
|    policy_gradient_loss | -0.00141     |
|    std                  | 5.18         |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=3044000, episode_reward=8.96 +/- 94.59
Episode length: 478.00 +/- 44.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 8.96        |
| time/                   |             |
|    total_timesteps      | 3044000     |
| train/                  |             |
|    approx_kl            | 0.005095925 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 170         |
|    n_updates            | 14860       |
|    policy_gradient_loss | -0.00178    |
|    std                  | 5.19        |
|    value_loss           | 663         |
-----------------------------------------
Eval num_timesteps=3046000, episode_reward=66.93 +/- 329.01
Episode length: 479.60 +/- 72.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 66.9         |
| time/                   |              |
|    total_timesteps      | 3046000      |
| train/                  |              |
|    approx_kl            | 0.0067495145 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 86.3         |
|    n_updates            | 14870        |
|    policy_gradient_loss | -0.00241     |
|    std                  | 5.21         |
|    value_loss           | 332          |
------------------------------------------
Eval num_timesteps=3048000, episode_reward=-105.56 +/- 52.81
Episode length: 415.20 +/- 74.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | -106         |
| time/                   |              |
|    total_timesteps      | 3048000      |
| train/                  |              |
|    approx_kl            | 0.0030183177 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 71.4         |
|    n_updates            | 14880        |
|    policy_gradient_loss | -0.00131     |
|    std                  | 5.24         |
|    value_loss           | 290          |
------------------------------------------
Eval num_timesteps=3050000, episode_reward=-65.04 +/- 77.98
Episode length: 513.00 +/- 100.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | -65         |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.006233058 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.1       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.001       |
|    loss                 | 85.7        |
|    n_updates            | 14890       |
|    policy_gradient_loss | 0.000439    |
|    std                  | 5.27        |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=3052000, episode_reward=-117.82 +/- 249.50
Episode length: 502.60 +/- 39.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 3052000      |
| train/                  |              |
|    approx_kl            | 0.0005903044 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 62.1         |
|    n_updates            | 14900        |
|    policy_gradient_loss | -0.000207    |
|    std                  | 5.29         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=3054000, episode_reward=45.15 +/- 333.53
Episode length: 465.60 +/- 80.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 45.1         |
| time/                   |              |
|    total_timesteps      | 3054000      |
| train/                  |              |
|    approx_kl            | 0.0010798132 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 177          |
|    n_updates            | 14910        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 5.31         |
|    value_loss           | 803          |
------------------------------------------
Eval num_timesteps=3056000, episode_reward=27.67 +/- 246.28
Episode length: 456.00 +/- 41.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 27.7         |
| time/                   |              |
|    total_timesteps      | 3056000      |
| train/                  |              |
|    approx_kl            | 0.0010910431 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 108          |
|    n_updates            | 14920        |
|    policy_gradient_loss | -0.00091     |
|    std                  | 5.35         |
|    value_loss           | 362          |
------------------------------------------
Eval num_timesteps=3058000, episode_reward=-61.00 +/- 109.55
Episode length: 537.20 +/- 88.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 537         |
|    mean_reward          | -61         |
| time/                   |             |
|    total_timesteps      | 3058000     |
| train/                  |             |
|    approx_kl            | 0.006108008 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.2       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 65.1        |
|    n_updates            | 14930       |
|    policy_gradient_loss | -0.0013     |
|    std                  | 5.38        |
|    value_loss           | 277         |
-----------------------------------------
Eval num_timesteps=3060000, episode_reward=-63.11 +/- 102.32
Episode length: 614.00 +/- 61.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 614          |
|    mean_reward          | -63.1        |
| time/                   |              |
|    total_timesteps      | 3060000      |
| train/                  |              |
|    approx_kl            | 0.0009966138 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 59.2         |
|    n_updates            | 14940        |
|    policy_gradient_loss | -0.000585    |
|    std                  | 5.4          |
|    value_loss           | 196          |
------------------------------------------
Eval num_timesteps=3062000, episode_reward=-87.76 +/- 44.23
Episode length: 490.00 +/- 80.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | -87.8        |
| time/                   |              |
|    total_timesteps      | 3062000      |
| train/                  |              |
|    approx_kl            | 0.0031450577 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 50.3         |
|    n_updates            | 14950        |
|    policy_gradient_loss | -0.00179     |
|    std                  | 5.42         |
|    value_loss           | 204          |
------------------------------------------
Eval num_timesteps=3064000, episode_reward=-27.15 +/- 215.63
Episode length: 448.60 +/- 73.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | -27.1        |
| time/                   |              |
|    total_timesteps      | 3064000      |
| train/                  |              |
|    approx_kl            | 0.0024189316 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 75.7         |
|    n_updates            | 14960        |
|    policy_gradient_loss | -0.00044     |
|    std                  | 5.43         |
|    value_loss           | 257          |
------------------------------------------
Eval num_timesteps=3066000, episode_reward=-119.56 +/- 34.27
Episode length: 431.80 +/- 67.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | -120         |
| time/                   |              |
|    total_timesteps      | 3066000      |
| train/                  |              |
|    approx_kl            | 0.0010611002 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 74.9         |
|    n_updates            | 14970        |
|    policy_gradient_loss | 0.000777     |
|    std                  | 5.43         |
|    value_loss           | 261          |
------------------------------------------
Eval num_timesteps=3068000, episode_reward=128.38 +/- 474.58
Episode length: 522.40 +/- 167.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 3068000      |
| train/                  |              |
|    approx_kl            | 0.0011185769 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 84.9         |
|    n_updates            | 14980        |
|    policy_gradient_loss | -0.00122     |
|    std                  | 5.44         |
|    value_loss           | 275          |
------------------------------------------
Eval num_timesteps=3070000, episode_reward=-187.98 +/- 104.19
Episode length: 478.40 +/- 98.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | -188         |
| time/                   |              |
|    total_timesteps      | 3070000      |
| train/                  |              |
|    approx_kl            | 0.0028753537 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 58.2         |
|    n_updates            | 14990        |
|    policy_gradient_loss | -0.00103     |
|    std                  | 5.45         |
|    value_loss           | 232          |
------------------------------------------
Eval num_timesteps=3072000, episode_reward=-183.43 +/- 15.12
Episode length: 355.60 +/- 36.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3074000, episode_reward=-61.37 +/- 182.62
Episode length: 462.00 +/- 113.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 462         |
|    mean_reward          | -61.4       |
| time/                   |             |
|    total_timesteps      | 3074000     |
| train/                  |             |
|    approx_kl            | 0.004044787 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.2       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 34.9        |
|    n_updates            | 15000       |
|    policy_gradient_loss | -0.000883   |
|    std                  | 5.47        |
|    value_loss           | 139         |
-----------------------------------------
Eval num_timesteps=3076000, episode_reward=-68.93 +/- 102.80
Episode length: 452.60 +/- 109.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 453        |
|    mean_reward          | -68.9      |
| time/                   |            |
|    total_timesteps      | 3076000    |
| train/                  |            |
|    approx_kl            | 0.00523657 |
|    clip_fraction        | 0.00645    |
|    clip_range           | 0.2        |
|    entropy_loss         | -12.3      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.001      |
|    loss                 | 49.8       |
|    n_updates            | 15010      |
|    policy_gradient_loss | -0.00137   |
|    std                  | 5.5        |
|    value_loss           | 180        |
----------------------------------------
Eval num_timesteps=3078000, episode_reward=-6.59 +/- 235.88
Episode length: 516.20 +/- 77.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -6.59        |
| time/                   |              |
|    total_timesteps      | 3078000      |
| train/                  |              |
|    approx_kl            | 0.0021209125 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 15020        |
|    policy_gradient_loss | 0.00138      |
|    std                  | 5.53         |
|    value_loss           | 562          |
------------------------------------------
Eval num_timesteps=3080000, episode_reward=-128.75 +/- 80.76
Episode length: 438.60 +/- 68.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 439         |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 3080000     |
| train/                  |             |
|    approx_kl            | 0.002220093 |
|    clip_fraction        | 0.00142     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 49          |
|    n_updates            | 15030       |
|    policy_gradient_loss | -0.000979   |
|    std                  | 5.55        |
|    value_loss           | 157         |
-----------------------------------------
Eval num_timesteps=3082000, episode_reward=-134.24 +/- 30.84
Episode length: 335.00 +/- 52.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | -134         |
| time/                   |              |
|    total_timesteps      | 3082000      |
| train/                  |              |
|    approx_kl            | 0.0029263971 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 44.7         |
|    n_updates            | 15040        |
|    policy_gradient_loss | -0.00052     |
|    std                  | 5.55         |
|    value_loss           | 150          |
------------------------------------------
Eval num_timesteps=3084000, episode_reward=-154.22 +/- 49.76
Episode length: 402.40 +/- 41.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | -154          |
| time/                   |               |
|    total_timesteps      | 3084000       |
| train/                  |               |
|    approx_kl            | 0.00058779545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.3         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 37.9          |
|    n_updates            | 15050         |
|    policy_gradient_loss | -0.000175     |
|    std                  | 5.55          |
|    value_loss           | 146           |
-------------------------------------------
Eval num_timesteps=3086000, episode_reward=-148.68 +/- 31.34
Episode length: 386.80 +/- 57.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | -149         |
| time/                   |              |
|    total_timesteps      | 3086000      |
| train/                  |              |
|    approx_kl            | 0.0034775804 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 27.1         |
|    n_updates            | 15060        |
|    policy_gradient_loss | -0.000929    |
|    std                  | 5.54         |
|    value_loss           | 106          |
------------------------------------------
Traceback (most recent call last):
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] The pipe has been ended
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 678, in <module>
    sim.run_full()
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 416, in run_full
    model.learn(total_timesteps=int(args.max_steps),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 194, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 206, in step
    return self.step_wait()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 321, in _recv_bytes
    raise EOFError
EOFError