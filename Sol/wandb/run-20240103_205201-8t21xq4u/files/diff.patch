diff --git a/Sol/Model/pybullet_drone_simulator.py b/Sol/Model/pybullet_drone_simulator.py
index bbe4ae1..5388f27 100644
--- a/Sol/Model/pybullet_drone_simulator.py
+++ b/Sol/Model/pybullet_drone_simulator.py
@@ -249,8 +249,8 @@ class PBDroneSimulator:
                                                  aviary_dim=np.array([-2, -2, 0, 2, 2, 2])) for i in
                                    range(self.num_cpu)])
         # train_env = VecCheckNan(train_env)
-        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
-                                 clip_obs=1)
+        # train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
+        #                          clip_obs=1)
 
         # eval_env = make_env(multi=False, gui=False, rank=0)
         #
@@ -258,12 +258,12 @@ class PBDroneSimulator:
                                                 aviary_dim=np.array([-2, -2, 0, 2, 2, 2])), ])
         # eval_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i) for i in range(self.num_cpu)])
         # eval_env = VecCheckNan(eval_env)
-        eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
-                                clip_obs=1)
+        # eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
+        #                         clip_obs=1)
 
-        # onpolicy_kwargs = dict(activation_fn=th.nn.ReLU,
-        #                        net_arch=dict(vf=[512, 512, 256, 128],
-        #                                      pi=[512, 512, 256, 128]))
+        onpolicy_kwargs = dict(activation_fn=th.nn.Tanh,
+                               net_arch=dict(vf=[512, 512, 256, 128],
+                                             pi=[512, 512, 256, 128]))
         # onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
 
         model = PPO("MlpPolicy",
@@ -278,8 +278,8 @@ class PBDroneSimulator:
                     learning_rate=1e-3,
                     tensorboard_log="./logs/ppo_tensorboard/",
                     device="auto",
-                    policy_kwargs=  # onpolicy_kwargs
-                    dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+                    policy_kwargs=   onpolicy_kwargs
+                    # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
                     )
 
         # tensorboard --logdir ./logs/ppo_tensorboard/
