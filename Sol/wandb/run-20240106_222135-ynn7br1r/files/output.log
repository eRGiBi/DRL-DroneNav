AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO 01.06.2024_22.21.57_1
C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py:155: UserWarning: You have specified a mini-batch size of 49152, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=2048 and n_envs=1)
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=2000, episode_reward=-2884.26 +/- 51.28
Episode length: 246.60 +/- 70.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 247       |
|    mean_reward     | -2.88e+03 |
| time/              |           |
|    total_timesteps | 2000      |
----------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-2919.74 +/- 32.26
Episode length: 202.20 +/- 47.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 4000         |
| train/                  |              |
|    approx_kl            | 0.0009256381 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.68        |
|    explained_variance   | -2.62e-06    |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+05      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00275     |
|    std                  | 1            |
|    value_loss           | 3.41e+05     |
------------------------------------------
Eval num_timesteps=6000, episode_reward=-2909.85 +/- 14.37
Episode length: 224.20 +/- 24.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 224           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 6000          |
| train/                  |               |
|    approx_kl            | 0.00019642082 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.68         |
|    explained_variance   | 0.00284       |
|    learning_rate        | 0.001         |
|    loss                 | 1.88e+05      |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000439     |
|    std                  | 1             |
|    value_loss           | 3.77e+05      |
-------------------------------------------
Eval num_timesteps=8000, episode_reward=-2890.66 +/- 28.14
Episode length: 262.00 +/- 30.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 8000         |
| train/                  |              |
|    approx_kl            | 8.577661e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.00126      |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+05      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.000371    |
|    std                  | 1            |
|    value_loss           | 3.01e+05     |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-2893.20 +/- 34.23
Episode length: 245.20 +/- 48.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 245           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 10000         |
| train/                  |               |
|    approx_kl            | 8.1403006e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.69         |
|    explained_variance   | 0.00175       |
|    learning_rate        | 0.001         |
|    loss                 | 1.88e+05      |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.00044      |
|    std                  | 1             |
|    value_loss           | 3.76e+05      |
-------------------------------------------
Eval num_timesteps=12000, episode_reward=-2927.05 +/- 32.85
Episode length: 216.60 +/- 30.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 217           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 12000         |
| train/                  |               |
|    approx_kl            | 7.5835735e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.7          |
|    explained_variance   | -3.22e-06     |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+05      |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.000414     |
|    std                  | 1.01          |
|    value_loss           | 3.37e+05      |
-------------------------------------------
Eval num_timesteps=14000, episode_reward=-2904.95 +/- 27.86
Episode length: 243.00 +/- 56.42
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 243            |
|    mean_reward          | -2.9e+03       |
| time/                   |                |
|    total_timesteps      | 14000          |
| train/                  |                |
|    approx_kl            | 0.000115303235 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.7           |
|    explained_variance   | 5.96e-07       |
|    learning_rate        | 0.001          |
|    loss                 | 1.5e+05        |
|    n_updates            | 60             |
|    policy_gradient_loss | -0.000556      |
|    std                  | 1.01           |
|    value_loss           | 3.01e+05       |
--------------------------------------------
Eval num_timesteps=16000, episode_reward=-2855.67 +/- 62.98
Episode length: 271.40 +/- 70.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 271           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 16000         |
| train/                  |               |
|    approx_kl            | 0.00012750222 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.71         |
|    explained_variance   | -5.03e-05     |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+05      |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000349     |
|    std                  | 1.01          |
|    value_loss           | 3.37e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=18000, episode_reward=-2894.97 +/- 19.81
Episode length: 235.80 +/- 34.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 18000        |
| train/                  |              |
|    approx_kl            | 8.400736e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | -2.42e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+05     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.000251    |
|    std                  | 1.01         |
|    value_loss           | 2.9e+05      |
------------------------------------------
Eval num_timesteps=20000, episode_reward=-2902.50 +/- 34.33
Episode length: 207.00 +/- 39.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 207           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 20000         |
| train/                  |               |
|    approx_kl            | 3.4011988e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.72         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.49e+05      |
|    n_updates            | 90            |
|    policy_gradient_loss | -7.72e-05     |
|    std                  | 1.01          |
|    value_loss           | 2.99e+05      |
-------------------------------------------
Eval num_timesteps=22000, episode_reward=-2922.57 +/- 37.84
Episode length: 184.80 +/- 24.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 185           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 3.6012003e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.72         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.87e+05      |
|    n_updates            | 100           |
|    policy_gradient_loss | -0.000152     |
|    std                  | 1.01          |
|    value_loss           | 3.74e+05      |
-------------------------------------------
Eval num_timesteps=24000, episode_reward=-2895.17 +/- 39.05
Episode length: 262.60 +/- 60.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 7.494353e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+05     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000451    |
|    std                  | 1.01         |
|    value_loss           | 2.98e+05     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-2870.45 +/- 15.92
Episode length: 269.60 +/- 23.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 270           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 26000         |
| train/                  |               |
|    approx_kl            | 0.00010322966 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.72         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+05      |
|    n_updates            | 120           |
|    policy_gradient_loss | -0.00053      |
|    std                  | 1.01          |
|    value_loss           | 3.36e+05      |
-------------------------------------------
Eval num_timesteps=28000, episode_reward=-2906.34 +/- 15.00
Episode length: 248.40 +/- 30.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 5.541509e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+05     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000104    |
|    std                  | 1.01         |
|    value_loss           | 3.35e+05     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-2922.79 +/- 33.67
Episode length: 213.20 +/- 60.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 213           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 0.00012373217 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.73         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+05      |
|    n_updates            | 140           |
|    policy_gradient_loss | -0.00077      |
|    std                  | 1.02          |
|    value_loss           | 3.14e+05      |
-------------------------------------------
Eval num_timesteps=32000, episode_reward=-2916.78 +/- 23.67
Episode length: 193.40 +/- 23.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 193           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 32000         |
| train/                  |               |
|    approx_kl            | 0.00021967935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.75         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+05      |
|    n_updates            | 150           |
|    policy_gradient_loss | -0.000706     |
|    std                  | 1.02          |
|    value_loss           | 2.97e+05      |
-------------------------------------------
Eval num_timesteps=34000, episode_reward=-2900.05 +/- 19.85
Episode length: 238.00 +/- 12.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 0.0001202134 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.76        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+05     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000223    |
|    std                  | 1.02         |
|    value_loss           | 3.34e+05     |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-2909.50 +/- 24.73
Episode length: 226.60 +/- 48.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 36000        |
| train/                  |              |
|    approx_kl            | 7.699948e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+05     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000332    |
|    std                  | 1.02         |
|    value_loss           | 3.71e+05     |
------------------------------------------
Eval num_timesteps=38000, episode_reward=-2901.04 +/- 38.40
Episode length: 216.00 +/- 33.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 216           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 0.00010164906 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.77         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+05      |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000581     |
|    std                  | 1.03          |
|    value_loss           | 3.71e+05      |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-2909.08 +/- 40.24
Episode length: 194.60 +/- 52.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | -2.91e+03   |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 7.91818e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.48e+05    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00023    |
|    std                  | 1.03        |
|    value_loss           | 2.96e+05    |
-----------------------------------------
Eval num_timesteps=42000, episode_reward=-2881.72 +/- 67.75
Episode length: 239.40 +/- 55.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 239           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 42000         |
| train/                  |               |
|    approx_kl            | 8.8761386e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.79         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+05      |
|    n_updates            | 200           |
|    policy_gradient_loss | -0.000369     |
|    std                  | 1.03          |
|    value_loss           | 3.38e+05      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=-2921.06 +/- 33.34
Episode length: 226.60 +/- 44.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 44000        |
| train/                  |              |
|    approx_kl            | 7.903535e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | -2.62e-06    |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+05     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000368    |
|    std                  | 1.03         |
|    value_loss           | 3.32e+05     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-2902.29 +/- 22.26
Episode length: 239.80 +/- 44.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 240           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 46000         |
| train/                  |               |
|    approx_kl            | 0.00010816395 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.8          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+05      |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.000304     |
|    std                  | 1.03          |
|    value_loss           | 3.32e+05      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=-2916.48 +/- 42.35
Episode length: 248.60 +/- 98.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 249           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 48000         |
| train/                  |               |
|    approx_kl            | 0.00011264329 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.8          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+05      |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.000543     |
|    std                  | 1.03          |
|    value_loss           | 2.95e+05      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-2922.02 +/- 17.81
Episode length: 200.60 +/- 32.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 201           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00020976635 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.81         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+05      |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000825     |
|    std                  | 1.04          |
|    value_loss           | 2.94e+05      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=-2876.54 +/- 25.78
Episode length: 248.60 +/- 44.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0002136993 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.82        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+05     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000753    |
|    std                  | 1.04         |
|    value_loss           | 4.04e+05     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-2883.09 +/- 38.96
Episode length: 279.60 +/- 87.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 54000         |
| train/                  |               |
|    approx_kl            | 0.00017929194 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.82         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+05      |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000528     |
|    std                  | 1.04          |
|    value_loss           | 2.58e+05      |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=-2905.01 +/- 26.31
Episode length: 200.60 +/- 31.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 201           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 56000         |
| train/                  |               |
|    approx_kl            | 0.00018351572 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+05      |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000688     |
|    std                  | 1.04          |
|    value_loss           | 3.3e+05       |
-------------------------------------------
Eval num_timesteps=58000, episode_reward=-2871.93 +/- 37.31
Episode length: 261.80 +/- 85.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 58000         |
| train/                  |               |
|    approx_kl            | 0.00016249754 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+05      |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000645     |
|    std                  | 1.04          |
|    value_loss           | 2.93e+05      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-2865.56 +/- 39.95
Episode length: 263.40 +/- 24.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0001670022 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.82        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000816    |
|    std                  | 1.04         |
|    value_loss           | 2.57e+05     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-2895.94 +/- 15.10
Episode length: 268.80 +/- 72.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 62000         |
| train/                  |               |
|    approx_kl            | 0.00017617937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+05      |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000488     |
|    std                  | 1.04          |
|    value_loss           | 2.56e+05      |
-------------------------------------------
Eval num_timesteps=64000, episode_reward=-2879.98 +/- 58.26
Episode length: 251.40 +/- 60.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 64000         |
| train/                  |               |
|    approx_kl            | 0.00013763455 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+05      |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000547     |
|    std                  | 1.04          |
|    value_loss           | 2.56e+05      |
-------------------------------------------
Eval num_timesteps=66000, episode_reward=-2879.72 +/- 90.22
Episode length: 248.20 +/- 94.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 248           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | 0.00015652488 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+05      |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000386     |
|    std                  | 1.04          |
|    value_loss           | 2.92e+05      |
-------------------------------------------
Eval num_timesteps=68000, episode_reward=-2834.66 +/- 58.35
Episode length: 316.00 +/- 134.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 0.00013821418 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000393     |
|    std                  | 1.04          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=70000, episode_reward=-2832.61 +/- 59.38
Episode length: 288.00 +/- 68.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 288           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 0.00014961616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+05      |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000558     |
|    std                  | 1.04          |
|    value_loss           | 2.55e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=72000, episode_reward=-2885.53 +/- 46.60
Episode length: 255.20 +/- 82.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 0.00011074322 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.85         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+05      |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000485     |
|    std                  | 1.04          |
|    value_loss           | 3.27e+05      |
-------------------------------------------
Eval num_timesteps=74000, episode_reward=-2896.28 +/- 46.32
Episode length: 257.60 +/- 83.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 9.404367e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.85        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+05     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000234    |
|    std                  | 1.05         |
|    value_loss           | 2.54e+05     |
------------------------------------------
Eval num_timesteps=76000, episode_reward=-2875.72 +/- 46.80
Episode length: 230.40 +/- 27.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0002492906 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000956    |
|    std                  | 1.05         |
|    value_loss           | 2.14e+05     |
------------------------------------------
Eval num_timesteps=78000, episode_reward=-2895.61 +/- 32.20
Episode length: 259.80 +/- 50.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 260           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 0.00033818008 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.86         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+05      |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.00106      |
|    std                  | 1.05          |
|    value_loss           | 2.9e+05       |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=-2872.68 +/- 78.37
Episode length: 254.40 +/- 71.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 254           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00011302007 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.85         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+05      |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000358     |
|    std                  | 1.04          |
|    value_loss           | 2.9e+05       |
-------------------------------------------
Eval num_timesteps=82000, episode_reward=-2828.23 +/- 43.42
Episode length: 318.00 +/- 92.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | 0.00017178207 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+05      |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000878     |
|    std                  | 1.04          |
|    value_loss           | 2.9e+05       |
-------------------------------------------
New best mean reward!
Eval num_timesteps=84000, episode_reward=-2913.70 +/- 26.27
Episode length: 233.60 +/- 42.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 234           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 84000         |
| train/                  |               |
|    approx_kl            | 0.00023661085 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.83         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000564     |
|    std                  | 1.04          |
|    value_loss           | 2.53e+05      |
-------------------------------------------
Eval num_timesteps=86000, episode_reward=-2880.66 +/- 32.36
Episode length: 280.00 +/- 32.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 280       |
|    mean_reward     | -2.88e+03 |
| time/              |           |
|    total_timesteps | 86000     |
----------------------------------
Eval num_timesteps=88000, episode_reward=-2897.20 +/- 24.87
Episode length: 235.60 +/- 59.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 236           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 88000         |
| train/                  |               |
|    approx_kl            | 0.00010180156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000177     |
|    std                  | 1.04          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-2859.33 +/- 87.62
Episode length: 290.80 +/- 89.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | -2.86e+03    |
| time/                   |              |
|    total_timesteps      | 90000        |
| train/                  |              |
|    approx_kl            | 6.665458e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+05     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.000256    |
|    std                  | 1.04         |
|    value_loss           | 2.88e+05     |
------------------------------------------
Eval num_timesteps=92000, episode_reward=-2882.25 +/- 18.85
Episode length: 297.60 +/- 18.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 298           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 92000         |
| train/                  |               |
|    approx_kl            | 9.0841815e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+05      |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.0003       |
|    std                  | 1.04          |
|    value_loss           | 2.89e+05      |
-------------------------------------------
Eval num_timesteps=94000, episode_reward=-2925.35 +/- 63.32
Episode length: 307.00 +/- 107.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 94000         |
| train/                  |               |
|    approx_kl            | 0.00020139269 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.85         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000764     |
|    std                  | 1.04          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=96000, episode_reward=-2914.92 +/- 39.20
Episode length: 253.20 +/- 44.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 253           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 96000         |
| train/                  |               |
|    approx_kl            | 0.00018695332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.85         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000529     |
|    std                  | 1.05          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=-2882.22 +/- 24.11
Episode length: 289.20 +/- 46.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 98000         |
| train/                  |               |
|    approx_kl            | 0.00021292779 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.86         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000911     |
|    std                  | 1.05          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-2871.23 +/- 68.58
Episode length: 349.40 +/- 43.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 349           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 100000        |
| train/                  |               |
|    approx_kl            | 0.00018698964 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.86         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000316     |
|    std                  | 1.05          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=102000, episode_reward=-2857.92 +/- 129.03
Episode length: 324.80 +/- 104.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | -2.86e+03    |
| time/                   |              |
|    total_timesteps      | 102000       |
| train/                  |              |
|    approx_kl            | 7.760947e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+05      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000182    |
|    std                  | 1.05         |
|    value_loss           | 2.6e+05      |
------------------------------------------
Eval num_timesteps=104000, episode_reward=-2893.31 +/- 34.93
Episode length: 304.60 +/- 57.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 104000        |
| train/                  |               |
|    approx_kl            | 0.00012350234 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.88         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000511     |
|    std                  | 1.05          |
|    value_loss           | 2.51e+05      |
-------------------------------------------
Eval num_timesteps=106000, episode_reward=-2878.43 +/- 98.16
Episode length: 313.60 +/- 82.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 106000       |
| train/                  |              |
|    approx_kl            | 9.614145e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.88        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.43e+05     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000201    |
|    std                  | 1.05         |
|    value_loss           | 2.87e+05     |
------------------------------------------
Eval num_timesteps=108000, episode_reward=-2852.09 +/- 57.20
Episode length: 364.00 +/- 133.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 108000        |
| train/                  |               |
|    approx_kl            | 0.00010787693 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.87         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+05      |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.000295     |
|    std                  | 1.05          |
|    value_loss           | 2.87e+05      |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-2882.82 +/- 62.60
Episode length: 259.00 +/- 72.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 259           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | 0.00023223486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.87         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+05      |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.00082      |
|    std                  | 1.05          |
|    value_loss           | 2.51e+05      |
-------------------------------------------
Eval num_timesteps=112000, episode_reward=-2842.43 +/- 67.27
Episode length: 278.40 +/- 50.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 112000       |
| train/                  |              |
|    approx_kl            | 8.764627e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.88        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+05     |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.000176    |
|    std                  | 1.05         |
|    value_loss           | 2.54e+05     |
------------------------------------------
Eval num_timesteps=114000, episode_reward=-2851.06 +/- 88.44
Episode length: 291.00 +/- 81.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 0.00020711293 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.89         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+05      |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.00061      |
|    std                  | 1.06          |
|    value_loss           | 2.5e+05       |
-------------------------------------------
Eval num_timesteps=116000, episode_reward=-2917.32 +/- 41.06
Episode length: 300.40 +/- 46.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 116000        |
| train/                  |               |
|    approx_kl            | 0.00021865548 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.9          |
|    explained_variance   | -9.54e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+05      |
|    n_updates            | 560           |
|    policy_gradient_loss | -0.0006       |
|    std                  | 1.06          |
|    value_loss           | 2.78e+05      |
-------------------------------------------
Eval num_timesteps=118000, episode_reward=-2883.26 +/- 31.70
Episode length: 323.40 +/- 68.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 118000        |
| train/                  |               |
|    approx_kl            | 0.00027638033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.92         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+05      |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.000575     |
|    std                  | 1.06          |
|    value_loss           | 2.14e+05      |
-------------------------------------------
Eval num_timesteps=120000, episode_reward=-2854.86 +/- 11.12
Episode length: 280.60 +/- 23.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 281        |
|    mean_reward          | -2.85e+03  |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.00016651 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.93      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 1.42e+05   |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.000456  |
|    std                  | 1.07       |
|    value_loss           | 2.85e+05   |
----------------------------------------
Eval num_timesteps=122000, episode_reward=-2872.80 +/- 50.03
Episode length: 300.00 +/- 79.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 122000        |
| train/                  |               |
|    approx_kl            | 0.00017257725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.94         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+05      |
|    n_updates            | 590           |
|    policy_gradient_loss | -0.000579     |
|    std                  | 1.07          |
|    value_loss           | 2.14e+05      |
-------------------------------------------
Eval num_timesteps=124000, episode_reward=-2845.37 +/- 72.52
Episode length: 354.80 +/- 79.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 355           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 124000        |
| train/                  |               |
|    approx_kl            | 0.00013533197 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.96         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+05      |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000322     |
|    std                  | 1.08          |
|    value_loss           | 2.49e+05      |
-------------------------------------------
Eval num_timesteps=126000, episode_reward=-2890.35 +/- 23.61
Episode length: 313.00 +/- 48.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 313          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 126000       |
| train/                  |              |
|    approx_kl            | 0.0003085547 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.89e+04     |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.000765    |
|    std                  | 1.08         |
|    value_loss           | 1.78e+05     |
------------------------------------------
Eval num_timesteps=128000, episode_reward=-2900.70 +/- 49.36
Episode length: 353.80 +/- 128.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 354           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 128000        |
| train/                  |               |
|    approx_kl            | 0.00033560305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.99         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+05       |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.000538     |
|    std                  | 1.09          |
|    value_loss           | 2.21e+05      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-2833.77 +/- 71.06
Episode length: 341.60 +/- 72.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 342          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0002215729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000422    |
|    std                  | 1.09         |
|    value_loss           | 2.49e+05     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-2951.55 +/- 29.63
Episode length: 204.80 +/- 26.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 205           |
|    mean_reward          | -2.95e+03     |
| time/                   |               |
|    total_timesteps      | 132000        |
| train/                  |               |
|    approx_kl            | 0.00019771469 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.03         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+05      |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000546     |
|    std                  | 1.1           |
|    value_loss           | 2.48e+05      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=-2879.33 +/- 51.87
Episode length: 278.40 +/- 44.43
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 278            |
|    mean_reward          | -2.88e+03      |
| time/                   |                |
|    total_timesteps      | 134000         |
| train/                  |                |
|    approx_kl            | 0.000119479286 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.05          |
|    explained_variance   | 5.96e-08       |
|    learning_rate        | 0.001          |
|    loss                 | 1.42e+05       |
|    n_updates            | 650            |
|    policy_gradient_loss | -0.000227      |
|    std                  | 1.1            |
|    value_loss           | 2.83e+05       |
--------------------------------------------
Eval num_timesteps=136000, episode_reward=-2887.70 +/- 60.29
Episode length: 298.40 +/- 86.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 136000       |
| train/                  |              |
|    approx_kl            | 0.0001288144 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000358    |
|    std                  | 1.1          |
|    value_loss           | 2.47e+05     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-2887.71 +/- 72.78
Episode length: 291.80 +/- 87.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 138000        |
| train/                  |               |
|    approx_kl            | 0.00010686953 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+05      |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000313     |
|    std                  | 1.11          |
|    value_loss           | 2.82e+05      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-2920.60 +/- 39.10
Episode length: 251.00 +/- 61.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 140000        |
| train/                  |               |
|    approx_kl            | 0.00016425364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.08         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000469     |
|    std                  | 1.11          |
|    value_loss           | 2.12e+05      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=-2897.94 +/- 41.16
Episode length: 297.40 +/- 64.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 142000       |
| train/                  |              |
|    approx_kl            | 0.0002321408 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000748    |
|    std                  | 1.11         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-2860.73 +/- 39.66
Episode length: 304.80 +/- 34.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 144000        |
| train/                  |               |
|    approx_kl            | 0.00022461466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.1          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+05      |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000844     |
|    std                  | 1.11          |
|    value_loss           | 2.46e+05      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=-2915.11 +/- 30.95
Episode length: 256.40 +/- 34.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 146000       |
| train/                  |              |
|    approx_kl            | 0.0002262124 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+05     |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.000939    |
|    std                  | 1.11         |
|    value_loss           | 2.79e+05     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-2902.43 +/- 31.38
Episode length: 232.80 +/- 36.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 233           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 148000        |
| train/                  |               |
|    approx_kl            | 0.00020877569 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.11         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+05      |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000559     |
|    std                  | 1.12          |
|    value_loss           | 2.46e+05      |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=-2924.38 +/- 46.17
Episode length: 248.80 +/- 69.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 249           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 150000        |
| train/                  |               |
|    approx_kl            | 0.00025062586 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.13         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+05       |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.000676     |
|    std                  | 1.12          |
|    value_loss           | 2.81e+05      |
-------------------------------------------
Eval num_timesteps=152000, episode_reward=-2926.94 +/- 47.70
Episode length: 254.20 +/- 70.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 254           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 0.00031345102 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.13         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+05      |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.000956     |
|    std                  | 1.12          |
|    value_loss           | 2.46e+05      |
-------------------------------------------
Eval num_timesteps=154000, episode_reward=-2899.14 +/- 84.87
Episode length: 258.40 +/- 94.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 258           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 0.00019336343 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+05       |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.000347     |
|    std                  | 1.12          |
|    value_loss           | 2.8e+05       |
-------------------------------------------
Eval num_timesteps=156000, episode_reward=-2888.97 +/- 64.00
Episode length: 258.80 +/- 59.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 259           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 0.00011298148 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+05      |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000482     |
|    std                  | 1.12          |
|    value_loss           | 2.1e+05       |
-------------------------------------------
Eval num_timesteps=158000, episode_reward=-2910.69 +/- 43.38
Episode length: 268.80 +/- 71.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 158000        |
| train/                  |               |
|    approx_kl            | 0.00012885517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+05      |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000476     |
|    std                  | 1.12          |
|    value_loss           | 3.15e+05      |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-2894.45 +/- 63.23
Episode length: 258.20 +/- 46.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 258           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 0.00011129945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.14         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+05      |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.000246     |
|    std                  | 1.13          |
|    value_loss           | 2.44e+05      |
-------------------------------------------
Eval num_timesteps=162000, episode_reward=-2930.99 +/- 30.03
Episode length: 229.40 +/- 52.21
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 229            |
|    mean_reward          | -2.93e+03      |
| time/                   |                |
|    total_timesteps      | 162000         |
| train/                  |                |
|    approx_kl            | 0.000115318166 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.15          |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.001          |
|    loss                 | 1.57e+05       |
|    n_updates            | 790            |
|    policy_gradient_loss | -0.000402      |
|    std                  | 1.13           |
|    value_loss           | 3.14e+05       |
--------------------------------------------
Eval num_timesteps=164000, episode_reward=-2899.71 +/- 48.61
Episode length: 301.00 +/- 94.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 0.00010525185 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+05      |
|    n_updates            | 800           |
|    policy_gradient_loss | -0.000539     |
|    std                  | 1.13          |
|    value_loss           | 2.62e+05      |
-------------------------------------------
Eval num_timesteps=166000, episode_reward=-2872.92 +/- 51.57
Episode length: 335.40 +/- 95.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 8.031688e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+05     |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000301    |
|    std                  | 1.13         |
|    value_loss           | 2.44e+05     |
------------------------------------------
Eval num_timesteps=168000, episode_reward=-2856.11 +/- 77.13
Episode length: 334.20 +/- 86.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 334           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 0.00019279437 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.000464     |
|    std                  | 1.13          |
|    value_loss           | 2.42e+05      |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=-2840.95 +/- 134.41
Episode length: 295.20 +/- 103.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 170000        |
| train/                  |               |
|    approx_kl            | 9.9822646e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+05      |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.13          |
|    value_loss           | 3.12e+05      |
-------------------------------------------
Eval num_timesteps=172000, episode_reward=-2871.30 +/- 58.30
Episode length: 368.60 +/- 143.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 369       |
|    mean_reward     | -2.87e+03 |
| time/              |           |
|    total_timesteps | 172000    |
----------------------------------
Eval num_timesteps=174000, episode_reward=-2910.65 +/- 42.70
Episode length: 269.80 +/- 65.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 174000       |
| train/                  |              |
|    approx_kl            | 8.591171e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.000418    |
|    std                  | 1.13         |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=176000, episode_reward=-2884.18 +/- 33.03
Episode length: 305.80 +/- 61.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 176000        |
| train/                  |               |
|    approx_kl            | 5.7132507e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.16         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+05      |
|    n_updates            | 850           |
|    policy_gradient_loss | -5.1e-05      |
|    std                  | 1.13          |
|    value_loss           | 2.24e+05      |
-------------------------------------------
Eval num_timesteps=178000, episode_reward=-2904.64 +/- 28.21
Episode length: 284.40 +/- 62.61
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 284            |
|    mean_reward          | -2.9e+03       |
| time/                   |                |
|    total_timesteps      | 178000         |
| train/                  |                |
|    approx_kl            | 0.000112385955 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.17          |
|    explained_variance   | 0              |
|    learning_rate        | 0.001          |
|    loss                 | 1.21e+05       |
|    n_updates            | 860            |
|    policy_gradient_loss | -0.000567      |
|    std                  | 1.13           |
|    value_loss           | 2.43e+05       |
--------------------------------------------
Eval num_timesteps=180000, episode_reward=-2878.96 +/- 34.67
Episode length: 312.00 +/- 67.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 180000        |
| train/                  |               |
|    approx_kl            | 0.00014208394 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.18         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.00052      |
|    std                  | 1.14          |
|    value_loss           | 2.42e+05      |
-------------------------------------------
Eval num_timesteps=182000, episode_reward=-2906.24 +/- 45.23
Episode length: 226.00 +/- 50.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 226           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 182000        |
| train/                  |               |
|    approx_kl            | 0.00021783361 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.2          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.000667     |
|    std                  | 1.14          |
|    value_loss           | 2.42e+05      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=-2864.45 +/- 73.00
Episode length: 336.00 +/- 121.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 336          |
|    mean_reward          | -2.86e+03    |
| time/                   |              |
|    total_timesteps      | 184000       |
| train/                  |              |
|    approx_kl            | 6.865253e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+05     |
|    n_updates            | 890          |
|    policy_gradient_loss | 4.13e-05     |
|    std                  | 1.15         |
|    value_loss           | 2.41e+05     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=-2879.00 +/- 112.10
Episode length: 262.20 +/- 80.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 186000        |
| train/                  |               |
|    approx_kl            | 5.4939388e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.22         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+05      |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000161     |
|    std                  | 1.15          |
|    value_loss           | 2.7e+05       |
-------------------------------------------
Eval num_timesteps=188000, episode_reward=-2915.71 +/- 30.27
Episode length: 260.60 +/- 52.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 188000       |
| train/                  |              |
|    approx_kl            | 0.0001061699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+05     |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000357    |
|    std                  | 1.15         |
|    value_loss           | 2.38e+05     |
------------------------------------------
Eval num_timesteps=190000, episode_reward=-2846.51 +/- 38.22
Episode length: 322.20 +/- 47.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | -2.85e+03   |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 9.30535e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+05    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.000123   |
|    std                  | 1.15        |
|    value_loss           | 2.42e+05    |
-----------------------------------------
Eval num_timesteps=192000, episode_reward=-2861.50 +/- 99.64
Episode length: 342.80 +/- 109.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 343           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 192000        |
| train/                  |               |
|    approx_kl            | 0.00011694411 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.24         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+05      |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.000561     |
|    std                  | 1.15          |
|    value_loss           | 2.76e+05      |
-------------------------------------------
Eval num_timesteps=194000, episode_reward=-2906.41 +/- 102.41
Episode length: 266.80 +/- 91.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 194000       |
| train/                  |              |
|    approx_kl            | 0.0001484335 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.25        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.0004      |
|    std                  | 1.16         |
|    value_loss           | 2.06e+05     |
------------------------------------------
Eval num_timesteps=196000, episode_reward=-2879.15 +/- 70.75
Episode length: 279.80 +/- 85.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 196000        |
| train/                  |               |
|    approx_kl            | 0.00014218473 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.26         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+05      |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000521     |
|    std                  | 1.16          |
|    value_loss           | 2.74e+05      |
-------------------------------------------
Eval num_timesteps=198000, episode_reward=-2886.78 +/- 115.95
Episode length: 280.40 +/- 72.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 198000        |
| train/                  |               |
|    approx_kl            | 0.00016552291 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.28         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+05       |
|    n_updates            | 960           |
|    policy_gradient_loss | -0.00047      |
|    std                  | 1.17          |
|    value_loss           | 2.4e+05       |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-2867.41 +/- 53.11
Episode length: 262.40 +/- 32.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 0.00012469591 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.29         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+05       |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.000333     |
|    std                  | 1.17          |
|    value_loss           | 2.4e+05       |
-------------------------------------------
Eval num_timesteps=202000, episode_reward=-2872.65 +/- 56.17
Episode length: 303.20 +/- 66.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 303            |
|    mean_reward          | -2.87e+03      |
| time/                   |                |
|    total_timesteps      | 202000         |
| train/                  |                |
|    approx_kl            | 0.000119422795 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.3           |
|    explained_variance   | 5.96e-08       |
|    learning_rate        | 0.001          |
|    loss                 | 1.03e+05       |
|    n_updates            | 980            |
|    policy_gradient_loss | -0.000341      |
|    std                  | 1.17           |
|    value_loss           | 2.06e+05       |
--------------------------------------------
Eval num_timesteps=204000, episode_reward=-2936.18 +/- 33.48
Episode length: 226.00 +/- 36.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | -2.94e+03   |
| time/                   |             |
|    total_timesteps      | 204000      |
| train/                  |             |
|    approx_kl            | 0.000143062 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 1.26e+05    |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.000515   |
|    std                  | 1.17        |
|    value_loss           | 2.53e+05    |
-----------------------------------------
Eval num_timesteps=206000, episode_reward=-2932.34 +/- 36.81
Episode length: 210.80 +/- 34.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 211           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 0.00018273041 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.31         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000516     |
|    std                  | 1.17          |
|    value_loss           | 2.53e+05      |
-------------------------------------------
Eval num_timesteps=208000, episode_reward=-2914.45 +/- 66.49
Episode length: 226.80 +/- 66.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 208000       |
| train/                  |              |
|    approx_kl            | 5.920051e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.31        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+05     |
|    n_updates            | 1010         |
|    policy_gradient_loss | -7.64e-05    |
|    std                  | 1.17         |
|    value_loss           | 3.06e+05     |
------------------------------------------
Eval num_timesteps=210000, episode_reward=-2933.36 +/- 24.78
Episode length: 251.00 +/- 63.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 5.4261356e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.31         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.36e+05      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.000245     |
|    std                  | 1.17          |
|    value_loss           | 2.72e+05      |
-------------------------------------------
Eval num_timesteps=212000, episode_reward=-2880.57 +/- 59.73
Episode length: 263.60 +/- 16.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 6.266267e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.36e+05     |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.000264    |
|    std                  | 1.18         |
|    value_loss           | 2.73e+05     |
------------------------------------------
Eval num_timesteps=214000, episode_reward=-2868.88 +/- 33.24
Episode length: 343.60 +/- 35.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 214000       |
| train/                  |              |
|    approx_kl            | 0.0002213697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.52e+04     |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.18         |
|    value_loss           | 1.71e+05     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-2923.08 +/- 49.41
Episode length: 239.60 +/- 36.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 240           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 216000        |
| train/                  |               |
|    approx_kl            | 0.00021592615 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.32         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.52e+05      |
|    n_updates            | 1050          |
|    policy_gradient_loss | -0.000197     |
|    std                  | 1.18          |
|    value_loss           | 3.05e+05      |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=-2920.72 +/- 15.51
Episode length: 282.60 +/- 90.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 283           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 218000        |
| train/                  |               |
|    approx_kl            | 0.00027980199 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.32         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+05      |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.000903     |
|    std                  | 1.18          |
|    value_loss           | 2.03e+05      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=-2825.86 +/- 88.59
Episode length: 372.20 +/- 108.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 220000        |
| train/                  |               |
|    approx_kl            | 0.00028877714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.33         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000838     |
|    std                  | 1.18          |
|    value_loss           | 2.37e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=222000, episode_reward=-2837.04 +/- 113.21
Episode length: 383.40 +/- 106.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 222000        |
| train/                  |               |
|    approx_kl            | 0.00019791967 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.33         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 1080          |
|    policy_gradient_loss | -0.000591     |
|    std                  | 1.18          |
|    value_loss           | 2.37e+05      |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=-2852.37 +/- 72.16
Episode length: 321.80 +/- 110.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 322           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 224000        |
| train/                  |               |
|    approx_kl            | 0.00016195056 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.34         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 1090          |
|    policy_gradient_loss | -0.000562     |
|    std                  | 1.18          |
|    value_loss           | 2.37e+05      |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=-2926.83 +/- 37.17
Episode length: 252.00 +/- 58.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 252           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 226000        |
| train/                  |               |
|    approx_kl            | 0.00017546254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.34         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+05      |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.000598     |
|    std                  | 1.18          |
|    value_loss           | 2.37e+05      |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=-2897.55 +/- 77.75
Episode length: 261.60 +/- 58.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 228000        |
| train/                  |               |
|    approx_kl            | 0.00013980162 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.34         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+05      |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.000519     |
|    std                  | 1.18          |
|    value_loss           | 2.66e+05      |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=-2872.40 +/- 61.89
Episode length: 308.40 +/- 108.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 308           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 230000        |
| train/                  |               |
|    approx_kl            | 0.00032964553 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.35         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+05      |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.000796     |
|    std                  | 1.19          |
|    value_loss           | 2.03e+05      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=-2895.95 +/- 53.69
Episode length: 306.60 +/- 79.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 232000        |
| train/                  |               |
|    approx_kl            | 0.00012373048 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.36         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+05      |
|    n_updates            | 1130          |
|    policy_gradient_loss | -3.46e-05     |
|    std                  | 1.19          |
|    value_loss           | 2.69e+05      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=-2937.36 +/- 25.26
Episode length: 231.80 +/- 42.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 232           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 234000        |
| train/                  |               |
|    approx_kl            | 0.00014094263 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.37         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+05      |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000503     |
|    std                  | 1.19          |
|    value_loss           | 2.69e+05      |
-------------------------------------------
Eval num_timesteps=236000, episode_reward=-2888.47 +/- 40.26
Episode length: 289.20 +/- 35.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 8.9158624e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.38         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+05      |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000157     |
|    std                  | 1.19          |
|    value_loss           | 2.69e+05      |
-------------------------------------------
Eval num_timesteps=238000, episode_reward=-2901.57 +/- 26.16
Episode length: 260.60 +/- 64.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 6.982917e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+05     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.000347    |
|    std                  | 1.19         |
|    value_loss           | 2.69e+05     |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-2903.53 +/- 49.30
Episode length: 287.60 +/- 84.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 288           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 0.00014233959 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.39         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1e+05         |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.000451     |
|    std                  | 1.2           |
|    value_loss           | 2.01e+05      |
-------------------------------------------
Eval num_timesteps=242000, episode_reward=-2900.75 +/- 21.23
Episode length: 325.40 +/- 76.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 325           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00020357186 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.4          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+05      |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000742     |
|    std                  | 1.2           |
|    value_loss           | 2.35e+05      |
-------------------------------------------
Eval num_timesteps=244000, episode_reward=-2873.62 +/- 44.87
Episode length: 295.20 +/- 59.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 0.00018003135 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.41         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+05      |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.000326     |
|    std                  | 1.2           |
|    value_loss           | 2.34e+05      |
-------------------------------------------
Eval num_timesteps=246000, episode_reward=-2897.44 +/- 31.72
Episode length: 240.80 +/- 45.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 241           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 0.00026404543 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.42         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1e+05         |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.21          |
|    value_loss           | 2.01e+05      |
-------------------------------------------
Eval num_timesteps=248000, episode_reward=-2875.19 +/- 69.70
Episode length: 277.40 +/- 100.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 277           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 0.00027078885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+05       |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.000357     |
|    std                  | 1.21          |
|    value_loss           | 2.2e+05       |
-------------------------------------------
Eval num_timesteps=250000, episode_reward=-2825.16 +/- 64.32
Episode length: 315.60 +/- 76.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00020863555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.44         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+05      |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.21          |
|    value_loss           | 2.33e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=252000, episode_reward=-2835.14 +/- 69.28
Episode length: 276.40 +/- 52.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 0.00017567433 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.46         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+05       |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.000217     |
|    std                  | 1.22          |
|    value_loss           | 2.99e+05      |
-------------------------------------------
Eval num_timesteps=254000, episode_reward=-2920.34 +/- 39.47
Episode length: 243.40 +/- 33.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 243          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 4.679925e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.46        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.000133    |
|    std                  | 1.22         |
|    value_loss           | 2.33e+05     |
------------------------------------------
Eval num_timesteps=256000, episode_reward=-2884.48 +/- 82.45
Episode length: 276.80 +/- 54.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 277       |
|    mean_reward     | -2.88e+03 |
| time/              |           |
|    total_timesteps | 256000    |
----------------------------------
Eval num_timesteps=258000, episode_reward=-2844.04 +/- 17.60
Episode length: 368.40 +/- 81.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 258000       |
| train/                  |              |
|    approx_kl            | 6.224029e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.46        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.99e+04     |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.000315    |
|    std                  | 1.22         |
|    value_loss           | 2e+05        |
------------------------------------------
Eval num_timesteps=260000, episode_reward=-2873.18 +/- 42.74
Episode length: 299.00 +/- 45.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 260000        |
| train/                  |               |
|    approx_kl            | 0.00017252541 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.46         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+05      |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000626     |
|    std                  | 1.22          |
|    value_loss           | 2.32e+05      |
-------------------------------------------
Eval num_timesteps=262000, episode_reward=-2884.27 +/- 91.14
Episode length: 370.60 +/- 70.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 371           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 262000        |
| train/                  |               |
|    approx_kl            | 0.00011879933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.46         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+05      |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.000425     |
|    std                  | 1.22          |
|    value_loss           | 2.32e+05      |
-------------------------------------------
Eval num_timesteps=264000, episode_reward=-2849.83 +/- 76.71
Episode length: 334.20 +/- 81.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 334         |
|    mean_reward          | -2.85e+03   |
| time/                   |             |
|    total_timesteps      | 264000      |
| train/                  |             |
|    approx_kl            | 9.67044e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.46       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.94e+04    |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.000458   |
|    std                  | 1.21        |
|    value_loss           | 1.99e+05    |
-----------------------------------------
Eval num_timesteps=266000, episode_reward=-2846.29 +/- 142.18
Episode length: 337.00 +/- 136.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | -2.85e+03    |
| time/                   |              |
|    total_timesteps      | 266000       |
| train/                  |              |
|    approx_kl            | 9.728348e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.45        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.000466    |
|    std                  | 1.21         |
|    value_loss           | 2.32e+05     |
------------------------------------------
Eval num_timesteps=268000, episode_reward=-2850.40 +/- 103.47
Episode length: 335.20 +/- 103.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 335           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 268000        |
| train/                  |               |
|    approx_kl            | 0.00017607518 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.45         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+05      |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.000481     |
|    std                  | 1.22          |
|    value_loss           | 2.31e+05      |
-------------------------------------------
Eval num_timesteps=270000, episode_reward=-2797.04 +/- 120.68
Episode length: 352.60 +/- 76.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 353           |
|    mean_reward          | -2.8e+03      |
| time/                   |               |
|    total_timesteps      | 270000        |
| train/                  |               |
|    approx_kl            | 0.00010407803 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.46         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+05      |
|    n_updates            | 1310          |
|    policy_gradient_loss | -0.000306     |
|    std                  | 1.22          |
|    value_loss           | 2.96e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=272000, episode_reward=-2858.52 +/- 48.17
Episode length: 366.20 +/- 72.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 366           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 272000        |
| train/                  |               |
|    approx_kl            | 0.00011159293 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.47         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+05      |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.000601     |
|    std                  | 1.22          |
|    value_loss           | 2.64e+05      |
-------------------------------------------
Eval num_timesteps=274000, episode_reward=-2908.57 +/- 37.56
Episode length: 220.20 +/- 51.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 274000       |
| train/                  |              |
|    approx_kl            | 0.0002506267 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+05     |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.000902    |
|    std                  | 1.23         |
|    value_loss           | 2.3e+05      |
------------------------------------------
Eval num_timesteps=276000, episode_reward=-2905.98 +/- 43.13
Episode length: 215.60 +/- 39.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 216          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 276000       |
| train/                  |              |
|    approx_kl            | 0.0002854698 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000473    |
|    std                  | 1.23         |
|    value_loss           | 2.31e+05     |
------------------------------------------
Eval num_timesteps=278000, episode_reward=-2868.53 +/- 75.34
Episode length: 306.00 +/- 84.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | 0.00026141654 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.5          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 9.91e+04      |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000648     |
|    std                  | 1.23          |
|    value_loss           | 1.98e+05      |
-------------------------------------------
Eval num_timesteps=280000, episode_reward=-2786.19 +/- 90.26
Episode length: 351.80 +/- 79.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 352           |
|    mean_reward          | -2.79e+03     |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00030263184 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.51         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.24e+04      |
|    n_updates            | 1360          |
|    policy_gradient_loss | -0.000896     |
|    std                  | 1.24          |
|    value_loss           | 1.65e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=282000, episode_reward=-2855.33 +/- 36.90
Episode length: 294.40 +/- 64.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 282000        |
| train/                  |               |
|    approx_kl            | 0.00020464763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.52         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 9.87e+04      |
|    n_updates            | 1370          |
|    policy_gradient_loss | -0.00021      |
|    std                  | 1.24          |
|    value_loss           | 1.97e+05      |
-------------------------------------------
Eval num_timesteps=284000, episode_reward=-2830.08 +/- 92.83
Episode length: 344.00 +/- 113.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 344           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 284000        |
| train/                  |               |
|    approx_kl            | 9.3560084e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.53         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.000183     |
|    std                  | 1.24          |
|    value_loss           | 2.25e+05      |
-------------------------------------------
Eval num_timesteps=286000, episode_reward=-2818.03 +/- 107.20
Episode length: 313.40 +/- 80.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | -2.82e+03     |
| time/                   |               |
|    total_timesteps      | 286000        |
| train/                  |               |
|    approx_kl            | 0.00012324555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.53         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+05      |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.000448     |
|    std                  | 1.24          |
|    value_loss           | 2.29e+05      |
-------------------------------------------
Eval num_timesteps=288000, episode_reward=-2810.88 +/- 30.95
Episode length: 400.00 +/- 59.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | -2.81e+03     |
| time/                   |               |
|    total_timesteps      | 288000        |
| train/                  |               |
|    approx_kl            | 0.00017783706 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.55         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.83e+04      |
|    n_updates            | 1400          |
|    policy_gradient_loss | -0.000489     |
|    std                  | 1.25          |
|    value_loss           | 1.97e+05      |
-------------------------------------------
Eval num_timesteps=290000, episode_reward=-2879.50 +/- 104.89
Episode length: 256.20 +/- 98.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 256           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 0.00015522481 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.57         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+05      |
|    n_updates            | 1410          |
|    policy_gradient_loss | -0.000379     |
|    std                  | 1.25          |
|    value_loss           | 2.29e+05      |
-------------------------------------------
Eval num_timesteps=292000, episode_reward=-2847.12 +/- 51.95
Episode length: 293.80 +/- 53.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 292000        |
| train/                  |               |
|    approx_kl            | 0.00020813214 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.58         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+05      |
|    n_updates            | 1420          |
|    policy_gradient_loss | -0.000652     |
|    std                  | 1.26          |
|    value_loss           | 2.29e+05      |
-------------------------------------------
Eval num_timesteps=294000, episode_reward=-2905.05 +/- 55.10
Episode length: 246.80 +/- 82.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 294000       |
| train/                  |              |
|    approx_kl            | 0.0001579749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.000259    |
|    std                  | 1.26         |
|    value_loss           | 2.28e+05     |
------------------------------------------
Eval num_timesteps=296000, episode_reward=-2890.54 +/- 82.89
Episode length: 223.60 +/- 64.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 224           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 296000        |
| train/                  |               |
|    approx_kl            | 5.8124453e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.6          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+05      |
|    n_updates            | 1440          |
|    policy_gradient_loss | -0.000105     |
|    std                  | 1.26          |
|    value_loss           | 2.5e+05       |
-------------------------------------------
Eval num_timesteps=298000, episode_reward=-2917.39 +/- 33.39
Episode length: 254.80 +/- 40.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 298000       |
| train/                  |              |
|    approx_kl            | 8.550368e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.61        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.000532    |
|    std                  | 1.27         |
|    value_loss           | 2.28e+05     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-2866.93 +/- 53.38
Episode length: 252.40 +/- 71.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 252           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 300000        |
| train/                  |               |
|    approx_kl            | 0.00015419515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.62         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+05      |
|    n_updates            | 1460          |
|    policy_gradient_loss | -0.000623     |
|    std                  | 1.27          |
|    value_loss           | 2.28e+05      |
-------------------------------------------
Eval num_timesteps=302000, episode_reward=-2817.34 +/- 75.94
Episode length: 307.00 +/- 90.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 302000       |
| train/                  |              |
|    approx_kl            | 0.0003167844 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.63        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.78e+04     |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.27         |
|    value_loss           | 1.96e+05     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-2888.03 +/- 52.04
Episode length: 287.80 +/- 65.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 304000       |
| train/                  |              |
|    approx_kl            | 0.0002737317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.64        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+05     |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.000579    |
|    std                  | 1.28         |
|    value_loss           | 2.57e+05     |
------------------------------------------
Eval num_timesteps=306000, episode_reward=-2892.80 +/- 26.19
Episode length: 255.00 +/- 39.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 306000        |
| train/                  |               |
|    approx_kl            | 0.00017801375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.65         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+05      |
|    n_updates            | 1490          |
|    policy_gradient_loss | -0.000387     |
|    std                  | 1.28          |
|    value_loss           | 2.27e+05      |
-------------------------------------------
Eval num_timesteps=308000, episode_reward=-2832.11 +/- 60.36
Episode length: 417.40 +/- 150.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 308000       |
| train/                  |              |
|    approx_kl            | 8.879829e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.65        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+05      |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.000283    |
|    std                  | 1.28         |
|    value_loss           | 2.59e+05     |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-2859.38 +/- 66.93
Episode length: 312.40 +/- 117.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 310000        |
| train/                  |               |
|    approx_kl            | 9.6926466e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.65         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 1510          |
|    policy_gradient_loss | -0.000362     |
|    std                  | 1.28          |
|    value_loss           | 2.27e+05      |
-------------------------------------------
Eval num_timesteps=312000, episode_reward=-2886.22 +/- 62.77
Episode length: 317.20 +/- 138.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 317           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 312000        |
| train/                  |               |
|    approx_kl            | 0.00017903431 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.65         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 1520          |
|    policy_gradient_loss | -0.000788     |
|    std                  | 1.28          |
|    value_loss           | 2.27e+05      |
-------------------------------------------
Eval num_timesteps=314000, episode_reward=-2674.32 +/- 369.01
Episode length: 413.80 +/- 255.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | -2.67e+03     |
| time/                   |               |
|    total_timesteps      | 314000        |
| train/                  |               |
|    approx_kl            | 0.00017281828 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.67         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.000555     |
|    std                  | 1.29          |
|    value_loss           | 2.26e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=316000, episode_reward=-2876.64 +/- 37.53
Episode length: 281.40 +/- 102.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 316000       |
| train/                  |              |
|    approx_kl            | 0.0001723523 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.69        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+05     |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.000789    |
|    std                  | 1.29         |
|    value_loss           | 2.9e+05      |
------------------------------------------
Eval num_timesteps=318000, episode_reward=-2876.84 +/- 57.98
Episode length: 326.80 +/- 106.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 327          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 318000       |
| train/                  |              |
|    approx_kl            | 0.0001301356 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+05     |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.000202    |
|    std                  | 1.3          |
|    value_loss           | 2.55e+05     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-2871.66 +/- 66.88
Episode length: 277.00 +/- 34.48
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 277            |
|    mean_reward          | -2.87e+03      |
| time/                   |                |
|    total_timesteps      | 320000         |
| train/                  |                |
|    approx_kl            | 0.000115252624 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.7           |
|    explained_variance   | 0              |
|    learning_rate        | 0.001          |
|    loss                 | 1.13e+05       |
|    n_updates            | 1560           |
|    policy_gradient_loss | -0.000561      |
|    std                  | 1.3            |
|    value_loss           | 2.25e+05       |
--------------------------------------------
Eval num_timesteps=322000, episode_reward=-2910.45 +/- 35.46
Episode length: 228.40 +/- 62.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 228           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 322000        |
| train/                  |               |
|    approx_kl            | 0.00027769333 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.7          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.68e+04      |
|    n_updates            | 1570          |
|    policy_gradient_loss | -0.000975     |
|    std                  | 1.3           |
|    value_loss           | 1.94e+05      |
-------------------------------------------
Eval num_timesteps=324000, episode_reward=-2817.06 +/- 84.18
Episode length: 307.60 +/- 87.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 308           |
|    mean_reward          | -2.82e+03     |
| time/                   |               |
|    total_timesteps      | 324000        |
| train/                  |               |
|    approx_kl            | 0.00022863664 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.71         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+05      |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.000585     |
|    std                  | 1.3           |
|    value_loss           | 2.57e+05      |
-------------------------------------------
Eval num_timesteps=326000, episode_reward=-2843.84 +/- 79.07
Episode length: 338.20 +/- 77.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 338           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 326000        |
| train/                  |               |
|    approx_kl            | 0.00012229226 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.71         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+05      |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.000249     |
|    std                  | 1.3           |
|    value_loss           | 2.57e+05      |
-------------------------------------------
Eval num_timesteps=328000, episode_reward=-2905.92 +/- 45.28
Episode length: 237.20 +/- 50.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 237           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 328000        |
| train/                  |               |
|    approx_kl            | 0.00012808031 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.71         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+05      |
|    n_updates            | 1600          |
|    policy_gradient_loss | -0.000397     |
|    std                  | 1.3           |
|    value_loss           | 2.25e+05      |
-------------------------------------------
Eval num_timesteps=330000, episode_reward=-2882.47 +/- 43.37
Episode length: 302.80 +/- 71.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 6.739766e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -8.09e-05    |
|    std                  | 1.3          |
|    value_loss           | 2.56e+05     |
------------------------------------------
Eval num_timesteps=332000, episode_reward=-2869.40 +/- 66.55
Episode length: 290.00 +/- 75.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 7.190884e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.000332    |
|    std                  | 1.3          |
|    value_loss           | 2.56e+05     |
------------------------------------------
Eval num_timesteps=334000, episode_reward=-2844.73 +/- 53.81
Episode length: 290.40 +/- 46.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 334000        |
| train/                  |               |
|    approx_kl            | 0.00018187263 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.72         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.62e+04      |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.000794     |
|    std                  | 1.3           |
|    value_loss           | 1.92e+05      |
-------------------------------------------
Eval num_timesteps=336000, episode_reward=-2920.70 +/- 25.11
Episode length: 242.20 +/- 47.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 242           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 336000        |
| train/                  |               |
|    approx_kl            | 0.00015522452 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.73         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+05      |
|    n_updates            | 1640          |
|    policy_gradient_loss | -0.000225     |
|    std                  | 1.31          |
|    value_loss           | 2.88e+05      |
-------------------------------------------
Eval num_timesteps=338000, episode_reward=-2892.57 +/- 27.06
Episode length: 262.20 +/- 56.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 338000        |
| train/                  |               |
|    approx_kl            | 0.00016559951 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.59e+04      |
|    n_updates            | 1650          |
|    policy_gradient_loss | -0.000645     |
|    std                  | 1.31          |
|    value_loss           | 1.92e+05      |
-------------------------------------------
Eval num_timesteps=340000, episode_reward=-2849.03 +/- 73.09
Episode length: 301.40 +/- 75.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | -2.85e+03    |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0002499463 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000511    |
|    std                  | 1.31         |
|    value_loss           | 2.56e+05     |
------------------------------------------
Eval num_timesteps=342000, episode_reward=-2890.81 +/- 70.81
Episode length: 306.00 +/- 77.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 306       |
|    mean_reward     | -2.89e+03 |
| time/              |           |
|    total_timesteps | 342000    |
----------------------------------
Eval num_timesteps=344000, episode_reward=-2912.46 +/- 37.34
Episode length: 258.40 +/- 74.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 258           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 344000        |
| train/                  |               |
|    approx_kl            | 0.00020259921 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.74         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+05      |
|    n_updates            | 1670          |
|    policy_gradient_loss | -0.00035      |
|    std                  | 1.31          |
|    value_loss           | 2.24e+05      |
-------------------------------------------
Eval num_timesteps=346000, episode_reward=-2902.70 +/- 36.74
Episode length: 251.40 +/- 31.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 346000        |
| train/                  |               |
|    approx_kl            | 0.00016835483 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.75         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 1680          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 1.31          |
|    value_loss           | 2.53e+05      |
-------------------------------------------
Eval num_timesteps=348000, episode_reward=-2741.65 +/- 81.37
Episode length: 473.60 +/- 87.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | -2.74e+03     |
| time/                   |               |
|    total_timesteps      | 348000        |
| train/                  |               |
|    approx_kl            | 0.00025384716 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.76         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+05      |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000835     |
|    std                  | 1.32          |
|    value_loss           | 2.55e+05      |
-------------------------------------------
Eval num_timesteps=350000, episode_reward=-2886.05 +/- 62.67
Episode length: 302.60 +/- 64.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0003217173 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+04     |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.000923    |
|    std                  | 1.32         |
|    value_loss           | 1.91e+05     |
------------------------------------------
Eval num_timesteps=352000, episode_reward=-2819.88 +/- 67.38
Episode length: 371.80 +/- 115.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 372          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 352000       |
| train/                  |              |
|    approx_kl            | 0.0005654696 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.97e+04     |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.33         |
|    value_loss           | 1.59e+05     |
------------------------------------------
Eval num_timesteps=354000, episode_reward=-2830.16 +/- 42.04
Episode length: 368.20 +/- 71.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 354000       |
| train/                  |              |
|    approx_kl            | 0.0002911938 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.37e+04     |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00018     |
|    std                  | 1.33         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=356000, episode_reward=-2913.30 +/- 38.03
Episode length: 266.80 +/- 66.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 356000       |
| train/                  |              |
|    approx_kl            | 0.0001301259 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.82        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+05     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.000298    |
|    std                  | 1.34         |
|    value_loss           | 2.23e+05     |
------------------------------------------
Eval num_timesteps=358000, episode_reward=-2869.79 +/- 29.52
Episode length: 319.60 +/- 45.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | -2.87e+03    |
| time/                   |              |
|    total_timesteps      | 358000       |
| train/                  |              |
|    approx_kl            | 0.0005307184 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 6.39e+04     |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.34         |
|    value_loss           | 1.28e+05     |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-2907.63 +/- 29.79
Episode length: 269.60 +/- 43.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0007408658 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+05     |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.000995    |
|    std                  | 1.34         |
|    value_loss           | 2.83e+05     |
------------------------------------------
Eval num_timesteps=362000, episode_reward=-2926.33 +/- 30.13
Episode length: 216.60 +/- 40.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 217           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 362000        |
| train/                  |               |
|    approx_kl            | 0.00019339807 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.84         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 1.34          |
|    value_loss           | 2.53e+05      |
-------------------------------------------
Eval num_timesteps=364000, episode_reward=-2904.46 +/- 29.09
Episode length: 254.20 +/- 66.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 254         |
|    mean_reward          | -2.9e+03    |
| time/                   |             |
|    total_timesteps      | 364000      |
| train/                  |             |
|    approx_kl            | 9.07167e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.85       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.42e+05    |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.000346   |
|    std                  | 1.34        |
|    value_loss           | 2.84e+05    |
-----------------------------------------
Eval num_timesteps=366000, episode_reward=-2904.94 +/- 20.95
Episode length: 226.60 +/- 30.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 227           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 366000        |
| train/                  |               |
|    approx_kl            | 9.3371025e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.85         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+05      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.000392     |
|    std                  | 1.35          |
|    value_loss           | 2.22e+05      |
-------------------------------------------
Eval num_timesteps=368000, episode_reward=-2894.08 +/- 57.16
Episode length: 248.40 +/- 50.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | -2.89e+03   |
| time/                   |             |
|    total_timesteps      | 368000      |
| train/                  |             |
|    approx_kl            | 7.38306e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.85       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+05     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.000183   |
|    std                  | 1.34        |
|    value_loss           | 2.59e+05    |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=-2914.40 +/- 41.20
Episode length: 233.20 +/- 60.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 8.026493e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.85        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+05      |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000448    |
|    std                  | 1.34         |
|    value_loss           | 2.21e+05     |
------------------------------------------
Eval num_timesteps=372000, episode_reward=-2930.26 +/- 27.86
Episode length: 229.20 +/- 37.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 229          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 372000       |
| train/                  |              |
|    approx_kl            | 7.974429e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+05     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000261    |
|    std                  | 1.34         |
|    value_loss           | 2.83e+05     |
------------------------------------------
Eval num_timesteps=374000, episode_reward=-2865.52 +/- 101.52
Episode length: 275.20 +/- 88.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 275           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 374000        |
| train/                  |               |
|    approx_kl            | 0.00012424134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.85         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 1820          |
|    policy_gradient_loss | -0.000612     |
|    std                  | 1.35          |
|    value_loss           | 2.39e+05      |
-------------------------------------------
Eval num_timesteps=376000, episode_reward=-2882.48 +/- 56.79
Episode length: 256.00 +/- 68.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 256           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 376000        |
| train/                  |               |
|    approx_kl            | 0.00012744436 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.86         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+05      |
|    n_updates            | 1830          |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.35          |
|    value_loss           | 2.51e+05      |
-------------------------------------------
Eval num_timesteps=378000, episode_reward=-2895.93 +/- 51.85
Episode length: 271.00 +/- 88.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 271           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 378000        |
| train/                  |               |
|    approx_kl            | 0.00012680862 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.86         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+05      |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000588     |
|    std                  | 1.35          |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=380000, episode_reward=-2879.35 +/- 65.12
Episode length: 255.20 +/- 78.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 380000        |
| train/                  |               |
|    approx_kl            | 0.00019444607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+05      |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.000538     |
|    std                  | 1.35          |
|    value_loss           | 2.51e+05      |
-------------------------------------------
Eval num_timesteps=382000, episode_reward=-2893.39 +/- 102.33
Episode length: 259.60 +/- 132.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 260           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 382000        |
| train/                  |               |
|    approx_kl            | 0.00012719823 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.87         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+05       |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.000525     |
|    std                  | 1.35          |
|    value_loss           | 2.2e+05       |
-------------------------------------------
Eval num_timesteps=384000, episode_reward=-2922.00 +/- 68.08
Episode length: 213.60 +/- 61.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 214           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 384000        |
| train/                  |               |
|    approx_kl            | 0.00015389235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.88         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+05      |
|    n_updates            | 1870          |
|    policy_gradient_loss | -0.000436     |
|    std                  | 1.36          |
|    value_loss           | 2.81e+05      |
-------------------------------------------
Eval num_timesteps=386000, episode_reward=-2863.87 +/- 149.37
Episode length: 288.60 +/- 145.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 289          |
|    mean_reward          | -2.86e+03    |
| time/                   |              |
|    total_timesteps      | 386000       |
| train/                  |              |
|    approx_kl            | 8.910574e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+05     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.000311    |
|    std                  | 1.36         |
|    value_loss           | 2.19e+05     |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-2936.49 +/- 30.39
Episode length: 245.40 +/- 77.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 245           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 388000        |
| train/                  |               |
|    approx_kl            | 0.00013894486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.88         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+05      |
|    n_updates            | 1890          |
|    policy_gradient_loss | -0.000194     |
|    std                  | 1.36          |
|    value_loss           | 2.19e+05      |
-------------------------------------------
Eval num_timesteps=390000, episode_reward=-2893.59 +/- 42.64
Episode length: 295.80 +/- 88.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 390000        |
| train/                  |               |
|    approx_kl            | 0.00025621022 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.89         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+05      |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.0011       |
|    std                  | 1.36          |
|    value_loss           | 2.87e+05      |
-------------------------------------------
Eval num_timesteps=392000, episode_reward=-2908.12 +/- 57.08
Episode length: 240.80 +/- 79.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 392000       |
| train/                  |              |
|    approx_kl            | 0.0002680656 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.91        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+05     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.000557    |
|    std                  | 1.37         |
|    value_loss           | 2.49e+05     |
------------------------------------------
Eval num_timesteps=394000, episode_reward=-2924.04 +/- 21.06
Episode length: 227.60 +/- 31.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 228           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 394000        |
| train/                  |               |
|    approx_kl            | 0.00019448009 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.92         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+05      |
|    n_updates            | 1920          |
|    policy_gradient_loss | -0.000386     |
|    std                  | 1.37          |
|    value_loss           | 2.18e+05      |
-------------------------------------------
Eval num_timesteps=396000, episode_reward=-2872.92 +/- 34.48
Episode length: 304.20 +/- 75.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 396000        |
| train/                  |               |
|    approx_kl            | 7.6064316e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.92         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+05      |
|    n_updates            | 1930          |
|    policy_gradient_loss | 3.32e-05      |
|    std                  | 1.37          |
|    value_loss           | 2.49e+05      |
-------------------------------------------
Eval num_timesteps=398000, episode_reward=-2912.85 +/- 16.98
Episode length: 257.00 +/- 52.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 398000       |
| train/                  |              |
|    approx_kl            | 5.356816e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.000313    |
|    std                  | 1.37         |
|    value_loss           | 2.48e+05     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-2900.47 +/- 41.08
Episode length: 275.20 +/- 112.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 8.927818e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.000198    |
|    std                  | 1.37         |
|    value_loss           | 2.48e+05     |
------------------------------------------
Eval num_timesteps=402000, episode_reward=-2925.33 +/- 39.00
Episode length: 240.00 +/- 43.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 240           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 402000        |
| train/                  |               |
|    approx_kl            | 0.00014930152 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.94         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+05      |
|    n_updates            | 1960          |
|    policy_gradient_loss | -0.000384     |
|    std                  | 1.38          |
|    value_loss           | 2.48e+05      |
-------------------------------------------
Eval num_timesteps=404000, episode_reward=-2857.37 +/- 57.93
Episode length: 313.40 +/- 123.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 313          |
|    mean_reward          | -2.86e+03    |
| time/                   |              |
|    total_timesteps      | 404000       |
| train/                  |              |
|    approx_kl            | 0.0007681559 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.95        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.8e+04      |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.38         |
|    value_loss           | 1.56e+05     |
------------------------------------------
Eval num_timesteps=406000, episode_reward=-2920.04 +/- 54.81
Episode length: 238.60 +/- 51.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 239           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 406000        |
| train/                  |               |
|    approx_kl            | 0.00035009524 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.96         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 1980          |
|    policy_gradient_loss | 0.000491      |
|    std                  | 1.38          |
|    value_loss           | 2.16e+05      |
-------------------------------------------
Eval num_timesteps=408000, episode_reward=-2871.74 +/- 62.73
Episode length: 312.00 +/- 72.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 408000        |
| train/                  |               |
|    approx_kl            | 8.0994825e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.95         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 1990          |
|    policy_gradient_loss | -0.000276     |
|    std                  | 1.38          |
|    value_loss           | 2.17e+05      |
-------------------------------------------
Eval num_timesteps=410000, episode_reward=-2871.25 +/- 20.59
Episode length: 312.40 +/- 37.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 410000        |
| train/                  |               |
|    approx_kl            | 0.00010379948 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.96         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 1.38          |
|    value_loss           | 2.16e+05      |
-------------------------------------------
Eval num_timesteps=412000, episode_reward=-2840.54 +/- 40.88
Episode length: 343.00 +/- 89.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0001370697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+05     |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00031     |
|    std                  | 1.38         |
|    value_loss           | 2.16e+05     |
------------------------------------------
Eval num_timesteps=414000, episode_reward=-2861.86 +/- 76.99
Episode length: 317.80 +/- 94.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 414000        |
| train/                  |               |
|    approx_kl            | 0.00013121252 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.97         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.000409     |
|    std                  | 1.39          |
|    value_loss           | 2.16e+05      |
-------------------------------------------
Eval num_timesteps=416000, episode_reward=-2900.71 +/- 21.13
Episode length: 289.40 +/- 49.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | 0.00018213611 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.98         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.39          |
|    value_loss           | 2.15e+05      |
-------------------------------------------
Eval num_timesteps=418000, episode_reward=-2854.34 +/- 62.71
Episode length: 301.00 +/- 83.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | -2.85e+03    |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0003125213 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.99        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+05     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.4          |
|    value_loss           | 2.46e+05     |
------------------------------------------
Eval num_timesteps=420000, episode_reward=-2923.56 +/- 20.06
Episode length: 234.80 +/- 57.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 235           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 420000        |
| train/                  |               |
|    approx_kl            | 0.00026869067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7            |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+05       |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.000618     |
|    std                  | 1.4           |
|    value_loss           | 2.59e+05      |
-------------------------------------------
Eval num_timesteps=422000, episode_reward=-2853.35 +/- 48.46
Episode length: 356.20 +/- 120.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 356           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 0.00025641546 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7            |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+05      |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.000667     |
|    std                  | 1.4           |
|    value_loss           | 2.45e+05      |
-------------------------------------------
Eval num_timesteps=424000, episode_reward=-2899.20 +/- 60.76
Episode length: 248.40 +/- 105.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 248           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 424000        |
| train/                  |               |
|    approx_kl            | 0.00018421054 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7            |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+05      |
|    n_updates            | 2070          |
|    policy_gradient_loss | -0.000343     |
|    std                  | 1.4           |
|    value_loss           | 2.75e+05      |
-------------------------------------------
Eval num_timesteps=426000, episode_reward=-2893.05 +/- 73.85
Episode length: 251.20 +/- 121.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 426000        |
| train/                  |               |
|    approx_kl            | 0.00014751067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7            |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 9.22e+04      |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.00048      |
|    std                  | 1.4           |
|    value_loss           | 1.84e+05      |
-------------------------------------------
Eval num_timesteps=428000, episode_reward=-2876.14 +/- 64.38
Episode length: 280.60 +/- 85.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 281       |
|    mean_reward     | -2.88e+03 |
| time/              |           |
|    total_timesteps | 428000    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-2923.55 +/- 36.78
Episode length: 216.20 +/- 44.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 216           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 430000        |
| train/                  |               |
|    approx_kl            | 0.00014470122 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.01         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+05      |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.000321     |
|    std                  | 1.4           |
|    value_loss           | 2.44e+05      |
-------------------------------------------
Eval num_timesteps=432000, episode_reward=-2880.29 +/- 50.65
Episode length: 271.60 +/- 73.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 272           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 432000        |
| train/                  |               |
|    approx_kl            | 0.00016077005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+05      |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.000285     |
|    std                  | 1.4           |
|    value_loss           | 2.74e+05      |
-------------------------------------------
Eval num_timesteps=434000, episode_reward=-2878.56 +/- 53.71
Episode length: 297.20 +/- 86.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 434000       |
| train/                  |              |
|    approx_kl            | 3.765506e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+05     |
|    n_updates            | 2110         |
|    policy_gradient_loss | -4.79e-05    |
|    std                  | 1.4          |
|    value_loss           | 2.44e+05     |
------------------------------------------
Eval num_timesteps=436000, episode_reward=-2881.78 +/- 49.55
Episode length: 272.40 +/- 77.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 436000       |
| train/                  |              |
|    approx_kl            | 8.910571e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.17e+04     |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000473    |
|    std                  | 1.4          |
|    value_loss           | 1.83e+05     |
------------------------------------------
Eval num_timesteps=438000, episode_reward=-2843.73 +/- 35.36
Episode length: 309.80 +/- 50.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 438000       |
| train/                  |              |
|    approx_kl            | 0.0002008623 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.000544    |
|    std                  | 1.41         |
|    value_loss           | 2.28e+05     |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-2886.48 +/- 44.86
Episode length: 259.60 +/- 64.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0001326041 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.03        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+05     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000188    |
|    std                  | 1.41         |
|    value_loss           | 2.43e+05     |
------------------------------------------
Eval num_timesteps=442000, episode_reward=-2843.33 +/- 58.44
Episode length: 380.00 +/- 127.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 442000        |
| train/                  |               |
|    approx_kl            | 0.00016483231 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.000523     |
|    std                  | 1.42          |
|    value_loss           | 2.43e+05      |
-------------------------------------------
Eval num_timesteps=444000, episode_reward=-2920.12 +/- 28.99
Episode length: 221.80 +/- 44.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 222           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 444000        |
| train/                  |               |
|    approx_kl            | 0.00016216809 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+05      |
|    n_updates            | 2160          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 1.41          |
|    value_loss           | 2.3e+05       |
-------------------------------------------
Eval num_timesteps=446000, episode_reward=-2905.25 +/- 39.43
Episode length: 282.20 +/- 33.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 282           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 446000        |
| train/                  |               |
|    approx_kl            | 0.00022776486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.03         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.000907     |
|    std                  | 1.41          |
|    value_loss           | 2.42e+05      |
-------------------------------------------
Eval num_timesteps=448000, episode_reward=-2843.66 +/- 99.11
Episode length: 301.80 +/- 91.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | -2.84e+03     |
| time/                   |               |
|    total_timesteps      | 448000        |
| train/                  |               |
|    approx_kl            | 0.00011743276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 2180          |
|    policy_gradient_loss | -0.000113     |
|    std                  | 1.41          |
|    value_loss           | 2.12e+05      |
-------------------------------------------
Eval num_timesteps=450000, episode_reward=-2899.20 +/- 20.84
Episode length: 277.60 +/- 24.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 7.861544e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+05     |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00022     |
|    std                  | 1.41         |
|    value_loss           | 2.36e+05     |
------------------------------------------
Eval num_timesteps=452000, episode_reward=-2851.94 +/- 56.64
Episode length: 359.00 +/- 102.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 359           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 452000        |
| train/                  |               |
|    approx_kl            | 8.5217674e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000251     |
|    std                  | 1.42          |
|    value_loss           | 2.12e+05      |
-------------------------------------------
Eval num_timesteps=454000, episode_reward=-2909.69 +/- 28.46
Episode length: 238.80 +/- 41.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 454000       |
| train/                  |              |
|    approx_kl            | 0.0002956829 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.1e+04      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.000963    |
|    std                  | 1.42         |
|    value_loss           | 1.82e+05     |
------------------------------------------
Eval num_timesteps=456000, episode_reward=-2891.30 +/- 44.04
Episode length: 251.40 +/- 70.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 251           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 456000        |
| train/                  |               |
|    approx_kl            | 0.00028862854 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.000212     |
|    std                  | 1.43          |
|    value_loss           | 2.11e+05      |
-------------------------------------------
Eval num_timesteps=458000, episode_reward=-2858.36 +/- 78.04
Episode length: 338.40 +/- 88.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 338           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 458000        |
| train/                  |               |
|    approx_kl            | 0.00022820767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 2230          |
|    policy_gradient_loss | -0.000545     |
|    std                  | 1.43          |
|    value_loss           | 2.11e+05      |
-------------------------------------------
Eval num_timesteps=460000, episode_reward=-2886.29 +/- 62.38
Episode length: 252.20 +/- 86.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 460000       |
| train/                  |              |
|    approx_kl            | 0.0003040781 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.000833    |
|    std                  | 1.43         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=462000, episode_reward=-2908.59 +/- 25.68
Episode length: 257.40 +/- 49.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 257           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 462000        |
| train/                  |               |
|    approx_kl            | 0.00035043262 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.09         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+05       |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000497     |
|    std                  | 1.43          |
|    value_loss           | 2.41e+05      |
-------------------------------------------
Eval num_timesteps=464000, episode_reward=-2902.01 +/- 38.10
Episode length: 269.40 +/- 58.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 464000        |
| train/                  |               |
|    approx_kl            | 0.00029479276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+05       |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.000492     |
|    std                  | 1.44          |
|    value_loss           | 2.4e+05       |
-------------------------------------------
Eval num_timesteps=466000, episode_reward=-2851.00 +/- 100.98
Episode length: 274.20 +/- 104.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 274           |
|    mean_reward          | -2.85e+03     |
| time/                   |               |
|    total_timesteps      | 466000        |
| train/                  |               |
|    approx_kl            | 0.00022084918 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.11         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.03e+04      |
|    n_updates            | 2270          |
|    policy_gradient_loss | -0.000785     |
|    std                  | 1.44          |
|    value_loss           | 1.81e+05      |
-------------------------------------------
Eval num_timesteps=468000, episode_reward=-2905.09 +/- 37.77
Episode length: 226.00 +/- 57.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 468000       |
| train/                  |              |
|    approx_kl            | 0.0001513073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000159    |
|    std                  | 1.44         |
|    value_loss           | 2.1e+05      |
------------------------------------------
Eval num_timesteps=470000, episode_reward=-2886.27 +/- 33.29
Episode length: 288.00 +/- 65.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 470000       |
| train/                  |              |
|    approx_kl            | 5.033429e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+05      |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00017     |
|    std                  | 1.44         |
|    value_loss           | 2.39e+05     |
------------------------------------------
Eval num_timesteps=472000, episode_reward=-2885.24 +/- 25.23
Episode length: 245.00 +/- 39.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 245           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 472000        |
| train/                  |               |
|    approx_kl            | 0.00016499654 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.11         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+05      |
|    n_updates            | 2300          |
|    policy_gradient_loss | -0.000507     |
|    std                  | 1.44          |
|    value_loss           | 2.28e+05      |
-------------------------------------------
Eval num_timesteps=474000, episode_reward=-2846.70 +/- 82.05
Episode length: 336.40 +/- 143.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 336         |
|    mean_reward          | -2.85e+03   |
| time/                   |             |
|    total_timesteps      | 474000      |
| train/                  |             |
|    approx_kl            | 0.000148808 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.19e+05    |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00029    |
|    std                  | 1.44        |
|    value_loss           | 2.38e+05    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=-2914.34 +/- 20.52
Episode length: 210.20 +/- 37.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 476000       |
| train/                  |              |
|    approx_kl            | 0.0001031954 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.000494    |
|    std                  | 1.45         |
|    value_loss           | 2.09e+05     |
------------------------------------------
Eval num_timesteps=478000, episode_reward=-2903.15 +/- 37.59
Episode length: 219.00 +/- 48.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 219           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 478000        |
| train/                  |               |
|    approx_kl            | 0.00017058905 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.13         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.000444     |
|    std                  | 1.45          |
|    value_loss           | 2.39e+05      |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=-2897.90 +/- 40.97
Episode length: 274.00 +/- 48.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 274           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 4.8720132e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+05      |
|    n_updates            | 2340          |
|    policy_gradient_loss | -3.42e-05     |
|    std                  | 1.44          |
|    value_loss           | 2.38e+05      |
-------------------------------------------
Eval num_timesteps=482000, episode_reward=-2901.27 +/- 21.18
Episode length: 255.20 +/- 30.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 482000        |
| train/                  |               |
|    approx_kl            | 0.00016113123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.13         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.00071      |
|    std                  | 1.45          |
|    value_loss           | 2.09e+05      |
-------------------------------------------
Eval num_timesteps=484000, episode_reward=-2921.54 +/- 22.80
Episode length: 222.80 +/- 42.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 484000       |
| train/                  |              |
|    approx_kl            | 0.0001618693 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+05     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.000383    |
|    std                  | 1.45         |
|    value_loss           | 2.66e+05     |
------------------------------------------
Eval num_timesteps=486000, episode_reward=-2910.68 +/- 40.47
Episode length: 238.40 +/- 28.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 8.965249e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.000296    |
|    std                  | 1.44         |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-2880.67 +/- 41.45
Episode length: 264.20 +/- 43.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 264           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 488000        |
| train/                  |               |
|    approx_kl            | 0.00015353461 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+05      |
|    n_updates            | 2380          |
|    policy_gradient_loss | -0.000288     |
|    std                  | 1.44          |
|    value_loss           | 2.36e+05      |
-------------------------------------------
Eval num_timesteps=490000, episode_reward=-2875.07 +/- 109.74
Episode length: 253.60 +/- 105.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 254           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 490000        |
| train/                  |               |
|    approx_kl            | 0.00019339827 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.11         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+05      |
|    n_updates            | 2390          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 1.44          |
|    value_loss           | 2.66e+05      |
-------------------------------------------
Eval num_timesteps=492000, episode_reward=-2895.69 +/- 41.87
Episode length: 280.40 +/- 152.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 5.1932613e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 2400          |
|    policy_gradient_loss | -0.000173     |
|    std                  | 1.44          |
|    value_loss           | 2.07e+05      |
-------------------------------------------
Eval num_timesteps=494000, episode_reward=-2823.28 +/- 90.93
Episode length: 307.80 +/- 63.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 308           |
|    mean_reward          | -2.82e+03     |
| time/                   |               |
|    total_timesteps      | 494000        |
| train/                  |               |
|    approx_kl            | 0.00016225758 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+05      |
|    n_updates            | 2410          |
|    policy_gradient_loss | -0.000288     |
|    std                  | 1.45          |
|    value_loss           | 2.03e+05      |
-------------------------------------------
Eval num_timesteps=496000, episode_reward=-2911.10 +/- 27.09
Episode length: 221.40 +/- 26.87
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 221            |
|    mean_reward          | -2.91e+03      |
| time/                   |                |
|    total_timesteps      | 496000         |
| train/                  |                |
|    approx_kl            | 0.000100461504 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.13          |
|    explained_variance   | 1.19e-07       |
|    learning_rate        | 0.001          |
|    loss                 | 1.32e+05       |
|    n_updates            | 2420           |
|    policy_gradient_loss | -0.000289      |
|    std                  | 1.45           |
|    value_loss           | 2.65e+05       |
--------------------------------------------
Eval num_timesteps=498000, episode_reward=-2897.07 +/- 45.44
Episode length: 259.00 +/- 60.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | -2.9e+03    |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 9.84449e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.18e+05    |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00036    |
|    std                  | 1.45        |
|    value_loss           | 2.36e+05    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=-2882.12 +/- 25.29
Episode length: 280.60 +/- 44.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 9.4799296e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.14         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+05      |
|    n_updates            | 2440          |
|    policy_gradient_loss | -0.00029      |
|    std                  | 1.45          |
|    value_loss           | 2.35e+05      |
-------------------------------------------
Eval num_timesteps=502000, episode_reward=-2873.47 +/- 63.14
Episode length: 303.80 +/- 91.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 502000        |
| train/                  |               |
|    approx_kl            | 0.00014221223 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.15         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+05      |
|    n_updates            | 2450          |
|    policy_gradient_loss | -0.000474     |
|    std                  | 1.46          |
|    value_loss           | 2.06e+05      |
-------------------------------------------
Eval num_timesteps=504000, episode_reward=-2888.58 +/- 51.20
Episode length: 266.60 +/- 76.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 267           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 504000        |
| train/                  |               |
|    approx_kl            | 0.00023055926 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.16         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+05      |
|    n_updates            | 2460          |
|    policy_gradient_loss | -0.000583     |
|    std                  | 1.46          |
|    value_loss           | 2.64e+05      |
-------------------------------------------
Eval num_timesteps=506000, episode_reward=-2878.86 +/- 18.63
Episode length: 304.00 +/- 43.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0006620417 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+05     |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.46         |
|    value_loss           | 2.35e+05     |
------------------------------------------
Eval num_timesteps=508000, episode_reward=-2895.86 +/- 43.98
Episode length: 256.00 +/- 63.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 0.0006155425 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.84e+04     |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.000733    |
|    std                  | 1.47         |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=510000, episode_reward=-2865.32 +/- 38.22
Episode length: 262.20 +/- 49.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 262           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 0.00033917083 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.19         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+05      |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.000477     |
|    std                  | 1.47          |
|    value_loss           | 2.06e+05      |
-------------------------------------------
Eval num_timesteps=512000, episode_reward=-2879.89 +/- 36.35
Episode length: 331.00 +/- 47.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 331       |
|    mean_reward     | -2.88e+03 |
| time/              |           |
|    total_timesteps | 512000    |
----------------------------------
Eval num_timesteps=514000, episode_reward=-2892.27 +/- 27.06
Episode length: 293.80 +/- 89.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | -2.89e+03     |
| time/                   |               |
|    total_timesteps      | 514000        |
| train/                  |               |
|    approx_kl            | 0.00028680725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.2          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+05      |
|    n_updates            | 2500          |
|    policy_gradient_loss | -0.0006       |
|    std                  | 1.48          |
|    value_loss           | 2.05e+05      |
-------------------------------------------
Eval num_timesteps=516000, episode_reward=-2870.97 +/- 40.58
Episode length: 303.80 +/- 64.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | -2.87e+03     |
| time/                   |               |
|    total_timesteps      | 516000        |
| train/                  |               |
|    approx_kl            | 0.00035304832 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.21         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.82e+04      |
|    n_updates            | 2510          |
|    policy_gradient_loss | -0.000661     |
|    std                  | 1.48          |
|    value_loss           | 1.77e+05      |
-------------------------------------------
Eval num_timesteps=518000, episode_reward=-2883.95 +/- 70.72
Episode length: 306.80 +/- 90.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 518000       |
| train/                  |              |
|    approx_kl            | 0.0007436654 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.81e+04     |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.48         |
|    value_loss           | 1.76e+05     |
------------------------------------------
Eval num_timesteps=520000, episode_reward=-2848.17 +/- 34.05
Episode length: 379.80 +/- 76.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | -2.85e+03    |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0015756909 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.39e+04     |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 1.48         |
|    value_loss           | 1.48e+05     |
------------------------------------------
Eval num_timesteps=522000, episode_reward=-2840.82 +/- 35.83
Episode length: 339.20 +/- 90.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 522000       |
| train/                  |              |
|    approx_kl            | 0.0005840879 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.36e+04     |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.000281    |
|    std                  | 1.48         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=524000, episode_reward=-2817.88 +/- 110.13
Episode length: 320.40 +/- 70.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 524000       |
| train/                  |              |
|    approx_kl            | 0.0011092976 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.65e+04     |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.47         |
|    value_loss           | 1.53e+05     |
------------------------------------------
Eval num_timesteps=526000, episode_reward=-2704.63 +/- 235.93
Episode length: 469.60 +/- 167.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | -2.7e+03     |
| time/                   |              |
|    total_timesteps      | 526000       |
| train/                  |              |
|    approx_kl            | 0.0008798756 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.78e+04     |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000998    |
|    std                  | 1.47         |
|    value_loss           | 1.76e+05     |
------------------------------------------
Eval num_timesteps=528000, episode_reward=-2746.11 +/- 79.64
Episode length: 536.00 +/- 130.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 536          |
|    mean_reward          | -2.75e+03    |
| time/                   |              |
|    total_timesteps      | 528000       |
| train/                  |              |
|    approx_kl            | 0.0016055913 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.21        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 5.93e+04     |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.48         |
|    value_loss           | 1.19e+05     |
------------------------------------------
Eval num_timesteps=530000, episode_reward=-2625.02 +/- 165.95
Episode length: 582.00 +/- 175.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 582          |
|    mean_reward          | -2.63e+03    |
| time/                   |              |
|    total_timesteps      | 530000       |
| train/                  |              |
|    approx_kl            | 0.0014264511 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+04     |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000843    |
|    std                  | 1.49         |
|    value_loss           | 1.47e+05     |
------------------------------------------
New best mean reward!
Eval num_timesteps=532000, episode_reward=-2753.76 +/- 82.59
Episode length: 540.20 +/- 218.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | -2.75e+03    |
| time/                   |              |
|    total_timesteps      | 532000       |
| train/                  |              |
|    approx_kl            | 0.0010755259 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.9e+04      |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.49         |
|    value_loss           | 1.18e+05     |
------------------------------------------
Eval num_timesteps=534000, episode_reward=-2680.21 +/- 268.14
Episode length: 594.60 +/- 266.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 595         |
|    mean_reward          | -2.68e+03   |
| time/                   |             |
|    total_timesteps      | 534000      |
| train/                  |             |
|    approx_kl            | 0.002986097 |
|    clip_fraction        | 0.00459     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.91e+04    |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00384    |
|    std                  | 1.49        |
|    value_loss           | 1.18e+05    |
-----------------------------------------
Eval num_timesteps=536000, episode_reward=-2707.19 +/- 52.60
Episode length: 591.40 +/- 133.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 591          |
|    mean_reward          | -2.71e+03    |
| time/                   |              |
|    total_timesteps      | 536000       |
| train/                  |              |
|    approx_kl            | 0.0015101908 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+04     |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.000905    |
|    std                  | 1.49         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=538000, episode_reward=-2760.55 +/- 195.31
Episode length: 540.60 +/- 113.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 541        |
|    mean_reward          | -2.76e+03  |
| time/                   |            |
|    total_timesteps      | 538000     |
| train/                  |            |
|    approx_kl            | 0.01873133 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.23      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.001      |
|    loss                 | 4.46e+04   |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.00794   |
|    std                  | 1.49       |
|    value_loss           | 8.93e+04   |
----------------------------------------
Eval num_timesteps=540000, episode_reward=-2807.72 +/- 85.62
Episode length: 415.40 +/- 137.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 415         |
|    mean_reward          | -2.81e+03   |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.009489553 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.9e+04     |
|    n_updates            | 2630        |
|    policy_gradient_loss | 0.000859    |
|    std                  | 1.49        |
|    value_loss           | 1.18e+05    |
-----------------------------------------
Eval num_timesteps=542000, episode_reward=-2785.97 +/- 85.88
Episode length: 430.00 +/- 81.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | -2.79e+03    |
| time/                   |              |
|    total_timesteps      | 542000       |
| train/                  |              |
|    approx_kl            | 0.0010566509 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.29e+04     |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.000645    |
|    std                  | 1.49         |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=544000, episode_reward=-2759.44 +/- 111.64
Episode length: 428.60 +/- 115.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | -2.76e+03    |
| time/                   |              |
|    total_timesteps      | 544000       |
| train/                  |              |
|    approx_kl            | 0.0007332621 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.27e+04     |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.49         |
|    value_loss           | 1.45e+05     |
------------------------------------------
Eval num_timesteps=546000, episode_reward=-2715.20 +/- 100.33
Episode length: 401.80 +/- 90.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | -2.72e+03    |
| time/                   |              |
|    total_timesteps      | 546000       |
| train/                  |              |
|    approx_kl            | 0.0008292262 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.31e+04     |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.49         |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=548000, episode_reward=-2738.54 +/- 65.69
Episode length: 386.00 +/- 47.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | -2.74e+03    |
| time/                   |              |
|    total_timesteps      | 548000       |
| train/                  |              |
|    approx_kl            | 0.0006827306 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.67e+04     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.000548    |
|    std                  | 1.49         |
|    value_loss           | 1.73e+05     |
------------------------------------------
Eval num_timesteps=550000, episode_reward=-2664.56 +/- 96.30
Episode length: 489.60 +/- 81.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | -2.66e+03     |
| time/                   |               |
|    total_timesteps      | 550000        |
| train/                  |               |
|    approx_kl            | 0.00015787099 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.24         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 8.66e+04      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.000237     |
|    std                  | 1.49          |
|    value_loss           | 1.73e+05      |
-------------------------------------------
Eval num_timesteps=552000, episode_reward=-2692.39 +/- 88.58
Episode length: 438.40 +/- 89.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | -2.69e+03    |
| time/                   |              |
|    total_timesteps      | 552000       |
| train/                  |              |
|    approx_kl            | 0.0010646249 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.27e+04     |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.49         |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=554000, episode_reward=-2684.24 +/- 107.28
Episode length: 445.60 +/- 138.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | -2.68e+03    |
| time/                   |              |
|    total_timesteps      | 554000       |
| train/                  |              |
|    approx_kl            | 0.0006367826 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.25e+04     |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.49         |
|    value_loss           | 1.45e+05     |
------------------------------------------
Eval num_timesteps=556000, episode_reward=-2702.70 +/- 120.56
Episode length: 479.40 +/- 71.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 479           |
|    mean_reward          | -2.7e+03      |
| time/                   |               |
|    total_timesteps      | 556000        |
| train/                  |               |
|    approx_kl            | 0.00049719936 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.25         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.63e+04      |
|    n_updates            | 2710          |
|    policy_gradient_loss | -0.000263     |
|    std                  | 1.49          |
|    value_loss           | 1.73e+05      |
-------------------------------------------
Eval num_timesteps=558000, episode_reward=-2731.73 +/- 67.05
Episode length: 364.60 +/- 73.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 365           |
|    mean_reward          | -2.73e+03     |
| time/                   |               |
|    total_timesteps      | 558000        |
| train/                  |               |
|    approx_kl            | 0.00030263618 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.25         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 7.26e+04      |
|    n_updates            | 2720          |
|    policy_gradient_loss | -0.000475     |
|    std                  | 1.49          |
|    value_loss           | 1.45e+05      |
-------------------------------------------
Eval num_timesteps=560000, episode_reward=-2791.72 +/- 40.48
Episode length: 334.20 +/- 96.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 334          |
|    mean_reward          | -2.79e+03    |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0002729317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.62e+04     |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.000303    |
|    std                  | 1.49         |
|    value_loss           | 1.72e+05     |
------------------------------------------
Eval num_timesteps=562000, episode_reward=-2653.43 +/- 113.60
Episode length: 440.20 +/- 115.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | -2.65e+03     |
| time/                   |               |
|    total_timesteps      | 562000        |
| train/                  |               |
|    approx_kl            | 0.00023919539 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.25         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 7.22e+04      |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 1.5           |
|    value_loss           | 1.44e+05      |
-------------------------------------------
Eval num_timesteps=564000, episode_reward=-2727.30 +/- 73.79
Episode length: 401.60 +/- 83.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | -2.73e+03    |
| time/                   |              |
|    total_timesteps      | 564000       |
| train/                  |              |
|    approx_kl            | 0.0003300052 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.57e+04     |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.5          |
|    value_loss           | 1.71e+05     |
------------------------------------------
Eval num_timesteps=566000, episode_reward=-2758.60 +/- 86.80
Episode length: 320.20 +/- 89.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | -2.76e+03    |
| time/                   |              |
|    total_timesteps      | 566000       |
| train/                  |              |
|    approx_kl            | 0.0005203013 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.22e+04     |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.000942    |
|    std                  | 1.5          |
|    value_loss           | 1.64e+05     |
------------------------------------------
Eval num_timesteps=568000, episode_reward=-2764.86 +/- 64.91
Episode length: 336.40 +/- 75.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 336           |
|    mean_reward          | -2.76e+03     |
| time/                   |               |
|    total_timesteps      | 568000        |
| train/                  |               |
|    approx_kl            | 0.00029470024 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.26         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.57e+04      |
|    n_updates            | 2770          |
|    policy_gradient_loss | -0.00051      |
|    std                  | 1.5           |
|    value_loss           | 1.71e+05      |
-------------------------------------------
Eval num_timesteps=570000, episode_reward=-2793.48 +/- 80.65
Episode length: 328.60 +/- 64.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 329           |
|    mean_reward          | -2.79e+03     |
| time/                   |               |
|    total_timesteps      | 570000        |
| train/                  |               |
|    approx_kl            | 0.00049157394 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.27         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 7.2e+04       |
|    n_updates            | 2780          |
|    policy_gradient_loss | -0.00109      |
|    std                  | 1.5           |
|    value_loss           | 1.44e+05      |
-------------------------------------------
Eval num_timesteps=572000, episode_reward=-2771.73 +/- 79.91
Episode length: 351.60 +/- 74.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | -2.77e+03    |
| time/                   |              |
|    total_timesteps      | 572000       |
| train/                  |              |
|    approx_kl            | 0.0006557348 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.58e+04     |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.5          |
|    value_loss           | 1.72e+05     |
------------------------------------------
Eval num_timesteps=574000, episode_reward=-2652.39 +/- 132.98
Episode length: 519.80 +/- 208.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | -2.65e+03    |
| time/                   |              |
|    total_timesteps      | 574000       |
| train/                  |              |
|    approx_kl            | 0.0015119615 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.18e+04     |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.5          |
|    value_loss           | 1.44e+05     |
------------------------------------------
Eval num_timesteps=576000, episode_reward=-2723.60 +/- 67.51
Episode length: 467.60 +/- 80.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 468           |
|    mean_reward          | -2.72e+03     |
| time/                   |               |
|    total_timesteps      | 576000        |
| train/                  |               |
|    approx_kl            | 0.00095062325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.28         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 7.19e+04      |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.00108      |
|    std                  | 1.51          |
|    value_loss           | 1.44e+05      |
-------------------------------------------
Eval num_timesteps=578000, episode_reward=-2840.10 +/- 35.92
Episode length: 404.80 +/- 145.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | -2.84e+03    |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0003788884 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.2e+04      |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.000414    |
|    std                  | 1.51         |
|    value_loss           | 1.44e+05     |
------------------------------------------
Eval num_timesteps=580000, episode_reward=-2676.71 +/- 48.53
Episode length: 460.20 +/- 73.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | -2.68e+03   |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.008066168 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 4.42e+04    |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.00393    |
|    std                  | 1.52        |
|    value_loss           | 8.85e+04    |
-----------------------------------------
Eval num_timesteps=582000, episode_reward=-2717.32 +/- 85.09
Episode length: 461.60 +/- 105.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | -2.72e+03    |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0010337185 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.22e+04     |
|    n_updates            | 2840         |
|    policy_gradient_loss | 0.000362     |
|    std                  | 1.53         |
|    value_loss           | 1.64e+05     |
------------------------------------------
Eval num_timesteps=584000, episode_reward=-2760.76 +/- 98.39
Episode length: 374.40 +/- 110.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | -2.76e+03    |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0039607584 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 5.82e+04     |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.00409     |
|    std                  | 1.54         |
|    value_loss           | 1.16e+05     |
------------------------------------------
Eval num_timesteps=586000, episode_reward=-2709.34 +/- 164.47
Episode length: 449.20 +/- 201.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 449         |
|    mean_reward          | -2.71e+03   |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.008316653 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.37       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 5.81e+04    |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0043     |
|    std                  | 1.54        |
|    value_loss           | 1.16e+05    |
-----------------------------------------
Eval num_timesteps=588000, episode_reward=-2795.60 +/- 77.80
Episode length: 322.40 +/- 76.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | -2.8e+03     |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0022118613 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.38        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.18e+04     |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.55         |
|    value_loss           | 1.44e+05     |
------------------------------------------
Eval num_timesteps=590000, episode_reward=-2805.14 +/- 19.10
Episode length: 385.40 +/- 95.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | -2.81e+03    |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0010432787 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.38        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.55e+04     |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.000121    |
|    std                  | 1.55         |
|    value_loss           | 1.71e+05     |
------------------------------------------
Eval num_timesteps=592000, episode_reward=-2699.10 +/- 136.95
Episode length: 506.20 +/- 61.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | -2.7e+03     |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0013705695 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.19e+04     |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.55         |
|    value_loss           | 1.44e+05     |
------------------------------------------
Eval num_timesteps=594000, episode_reward=-2699.99 +/- 106.68
Episode length: 454.00 +/- 33.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | -2.7e+03     |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 0.0022091097 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.88e+04     |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.55         |
|    value_loss           | 1.98e+05     |
------------------------------------------
Eval num_timesteps=596000, episode_reward=-2734.74 +/- 140.85
Episode length: 427.60 +/- 258.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | -2.73e+03   |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.003951745 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.4        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.14e+04    |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00275    |
|    std                  | 1.55        |
|    value_loss           | 1.43e+05    |
-----------------------------------------
Eval num_timesteps=598000, episode_reward=-2813.65 +/- 37.62
Episode length: 458.00 +/- 134.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 458       |
|    mean_reward     | -2.81e+03 |
| time/              |           |
|    total_timesteps | 598000    |
----------------------------------
Eval num_timesteps=600000, episode_reward=-2743.80 +/- 158.84
Episode length: 611.00 +/- 133.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 611          |
|    mean_reward          | -2.74e+03    |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0016368009 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.49e+04     |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 1.55         |
|    value_loss           | 1.7e+05      |
------------------------------------------
Eval num_timesteps=602000, episode_reward=-2821.46 +/- 64.40
Episode length: 365.20 +/- 120.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 602000       |
| train/                  |              |
|    approx_kl            | 0.0050002993 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.45e+04     |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.55         |
|    value_loss           | 8.89e+04     |
------------------------------------------
Eval num_timesteps=604000, episode_reward=-2787.37 +/- 57.37
Episode length: 378.20 +/- 76.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | -2.79e+03    |
| time/                   |              |
|    total_timesteps      | 604000       |
| train/                  |              |
|    approx_kl            | 0.0059768856 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.41        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.77e+04     |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.00353     |
|    std                  | 1.56         |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=606000, episode_reward=-2763.04 +/- 119.38
Episode length: 432.80 +/- 164.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | -2.76e+03    |
| time/                   |              |
|    total_timesteps      | 606000       |
| train/                  |              |
|    approx_kl            | 0.0047201305 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.77e+04     |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.56         |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=608000, episode_reward=-2831.64 +/- 59.27
Episode length: 374.40 +/- 142.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 608000       |
| train/                  |              |
|    approx_kl            | 0.0027936874 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.44        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.49e+04     |
|    n_updates            | 2960         |
|    policy_gradient_loss | 0.00142      |
|    std                  | 1.57         |
|    value_loss           | 1.7e+05      |
------------------------------------------
Eval num_timesteps=610000, episode_reward=-2757.05 +/- 121.77
Episode length: 353.40 +/- 139.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 353           |
|    mean_reward          | -2.76e+03     |
| time/                   |               |
|    total_timesteps      | 610000        |
| train/                  |               |
|    approx_kl            | 0.00019730566 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.44         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.48e+04      |
|    n_updates            | 2970          |
|    policy_gradient_loss | 5.99e-05      |
|    std                  | 1.57          |
|    value_loss           | 1.7e+05       |
-------------------------------------------
Eval num_timesteps=612000, episode_reward=-2898.56 +/- 57.68
Episode length: 347.60 +/- 120.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 348          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 612000       |
| train/                  |              |
|    approx_kl            | 0.0018902869 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 5.76e+04     |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1.57         |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=614000, episode_reward=-2792.58 +/- 20.75
Episode length: 408.60 +/- 78.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | -2.79e+03   |
| time/                   |             |
|    total_timesteps      | 614000      |
| train/                  |             |
|    approx_kl            | 0.002581981 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.46       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.1e+04     |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 1.58        |
|    value_loss           | 1.42e+05    |
-----------------------------------------
Eval num_timesteps=616000, episode_reward=-2829.90 +/- 45.26
Episode length: 414.60 +/- 132.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 415           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 616000        |
| train/                  |               |
|    approx_kl            | 0.00084972545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.47         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.82e+04      |
|    n_updates            | 3000          |
|    policy_gradient_loss | -0.000244     |
|    std                  | 1.58          |
|    value_loss           | 1.96e+05      |
-------------------------------------------
Eval num_timesteps=618000, episode_reward=-2802.30 +/- 52.89
Episode length: 343.20 +/- 83.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 343           |
|    mean_reward          | -2.8e+03      |
| time/                   |               |
|    total_timesteps      | 618000        |
| train/                  |               |
|    approx_kl            | 0.00047143322 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.47         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 7.11e+04      |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.00137      |
|    std                  | 1.58          |
|    value_loss           | 1.42e+05      |
-------------------------------------------
Eval num_timesteps=620000, episode_reward=-2907.89 +/- 52.74
Episode length: 269.20 +/- 109.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -2.91e+03     |
| time/                   |               |
|    total_timesteps      | 620000        |
| train/                  |               |
|    approx_kl            | 0.00060054497 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.47         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.79e+04      |
|    n_updates            | 3020          |
|    policy_gradient_loss | -0.000589     |
|    std                  | 1.58          |
|    value_loss           | 1.96e+05      |
-------------------------------------------
Eval num_timesteps=622000, episode_reward=-2831.68 +/- 38.60
Episode length: 387.60 +/- 74.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 622000       |
| train/                  |              |
|    approx_kl            | 0.0008358517 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.45e+04     |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.000717    |
|    std                  | 1.58         |
|    value_loss           | 1.69e+05     |
------------------------------------------
Eval num_timesteps=624000, episode_reward=-2790.95 +/- 74.98
Episode length: 516.40 +/- 48.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -2.79e+03    |
| time/                   |              |
|    total_timesteps      | 624000       |
| train/                  |              |
|    approx_kl            | 0.0019011892 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.43e+04     |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.59         |
|    value_loss           | 1.69e+05     |
------------------------------------------
Eval num_timesteps=626000, episode_reward=-2823.16 +/- 75.78
Episode length: 355.80 +/- 114.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 356          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 626000       |
| train/                  |              |
|    approx_kl            | 0.0018692808 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.41e+04     |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.000947    |
|    std                  | 1.59         |
|    value_loss           | 1.68e+05     |
------------------------------------------
Eval num_timesteps=628000, episode_reward=-2832.28 +/- 77.49
Episode length: 343.00 +/- 108.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 343           |
|    mean_reward          | -2.83e+03     |
| time/                   |               |
|    total_timesteps      | 628000        |
| train/                  |               |
|    approx_kl            | 0.00049338915 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.48         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 9.76e+04      |
|    n_updates            | 3060          |
|    policy_gradient_loss | -0.000273     |
|    std                  | 1.59          |
|    value_loss           | 1.95e+05      |
-------------------------------------------
Eval num_timesteps=630000, episode_reward=-2877.56 +/- 46.42
Episode length: 287.40 +/- 31.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 287           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 630000        |
| train/                  |               |
|    approx_kl            | 0.00028197162 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.49         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.4e+04       |
|    n_updates            | 3070          |
|    policy_gradient_loss | -0.00045      |
|    std                  | 1.59          |
|    value_loss           | 1.68e+05      |
-------------------------------------------
Eval num_timesteps=632000, episode_reward=-2862.55 +/- 30.21
Episode length: 360.60 +/- 131.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 361           |
|    mean_reward          | -2.86e+03     |
| time/                   |               |
|    total_timesteps      | 632000        |
| train/                  |               |
|    approx_kl            | 0.00017322772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.49         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 9.75e+04      |
|    n_updates            | 3080          |
|    policy_gradient_loss | -0.000296     |
|    std                  | 1.59          |
|    value_loss           | 1.95e+05      |
-------------------------------------------
Eval num_timesteps=634000, episode_reward=-2825.60 +/- 42.36
Episode length: 330.40 +/- 66.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 634000       |
| train/                  |              |
|    approx_kl            | 0.0009822258 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.39e+04     |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.59         |
|    value_loss           | 1.68e+05     |
------------------------------------------
Eval num_timesteps=636000, episode_reward=-2833.65 +/- 108.02
Episode length: 347.20 +/- 122.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 636000       |
| train/                  |              |
|    approx_kl            | 0.0069651715 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.05e+04     |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00352     |
|    std                  | 1.59         |
|    value_loss           | 1.41e+05     |
------------------------------------------
Eval num_timesteps=638000, episode_reward=-2847.45 +/- 41.14
Episode length: 371.40 +/- 106.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 371        |
|    mean_reward          | -2.85e+03  |
| time/                   |            |
|    total_timesteps      | 638000     |
| train/                  |            |
|    approx_kl            | 0.00333951 |
|    clip_fraction        | 0.00552    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.5       |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 8.38e+04   |
|    n_updates            | 3110       |
|    policy_gradient_loss | -0.00209   |
|    std                  | 1.59       |
|    value_loss           | 1.68e+05   |
----------------------------------------
Eval num_timesteps=640000, episode_reward=-2821.55 +/- 43.97
Episode length: 415.40 +/- 77.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 415           |
|    mean_reward          | -2.82e+03     |
| time/                   |               |
|    total_timesteps      | 640000        |
| train/                  |               |
|    approx_kl            | 0.00032550568 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.5          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 9.71e+04      |
|    n_updates            | 3120          |
|    policy_gradient_loss | 0.00081       |
|    std                  | 1.59          |
|    value_loss           | 1.94e+05      |
-------------------------------------------
Eval num_timesteps=642000, episode_reward=-2745.16 +/- 203.55
Episode length: 485.20 +/- 197.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 485          |
|    mean_reward          | -2.75e+03    |
| time/                   |              |
|    total_timesteps      | 642000       |
| train/                  |              |
|    approx_kl            | 0.0041149734 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.02e+04     |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.00358     |
|    std                  | 1.6          |
|    value_loss           | 1.4e+05      |
------------------------------------------
Eval num_timesteps=644000, episode_reward=-2833.82 +/- 62.39
Episode length: 324.00 +/- 64.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 324          |
|    mean_reward          | -2.83e+03    |
| time/                   |              |
|    total_timesteps      | 644000       |
| train/                  |              |
|    approx_kl            | 0.0016422544 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.69e+04     |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.000502    |
|    std                  | 1.6          |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=646000, episode_reward=-2809.17 +/- 159.92
Episode length: 368.20 +/- 127.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | -2.81e+03   |
| time/                   |             |
|    total_timesteps      | 646000      |
| train/                  |             |
|    approx_kl            | 0.006218202 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.51       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.01e+04    |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.0034     |
|    std                  | 1.6         |
|    value_loss           | 1.4e+05     |
-----------------------------------------
Eval num_timesteps=648000, episode_reward=-2808.72 +/- 77.07
Episode length: 397.60 +/- 95.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | -2.81e+03   |
| time/                   |             |
|    total_timesteps      | 648000      |
| train/                  |             |
|    approx_kl            | 0.003217929 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.51       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 5.67e+04    |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 1.6         |
|    value_loss           | 1.13e+05    |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=-2800.10 +/- 60.07
Episode length: 394.00 +/- 104.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | -2.8e+03     |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0032275012 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.32e+04     |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.6          |
|    value_loss           | 1.66e+05     |
------------------------------------------
Eval num_timesteps=652000, episode_reward=-2702.79 +/- 107.39
Episode length: 457.00 +/- 150.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 457         |
|    mean_reward          | -2.7e+03    |
| time/                   |             |
|    total_timesteps      | 652000      |
| train/                  |             |
|    approx_kl            | 0.011333623 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 5.67e+04    |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00436    |
|    std                  | 1.6         |
|    value_loss           | 1.13e+05    |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=-2658.57 +/- 105.48
Episode length: 513.20 +/- 92.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | -2.66e+03   |
| time/                   |             |
|    total_timesteps      | 654000      |
| train/                  |             |
|    approx_kl            | 0.019863421 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 5.66e+04    |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00613    |
|    std                  | 1.61        |
|    value_loss           | 1.13e+05    |
-----------------------------------------
Eval num_timesteps=656000, episode_reward=-2612.89 +/- 151.85
Episode length: 715.40 +/- 166.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 715          |
|    mean_reward          | -2.61e+03    |
| time/                   |              |
|    total_timesteps      | 656000       |
| train/                  |              |
|    approx_kl            | 0.0068580853 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.55        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.32e+04     |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.62         |
|    value_loss           | 8.63e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=658000, episode_reward=-2657.31 +/- 58.50
Episode length: 635.20 +/- 63.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 635          |
|    mean_reward          | -2.66e+03    |
| time/                   |              |
|    total_timesteps      | 658000       |
| train/                  |              |
|    approx_kl            | 0.0126794595 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.66e+04     |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00559     |
|    std                  | 1.63         |
|    value_loss           | 1.13e+05     |
------------------------------------------
Eval num_timesteps=660000, episode_reward=-2515.17 +/- 247.79
Episode length: 732.80 +/- 109.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 733         |
|    mean_reward          | -2.52e+03   |
| time/                   |             |
|    total_timesteps      | 660000      |
| train/                  |             |
|    approx_kl            | 0.009554952 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 4.32e+04    |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 1.64        |
|    value_loss           | 8.63e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=662000, episode_reward=-2496.10 +/- 156.11
Episode length: 844.40 +/- 87.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 844         |
|    mean_reward          | -2.5e+03    |
| time/                   |             |
|    total_timesteps      | 662000      |
| train/                  |             |
|    approx_kl            | 0.007640969 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 3.01e+04    |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 1.65        |
|    value_loss           | 6.03e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=664000, episode_reward=-2454.17 +/- 130.58
Episode length: 875.80 +/- 88.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 876         |
|    mean_reward          | -2.45e+03   |
| time/                   |             |
|    total_timesteps      | 664000      |
| train/                  |             |
|    approx_kl            | 0.006939506 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 4.32e+04    |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00341    |
|    std                  | 1.66        |
|    value_loss           | 8.63e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=666000, episode_reward=-2506.30 +/- 139.13
Episode length: 746.60 +/- 138.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 747          |
|    mean_reward          | -2.51e+03    |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0053019677 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.97e+04     |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.000747    |
|    std                  | 1.66         |
|    value_loss           | 5.95e+04     |
------------------------------------------
Eval num_timesteps=668000, episode_reward=-2655.79 +/- 194.41
Episode length: 579.40 +/- 285.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 579         |
|    mean_reward          | -2.66e+03   |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.013615973 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 4.31e+04    |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00539    |
|    std                  | 1.67        |
|    value_loss           | 8.62e+04    |
-----------------------------------------
Eval num_timesteps=670000, episode_reward=-2312.83 +/- 397.82
Episode length: 749.20 +/- 193.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 749          |
|    mean_reward          | -2.31e+03    |
| time/                   |              |
|    total_timesteps      | 670000       |
| train/                  |              |
|    approx_kl            | 0.0035518224 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.99e+04     |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.68         |
|    value_loss           | 5.98e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=672000, episode_reward=-2531.97 +/- 199.30
Episode length: 822.00 +/- 303.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 822         |
|    mean_reward          | -2.53e+03   |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.004822095 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.69       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 4.33e+04    |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.67        |
|    value_loss           | 8.65e+04    |
-----------------------------------------
Eval num_timesteps=674000, episode_reward=-2401.86 +/- 275.74
Episode length: 637.20 +/- 142.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 637         |
|    mean_reward          | -2.4e+03    |
| time/                   |             |
|    total_timesteps      | 674000      |
| train/                  |             |
|    approx_kl            | 0.012120608 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 4.34e+04    |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00532    |
|    std                  | 1.67        |
|    value_loss           | 8.68e+04    |
-----------------------------------------
Eval num_timesteps=676000, episode_reward=-2379.51 +/- 189.02
Episode length: 639.80 +/- 40.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 640         |
|    mean_reward          | -2.38e+03   |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.015073286 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.33e+04    |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00477    |
|    std                  | 1.67        |
|    value_loss           | 8.67e+04    |
-----------------------------------------
Eval num_timesteps=678000, episode_reward=-2600.38 +/- 53.84
Episode length: 693.60 +/- 83.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 694        |
|    mean_reward          | -2.6e+03   |
| time/                   |            |
|    total_timesteps      | 678000     |
| train/                  |            |
|    approx_kl            | 0.00926519 |
|    clip_fraction        | 0.0367     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.69      |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.001      |
|    loss                 | 5.68e+04   |
|    n_updates            | 3310       |
|    policy_gradient_loss | -0.00327   |
|    std                  | 1.68       |
|    value_loss           | 1.14e+05   |
----------------------------------------
Eval num_timesteps=680000, episode_reward=-2580.99 +/- 60.09
Episode length: 735.60 +/- 74.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 736         |
|    mean_reward          | -2.58e+03   |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.007963141 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 4.34e+04    |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 1.69        |
|    value_loss           | 8.68e+04    |
-----------------------------------------
Eval num_timesteps=682000, episode_reward=-2713.85 +/- 52.02
Episode length: 631.80 +/- 108.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 632         |
|    mean_reward          | -2.71e+03   |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.017219706 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.68e+04    |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.00709    |
|    std                  | 1.69        |
|    value_loss           | 1.14e+05    |
-----------------------------------------
Eval num_timesteps=684000, episode_reward=-2589.58 +/- 213.90
Episode length: 732.00 +/- 57.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 732       |
|    mean_reward     | -2.59e+03 |
| time/              |           |
|    total_timesteps | 684000    |
----------------------------------
Eval num_timesteps=686000, episode_reward=-2605.24 +/- 47.98
Episode length: 846.80 +/- 49.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 847         |
|    mean_reward          | -2.61e+03   |
| time/                   |             |
|    total_timesteps      | 686000      |
| train/                  |             |
|    approx_kl            | 0.010836999 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.33e+04    |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00439    |
|    std                  | 1.7         |
|    value_loss           | 8.66e+04    |
-----------------------------------------
Eval num_timesteps=688000, episode_reward=-2548.74 +/- 49.55
Episode length: 871.40 +/- 86.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 871         |
|    mean_reward          | -2.55e+03   |
| time/                   |             |
|    total_timesteps      | 688000      |
| train/                  |             |
|    approx_kl            | 0.015649611 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.33e+04    |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.00567    |
|    std                  | 1.7         |
|    value_loss           | 8.66e+04    |
-----------------------------------------
Eval num_timesteps=690000, episode_reward=-2571.49 +/- 99.63
Episode length: 893.20 +/- 120.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 893          |
|    mean_reward          | -2.57e+03    |
| time/                   |              |
|    total_timesteps      | 690000       |
| train/                  |              |
|    approx_kl            | 0.0069817603 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+04     |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00603     |
|    std                  | 1.71         |
|    value_loss           | 3.32e+04     |
------------------------------------------
Eval num_timesteps=692000, episode_reward=-2626.60 +/- 71.11
Episode length: 798.80 +/- 96.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 799          |
|    mean_reward          | -2.63e+03    |
| time/                   |              |
|    total_timesteps      | 692000       |
| train/                  |              |
|    approx_kl            | 0.0150403585 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.34e+04     |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00605     |
|    std                  | 1.71         |
|    value_loss           | 8.68e+04     |
------------------------------------------
Eval num_timesteps=694000, episode_reward=-869.71 +/- 2676.34
Episode length: 1111.60 +/- 191.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.11e+03    |
|    mean_reward          | -870        |
| time/                   |             |
|    total_timesteps      | 694000      |
| train/                  |             |
|    approx_kl            | 0.013620278 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.34e+04    |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00408    |
|    std                  | 1.72        |
|    value_loss           | 8.68e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=696000, episode_reward=-2701.99 +/- 89.76
Episode length: 638.00 +/- 180.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 638         |
|    mean_reward          | -2.7e+03    |
| time/                   |             |
|    total_timesteps      | 696000      |
| train/                  |             |
|    approx_kl            | 0.006296203 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 3e+04       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00372    |
|    std                  | 1.72        |
|    value_loss           | 6e+04       |
-----------------------------------------
Eval num_timesteps=698000, episode_reward=-2572.32 +/- 410.74
Episode length: 608.40 +/- 334.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 608          |
|    mean_reward          | -2.57e+03    |
| time/                   |              |
|    total_timesteps      | 698000       |
| train/                  |              |
|    approx_kl            | 0.0025929413 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 3e+04        |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.73         |
|    value_loss           | 6e+04        |
------------------------------------------
Eval num_timesteps=700000, episode_reward=-2638.91 +/- 87.04
Episode length: 765.00 +/- 168.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 765         |
|    mean_reward          | -2.64e+03   |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.008200633 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.72e+04    |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 1.74        |
|    value_loss           | 1.14e+05    |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=-2632.22 +/- 155.79
Episode length: 608.20 +/- 119.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 608         |
|    mean_reward          | -2.63e+03   |
| time/                   |             |
|    total_timesteps      | 702000      |
| train/                  |             |
|    approx_kl            | 0.003105598 |
|    clip_fraction        | 0.0061      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.83       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.07e+04    |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.0018     |
|    std                  | 1.74        |
|    value_loss           | 6.15e+04    |
-----------------------------------------
Eval num_timesteps=704000, episode_reward=-2802.86 +/- 104.67
Episode length: 620.20 +/- 146.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 620         |
|    mean_reward          | -2.8e+03    |
| time/                   |             |
|    total_timesteps      | 704000      |
| train/                  |             |
|    approx_kl            | 0.009215904 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.84       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 4.36e+04    |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00408    |
|    std                  | 1.75        |
|    value_loss           | 8.73e+04    |
-----------------------------------------
Eval num_timesteps=706000, episode_reward=-2740.03 +/- 107.31
Episode length: 589.40 +/- 142.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 589         |
|    mean_reward          | -2.74e+03   |
| time/                   |             |
|    total_timesteps      | 706000      |
| train/                  |             |
|    approx_kl            | 0.005953945 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.86       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3e+04       |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.0033     |
|    std                  | 1.75        |
|    value_loss           | 6.01e+04    |
-----------------------------------------
Eval num_timesteps=708000, episode_reward=-2549.48 +/- 197.75
Episode length: 629.40 +/- 106.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 629          |
|    mean_reward          | -2.55e+03    |
| time/                   |              |
|    total_timesteps      | 708000       |
| train/                  |              |
|    approx_kl            | 0.0075279837 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.87        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 5.74e+04     |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00521     |
|    std                  | 1.76         |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=710000, episode_reward=-2706.23 +/- 120.38
Episode length: 523.00 +/- 113.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 523         |
|    mean_reward          | -2.71e+03   |
| time/                   |             |
|    total_timesteps      | 710000      |
| train/                  |             |
|    approx_kl            | 0.010074429 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 5.72e+04    |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00542    |
|    std                  | 1.76        |
|    value_loss           | 1.14e+05    |
-----------------------------------------
Eval num_timesteps=712000, episode_reward=-2502.56 +/- 144.39
Episode length: 541.60 +/- 48.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | -2.5e+03     |
| time/                   |              |
|    total_timesteps      | 712000       |
| train/                  |              |
|    approx_kl            | 0.0126831625 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.87        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.73e+04     |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.00361     |
|    std                  | 1.75         |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=714000, episode_reward=-2659.43 +/- 168.06
Episode length: 462.40 +/- 137.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 462         |
|    mean_reward          | -2.66e+03   |
| time/                   |             |
|    total_timesteps      | 714000      |
| train/                  |             |
|    approx_kl            | 0.015753783 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.75e+04    |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 1.75        |
|    value_loss           | 1.15e+05    |
-----------------------------------------
Eval num_timesteps=716000, episode_reward=-2595.78 +/- 235.02
Episode length: 465.80 +/- 158.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | -2.6e+03    |
| time/                   |             |
|    total_timesteps      | 716000      |
| train/                  |             |
|    approx_kl            | 0.010786733 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.88       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.79e+04    |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00786    |
|    std                  | 1.77        |
|    value_loss           | 1.16e+05    |
-----------------------------------------
Eval num_timesteps=718000, episode_reward=-2588.78 +/- 240.32
Episode length: 457.40 +/- 177.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 457         |
|    mean_reward          | -2.59e+03   |
| time/                   |             |
|    total_timesteps      | 718000      |
| train/                  |             |
|    approx_kl            | 0.030643117 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.39e+04    |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.000597   |
|    std                  | 1.79        |
|    value_loss           | 8.78e+04    |
-----------------------------------------
Eval num_timesteps=720000, episode_reward=-2659.98 +/- 121.02
Episode length: 505.40 +/- 92.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -2.66e+03    |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0040533617 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.95        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.41e+04     |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.000628    |
|    std                  | 1.79         |
|    value_loss           | 1.68e+05     |
------------------------------------------
Eval num_timesteps=722000, episode_reward=-2675.73 +/- 104.13
Episode length: 476.60 +/- 74.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | -2.68e+03   |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.014654789 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.96       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.74e+04    |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00568    |
|    std                  | 1.8         |
|    value_loss           | 1.15e+05    |
-----------------------------------------
Eval num_timesteps=724000, episode_reward=-2761.31 +/- 34.06
Episode length: 388.20 +/- 46.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | -2.76e+03   |
| time/                   |             |
|    total_timesteps      | 724000      |
| train/                  |             |
|    approx_kl            | 0.009352513 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.04e+04    |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 1.81        |
|    value_loss           | 1.41e+05    |
-----------------------------------------
Eval num_timesteps=726000, episode_reward=-2776.16 +/- 84.83
Episode length: 365.80 +/- 100.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 366        |
|    mean_reward          | -2.78e+03  |
| time/                   |            |
|    total_timesteps      | 726000     |
| train/                  |            |
|    approx_kl            | 0.02069781 |
|    clip_fraction        | 0.0917     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8         |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.001      |
|    loss                 | 5.69e+04   |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.00655   |
|    std                  | 1.81       |
|    value_loss           | 1.14e+05   |
----------------------------------------
Eval num_timesteps=728000, episode_reward=-2800.01 +/- 95.14
Episode length: 322.40 +/- 90.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | -2.8e+03     |
| time/                   |              |
|    total_timesteps      | 728000       |
| train/                  |              |
|    approx_kl            | 0.0051120096 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.69e+04     |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00056     |
|    std                  | 1.82         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=730000, episode_reward=-2815.06 +/- 123.50
Episode length: 307.60 +/- 104.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -2.82e+03    |
| time/                   |              |
|    total_timesteps      | 730000       |
| train/                  |              |
|    approx_kl            | 0.0058064014 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.33e+04     |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.00294     |
|    std                  | 1.82         |
|    value_loss           | 1.67e+05     |
------------------------------------------
Eval num_timesteps=732000, episode_reward=-2878.60 +/- 44.71
Episode length: 301.00 +/- 73.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 732000       |
| train/                  |              |
|    approx_kl            | 0.0068297926 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.35e+04     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.00298     |
|    std                  | 1.82         |
|    value_loss           | 1.67e+05     |
------------------------------------------
Eval num_timesteps=734000, episode_reward=-2902.62 +/- 79.99
Episode length: 332.60 +/- 66.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 333          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 734000       |
| train/                  |              |
|    approx_kl            | 0.0026181699 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+04      |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1.82         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=736000, episode_reward=-2832.72 +/- 72.22
Episode length: 322.40 +/- 65.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 322        |
|    mean_reward          | -2.83e+03  |
| time/                   |            |
|    total_timesteps      | 736000     |
| train/                  |            |
|    approx_kl            | 0.00177755 |
|    clip_fraction        | 0.00107    |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.01      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 9.65e+04   |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.00118   |
|    std                  | 1.82       |
|    value_loss           | 1.93e+05   |
----------------------------------------
Eval num_timesteps=738000, episode_reward=-3006.08 +/- 129.51
Episode length: 337.60 +/- 63.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | -3.01e+03    |
| time/                   |              |
|    total_timesteps      | 738000       |
| train/                  |              |
|    approx_kl            | 0.0022692247 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.3e+04      |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 1.82         |
|    value_loss           | 1.66e+05     |
------------------------------------------
Eval num_timesteps=740000, episode_reward=-3001.63 +/- 106.27
Episode length: 313.60 +/- 81.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 314         |
|    mean_reward          | -3e+03      |
| time/                   |             |
|    total_timesteps      | 740000      |
| train/                  |             |
|    approx_kl            | 0.001677145 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.29e+04    |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.000192   |
|    std                  | 1.82        |
|    value_loss           | 1.86e+05    |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=-2964.05 +/- 85.18
Episode length: 342.80 +/- 86.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | -2.96e+03    |
| time/                   |              |
|    total_timesteps      | 742000       |
| train/                  |              |
|    approx_kl            | 0.0020649591 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.34e+04     |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.82         |
|    value_loss           | 1.67e+05     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=-2986.06 +/- 122.86
Episode length: 341.20 +/- 116.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 744000       |
| train/                  |              |
|    approx_kl            | 0.0030824468 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.33e+04     |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.82         |
|    value_loss           | 1.67e+05     |
------------------------------------------
Eval num_timesteps=746000, episode_reward=-2903.38 +/- 44.53
Episode length: 295.00 +/- 89.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 746000       |
| train/                  |              |
|    approx_kl            | 0.0019112765 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.62e+04     |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 1.82         |
|    value_loss           | 1.92e+05     |
------------------------------------------
Eval num_timesteps=748000, episode_reward=-2947.77 +/- 106.89
Episode length: 349.40 +/- 98.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 349          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 748000       |
| train/                  |              |
|    approx_kl            | 0.0031891146 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.06e+04     |
|    n_updates            | 3650         |
|    policy_gradient_loss | -0.0026      |
|    std                  | 1.82         |
|    value_loss           | 1.41e+05     |
------------------------------------------
Eval num_timesteps=750000, episode_reward=-2926.64 +/- 151.77
Episode length: 375.80 +/- 76.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0024934835 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.95e+04     |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000546    |
|    std                  | 1.82         |
|    value_loss           | 1.39e+05     |
------------------------------------------
Eval num_timesteps=752000, episode_reward=-3014.42 +/- 119.50
Episode length: 293.00 +/- 72.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | -3.01e+03    |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0021591333 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.96e+04     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.82         |
|    value_loss           | 1.39e+05     |
------------------------------------------
Eval num_timesteps=754000, episode_reward=-2971.28 +/- 56.41
Episode length: 250.00 +/- 48.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 250           |
|    mean_reward          | -2.97e+03     |
| time/                   |               |
|    total_timesteps      | 754000        |
| train/                  |               |
|    approx_kl            | 0.00084445835 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+05      |
|    n_updates            | 3680          |
|    policy_gradient_loss | 0.000487      |
|    std                  | 1.83          |
|    value_loss           | 2.1e+05       |
-------------------------------------------
Eval num_timesteps=756000, episode_reward=-2965.11 +/- 22.85
Episode length: 302.20 +/- 74.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | -2.97e+03     |
| time/                   |               |
|    total_timesteps      | 756000        |
| train/                  |               |
|    approx_kl            | 0.00095498847 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.61e+04      |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.00154      |
|    std                  | 1.83          |
|    value_loss           | 1.92e+05      |
-------------------------------------------
Eval num_timesteps=758000, episode_reward=-2929.04 +/- 24.42
Episode length: 287.20 +/- 101.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 758000       |
| train/                  |              |
|    approx_kl            | 0.0012992315 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.59e+04     |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.000754    |
|    std                  | 1.83         |
|    value_loss           | 1.92e+05     |
------------------------------------------
Eval num_timesteps=760000, episode_reward=-2879.93 +/- 102.33
Episode length: 331.60 +/- 71.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 332          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0013110527 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.3e+04      |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.83         |
|    value_loss           | 1.66e+05     |
------------------------------------------
Eval num_timesteps=762000, episode_reward=-2888.18 +/- 96.62
Episode length: 291.00 +/- 74.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0017434419 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.25e+04     |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000731    |
|    std                  | 1.83         |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=764000, episode_reward=-2966.63 +/- 135.85
Episode length: 309.80 +/- 139.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | -2.97e+03    |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0039892783 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.24e+04     |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 1.83         |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=766000, episode_reward=-2943.28 +/- 145.59
Episode length: 371.80 +/- 78.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 766000        |
| train/                  |               |
|    approx_kl            | 0.00071624236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.55e+04      |
|    n_updates            | 3740          |
|    policy_gradient_loss | -0.000295     |
|    std                  | 1.83          |
|    value_loss           | 1.91e+05      |
-------------------------------------------
Eval num_timesteps=768000, episode_reward=-2944.79 +/- 63.28
Episode length: 383.00 +/- 35.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 383       |
|    mean_reward     | -2.94e+03 |
| time/              |           |
|    total_timesteps | 768000    |
----------------------------------
Eval num_timesteps=770000, episode_reward=-2884.95 +/- 159.33
Episode length: 323.00 +/- 130.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | -2.88e+03    |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 0.0015181992 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.24e+04     |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.83         |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=772000, episode_reward=-3058.28 +/- 182.93
Episode length: 416.60 +/- 29.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | -3.06e+03     |
| time/                   |               |
|    total_timesteps      | 772000        |
| train/                  |               |
|    approx_kl            | 0.00063102145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 9.55e+04      |
|    n_updates            | 3760          |
|    policy_gradient_loss | -0.000399     |
|    std                  | 1.83          |
|    value_loss           | 1.91e+05      |
-------------------------------------------
Eval num_timesteps=774000, episode_reward=-2947.18 +/- 104.99
Episode length: 292.60 +/- 80.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 293         |
|    mean_reward          | -2.95e+03   |
| time/                   |             |
|    total_timesteps      | 774000      |
| train/                  |             |
|    approx_kl            | 0.004104672 |
|    clip_fraction        | 0.00854     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 8.2e+04     |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 1.83        |
|    value_loss           | 1.64e+05    |
-----------------------------------------
Eval num_timesteps=776000, episode_reward=-3021.89 +/- 128.27
Episode length: 346.00 +/- 44.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 346         |
|    mean_reward          | -3.02e+03   |
| time/                   |             |
|    total_timesteps      | 776000      |
| train/                  |             |
|    approx_kl            | 0.004203423 |
|    clip_fraction        | 0.00698     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 8.23e+04    |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00242    |
|    std                  | 1.83        |
|    value_loss           | 1.65e+05    |
-----------------------------------------
Eval num_timesteps=778000, episode_reward=-3092.28 +/- 93.13
Episode length: 344.00 +/- 43.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 778000       |
| train/                  |              |
|    approx_kl            | 0.0024519588 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+04     |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.000946    |
|    std                  | 1.83         |
|    value_loss           | 1.9e+05      |
------------------------------------------
Eval num_timesteps=780000, episode_reward=-2921.23 +/- 42.23
Episode length: 256.00 +/- 63.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0006787658 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+04     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000515    |
|    std                  | 1.83         |
|    value_loss           | 1.9e+05      |
------------------------------------------
Eval num_timesteps=782000, episode_reward=-2976.06 +/- 77.08
Episode length: 350.80 +/- 84.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 351           |
|    mean_reward          | -2.98e+03     |
| time/                   |               |
|    total_timesteps      | 782000        |
| train/                  |               |
|    approx_kl            | 0.00055957725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+05      |
|    n_updates            | 3810          |
|    policy_gradient_loss | -0.000378     |
|    std                  | 1.83          |
|    value_loss           | 2.16e+05      |
-------------------------------------------
Eval num_timesteps=784000, episode_reward=-2947.98 +/- 100.32
Episode length: 324.60 +/- 42.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 784000       |
| train/                  |              |
|    approx_kl            | 0.0011541597 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.47e+04     |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.83         |
|    value_loss           | 1.9e+05      |
------------------------------------------
Eval num_timesteps=786000, episode_reward=-2976.03 +/- 78.38
Episode length: 280.40 +/- 107.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | -2.98e+03    |
| time/                   |              |
|    total_timesteps      | 786000       |
| train/                  |              |
|    approx_kl            | 0.0012311611 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+04     |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.83         |
|    value_loss           | 1.91e+05     |
------------------------------------------
Eval num_timesteps=788000, episode_reward=-2973.09 +/- 82.81
Episode length: 285.40 +/- 110.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 285           |
|    mean_reward          | -2.97e+03     |
| time/                   |               |
|    total_timesteps      | 788000        |
| train/                  |               |
|    approx_kl            | 0.00077215314 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+05      |
|    n_updates            | 3840          |
|    policy_gradient_loss | -0.000512     |
|    std                  | 1.83          |
|    value_loss           | 2.42e+05      |
-------------------------------------------
Eval num_timesteps=790000, episode_reward=-3032.29 +/- 116.81
Episode length: 332.20 +/- 32.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 332          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 790000       |
| train/                  |              |
|    approx_kl            | 0.0015022391 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.21e+04     |
|    n_updates            | 3850         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.83         |
|    value_loss           | 1.64e+05     |
------------------------------------------
Eval num_timesteps=792000, episode_reward=-2914.64 +/- 38.62
Episode length: 277.20 +/- 68.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 792000       |
| train/                  |              |
|    approx_kl            | 0.0018557858 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.45e+04     |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.000889    |
|    std                  | 1.83         |
|    value_loss           | 1.89e+05     |
------------------------------------------
Eval num_timesteps=794000, episode_reward=-3004.02 +/- 114.79
Episode length: 254.80 +/- 37.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | -3e+03       |
| time/                   |              |
|    total_timesteps      | 794000       |
| train/                  |              |
|    approx_kl            | 0.0006280942 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.42e+04     |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.000285    |
|    std                  | 1.83         |
|    value_loss           | 1.89e+05     |
------------------------------------------
Eval num_timesteps=796000, episode_reward=-3067.30 +/- 91.27
Episode length: 305.40 +/- 55.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 796000       |
| train/                  |              |
|    approx_kl            | 0.0016326236 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.48e+04     |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.000926    |
|    std                  | 1.83         |
|    value_loss           | 1.9e+05      |
------------------------------------------
Eval num_timesteps=798000, episode_reward=-3197.07 +/- 98.16
Episode length: 324.60 +/- 32.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | -3.2e+03    |
| time/                   |             |
|    total_timesteps      | 798000      |
| train/                  |             |
|    approx_kl            | 0.004444985 |
|    clip_fraction        | 0.00825     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 9.45e+04    |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.83        |
|    value_loss           | 1.89e+05    |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=-3164.16 +/- 326.14
Episode length: 323.20 +/- 159.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 0.0031045764 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+05     |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.83         |
|    value_loss           | 2.15e+05     |
------------------------------------------
Eval num_timesteps=802000, episode_reward=-3207.42 +/- 146.55
Episode length: 340.60 +/- 90.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 341         |
|    mean_reward          | -3.21e+03   |
| time/                   |             |
|    total_timesteps      | 802000      |
| train/                  |             |
|    approx_kl            | 0.002240512 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 8.14e+04    |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 1.83        |
|    value_loss           | 1.63e+05    |
-----------------------------------------
Eval num_timesteps=804000, episode_reward=-3120.12 +/- 167.27
Episode length: 308.00 +/- 38.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -3.12e+03    |
| time/                   |              |
|    total_timesteps      | 804000       |
| train/                  |              |
|    approx_kl            | 0.0020543253 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.86e+04     |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.83         |
|    value_loss           | 1.37e+05     |
------------------------------------------
Eval num_timesteps=806000, episode_reward=-3061.68 +/- 72.47
Episode length: 306.40 +/- 71.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | -3.06e+03   |
| time/                   |             |
|    total_timesteps      | 806000      |
| train/                  |             |
|    approx_kl            | 0.005622343 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 8.12e+04    |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.0018     |
|    std                  | 1.84        |
|    value_loss           | 1.63e+05    |
-----------------------------------------
Eval num_timesteps=808000, episode_reward=-3103.02 +/- 65.00
Episode length: 328.80 +/- 56.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 329        |
|    mean_reward          | -3.1e+03   |
| time/                   |            |
|    total_timesteps      | 808000     |
| train/                  |            |
|    approx_kl            | 0.00365045 |
|    clip_fraction        | 0.00513    |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.05      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 6.81e+04   |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00133   |
|    std                  | 1.84       |
|    value_loss           | 1.36e+05   |
----------------------------------------
Eval num_timesteps=810000, episode_reward=-3012.85 +/- 104.00
Episode length: 255.00 +/- 38.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | -3.01e+03     |
| time/                   |               |
|    total_timesteps      | 810000        |
| train/                  |               |
|    approx_kl            | 0.00069876044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+05      |
|    n_updates            | 3950          |
|    policy_gradient_loss | -0.000611     |
|    std                  | 1.84          |
|    value_loss           | 2.13e+05      |
-------------------------------------------
Eval num_timesteps=812000, episode_reward=-3173.06 +/- 211.88
Episode length: 359.00 +/- 165.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | -3.17e+03   |
| time/                   |             |
|    total_timesteps      | 812000      |
| train/                  |             |
|    approx_kl            | 0.010338573 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 8.11e+04    |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00413    |
|    std                  | 1.84        |
|    value_loss           | 1.62e+05    |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=-3183.18 +/- 113.94
Episode length: 357.60 +/- 68.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | -3.18e+03   |
| time/                   |             |
|    total_timesteps      | 814000      |
| train/                  |             |
|    approx_kl            | 0.009576393 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.12e+04    |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00317    |
|    std                  | 1.84        |
|    value_loss           | 1.62e+05    |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=-2989.49 +/- 137.01
Episode length: 337.40 +/- 102.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 337         |
|    mean_reward          | -2.99e+03   |
| time/                   |             |
|    total_timesteps      | 816000      |
| train/                  |             |
|    approx_kl            | 0.002176771 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.41e+04    |
|    n_updates            | 3980        |
|    policy_gradient_loss | -0.00145    |
|    std                  | 1.84        |
|    value_loss           | 1.88e+05    |
-----------------------------------------
Eval num_timesteps=818000, episode_reward=-3156.25 +/- 122.87
Episode length: 351.00 +/- 127.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 818000       |
| train/                  |              |
|    approx_kl            | 0.0012766755 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.12e+04     |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.000187    |
|    std                  | 1.85         |
|    value_loss           | 1.63e+05     |
------------------------------------------
Eval num_timesteps=820000, episode_reward=-3125.11 +/- 102.26
Episode length: 251.20 +/- 84.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | -3.13e+03   |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.001912221 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.37e+04    |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00145    |
|    std                  | 1.85        |
|    value_loss           | 1.88e+05    |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=-3162.11 +/- 121.36
Episode length: 307.40 +/- 12.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 822000       |
| train/                  |              |
|    approx_kl            | 0.0008216252 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.07        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.41e+04     |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.000631    |
|    std                  | 1.85         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=824000, episode_reward=-3146.36 +/- 77.69
Episode length: 254.40 +/- 39.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 824000       |
| train/                  |              |
|    approx_kl            | 0.0017758406 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.07        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.38e+04     |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.85         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=826000, episode_reward=-3096.46 +/- 140.29
Episode length: 248.80 +/- 61.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 826000       |
| train/                  |              |
|    approx_kl            | 0.0012183059 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.85         |
|    value_loss           | 2.14e+05     |
------------------------------------------
Eval num_timesteps=828000, episode_reward=-3092.73 +/- 22.27
Episode length: 225.80 +/- 27.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 828000       |
| train/                  |              |
|    approx_kl            | 0.0012420529 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.00064     |
|    std                  | 1.85         |
|    value_loss           | 2.13e+05     |
------------------------------------------
Eval num_timesteps=830000, episode_reward=-3032.53 +/- 57.05
Episode length: 184.20 +/- 37.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 184          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0010135924 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+05     |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.000548    |
|    std                  | 1.85         |
|    value_loss           | 2.38e+05     |
------------------------------------------
Eval num_timesteps=832000, episode_reward=-3086.42 +/- 111.12
Episode length: 293.20 +/- 78.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 832000       |
| train/                  |              |
|    approx_kl            | 0.0020639242 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+05     |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.85         |
|    value_loss           | 2.37e+05     |
------------------------------------------
Eval num_timesteps=834000, episode_reward=-3069.73 +/- 7.31
Episode length: 215.20 +/- 30.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 215          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0015820661 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.06e+04     |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.85         |
|    value_loss           | 1.61e+05     |
------------------------------------------
Eval num_timesteps=836000, episode_reward=-3005.44 +/- 202.61
Episode length: 281.20 +/- 94.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | -3.01e+03   |
| time/                   |             |
|    total_timesteps      | 836000      |
| train/                  |             |
|    approx_kl            | 0.003337035 |
|    clip_fraction        | 0.00356     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.19e+05    |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00202    |
|    std                  | 1.85        |
|    value_loss           | 2.38e+05    |
-----------------------------------------
Eval num_timesteps=838000, episode_reward=-3107.48 +/- 79.89
Episode length: 242.00 +/- 53.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 838000       |
| train/                  |              |
|    approx_kl            | 0.0007044596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.08        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.28e+04     |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.000646    |
|    std                  | 1.85         |
|    value_loss           | 1.86e+05     |
------------------------------------------
Eval num_timesteps=840000, episode_reward=-3056.28 +/- 30.37
Episode length: 235.80 +/- 45.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | -3.06e+03   |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.007226181 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.05e+04    |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.85        |
|    value_loss           | 1.41e+05    |
-----------------------------------------
Eval num_timesteps=842000, episode_reward=-3018.36 +/- 125.98
Episode length: 295.60 +/- 69.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 842000       |
| train/                  |              |
|    approx_kl            | 0.0042759157 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.09        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.27e+04     |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.000471    |
|    std                  | 1.86         |
|    value_loss           | 1.86e+05     |
------------------------------------------
Eval num_timesteps=844000, episode_reward=-2928.06 +/- 86.62
Episode length: 275.60 +/- 55.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0015909865 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.88e+04     |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.000769    |
|    std                  | 1.86         |
|    value_loss           | 1.78e+05     |
------------------------------------------
Eval num_timesteps=846000, episode_reward=-3077.30 +/- 126.99
Episode length: 300.20 +/- 126.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -3.08e+03   |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.001310975 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+05    |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.000652   |
|    std                  | 1.86        |
|    value_loss           | 2.1e+05     |
-----------------------------------------
Eval num_timesteps=848000, episode_reward=-2999.83 +/- 114.45
Episode length: 315.80 +/- 83.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | -3e+03        |
| time/                   |               |
|    total_timesteps      | 848000        |
| train/                  |               |
|    approx_kl            | 0.00032116394 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 4140          |
|    policy_gradient_loss | -0.000316     |
|    std                  | 1.86          |
|    value_loss           | 2.09e+05      |
-------------------------------------------
Eval num_timesteps=850000, episode_reward=-2882.94 +/- 61.12
Episode length: 268.00 +/- 72.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 268           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 850000        |
| train/                  |               |
|    approx_kl            | 0.00029238776 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.2e+04       |
|    n_updates            | 4150          |
|    policy_gradient_loss | -0.000591     |
|    std                  | 1.86          |
|    value_loss           | 1.84e+05      |
-------------------------------------------
Eval num_timesteps=852000, episode_reward=-3016.86 +/- 134.21
Episode length: 285.00 +/- 102.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 0.0033339947 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.1         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.95e+04     |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00237     |
|    std                  | 1.86         |
|    value_loss           | 1.59e+05     |
------------------------------------------
Eval num_timesteps=854000, episode_reward=-3062.35 +/- 119.68
Episode length: 292.80 +/- 101.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 293       |
|    mean_reward     | -3.06e+03 |
| time/              |           |
|    total_timesteps | 854000    |
----------------------------------
Eval num_timesteps=856000, episode_reward=-3087.97 +/- 114.63
Episode length: 312.40 +/- 104.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 856000       |
| train/                  |              |
|    approx_kl            | 0.0035492056 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.11        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.93e+04     |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.000922    |
|    std                  | 1.87         |
|    value_loss           | 1.59e+05     |
------------------------------------------
Eval num_timesteps=858000, episode_reward=-3124.67 +/- 134.61
Episode length: 339.40 +/- 107.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 339        |
|    mean_reward          | -3.12e+03  |
| time/                   |            |
|    total_timesteps      | 858000     |
| train/                  |            |
|    approx_kl            | 0.01036899 |
|    clip_fraction        | 0.0674     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.11      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 7.23e+04   |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.00288   |
|    std                  | 1.87       |
|    value_loss           | 1.45e+05   |
----------------------------------------
Eval num_timesteps=860000, episode_reward=-3104.41 +/- 94.11
Episode length: 303.60 +/- 85.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 304         |
|    mean_reward          | -3.1e+03    |
| time/                   |             |
|    total_timesteps      | 860000      |
| train/                  |             |
|    approx_kl            | 0.004347591 |
|    clip_fraction        | 0.00952     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.99e+04    |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 1.87        |
|    value_loss           | 1.8e+05     |
-----------------------------------------
Eval num_timesteps=862000, episode_reward=-2916.05 +/- 128.15
Episode length: 307.40 +/- 44.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 307         |
|    mean_reward          | -2.92e+03   |
| time/                   |             |
|    total_timesteps      | 862000      |
| train/                  |             |
|    approx_kl            | 0.003518391 |
|    clip_fraction        | 0.00376     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.21e+04    |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 1.88        |
|    value_loss           | 1.84e+05    |
-----------------------------------------
Eval num_timesteps=864000, episode_reward=-3050.26 +/- 65.85
Episode length: 322.80 +/- 41.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | -3.05e+03     |
| time/                   |               |
|    total_timesteps      | 864000        |
| train/                  |               |
|    approx_kl            | 0.00063604536 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.16e+04      |
|    n_updates            | 4210          |
|    policy_gradient_loss | -0.000397     |
|    std                  | 1.88          |
|    value_loss           | 1.83e+05      |
-------------------------------------------
Eval num_timesteps=866000, episode_reward=-2946.24 +/- 45.00
Episode length: 221.40 +/- 51.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 866000       |
| train/                  |              |
|    approx_kl            | 0.0030777329 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 6.68e+04     |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.88         |
|    value_loss           | 1.34e+05     |
------------------------------------------
Eval num_timesteps=868000, episode_reward=-3084.31 +/- 89.03
Episode length: 295.00 +/- 35.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | -3.08e+03    |
| time/                   |              |
|    total_timesteps      | 868000       |
| train/                  |              |
|    approx_kl            | 0.0011213617 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 4230         |
|    policy_gradient_loss | 0.000417     |
|    std                  | 1.88         |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=870000, episode_reward=-3047.13 +/- 84.66
Episode length: 339.00 +/- 65.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 339           |
|    mean_reward          | -3.05e+03     |
| time/                   |               |
|    total_timesteps      | 870000        |
| train/                  |               |
|    approx_kl            | 0.00035418445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+05      |
|    n_updates            | 4240          |
|    policy_gradient_loss | -0.000137     |
|    std                  | 1.88          |
|    value_loss           | 2.33e+05      |
-------------------------------------------
Eval num_timesteps=872000, episode_reward=-3063.23 +/- 126.35
Episode length: 300.00 +/- 39.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 872000       |
| train/                  |              |
|    approx_kl            | 0.0022148518 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.88e+04     |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 1.87         |
|    value_loss           | 1.58e+05     |
------------------------------------------
Eval num_timesteps=874000, episode_reward=-3099.66 +/- 86.77
Episode length: 236.80 +/- 88.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 874000       |
| train/                  |              |
|    approx_kl            | 0.0047140177 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.89e+04     |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.000605    |
|    std                  | 1.87         |
|    value_loss           | 1.58e+05     |
------------------------------------------
Eval num_timesteps=876000, episode_reward=-3176.44 +/- 186.18
Episode length: 292.80 +/- 128.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | -3.18e+03    |
| time/                   |              |
|    total_timesteps      | 876000       |
| train/                  |              |
|    approx_kl            | 0.0014962683 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 9.11e+04     |
|    n_updates            | 4270         |
|    policy_gradient_loss | -0.000645    |
|    std                  | 1.87         |
|    value_loss           | 1.82e+05     |
------------------------------------------
Eval num_timesteps=878000, episode_reward=-3290.00 +/- 64.16
Episode length: 356.60 +/- 56.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 357         |
|    mean_reward          | -3.29e+03   |
| time/                   |             |
|    total_timesteps      | 878000      |
| train/                  |             |
|    approx_kl            | 0.006270345 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.13e+04    |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00307    |
|    std                  | 1.87        |
|    value_loss           | 1.83e+05    |
-----------------------------------------
Eval num_timesteps=880000, episode_reward=-3120.56 +/- 92.80
Episode length: 205.20 +/- 41.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 205          |
|    mean_reward          | -3.12e+03    |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0079928655 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.59e+04     |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 1.87         |
|    value_loss           | 1.72e+05     |
------------------------------------------
Eval num_timesteps=882000, episode_reward=-3232.57 +/- 148.70
Episode length: 312.40 +/- 77.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 312         |
|    mean_reward          | -3.23e+03   |
| time/                   |             |
|    total_timesteps      | 882000      |
| train/                  |             |
|    approx_kl            | 0.001333495 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+05    |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.000987   |
|    std                  | 1.88        |
|    value_loss           | 2.07e+05    |
-----------------------------------------
Eval num_timesteps=884000, episode_reward=-3169.68 +/- 112.06
Episode length: 259.80 +/- 82.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | -3.17e+03    |
| time/                   |              |
|    total_timesteps      | 884000       |
| train/                  |              |
|    approx_kl            | 0.0006489181 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 4310         |
|    policy_gradient_loss | -0.000724    |
|    std                  | 1.88         |
|    value_loss           | 2.32e+05     |
------------------------------------------
Eval num_timesteps=886000, episode_reward=-3093.13 +/- 92.34
Episode length: 216.00 +/- 61.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 216          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 886000       |
| train/                  |              |
|    approx_kl            | 0.0020738724 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.88         |
|    value_loss           | 2.31e+05     |
------------------------------------------
Eval num_timesteps=888000, episode_reward=-3104.92 +/- 138.14
Episode length: 236.80 +/- 114.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 888000       |
| train/                  |              |
|    approx_kl            | 0.0005342657 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 4330         |
|    policy_gradient_loss | 0.000367     |
|    std                  | 1.88         |
|    value_loss           | 2.56e+05     |
------------------------------------------
Eval num_timesteps=890000, episode_reward=-3061.98 +/- 38.88
Episode length: 175.40 +/- 33.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 890000       |
| train/                  |              |
|    approx_kl            | 0.0014013082 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.000879    |
|    std                  | 1.88         |
|    value_loss           | 2.07e+05     |
------------------------------------------
Eval num_timesteps=892000, episode_reward=-3160.19 +/- 70.54
Episode length: 250.40 +/- 46.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 892000       |
| train/                  |              |
|    approx_kl            | 0.0014339085 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.13        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+05     |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.000448    |
|    std                  | 1.88         |
|    value_loss           | 2.55e+05     |
------------------------------------------
Eval num_timesteps=894000, episode_reward=-3138.88 +/- 119.53
Episode length: 227.40 +/- 72.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 227           |
|    mean_reward          | -3.14e+03     |
| time/                   |               |
|    total_timesteps      | 894000        |
| train/                  |               |
|    approx_kl            | 0.00037482585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+05      |
|    n_updates            | 4360          |
|    policy_gradient_loss | -0.00027      |
|    std                  | 1.88          |
|    value_loss           | 2.56e+05      |
-------------------------------------------
Eval num_timesteps=896000, episode_reward=-3126.92 +/- 147.19
Episode length: 197.20 +/- 80.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 197           |
|    mean_reward          | -3.13e+03     |
| time/                   |               |
|    total_timesteps      | 896000        |
| train/                  |               |
|    approx_kl            | 0.00048760572 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+05      |
|    n_updates            | 4370          |
|    policy_gradient_loss | -0.000485     |
|    std                  | 1.88          |
|    value_loss           | 2.3e+05       |
-------------------------------------------
Eval num_timesteps=898000, episode_reward=-3060.51 +/- 86.39
Episode length: 210.00 +/- 57.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -3.06e+03   |
| time/                   |             |
|    total_timesteps      | 898000      |
| train/                  |             |
|    approx_kl            | 0.005977923 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.14       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.03e+05    |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 1.88        |
|    value_loss           | 2.07e+05    |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=-3079.74 +/- 73.86
Episode length: 182.80 +/- 53.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 183          |
|    mean_reward          | -3.08e+03    |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0017812409 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.000415    |
|    std                  | 1.88         |
|    value_loss           | 2.05e+05     |
------------------------------------------
Eval num_timesteps=902000, episode_reward=-3104.83 +/- 97.10
Episode length: 208.60 +/- 60.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 209          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 902000       |
| train/                  |              |
|    approx_kl            | 0.0038906266 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+05     |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 1.88         |
|    value_loss           | 2.3e+05      |
------------------------------------------
Eval num_timesteps=904000, episode_reward=-3076.96 +/- 46.10
Episode length: 191.80 +/- 44.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 192          |
|    mean_reward          | -3.08e+03    |
| time/                   |              |
|    total_timesteps      | 904000       |
| train/                  |              |
|    approx_kl            | 0.0006700712 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 4410         |
|    policy_gradient_loss | 0.000196     |
|    std                  | 1.88         |
|    value_loss           | 2.29e+05     |
------------------------------------------
Eval num_timesteps=906000, episode_reward=-3044.07 +/- 69.86
Episode length: 196.00 +/- 37.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | -3.04e+03   |
| time/                   |             |
|    total_timesteps      | 906000      |
| train/                  |             |
|    approx_kl            | 0.003700542 |
|    clip_fraction        | 0.00366     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.27e+05    |
|    n_updates            | 4420        |
|    policy_gradient_loss | -0.00234    |
|    std                  | 1.88        |
|    value_loss           | 2.54e+05    |
-----------------------------------------
Eval num_timesteps=908000, episode_reward=-3070.75 +/- 62.38
Episode length: 178.40 +/- 31.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 178          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 908000       |
| train/                  |              |
|    approx_kl            | 0.0031621247 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.000789    |
|    std                  | 1.88         |
|    value_loss           | 2.05e+05     |
------------------------------------------
Eval num_timesteps=910000, episode_reward=-3061.83 +/- 54.92
Episode length: 190.00 +/- 32.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 190          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0007678808 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.000342    |
|    std                  | 1.88         |
|    value_loss           | 2.28e+05     |
------------------------------------------
Eval num_timesteps=912000, episode_reward=-3174.27 +/- 177.11
Episode length: 240.20 +/- 97.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 240          |
|    mean_reward          | -3.17e+03    |
| time/                   |              |
|    total_timesteps      | 912000       |
| train/                  |              |
|    approx_kl            | 0.0024354304 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.89         |
|    value_loss           | 2.04e+05     |
------------------------------------------
Eval num_timesteps=914000, episode_reward=-3136.25 +/- 73.03
Episode length: 237.60 +/- 35.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -3.14e+03    |
| time/                   |              |
|    total_timesteps      | 914000       |
| train/                  |              |
|    approx_kl            | 0.0034507273 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.69e+04     |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.89         |
|    value_loss           | 1.54e+05     |
------------------------------------------
Eval num_timesteps=916000, episode_reward=-3161.93 +/- 123.74
Episode length: 246.60 +/- 72.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 916000       |
| train/                  |              |
|    approx_kl            | 0.0010938569 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+05     |
|    n_updates            | 4470         |
|    policy_gradient_loss | 0.000853     |
|    std                  | 1.89         |
|    value_loss           | 2.27e+05     |
------------------------------------------
Eval num_timesteps=918000, episode_reward=-3065.41 +/- 94.54
Episode length: 209.40 +/- 58.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 209           |
|    mean_reward          | -3.07e+03     |
| time/                   |               |
|    total_timesteps      | 918000        |
| train/                  |               |
|    approx_kl            | 0.00095968996 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.16         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 6.46e+04      |
|    n_updates            | 4480          |
|    policy_gradient_loss | -0.000877     |
|    std                  | 1.89          |
|    value_loss           | 1.29e+05      |
-------------------------------------------
Eval num_timesteps=920000, episode_reward=-3106.43 +/- 99.49
Episode length: 215.40 +/- 63.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 215          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0015562102 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.17        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.94e+04     |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.000739    |
|    std                  | 1.9          |
|    value_loss           | 1.79e+05     |
------------------------------------------
Eval num_timesteps=922000, episode_reward=-3186.71 +/- 123.26
Episode length: 343.40 +/- 100.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 343         |
|    mean_reward          | -3.19e+03   |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.005590598 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 8.89e+04    |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00247    |
|    std                  | 1.9         |
|    value_loss           | 1.78e+05    |
-----------------------------------------
Eval num_timesteps=924000, episode_reward=-3122.48 +/- 129.57
Episode length: 249.80 +/- 69.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 250           |
|    mean_reward          | -3.12e+03     |
| time/                   |               |
|    total_timesteps      | 924000        |
| train/                  |               |
|    approx_kl            | 0.00025350784 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.18         |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 9.63e+04      |
|    n_updates            | 4510          |
|    policy_gradient_loss | 2.6e-05       |
|    std                  | 1.9           |
|    value_loss           | 1.93e+05      |
-------------------------------------------
Eval num_timesteps=926000, episode_reward=-3220.78 +/- 73.33
Episode length: 306.20 +/- 46.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 306          |
|    mean_reward          | -3.22e+03    |
| time/                   |              |
|    total_timesteps      | 926000       |
| train/                  |              |
|    approx_kl            | 0.0010084847 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.9e+04      |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00036     |
|    std                  | 1.9          |
|    value_loss           | 1.78e+05     |
------------------------------------------
Eval num_timesteps=928000, episode_reward=-3153.04 +/- 157.07
Episode length: 310.20 +/- 98.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0004406563 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.86e+04     |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.000303    |
|    std                  | 1.9          |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=930000, episode_reward=-3239.79 +/- 139.05
Episode length: 370.40 +/- 119.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | -3.24e+03    |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 0.0005833241 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.19        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+05     |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.000938    |
|    std                  | 1.91         |
|    value_loss           | 2.02e+05     |
------------------------------------------
Eval num_timesteps=932000, episode_reward=-3164.30 +/- 124.34
Episode length: 353.40 +/- 39.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | -3.16e+03   |
| time/                   |             |
|    total_timesteps      | 932000      |
| train/                  |             |
|    approx_kl            | 0.002176059 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.89e+04    |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00149    |
|    std                  | 1.91        |
|    value_loss           | 1.78e+05    |
-----------------------------------------
Eval num_timesteps=934000, episode_reward=-3133.10 +/- 103.10
Episode length: 310.40 +/- 28.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | -3.13e+03    |
| time/                   |              |
|    total_timesteps      | 934000       |
| train/                  |              |
|    approx_kl            | 0.0032821381 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.19        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.87e+04     |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.000941    |
|    std                  | 1.91         |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=936000, episode_reward=-3106.48 +/- 163.44
Episode length: 311.80 +/- 67.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | -3.11e+03     |
| time/                   |               |
|    total_timesteps      | 936000        |
| train/                  |               |
|    approx_kl            | 0.00033176126 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.2          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 7.68e+04      |
|    n_updates            | 4570          |
|    policy_gradient_loss | -0.000111     |
|    std                  | 1.91          |
|    value_loss           | 1.54e+05      |
-------------------------------------------
Eval num_timesteps=938000, episode_reward=-3176.02 +/- 96.26
Episode length: 342.20 +/- 36.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | -3.18e+03   |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.006469787 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.66e+04    |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.00229    |
|    std                  | 1.92        |
|    value_loss           | 1.53e+05    |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=-3152.72 +/- 102.92
Episode length: 332.80 +/- 85.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 333       |
|    mean_reward     | -3.15e+03 |
| time/              |           |
|    total_timesteps | 940000    |
----------------------------------
Eval num_timesteps=942000, episode_reward=-3041.96 +/- 105.27
Episode length: 268.00 +/- 103.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | -3.04e+03    |
| time/                   |              |
|    total_timesteps      | 942000       |
| train/                  |              |
|    approx_kl            | 0.0012511913 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.81e+04     |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.92         |
|    value_loss           | 1.76e+05     |
------------------------------------------
Eval num_timesteps=944000, episode_reward=-3111.84 +/- 77.16
Episode length: 329.40 +/- 106.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 944000       |
| train/                  |              |
|    approx_kl            | 0.0014416941 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.83e+04     |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.000796    |
|    std                  | 1.92         |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=946000, episode_reward=-3159.79 +/- 111.57
Episode length: 321.60 +/- 60.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 946000       |
| train/                  |              |
|    approx_kl            | 0.0019853264 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.65e+04     |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.000957    |
|    std                  | 1.92         |
|    value_loss           | 1.53e+05     |
------------------------------------------
Eval num_timesteps=948000, episode_reward=-3013.85 +/- 71.82
Episode length: 238.00 +/- 56.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | -3.01e+03   |
| time/                   |             |
|    total_timesteps      | 948000      |
| train/                  |             |
|    approx_kl            | 0.006125613 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.42e+04    |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1.92        |
|    value_loss           | 1.28e+05    |
-----------------------------------------
Eval num_timesteps=950000, episode_reward=-3093.53 +/- 80.59
Episode length: 322.80 +/- 44.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | -3.09e+03   |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.004704184 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 8.83e+04    |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00247    |
|    std                  | 1.92        |
|    value_loss           | 1.77e+05    |
-----------------------------------------
Eval num_timesteps=952000, episode_reward=-2955.33 +/- 76.72
Episode length: 292.40 +/- 19.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | -2.96e+03   |
| time/                   |             |
|    total_timesteps      | 952000      |
| train/                  |             |
|    approx_kl            | 0.002880495 |
|    clip_fraction        | 0.00303     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 6.41e+04    |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.92        |
|    value_loss           | 1.28e+05    |
-----------------------------------------
Eval num_timesteps=954000, episode_reward=-3055.05 +/- 81.34
Episode length: 330.60 +/- 31.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 331          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 954000       |
| train/                  |              |
|    approx_kl            | 0.0051597874 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.77e+04     |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 1.92         |
|    value_loss           | 1.75e+05     |
------------------------------------------
Eval num_timesteps=956000, episode_reward=-3129.23 +/- 150.80
Episode length: 354.80 +/- 79.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | -3.13e+03   |
| time/                   |             |
|    total_timesteps      | 956000      |
| train/                  |             |
|    approx_kl            | 0.004074975 |
|    clip_fraction        | 0.00527     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.61e+04    |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 1.92        |
|    value_loss           | 1.52e+05    |
-----------------------------------------
Eval num_timesteps=958000, episode_reward=-2982.78 +/- 324.23
Episode length: 429.60 +/- 202.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | -2.98e+03   |
| time/                   |             |
|    total_timesteps      | 958000      |
| train/                  |             |
|    approx_kl            | 0.013825197 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.58e+04    |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00381    |
|    std                  | 1.92        |
|    value_loss           | 1.52e+05    |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=-3118.49 +/- 130.33
Episode length: 370.40 +/- 92.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | -3.12e+03    |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0020629847 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.75e+04     |
|    n_updates            | 4680         |
|    policy_gradient_loss | 0.000407     |
|    std                  | 1.93         |
|    value_loss           | 1.95e+05     |
------------------------------------------
Eval num_timesteps=962000, episode_reward=-3045.13 +/- 90.26
Episode length: 270.00 +/- 67.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 270        |
|    mean_reward          | -3.05e+03  |
| time/                   |            |
|    total_timesteps      | 962000     |
| train/                  |            |
|    approx_kl            | 0.00088055 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.22      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 8.71e+04   |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.000853  |
|    std                  | 1.93       |
|    value_loss           | 1.74e+05   |
----------------------------------------
Eval num_timesteps=964000, episode_reward=-3042.69 +/- 68.35
Episode length: 280.80 +/- 65.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | -3.04e+03   |
| time/                   |             |
|    total_timesteps      | 964000      |
| train/                  |             |
|    approx_kl            | 0.008458395 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.55e+04    |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.92        |
|    value_loss           | 1.51e+05    |
-----------------------------------------
Eval num_timesteps=966000, episode_reward=-2989.22 +/- 32.01
Episode length: 205.80 +/- 38.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 966000       |
| train/                  |              |
|    approx_kl            | 0.0048225895 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.7e+04      |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.92         |
|    value_loss           | 1.74e+05     |
------------------------------------------
Eval num_timesteps=968000, episode_reward=-2999.87 +/- 26.52
Episode length: 186.40 +/- 40.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 186          |
|    mean_reward          | -3e+03       |
| time/                   |              |
|    total_timesteps      | 968000       |
| train/                  |              |
|    approx_kl            | 0.0046938183 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+05     |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 1.92         |
|    value_loss           | 2.21e+05     |
------------------------------------------
Eval num_timesteps=970000, episode_reward=-3023.92 +/- 47.43
Episode length: 228.40 +/- 58.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 970000       |
| train/                  |              |
|    approx_kl            | 0.0023967095 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+05     |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.000336    |
|    std                  | 1.92         |
|    value_loss           | 2.15e+05     |
------------------------------------------
Eval num_timesteps=972000, episode_reward=-3023.32 +/- 8.06
Episode length: 194.60 +/- 26.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 195           |
|    mean_reward          | -3.02e+03     |
| time/                   |               |
|    total_timesteps      | 972000        |
| train/                  |               |
|    approx_kl            | 0.00057964365 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.22         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+05       |
|    n_updates            | 4740          |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.92          |
|    value_loss           | 2.2e+05       |
-------------------------------------------
Eval num_timesteps=974000, episode_reward=-3014.72 +/- 20.14
Episode length: 211.80 +/- 46.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 212           |
|    mean_reward          | -3.01e+03     |
| time/                   |               |
|    total_timesteps      | 974000        |
| train/                  |               |
|    approx_kl            | 0.00080376933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.22         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+05      |
|    n_updates            | 4750          |
|    policy_gradient_loss | -0.000362     |
|    std                  | 1.92          |
|    value_loss           | 2.44e+05      |
-------------------------------------------
Eval num_timesteps=976000, episode_reward=-3033.25 +/- 16.20
Episode length: 211.40 +/- 20.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | -3.03e+03   |
| time/                   |             |
|    total_timesteps      | 976000      |
| train/                  |             |
|    approx_kl            | 0.002427414 |
|    clip_fraction        | 0.000928    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+05    |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 1.93        |
|    value_loss           | 2.44e+05    |
-----------------------------------------
Eval num_timesteps=978000, episode_reward=-3038.39 +/- 23.48
Episode length: 265.20 +/- 45.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | -3.04e+03   |
| time/                   |             |
|    total_timesteps      | 978000      |
| train/                  |             |
|    approx_kl            | 0.004146438 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+05     |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00231    |
|    std                  | 1.93        |
|    value_loss           | 2.2e+05     |
-----------------------------------------
Eval num_timesteps=980000, episode_reward=-3055.60 +/- 131.84
Episode length: 259.80 +/- 82.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | -3.06e+03   |
| time/                   |             |
|    total_timesteps      | 980000      |
| train/                  |             |
|    approx_kl            | 0.001345176 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 9.85e+04    |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.000359   |
|    std                  | 1.93        |
|    value_loss           | 1.97e+05    |
-----------------------------------------
Eval num_timesteps=982000, episode_reward=-3006.50 +/- 30.65
Episode length: 240.80 +/- 56.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 241           |
|    mean_reward          | -3.01e+03     |
| time/                   |               |
|    total_timesteps      | 982000        |
| train/                  |               |
|    approx_kl            | 0.00025284503 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.22         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.001         |
|    loss                 | 7.49e+04      |
|    n_updates            | 4790          |
|    policy_gradient_loss | -0.000485     |
|    std                  | 1.92          |
|    value_loss           | 1.5e+05       |
-------------------------------------------
Eval num_timesteps=984000, episode_reward=-3058.24 +/- 64.84
Episode length: 225.40 +/- 88.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 225          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 984000       |
| train/                  |              |
|    approx_kl            | 0.0022594433 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+04      |
|    n_updates            | 4800         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 1.92         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=986000, episode_reward=-3054.59 +/- 75.31
Episode length: 225.60 +/- 90.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 986000       |
| train/                  |              |
|    approx_kl            | 0.0050981436 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+05     |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.93         |
|    value_loss           | 2.19e+05     |
------------------------------------------
Eval num_timesteps=988000, episode_reward=-3072.08 +/- 72.22
Episode length: 213.80 +/- 64.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 214        |
|    mean_reward          | -3.07e+03  |
| time/                   |            |
|    total_timesteps      | 988000     |
| train/                  |            |
|    approx_kl            | 0.00665975 |
|    clip_fraction        | 0.0202     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.23      |
|    explained_variance   | 1.79e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 1.09e+05   |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.00395   |
|    std                  | 1.93       |
|    value_loss           | 2.19e+05   |
----------------------------------------
Eval num_timesteps=990000, episode_reward=-3067.63 +/- 34.13
Episode length: 230.80 +/- 13.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 231           |
|    mean_reward          | -3.07e+03     |
| time/                   |               |
|    total_timesteps      | 990000        |
| train/                  |               |
|    approx_kl            | 0.00073249056 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.23         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.64e+04      |
|    n_updates            | 4830          |
|    policy_gradient_loss | 0.000303      |
|    std                  | 1.93          |
|    value_loss           | 1.73e+05      |
-------------------------------------------
Eval num_timesteps=992000, episode_reward=-3149.32 +/- 97.49
Episode length: 276.40 +/- 88.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 992000       |
| train/                  |              |
|    approx_kl            | 0.0010846527 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+05     |
|    n_updates            | 4840         |
|    policy_gradient_loss | -0.000798    |
|    std                  | 1.93         |
|    value_loss           | 2.36e+05     |
------------------------------------------
Eval num_timesteps=994000, episode_reward=-3105.00 +/- 77.72
Episode length: 252.80 +/- 35.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | -3.11e+03   |
| time/                   |             |
|    total_timesteps      | 994000      |
| train/                  |             |
|    approx_kl            | 0.004206685 |
|    clip_fraction        | 0.0042      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 8.63e+04    |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.00114    |
|    std                  | 1.93        |
|    value_loss           | 1.73e+05    |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=-3118.61 +/- 162.63
Episode length: 259.60 +/- 90.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | -3.12e+03   |
| time/                   |             |
|    total_timesteps      | 996000      |
| train/                  |             |
|    approx_kl            | 0.001716727 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.09e+05    |
|    n_updates            | 4860        |
|    policy_gradient_loss | -7.65e-05   |
|    std                  | 1.93        |
|    value_loss           | 2.18e+05    |
-----------------------------------------
Eval num_timesteps=998000, episode_reward=-3099.42 +/- 32.77
Episode length: 266.00 +/- 33.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | -3.1e+03    |
| time/                   |             |
|    total_timesteps      | 998000      |
| train/                  |             |
|    approx_kl            | 0.000984778 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 9.77e+04    |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.000553   |
|    std                  | 1.93        |
|    value_loss           | 1.96e+05    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-3009.37 +/- 77.50
Episode length: 252.80 +/- 49.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | -3.01e+03   |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.003392687 |
|    clip_fraction        | 0.00308     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 9.74e+04    |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 1.93        |
|    value_loss           | 1.95e+05    |
-----------------------------------------
Eval num_timesteps=1002000, episode_reward=-3077.05 +/- 37.40
Episode length: 255.20 +/- 38.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | -3.08e+03    |
| time/                   |              |
|    total_timesteps      | 1002000      |
| train/                  |              |
|    approx_kl            | 0.0026016915 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+05      |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000572    |
|    std                  | 1.93         |
|    value_loss           | 2.41e+05     |
------------------------------------------
Eval num_timesteps=1004000, episode_reward=-3059.84 +/- 52.50
Episode length: 281.60 +/- 62.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1004000      |
| train/                  |              |
|    approx_kl            | 0.0009012072 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.24        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+04      |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.000463    |
|    std                  | 1.94         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=1006000, episode_reward=-3074.39 +/- 66.64
Episode length: 246.60 +/- 84.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 1006000      |
| train/                  |              |
|    approx_kl            | 0.0022082748 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.55e+04     |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 1.94         |
|    value_loss           | 1.71e+05     |
------------------------------------------
Eval num_timesteps=1008000, episode_reward=-3033.73 +/- 96.64
Episode length: 255.40 +/- 77.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | -3.03e+03   |
| time/                   |             |
|    total_timesteps      | 1008000     |
| train/                  |             |
|    approx_kl            | 0.001698529 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.26       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 9.64e+04    |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.000306   |
|    std                  | 1.94        |
|    value_loss           | 1.93e+05    |
-----------------------------------------
Eval num_timesteps=1010000, episode_reward=-2992.99 +/- 96.58
Episode length: 238.00 +/- 85.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | -2.99e+03   |
| time/                   |             |
|    total_timesteps      | 1010000     |
| train/                  |             |
|    approx_kl            | 0.005933129 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.26       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.4e+04     |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00281    |
|    std                  | 1.95        |
|    value_loss           | 1.48e+05    |
-----------------------------------------
Eval num_timesteps=1012000, episode_reward=-3122.76 +/- 97.44
Episode length: 343.40 +/- 107.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 343         |
|    mean_reward          | -3.12e+03   |
| time/                   |             |
|    total_timesteps      | 1012000     |
| train/                  |             |
|    approx_kl            | 0.010894254 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8e+04       |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00435    |
|    std                  | 1.95        |
|    value_loss           | 1.6e+05     |
-----------------------------------------
Eval num_timesteps=1014000, episode_reward=-3096.59 +/- 169.52
Episode length: 397.20 +/- 34.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1014000      |
| train/                  |              |
|    approx_kl            | 0.0063811317 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.37e+04     |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 1.95         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=1016000, episode_reward=-3039.54 +/- 126.17
Episode length: 319.80 +/- 79.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 320         |
|    mean_reward          | -3.04e+03   |
| time/                   |             |
|    total_timesteps      | 1016000     |
| train/                  |             |
|    approx_kl            | 0.005407973 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.38e+04    |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 1.95        |
|    value_loss           | 1.48e+05    |
-----------------------------------------
Eval num_timesteps=1018000, episode_reward=-3140.51 +/- 115.01
Episode length: 361.40 +/- 89.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 361          |
|    mean_reward          | -3.14e+03    |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 0.0036051557 |
|    clip_fraction        | 0.0626       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.17e+04     |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 1.96         |
|    value_loss           | 1.23e+05     |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=-3143.42 +/- 172.24
Episode length: 399.00 +/- 125.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | -3.14e+03   |
| time/                   |             |
|    total_timesteps      | 1020000     |
| train/                  |             |
|    approx_kl            | 0.008502882 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 5.07e+04    |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00183    |
|    std                  | 1.97        |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=1022000, episode_reward=-3033.64 +/- 150.24
Episode length: 398.00 +/- 109.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | -3.03e+03   |
| time/                   |             |
|    total_timesteps      | 1022000     |
| train/                  |             |
|    approx_kl            | 0.011767896 |
|    clip_fraction        | 0.055       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.32       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.21e+04    |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.00433    |
|    std                  | 1.98        |
|    value_loss           | 1.24e+05    |
-----------------------------------------
Eval num_timesteps=1024000, episode_reward=-3022.21 +/- 146.57
Episode length: 419.20 +/- 46.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 419       |
|    mean_reward     | -3.02e+03 |
| time/              |           |
|    total_timesteps | 1024000   |
----------------------------------
Eval num_timesteps=1026000, episode_reward=-2923.05 +/- 109.10
Episode length: 346.80 +/- 77.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 1026000      |
| train/                  |              |
|    approx_kl            | 0.0017254946 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.34        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.000152    |
|    std                  | 1.99         |
|    value_loss           | 2.13e+05     |
------------------------------------------
Eval num_timesteps=1028000, episode_reward=-3052.52 +/- 97.57
Episode length: 248.40 +/- 56.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | -3.05e+03   |
| time/                   |             |
|    total_timesteps      | 1028000     |
| train/                  |             |
|    approx_kl            | 0.010722567 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 5.06e+04    |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 2           |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=1030000, episode_reward=-3190.55 +/- 173.13
Episode length: 340.80 +/- 147.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | -3.19e+03    |
| time/                   |              |
|    total_timesteps      | 1030000      |
| train/                  |              |
|    approx_kl            | 0.0072389543 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.38        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.36e+04     |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.0034      |
|    std                  | 2.01         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=1032000, episode_reward=-3141.79 +/- 98.15
Episode length: 337.20 +/- 117.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | -3.14e+03    |
| time/                   |              |
|    total_timesteps      | 1032000      |
| train/                  |              |
|    approx_kl            | 0.0020248024 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.4         |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.31e+04     |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.000333    |
|    std                  | 2.02         |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=1034000, episode_reward=-3049.50 +/- 90.53
Episode length: 212.00 +/- 56.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 212          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1034000      |
| train/                  |              |
|    approx_kl            | 0.0008714448 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.41e+04     |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.000769    |
|    std                  | 2.03         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=1036000, episode_reward=-3152.68 +/- 141.32
Episode length: 297.40 +/- 119.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 1036000      |
| train/                  |              |
|    approx_kl            | 0.0046754903 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.43        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.48e+04     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2.03         |
|    value_loss           | 1.7e+05      |
------------------------------------------
Eval num_timesteps=1038000, episode_reward=-3154.31 +/- 100.15
Episode length: 319.00 +/- 104.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 319          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 1038000      |
| train/                  |              |
|    approx_kl            | 0.0031562354 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.62e+04     |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 2.04         |
|    value_loss           | 1.92e+05     |
------------------------------------------
Eval num_timesteps=1040000, episode_reward=-3113.35 +/- 119.90
Episode length: 261.80 +/- 76.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 1040000      |
| train/                  |              |
|    approx_kl            | 0.0013807346 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.48e+04     |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.000798    |
|    std                  | 2.04         |
|    value_loss           | 1.7e+05      |
------------------------------------------
Eval num_timesteps=1042000, episode_reward=-3257.77 +/- 177.41
Episode length: 333.00 +/- 96.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 333          |
|    mean_reward          | -3.26e+03    |
| time/                   |              |
|    total_timesteps      | 1042000      |
| train/                  |              |
|    approx_kl            | 0.0008806633 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+04     |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.000374    |
|    std                  | 2.04         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=1044000, episode_reward=-3100.92 +/- 283.87
Episode length: 273.60 +/- 132.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1044000      |
| train/                  |              |
|    approx_kl            | 0.0016720104 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.42e+04     |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000848    |
|    std                  | 2.04         |
|    value_loss           | 1.68e+05     |
------------------------------------------
Eval num_timesteps=1046000, episode_reward=-3212.32 +/- 110.18
Episode length: 277.40 +/- 68.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | -3.21e+03    |
| time/                   |              |
|    total_timesteps      | 1046000      |
| train/                  |              |
|    approx_kl            | 0.0050384514 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.44        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+04     |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 2.04         |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=1048000, episode_reward=-3240.07 +/- 200.64
Episode length: 286.80 +/- 93.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | -3.24e+03   |
| time/                   |             |
|    total_timesteps      | 1048000     |
| train/                  |             |
|    approx_kl            | 0.003990917 |
|    clip_fraction        | 0.00601     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.45       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 7.31e+04    |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00106    |
|    std                  | 2.05        |
|    value_loss           | 1.46e+05    |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=-3228.89 +/- 216.56
Episode length: 315.80 +/- 98.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | -3.23e+03    |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0015765362 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.46        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.57e+04     |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.000613    |
|    std                  | 2.05         |
|    value_loss           | 1.91e+05     |
------------------------------------------
Eval num_timesteps=1052000, episode_reward=-3108.79 +/- 122.42
Episode length: 249.20 +/- 62.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 1052000      |
| train/                  |              |
|    approx_kl            | 0.0033152155 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.47        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.29e+04     |
|    n_updates            | 5130         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 2.06         |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=1054000, episode_reward=-3083.57 +/- 147.50
Episode length: 234.20 +/- 73.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | -3.08e+03   |
| time/                   |             |
|    total_timesteps      | 1054000     |
| train/                  |             |
|    approx_kl            | 0.004712215 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.48       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 7.27e+04    |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 2.06        |
|    value_loss           | 1.45e+05    |
-----------------------------------------
Eval num_timesteps=1056000, episode_reward=-3139.76 +/- 59.19
Episode length: 285.20 +/- 28.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | -3.14e+03    |
| time/                   |              |
|    total_timesteps      | 1056000      |
| train/                  |              |
|    approx_kl            | 0.0020269917 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+05     |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.000716    |
|    std                  | 2.07         |
|    value_loss           | 2.13e+05     |
------------------------------------------
Eval num_timesteps=1058000, episode_reward=-3127.93 +/- 87.54
Episode length: 232.20 +/- 62.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | -3.13e+03    |
| time/                   |              |
|    total_timesteps      | 1058000      |
| train/                  |              |
|    approx_kl            | 0.0015187692 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5         |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+04     |
|    n_updates            | 5160         |
|    policy_gradient_loss | -0.000762    |
|    std                  | 2.07         |
|    value_loss           | 1.91e+05     |
------------------------------------------
Eval num_timesteps=1060000, episode_reward=-3064.47 +/- 23.94
Episode length: 197.40 +/- 22.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 197          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1060000      |
| train/                  |              |
|    approx_kl            | 0.0014369223 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+05     |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.000959    |
|    std                  | 2.08         |
|    value_loss           | 2.13e+05     |
------------------------------------------
Eval num_timesteps=1062000, episode_reward=-3128.68 +/- 49.57
Episode length: 233.00 +/- 37.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | -3.13e+03   |
| time/                   |             |
|    total_timesteps      | 1062000     |
| train/                  |             |
|    approx_kl            | 0.001688681 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+05    |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00135    |
|    std                  | 2.08        |
|    value_loss           | 2.35e+05    |
-----------------------------------------
Eval num_timesteps=1064000, episode_reward=-3161.53 +/- 100.00
Episode length: 269.80 +/- 62.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 1064000      |
| train/                  |              |
|    approx_kl            | 0.0038026553 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+05     |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.08         |
|    value_loss           | 2.12e+05     |
------------------------------------------
Eval num_timesteps=1066000, episode_reward=-3137.02 +/- 156.57
Episode length: 273.40 +/- 62.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | -3.14e+03   |
| time/                   |             |
|    total_timesteps      | 1066000     |
| train/                  |             |
|    approx_kl            | 0.001093923 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+05    |
|    n_updates            | 5200        |
|    policy_gradient_loss | -5.89e-05   |
|    std                  | 2.08        |
|    value_loss           | 2.12e+05    |
-----------------------------------------
Eval num_timesteps=1068000, episode_reward=-3019.69 +/- 42.94
Episode length: 204.20 +/- 72.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 204          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 1068000      |
| train/                  |              |
|    approx_kl            | 0.0023520999 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.45e+04     |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 2.08         |
|    value_loss           | 1.89e+05     |
------------------------------------------
Eval num_timesteps=1070000, episode_reward=-3050.26 +/- 52.81
Episode length: 199.40 +/- 33.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1070000      |
| train/                  |              |
|    approx_kl            | 0.0020701275 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.51        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.42e+04     |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.000321    |
|    std                  | 2.08         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=1072000, episode_reward=-3169.93 +/- 109.12
Episode length: 245.00 +/- 61.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 245           |
|    mean_reward          | -3.17e+03     |
| time/                   |               |
|    total_timesteps      | 1072000       |
| train/                  |               |
|    approx_kl            | 0.00014028893 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.52         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+05      |
|    n_updates            | 5230          |
|    policy_gradient_loss | -1.99e-05     |
|    std                  | 2.08          |
|    value_loss           | 2.34e+05      |
-------------------------------------------
Eval num_timesteps=1074000, episode_reward=-3100.17 +/- 85.52
Episode length: 206.80 +/- 45.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1074000      |
| train/                  |              |
|    approx_kl            | 0.0027343873 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+05     |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 2.08         |
|    value_loss           | 2.33e+05     |
------------------------------------------
Eval num_timesteps=1076000, episode_reward=-3124.66 +/- 94.69
Episode length: 230.80 +/- 54.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | -3.12e+03    |
| time/                   |              |
|    total_timesteps      | 1076000      |
| train/                  |              |
|    approx_kl            | 0.0016280222 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.000869    |
|    std                  | 2.08         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=1078000, episode_reward=-3051.56 +/- 33.80
Episode length: 173.40 +/- 26.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 173          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1078000      |
| train/                  |              |
|    approx_kl            | 0.0056465706 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.52        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 2.08         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=-3164.24 +/- 109.43
Episode length: 261.20 +/- 58.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 261           |
|    mean_reward          | -3.16e+03     |
| time/                   |               |
|    total_timesteps      | 1080000       |
| train/                  |               |
|    approx_kl            | 0.00093882653 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.52         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+05      |
|    n_updates            | 5270          |
|    policy_gradient_loss | -0.000372     |
|    std                  | 2.08          |
|    value_loss           | 2.32e+05      |
-------------------------------------------
Eval num_timesteps=1082000, episode_reward=-3174.40 +/- 79.74
Episode length: 275.60 +/- 48.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | -3.17e+03     |
| time/                   |               |
|    total_timesteps      | 1082000       |
| train/                  |               |
|    approx_kl            | 0.00012682594 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.52         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+05      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000331     |
|    std                  | 2.09          |
|    value_loss           | 2.1e+05       |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=-3070.15 +/- 17.10
Episode length: 193.60 +/- 26.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 194          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 1084000      |
| train/                  |              |
|    approx_kl            | 0.0020057515 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 2.09         |
|    value_loss           | 2.1e+05      |
------------------------------------------
Eval num_timesteps=1086000, episode_reward=-3092.51 +/- 123.02
Episode length: 215.00 +/- 85.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 215          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 1086000      |
| train/                  |              |
|    approx_kl            | 0.0072674984 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.24e+04     |
|    n_updates            | 5300         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 2.09         |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=1088000, episode_reward=-3195.13 +/- 72.42
Episode length: 260.80 +/- 34.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | -3.2e+03    |
| time/                   |             |
|    total_timesteps      | 1088000     |
| train/                  |             |
|    approx_kl            | 0.002964038 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+05    |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.000708   |
|    std                  | 2.09        |
|    value_loss           | 2.01e+05    |
-----------------------------------------
Eval num_timesteps=1090000, episode_reward=-3107.82 +/- 30.39
Episode length: 222.80 +/- 34.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | -3.11e+03   |
| time/                   |             |
|    total_timesteps      | 1090000     |
| train/                  |             |
|    approx_kl            | 0.007600506 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.36e+04    |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00279    |
|    std                  | 2.09        |
|    value_loss           | 1.87e+05    |
-----------------------------------------
Eval num_timesteps=1092000, episode_reward=-3132.03 +/- 110.71
Episode length: 250.20 +/- 45.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | -3.13e+03   |
| time/                   |             |
|    total_timesteps      | 1092000     |
| train/                  |             |
|    approx_kl            | 0.003519835 |
|    clip_fraction        | 0.00552     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.15e+05    |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.000521   |
|    std                  | 2.09        |
|    value_loss           | 2.3e+05     |
-----------------------------------------
Eval num_timesteps=1094000, episode_reward=-3072.64 +/- 62.35
Episode length: 182.00 +/- 55.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 182           |
|    mean_reward          | -3.07e+03     |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 0.00031259254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.54         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 5340          |
|    policy_gradient_loss | -0.000185     |
|    std                  | 2.09          |
|    value_loss           | 2.09e+05      |
-------------------------------------------
Eval num_timesteps=1096000, episode_reward=-3096.86 +/- 47.42
Episode length: 220.40 +/- 29.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1096000      |
| train/                  |              |
|    approx_kl            | 0.0009056559 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+05     |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.000628    |
|    std                  | 2.1          |
|    value_loss           | 2.3e+05      |
------------------------------------------
Eval num_timesteps=1098000, episode_reward=-3083.59 +/- 31.67
Episode length: 242.00 +/- 29.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 242           |
|    mean_reward          | -3.08e+03     |
| time/                   |               |
|    total_timesteps      | 1098000       |
| train/                  |               |
|    approx_kl            | 0.00065675547 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.55         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 5360          |
|    policy_gradient_loss | -0.000421     |
|    std                  | 2.1           |
|    value_loss           | 2.08e+05      |
-------------------------------------------
Eval num_timesteps=1100000, episode_reward=-3045.82 +/- 35.66
Episode length: 196.40 +/- 36.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 196          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0011004615 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.55        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.000372    |
|    std                  | 2.1          |
|    value_loss           | 2.09e+05     |
------------------------------------------
Eval num_timesteps=1102000, episode_reward=-3095.77 +/- 23.85
Episode length: 219.00 +/- 15.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1102000      |
| train/                  |              |
|    approx_kl            | 0.0028223004 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.56        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.000889    |
|    std                  | 2.1          |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=1104000, episode_reward=-3057.43 +/- 26.71
Episode length: 219.80 +/- 31.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 220           |
|    mean_reward          | -3.06e+03     |
| time/                   |               |
|    total_timesteps      | 1104000       |
| train/                  |               |
|    approx_kl            | 0.00056011206 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 5390          |
|    policy_gradient_loss | 1.95e-05      |
|    std                  | 2.1           |
|    value_loss           | 2.08e+05      |
-------------------------------------------
Eval num_timesteps=1106000, episode_reward=-3066.01 +/- 38.76
Episode length: 220.60 +/- 22.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 1106000      |
| train/                  |              |
|    approx_kl            | 0.0018007256 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.89e+04     |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.11         |
|    value_loss           | 1.98e+05     |
------------------------------------------
Eval num_timesteps=1108000, episode_reward=-3061.65 +/- 32.71
Episode length: 248.80 +/- 33.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | -3.06e+03   |
| time/                   |             |
|    total_timesteps      | 1108000     |
| train/                  |             |
|    approx_kl            | 0.001040724 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+05    |
|    n_updates            | 5410        |
|    policy_gradient_loss | 0.0001      |
|    std                  | 2.1         |
|    value_loss           | 2.07e+05    |
-----------------------------------------
Eval num_timesteps=1110000, episode_reward=-3048.57 +/- 18.15
Episode length: 232.20 +/- 22.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 232       |
|    mean_reward     | -3.05e+03 |
| time/              |           |
|    total_timesteps | 1110000   |
----------------------------------
Eval num_timesteps=1112000, episode_reward=-3025.36 +/- 36.68
Episode length: 213.60 +/- 48.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 214           |
|    mean_reward          | -3.03e+03     |
| time/                   |               |
|    total_timesteps      | 1112000       |
| train/                  |               |
|    approx_kl            | 0.00080633955 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.56         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+05      |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000588     |
|    std                  | 2.1           |
|    value_loss           | 2.07e+05      |
-------------------------------------------
Eval num_timesteps=1114000, episode_reward=-3046.67 +/- 33.89
Episode length: 217.00 +/- 34.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 217         |
|    mean_reward          | -3.05e+03   |
| time/                   |             |
|    total_timesteps      | 1114000     |
| train/                  |             |
|    approx_kl            | 0.001719262 |
|    clip_fraction        | 0.000293    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.14e+05    |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 2.11        |
|    value_loss           | 2.28e+05    |
-----------------------------------------
Eval num_timesteps=1116000, episode_reward=-3014.41 +/- 31.59
Episode length: 187.40 +/- 31.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | -3.01e+03   |
| time/                   |             |
|    total_timesteps      | 1116000     |
| train/                  |             |
|    approx_kl            | 0.000668928 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.14e+05    |
|    n_updates            | 5440        |
|    policy_gradient_loss | 2.19e-05    |
|    std                  | 2.11        |
|    value_loss           | 2.27e+05    |
-----------------------------------------
Eval num_timesteps=1118000, episode_reward=-3027.44 +/- 44.34
Episode length: 167.60 +/- 54.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 168           |
|    mean_reward          | -3.03e+03     |
| time/                   |               |
|    total_timesteps      | 1118000       |
| train/                  |               |
|    approx_kl            | 0.00046480872 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.57         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+05      |
|    n_updates            | 5450          |
|    policy_gradient_loss | -0.000529     |
|    std                  | 2.11          |
|    value_loss           | 2.34e+05      |
-------------------------------------------
Eval num_timesteps=1120000, episode_reward=-3058.13 +/- 16.94
Episode length: 210.60 +/- 17.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 211          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1120000      |
| train/                  |              |
|    approx_kl            | 0.0010371011 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+05     |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.000284    |
|    std                  | 2.11         |
|    value_loss           | 2.27e+05     |
------------------------------------------
Eval num_timesteps=1122000, episode_reward=-3012.58 +/- 39.69
Episode length: 182.20 +/- 49.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 182           |
|    mean_reward          | -3.01e+03     |
| time/                   |               |
|    total_timesteps      | 1122000       |
| train/                  |               |
|    approx_kl            | 0.00071468705 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.57         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 5470          |
|    policy_gradient_loss | -0.000286     |
|    std                  | 2.11          |
|    value_loss           | 2.27e+05      |
-------------------------------------------
Eval num_timesteps=1124000, episode_reward=-3027.07 +/- 25.72
Episode length: 191.20 +/- 34.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 191          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1124000      |
| train/                  |              |
|    approx_kl            | 0.0002650373 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.00018     |
|    std                  | 2.11         |
|    value_loss           | 2.48e+05     |
------------------------------------------
Eval num_timesteps=1126000, episode_reward=-3049.66 +/- 32.25
Episode length: 222.80 +/- 43.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | -3.05e+03   |
| time/                   |             |
|    total_timesteps      | 1126000     |
| train/                  |             |
|    approx_kl            | 0.004152609 |
|    clip_fraction        | 0.00405     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+05    |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 2.11        |
|    value_loss           | 2.27e+05    |
-----------------------------------------
Eval num_timesteps=1128000, episode_reward=-3047.15 +/- 52.35
Episode length: 199.40 +/- 38.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1128000      |
| train/                  |              |
|    approx_kl            | 0.0055423137 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+05     |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 2.11         |
|    value_loss           | 2.26e+05     |
------------------------------------------
Eval num_timesteps=1130000, episode_reward=-3057.53 +/- 37.73
Episode length: 218.80 +/- 23.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1130000      |
| train/                  |              |
|    approx_kl            | 0.0006226316 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+05     |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00014     |
|    std                  | 2.11         |
|    value_loss           | 2.47e+05     |
------------------------------------------
Eval num_timesteps=1132000, episode_reward=-3072.98 +/- 18.53
Episode length: 221.80 +/- 21.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 222          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 1132000      |
| train/                  |              |
|    approx_kl            | 0.0006203532 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.000516    |
|    std                  | 2.11         |
|    value_loss           | 2.04e+05     |
------------------------------------------
Eval num_timesteps=1134000, episode_reward=-3031.49 +/- 33.53
Episode length: 211.00 +/- 54.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 211          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1134000      |
| train/                  |              |
|    approx_kl            | 0.0017858224 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+05     |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.000964    |
|    std                  | 2.11         |
|    value_loss           | 2.25e+05     |
------------------------------------------
Eval num_timesteps=1136000, episode_reward=-3035.50 +/- 42.06
Episode length: 205.60 +/- 39.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -3.04e+03    |
| time/                   |              |
|    total_timesteps      | 1136000      |
| train/                  |              |
|    approx_kl            | 0.0012330769 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+05     |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.000365    |
|    std                  | 2.11         |
|    value_loss           | 2.25e+05     |
------------------------------------------
Eval num_timesteps=1138000, episode_reward=-3099.30 +/- 48.82
Episode length: 261.80 +/- 30.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1138000      |
| train/                  |              |
|    approx_kl            | 0.0027218068 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+05     |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 2.11         |
|    value_loss           | 2.24e+05     |
------------------------------------------
Eval num_timesteps=1140000, episode_reward=-3031.58 +/- 43.10
Episode length: 217.00 +/- 49.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0010687385 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+05     |
|    n_updates            | 5560         |
|    policy_gradient_loss | -0.000727    |
|    std                  | 2.11         |
|    value_loss           | 2.25e+05     |
------------------------------------------
Eval num_timesteps=1142000, episode_reward=-3076.32 +/- 20.02
Episode length: 232.40 +/- 11.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | -3.08e+03    |
| time/                   |              |
|    total_timesteps      | 1142000      |
| train/                  |              |
|    approx_kl            | 0.0013500392 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+05     |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.000855    |
|    std                  | 2.11         |
|    value_loss           | 2.24e+05     |
------------------------------------------
Eval num_timesteps=1144000, episode_reward=-3058.39 +/- 58.38
Episode length: 218.60 +/- 47.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 219           |
|    mean_reward          | -3.06e+03     |
| time/                   |               |
|    total_timesteps      | 1144000       |
| train/                  |               |
|    approx_kl            | 0.00093013514 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.58         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+05      |
|    n_updates            | 5580          |
|    policy_gradient_loss | -0.000508     |
|    std                  | 2.11          |
|    value_loss           | 2.24e+05      |
-------------------------------------------
Eval num_timesteps=1146000, episode_reward=-3190.80 +/- 135.75
Episode length: 286.40 +/- 99.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | -3.19e+03   |
| time/                   |             |
|    total_timesteps      | 1146000     |
| train/                  |             |
|    approx_kl            | 0.000607637 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+05    |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.000507   |
|    std                  | 2.11        |
|    value_loss           | 2.24e+05    |
-----------------------------------------
Eval num_timesteps=1148000, episode_reward=-3063.84 +/- 36.22
Episode length: 229.80 +/- 48.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 230           |
|    mean_reward          | -3.06e+03     |
| time/                   |               |
|    total_timesteps      | 1148000       |
| train/                  |               |
|    approx_kl            | 0.00017924834 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.59         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+05      |
|    n_updates            | 5600          |
|    policy_gradient_loss | 6.03e-05      |
|    std                  | 2.12          |
|    value_loss           | 2.02e+05      |
-------------------------------------------
Eval num_timesteps=1150000, episode_reward=-3033.26 +/- 30.18
Episode length: 198.60 +/- 26.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1150000      |
| train/                  |              |
|    approx_kl            | 0.0063223424 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.01e+04     |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 2.12         |
|    value_loss           | 1.6e+05      |
------------------------------------------
Eval num_timesteps=1152000, episode_reward=-3020.80 +/- 58.75
Episode length: 254.60 +/- 97.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | -3.02e+03   |
| time/                   |             |
|    total_timesteps      | 1152000     |
| train/                  |             |
|    approx_kl            | 0.002274707 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+05    |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.000596   |
|    std                  | 2.12        |
|    value_loss           | 2.23e+05    |
-----------------------------------------
Eval num_timesteps=1154000, episode_reward=-3140.80 +/- 73.99
Episode length: 294.80 +/- 55.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | -3.14e+03    |
| time/                   |              |
|    total_timesteps      | 1154000      |
| train/                  |              |
|    approx_kl            | 0.0015804898 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.03e+04     |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.000978    |
|    std                  | 2.12         |
|    value_loss           | 1.81e+05     |
------------------------------------------
Eval num_timesteps=1156000, episode_reward=-3027.70 +/- 54.73
Episode length: 254.40 +/- 94.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 254           |
|    mean_reward          | -3.03e+03     |
| time/                   |               |
|    total_timesteps      | 1156000       |
| train/                  |               |
|    approx_kl            | 0.00092924875 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.59         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.05e+04      |
|    n_updates            | 5640          |
|    policy_gradient_loss | -0.000924     |
|    std                  | 2.11          |
|    value_loss           | 1.81e+05      |
-------------------------------------------
Eval num_timesteps=1158000, episode_reward=-3113.98 +/- 122.82
Episode length: 300.40 +/- 74.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 1158000      |
| train/                  |              |
|    approx_kl            | 0.0050662877 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.93e+04     |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 2.11         |
|    value_loss           | 1.39e+05     |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=-3033.79 +/- 95.08
Episode length: 270.20 +/- 85.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1160000      |
| train/                  |              |
|    approx_kl            | 0.0026164337 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.97e+04     |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.000181    |
|    std                  | 2.12         |
|    value_loss           | 1.59e+05     |
------------------------------------------
Eval num_timesteps=1162000, episode_reward=-3190.17 +/- 135.75
Episode length: 324.00 +/- 65.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | -3.19e+03   |
| time/                   |             |
|    total_timesteps      | 1162000     |
| train/                  |             |
|    approx_kl            | 0.004986425 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.6        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.9e+04     |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00227    |
|    std                  | 2.12        |
|    value_loss           | 1.38e+05    |
-----------------------------------------
Eval num_timesteps=1164000, episode_reward=-3176.09 +/- 123.49
Episode length: 323.60 +/- 94.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 324          |
|    mean_reward          | -3.18e+03    |
| time/                   |              |
|    total_timesteps      | 1164000      |
| train/                  |              |
|    approx_kl            | 0.0045743575 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.6         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.92e+04     |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 2.11         |
|    value_loss           | 1.38e+05     |
------------------------------------------
Eval num_timesteps=1166000, episode_reward=-3163.00 +/- 71.61
Episode length: 374.60 +/- 115.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 375         |
|    mean_reward          | -3.16e+03   |
| time/                   |             |
|    total_timesteps      | 1166000     |
| train/                  |             |
|    approx_kl            | 0.005218113 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.9e+04     |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 2.1         |
|    value_loss           | 1.38e+05    |
-----------------------------------------
Eval num_timesteps=1168000, episode_reward=-3276.46 +/- 282.19
Episode length: 352.40 +/- 151.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | -3.28e+03    |
| time/                   |              |
|    total_timesteps      | 1168000      |
| train/                  |              |
|    approx_kl            | 0.0050942106 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.57        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 6.9e+04      |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00458     |
|    std                  | 2.09         |
|    value_loss           | 1.38e+05     |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=-3151.95 +/- 176.55
Episode length: 350.00 +/- 45.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 350         |
|    mean_reward          | -3.15e+03   |
| time/                   |             |
|    total_timesteps      | 1170000     |
| train/                  |             |
|    approx_kl            | 0.008993103 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.55       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 8.97e+04    |
|    n_updates            | 5710        |
|    policy_gradient_loss | 6.06e-05    |
|    std                  | 2.09        |
|    value_loss           | 1.79e+05    |
-----------------------------------------
Eval num_timesteps=1172000, episode_reward=-3019.59 +/- 45.36
Episode length: 225.40 +/- 60.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | -3.02e+03   |
| time/                   |             |
|    total_timesteps      | 1172000     |
| train/                  |             |
|    approx_kl            | 0.013817555 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.61       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.84e+04    |
|    n_updates            | 5720        |
|    policy_gradient_loss | -0.00561    |
|    std                  | 2.16        |
|    value_loss           | 1.17e+05    |
-----------------------------------------
Eval num_timesteps=1174000, episode_reward=-3089.67 +/- 94.62
Episode length: 285.60 +/- 97.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | -3.09e+03   |
| time/                   |             |
|    total_timesteps      | 1174000     |
| train/                  |             |
|    approx_kl            | 0.013754971 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.73       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 7.95e+04    |
|    n_updates            | 5730        |
|    policy_gradient_loss | 0.00963     |
|    std                  | 2.2         |
|    value_loss           | 1.59e+05    |
-----------------------------------------
Eval num_timesteps=1176000, episode_reward=-3211.18 +/- 110.24
Episode length: 357.20 +/- 39.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 357         |
|    mean_reward          | -3.21e+03   |
| time/                   |             |
|    total_timesteps      | 1176000     |
| train/                  |             |
|    approx_kl            | 0.001690631 |
|    clip_fraction        | 0.0501      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.77       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 6.88e+04    |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.000183   |
|    std                  | 2.21        |
|    value_loss           | 1.38e+05    |
-----------------------------------------
Eval num_timesteps=1178000, episode_reward=-3101.35 +/- 165.97
Episode length: 302.60 +/- 99.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1178000      |
| train/                  |              |
|    approx_kl            | 0.0055496423 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 6.87e+04     |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 2.21         |
|    value_loss           | 1.37e+05     |
------------------------------------------
Eval num_timesteps=1180000, episode_reward=-3182.76 +/- 180.54
Episode length: 316.20 +/- 96.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | -3.18e+03    |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 0.0097147785 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.9e+04      |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 2.21         |
|    value_loss           | 1.58e+05     |
------------------------------------------
Eval num_timesteps=1182000, episode_reward=-3086.22 +/- 189.78
Episode length: 292.60 +/- 72.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 293         |
|    mean_reward          | -3.09e+03   |
| time/                   |             |
|    total_timesteps      | 1182000     |
| train/                  |             |
|    approx_kl            | 0.010485062 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.83e+04    |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00402    |
|    std                  | 2.21        |
|    value_loss           | 1.37e+05    |
-----------------------------------------
Eval num_timesteps=1184000, episode_reward=-3315.51 +/- 100.12
Episode length: 350.40 +/- 43.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | -3.32e+03    |
| time/                   |              |
|    total_timesteps      | 1184000      |
| train/                  |              |
|    approx_kl            | 0.0018298067 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.76e+04     |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000609    |
|    std                  | 2.21         |
|    value_loss           | 9.52e+04     |
------------------------------------------
Eval num_timesteps=1186000, episode_reward=-3312.19 +/- 213.07
Episode length: 375.00 +/- 82.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | -3.31e+03    |
| time/                   |              |
|    total_timesteps      | 1186000      |
| train/                  |              |
|    approx_kl            | 0.0025260712 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.9e+04      |
|    n_updates            | 5790         |
|    policy_gradient_loss | 0.000146     |
|    std                  | 2.22         |
|    value_loss           | 1.78e+05     |
------------------------------------------
Eval num_timesteps=1188000, episode_reward=-3190.79 +/- 145.85
Episode length: 268.00 +/- 82.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | -3.19e+03   |
| time/                   |             |
|    total_timesteps      | 1188000     |
| train/                  |             |
|    approx_kl            | 0.009025406 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.79       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.82e+04    |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00575    |
|    std                  | 2.23        |
|    value_loss           | 1.16e+05    |
-----------------------------------------
Eval num_timesteps=1190000, episode_reward=-3258.95 +/- 109.10
Episode length: 328.40 +/- 67.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | -3.26e+03   |
| time/                   |             |
|    total_timesteps      | 1190000     |
| train/                  |             |
|    approx_kl            | 0.005706582 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.84       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.53e+04    |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.000725   |
|    std                  | 2.25        |
|    value_loss           | 1.51e+05    |
-----------------------------------------
Eval num_timesteps=1192000, episode_reward=-3159.59 +/- 163.65
Episode length: 287.80 +/- 77.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -3.16e+03    |
| time/                   |              |
|    total_timesteps      | 1192000      |
| train/                  |              |
|    approx_kl            | 0.0023943454 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.86        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.92e+04     |
|    n_updates            | 5820         |
|    policy_gradient_loss | -4.26e-05    |
|    std                  | 2.25         |
|    value_loss           | 1.58e+05     |
------------------------------------------
Eval num_timesteps=1194000, episode_reward=-3099.18 +/- 175.02
Episode length: 259.00 +/- 70.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | -3.1e+03    |
| time/                   |             |
|    total_timesteps      | 1194000     |
| train/                  |             |
|    approx_kl            | 0.012814011 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.87       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.88e+04    |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.00383    |
|    std                  | 2.26        |
|    value_loss           | 1.58e+05    |
-----------------------------------------
Eval num_timesteps=1196000, episode_reward=-3077.19 +/- 111.86
Episode length: 235.40 +/- 48.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 235       |
|    mean_reward     | -3.08e+03 |
| time/              |           |
|    total_timesteps | 1196000   |
----------------------------------
Eval num_timesteps=1198000, episode_reward=-3062.55 +/- 97.84
Episode length: 287.20 +/- 36.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 287           |
|    mean_reward          | -3.06e+03     |
| time/                   |               |
|    total_timesteps      | 1198000       |
| train/                  |               |
|    approx_kl            | 0.00050121883 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.87         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 8.94e+04      |
|    n_updates            | 5840          |
|    policy_gradient_loss | -0.000772     |
|    std                  | 2.26          |
|    value_loss           | 1.79e+05      |
-------------------------------------------
Eval num_timesteps=1200000, episode_reward=-3104.85 +/- 60.96
Episode length: 250.80 +/- 69.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | -3.1e+03     |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0042814473 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.85e+04     |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.003       |
|    std                  | 2.26         |
|    value_loss           | 1.37e+05     |
------------------------------------------
Eval num_timesteps=1202000, episode_reward=-3136.84 +/- 130.76
Episode length: 244.60 +/- 126.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | -3.14e+03   |
| time/                   |             |
|    total_timesteps      | 1202000     |
| train/                  |             |
|    approx_kl            | 0.004224675 |
|    clip_fraction        | 0.00908     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.87       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.88e+04    |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.000562   |
|    std                  | 2.26        |
|    value_loss           | 1.78e+05    |
-----------------------------------------
Eval num_timesteps=1204000, episode_reward=-3048.49 +/- 23.28
Episode length: 227.20 +/- 60.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1204000      |
| train/                  |              |
|    approx_kl            | 0.0010163623 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.96e+04     |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.000142    |
|    std                  | 2.26         |
|    value_loss           | 1.99e+05     |
------------------------------------------
Eval num_timesteps=1206000, episode_reward=-3110.95 +/- 79.18
Episode length: 257.00 +/- 51.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | -3.11e+03    |
| time/                   |              |
|    total_timesteps      | 1206000      |
| train/                  |              |
|    approx_kl            | 0.0029170301 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.86e+04     |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 2.26         |
|    value_loss           | 1.57e+05     |
------------------------------------------
Eval num_timesteps=1208000, episode_reward=-3051.24 +/- 79.32
Episode length: 268.40 +/- 26.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | -3.05e+03   |
| time/                   |             |
|    total_timesteps      | 1208000     |
| train/                  |             |
|    approx_kl            | 0.002022094 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.88       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.44e+04    |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.000804   |
|    std                  | 2.26        |
|    value_loss           | 1.89e+05    |
-----------------------------------------
Eval num_timesteps=1210000, episode_reward=-3146.92 +/- 45.84
Episode length: 282.20 +/- 36.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | -3.15e+03    |
| time/                   |              |
|    total_timesteps      | 1210000      |
| train/                  |              |
|    approx_kl            | 0.0018058235 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.84e+04     |
|    n_updates            | 5900         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 2.26         |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=1212000, episode_reward=-3127.43 +/- 126.09
Episode length: 259.00 +/- 73.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | -3.13e+03    |
| time/                   |              |
|    total_timesteps      | 1212000      |
| train/                  |              |
|    approx_kl            | 0.0003958627 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.89e+04     |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.000218    |
|    std                  | 2.27         |
|    value_loss           | 1.78e+05     |
------------------------------------------
Eval num_timesteps=1214000, episode_reward=-3128.98 +/- 180.23
Episode length: 253.20 +/- 74.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | -3.13e+03    |
| time/                   |              |
|    total_timesteps      | 1214000      |
| train/                  |              |
|    approx_kl            | 0.0071201045 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.85e+04     |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 2.27         |
|    value_loss           | 1.57e+05     |
------------------------------------------
Eval num_timesteps=1216000, episode_reward=-3117.98 +/- 116.74
Episode length: 284.80 +/- 76.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | -3.12e+03   |
| time/                   |             |
|    total_timesteps      | 1216000     |
| train/                  |             |
|    approx_kl            | 0.001437722 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.85e+04    |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.0018     |
|    std                  | 2.27        |
|    value_loss           | 1.77e+05    |
-----------------------------------------
Eval num_timesteps=1218000, episode_reward=-3061.43 +/- 35.04
Episode length: 248.60 +/- 42.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1218000      |
| train/                  |              |
|    approx_kl            | 0.0037391218 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.84e+04     |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 2.27         |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=-3056.28 +/- 50.38
Episode length: 249.60 +/- 30.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1220000      |
| train/                  |              |
|    approx_kl            | 0.0021949331 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.8e+04      |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000786    |
|    std                  | 2.27         |
|    value_loss           | 1.76e+05     |
------------------------------------------
Eval num_timesteps=1222000, episode_reward=-3050.56 +/- 48.73
Episode length: 266.60 +/- 39.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | -3.05e+03   |
| time/                   |             |
|    total_timesteps      | 1222000     |
| train/                  |             |
|    approx_kl            | 0.003273761 |
|    clip_fraction        | 0.00356     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.08e+05    |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 2.27        |
|    value_loss           | 2.16e+05    |
-----------------------------------------
Eval num_timesteps=1224000, episode_reward=-3000.17 +/- 34.32
Episode length: 205.80 +/- 47.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -3e+03       |
| time/                   |              |
|    total_timesteps      | 1224000      |
| train/                  |              |
|    approx_kl            | 0.0030914142 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.79e+04     |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 2.27         |
|    value_loss           | 1.56e+05     |
------------------------------------------
Eval num_timesteps=1226000, episode_reward=-3073.30 +/- 26.15
Episode length: 230.20 +/- 19.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | -3.07e+03    |
| time/                   |              |
|    total_timesteps      | 1226000      |
| train/                  |              |
|    approx_kl            | 0.0009311498 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.38e+04     |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 2.27         |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=1228000, episode_reward=-3043.04 +/- 17.57
Episode length: 221.00 +/- 47.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | -3.04e+03    |
| time/                   |              |
|    total_timesteps      | 1228000      |
| train/                  |              |
|    approx_kl            | 0.0056148362 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00386     |
|    std                  | 2.27         |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=1230000, episode_reward=-3064.54 +/- 52.15
Episode length: 235.00 +/- 47.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 235          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1230000      |
| train/                  |              |
|    approx_kl            | 0.0011345621 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 8.79e+04     |
|    n_updates            | 6000         |
|    policy_gradient_loss | 0.000158     |
|    std                  | 2.28         |
|    value_loss           | 1.76e+05     |
------------------------------------------
Eval num_timesteps=1232000, episode_reward=-3108.75 +/- 69.12
Episode length: 277.00 +/- 39.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | -3.11e+03   |
| time/                   |             |
|    total_timesteps      | 1232000     |
| train/                  |             |
|    approx_kl            | 4.54846e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.9        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.08e+05    |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.000203   |
|    std                  | 2.28        |
|    value_loss           | 2.15e+05    |
-----------------------------------------
Eval num_timesteps=1234000, episode_reward=-3050.38 +/- 17.95
Episode length: 228.40 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | -3.05e+03    |
| time/                   |              |
|    total_timesteps      | 1234000      |
| train/                  |              |
|    approx_kl            | 0.0011991776 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 2.28         |
|    value_loss           | 2.15e+05     |
------------------------------------------
Eval num_timesteps=1236000, episode_reward=-3028.74 +/- 24.58
Episode length: 234.00 +/- 33.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1236000      |
| train/                  |              |
|    approx_kl            | 0.0017577859 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.16e+04     |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 2.28         |
|    value_loss           | 1.83e+05     |
------------------------------------------
Eval num_timesteps=1238000, episode_reward=-3090.21 +/- 33.91
Episode length: 246.20 +/- 39.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 1238000      |
| train/                  |              |
|    approx_kl            | 0.0023208172 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.74e+04     |
|    n_updates            | 6040         |
|    policy_gradient_loss | -0.000554    |
|    std                  | 2.28         |
|    value_loss           | 1.95e+05     |
------------------------------------------
Eval num_timesteps=1240000, episode_reward=-3093.48 +/- 65.72
Episode length: 237.80 +/- 34.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -3.09e+03    |
| time/                   |              |
|    total_timesteps      | 1240000      |
| train/                  |              |
|    approx_kl            | 0.0003578704 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 6050         |
|    policy_gradient_loss | 0.000149     |
|    std                  | 2.28         |
|    value_loss           | 2.15e+05     |
------------------------------------------
Eval num_timesteps=1242000, episode_reward=-3037.32 +/- 45.57
Episode length: 195.40 +/- 17.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 195          |
|    mean_reward          | -3.04e+03    |
| time/                   |              |
|    total_timesteps      | 1242000      |
| train/                  |              |
|    approx_kl            | 7.537895e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+05     |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.00037     |
|    std                  | 2.28         |
|    value_loss           | 2.14e+05     |
------------------------------------------
Eval num_timesteps=1244000, episode_reward=-3061.04 +/- 89.47
Episode length: 220.00 +/- 55.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | -3.06e+03    |
| time/                   |              |
|    total_timesteps      | 1244000      |
| train/                  |              |
|    approx_kl            | 0.0017702705 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.71e+04     |
|    n_updates            | 6070         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 2.28         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=1246000, episode_reward=-3040.61 +/- 43.83
Episode length: 203.40 +/- 34.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 203           |
|    mean_reward          | -3.04e+03     |
| time/                   |               |
|    total_timesteps      | 1246000       |
| train/                  |               |
|    approx_kl            | 0.00038877098 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.91         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+05      |
|    n_updates            | 6080          |
|    policy_gradient_loss | 0.000301      |
|    std                  | 2.28          |
|    value_loss           | 2.14e+05      |
-------------------------------------------
Eval num_timesteps=1248000, episode_reward=-3101.71 +/- 30.99
Episode length: 252.00 +/- 23.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | -3.1e+03    |
| time/                   |             |
|    total_timesteps      | 1248000     |
| train/                  |             |
|    approx_kl            | 0.003918639 |
|    clip_fraction        | 0.00503     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.91       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.69e+04    |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 2.29        |
|    value_loss           | 1.94e+05    |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=-3031.70 +/- 26.30
Episode length: 210.00 +/- 32.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1250000      |
| train/                  |              |
|    approx_kl            | 0.0039341217 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+04      |
|    n_updates            | 6100         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 2.29         |
|    value_loss           | 1.94e+05     |
------------------------------------------
Eval num_timesteps=1252000, episode_reward=-3028.87 +/- 18.23
Episode length: 224.40 +/- 24.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | -3.03e+03    |
| time/                   |              |
|    total_timesteps      | 1252000      |
| train/                  |              |
|    approx_kl            | 0.0023916394 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.66e+04     |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 2.29         |
|    value_loss           | 1.93e+05     |
------------------------------------------
Eval num_timesteps=1254000, episode_reward=-3017.78 +/- 31.14
Episode length: 206.40 +/- 40.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 1254000      |
| train/                  |              |
|    approx_kl            | 0.0047818995 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+05     |
|    n_updates            | 6120         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 2.29         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=1256000, episode_reward=-3007.34 +/- 33.31
Episode length: 198.20 +/- 29.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 198           |
|    mean_reward          | -3.01e+03     |
| time/                   |               |
|    total_timesteps      | 1256000       |
| train/                  |               |
|    approx_kl            | 0.00027871548 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 6130          |
|    policy_gradient_loss | 0.000191      |
|    std                  | 2.29          |
|    value_loss           | 2.12e+05      |
-------------------------------------------
Eval num_timesteps=1258000, episode_reward=-2997.23 +/- 29.76
Episode length: 183.00 +/- 4.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 183           |
|    mean_reward          | -3e+03        |
| time/                   |               |
|    total_timesteps      | 1258000       |
| train/                  |               |
|    approx_kl            | 0.00012455616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+05      |
|    n_updates            | 6140          |
|    policy_gradient_loss | -4.49e-05     |
|    std                  | 2.29          |
|    value_loss           | 2.12e+05      |
-------------------------------------------
Eval num_timesteps=1260000, episode_reward=-3016.66 +/- 22.63
Episode length: 206.60 +/- 22.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 207           |
|    mean_reward          | -3.02e+03     |
| time/                   |               |
|    total_timesteps      | 1260000       |
| train/                  |               |
|    approx_kl            | 0.00018864844 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.59e+04      |
|    n_updates            | 6150          |
|    policy_gradient_loss | -0.000237     |
|    std                  | 2.29          |
|    value_loss           | 1.92e+05      |
-------------------------------------------
Eval num_timesteps=1262000, episode_reward=-3011.30 +/- 22.52
Episode length: 199.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 200          |
|    mean_reward          | -3.01e+03    |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0006008671 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+05     |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.000894    |
|    std                  | 2.29         |
|    value_loss           | 2.31e+05     |
------------------------------------------
Eval num_timesteps=1264000, episode_reward=-3017.65 +/- 16.77
Episode length: 218.80 +/- 13.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | -3.02e+03    |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 0.0015182807 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.29         |
|    value_loss           | 2.11e+05     |
------------------------------------------
Eval num_timesteps=1266000, episode_reward=-3008.21 +/- 12.17
Episode length: 185.00 +/- 25.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 185          |
|    mean_reward          | -3.01e+03    |
| time/                   |              |
|    total_timesteps      | 1266000      |
| train/                  |              |
|    approx_kl            | 0.0018136518 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+05     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.000584    |
|    std                  | 2.29         |
|    value_loss           | 2.3e+05      |
------------------------------------------
Eval num_timesteps=1268000, episode_reward=-2989.90 +/- 29.14
Episode length: 188.20 +/- 18.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 188          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 1268000      |
| train/                  |              |
|    approx_kl            | 0.0008190554 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.000441    |
|    std                  | 2.3          |
|    value_loss           | 2.1e+05      |
------------------------------------------
Eval num_timesteps=1270000, episode_reward=-2992.17 +/- 28.57
Episode length: 197.60 +/- 23.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 198           |
|    mean_reward          | -2.99e+03     |
| time/                   |               |
|    total_timesteps      | 1270000       |
| train/                  |               |
|    approx_kl            | 0.00037607286 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.94         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 9.54e+04      |
|    n_updates            | 6200          |
|    policy_gradient_loss | -0.000261     |
|    std                  | 2.3           |
|    value_loss           | 1.91e+05      |
-------------------------------------------
Eval num_timesteps=1272000, episode_reward=-2979.71 +/- 40.21
Episode length: 204.40 +/- 22.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 204          |
|    mean_reward          | -2.98e+03    |
| time/                   |              |
|    total_timesteps      | 1272000      |
| train/                  |              |
|    approx_kl            | 0.0015260854 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.94        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 6210         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 2.3          |
|    value_loss           | 2.1e+05      |
------------------------------------------
Eval num_timesteps=1274000, episode_reward=-2976.02 +/- 26.52
Episode length: 182.80 +/- 34.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 183          |
|    mean_reward          | -2.98e+03    |
| time/                   |              |
|    total_timesteps      | 1274000      |
| train/                  |              |
|    approx_kl            | 0.0008765529 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 6220         |
|    policy_gradient_loss | -7.97e-05    |
|    std                  | 2.3          |
|    value_loss           | 2.29e+05     |
------------------------------------------
Eval num_timesteps=1276000, episode_reward=-2988.60 +/- 25.82
Episode length: 175.40 +/- 26.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 1276000      |
| train/                  |              |
|    approx_kl            | 0.0001993026 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+05     |
|    n_updates            | 6230         |
|    policy_gradient_loss | 4.89e-06     |
|    std                  | 2.31         |
|    value_loss           | 2.09e+05     |
------------------------------------------
Eval num_timesteps=1278000, episode_reward=-3001.04 +/- 18.91
Episode length: 192.00 +/- 12.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 192           |
|    mean_reward          | -3e+03        |
| time/                   |               |
|    total_timesteps      | 1278000       |
| train/                  |               |
|    approx_kl            | 0.00025829845 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.95         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+05      |
|    n_updates            | 6240          |
|    policy_gradient_loss | -0.000337     |
|    std                  | 2.31          |
|    value_loss           | 2.09e+05      |
-------------------------------------------
Eval num_timesteps=1280000, episode_reward=-3003.62 +/- 23.98
Episode length: 175.00 +/- 21.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | -3e+03   |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1282000, episode_reward=-2993.30 +/- 19.69
Episode length: 173.80 +/- 10.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 174          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 1282000      |
| train/                  |              |
|    approx_kl            | 0.0007856928 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+05     |
|    n_updates            | 6250         |
|    policy_gradient_loss | -0.000853    |
|    std                  | 2.31         |
|    value_loss           | 2.28e+05     |
------------------------------------------
Eval num_timesteps=1284000, episode_reward=-2980.55 +/- 45.55
Episode length: 192.80 +/- 24.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 193           |
|    mean_reward          | -2.98e+03     |
| time/                   |               |
|    total_timesteps      | 1284000       |
| train/                  |               |
|    approx_kl            | 0.00013270287 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.96         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 6260          |
|    policy_gradient_loss | -8.82e-05     |
|    std                  | 2.31          |
|    value_loss           | 2.09e+05      |
-------------------------------------------
Eval num_timesteps=1286000, episode_reward=-2963.87 +/- 32.07
Episode length: 218.00 +/- 17.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 218           |
|    mean_reward          | -2.96e+03     |
| time/                   |               |
|    total_timesteps      | 1286000       |
| train/                  |               |
|    approx_kl            | 0.00044357407 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.96         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+05      |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.000149     |
|    std                  | 2.31          |
|    value_loss           | 2.08e+05      |
-------------------------------------------
Eval num_timesteps=1288000, episode_reward=-2987.52 +/- 24.66
Episode length: 203.00 +/- 20.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 203          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 1288000      |
| train/                  |              |
|    approx_kl            | 0.0003976746 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.000573    |
|    std                  | 2.31         |
|    value_loss           | 2.08e+05     |
------------------------------------------
Eval num_timesteps=1290000, episode_reward=-2988.74 +/- 17.21
Episode length: 200.80 +/- 15.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | -2.99e+03    |
| time/                   |              |
|    total_timesteps      | 1290000      |
| train/                  |              |
|    approx_kl            | 0.0016880808 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+05     |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.31         |
|    value_loss           | 2.18e+05     |
------------------------------------------
Eval num_timesteps=1292000, episode_reward=-2945.76 +/- 13.82
Episode length: 234.40 +/- 44.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | -2.95e+03   |
| time/                   |             |
|    total_timesteps      | 1292000     |
| train/                  |             |
|    approx_kl            | 0.002405868 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+05    |
|    n_updates            | 6300        |
|    policy_gradient_loss | -0.00198    |
|    std                  | 2.32        |
|    value_loss           | 2.07e+05    |
-----------------------------------------
Eval num_timesteps=1294000, episode_reward=-2967.60 +/- 33.68
Episode length: 167.60 +/- 11.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 168          |
|    mean_reward          | -2.97e+03    |
| time/                   |              |
|    total_timesteps      | 1294000      |
| train/                  |              |
|    approx_kl            | 0.0015189754 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 2.32         |
|    value_loss           | 2.07e+05     |
------------------------------------------
Eval num_timesteps=1296000, episode_reward=-2976.31 +/- 24.89
Episode length: 173.40 +/- 30.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 173           |
|    mean_reward          | -2.98e+03     |
| time/                   |               |
|    total_timesteps      | 1296000       |
| train/                  |               |
|    approx_kl            | 0.00033753552 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.98         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+05      |
|    n_updates            | 6320          |
|    policy_gradient_loss | 0.000239      |
|    std                  | 2.32          |
|    value_loss           | 2.43e+05      |
-------------------------------------------
Eval num_timesteps=1298000, episode_reward=-2969.91 +/- 34.27
Episode length: 181.80 +/- 36.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 182          |
|    mean_reward          | -2.97e+03    |
| time/                   |              |
|    total_timesteps      | 1298000      |
| train/                  |              |
|    approx_kl            | 0.0013487575 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+05     |
|    n_updates            | 6330         |
|    policy_gradient_loss | -0.000844    |
|    std                  | 2.32         |
|    value_loss           | 2.07e+05     |
------------------------------------------
Eval num_timesteps=1300000, episode_reward=-2980.13 +/- 12.46
Episode length: 182.80 +/- 17.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 183           |
|    mean_reward          | -2.98e+03     |
| time/                   |               |
|    total_timesteps      | 1300000       |
| train/                  |               |
|    approx_kl            | 0.00012051285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.98         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+05      |
|    n_updates            | 6340          |
|    policy_gradient_loss | -0.000176     |
|    std                  | 2.33          |
|    value_loss           | 2.26e+05      |
-------------------------------------------
Eval num_timesteps=1302000, episode_reward=-2950.76 +/- 46.22
Episode length: 191.60 +/- 55.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 192          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 1302000      |
| train/                  |              |
|    approx_kl            | 0.0009599542 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 6350         |
|    policy_gradient_loss | -0.000468    |
|    std                  | 2.32         |
|    value_loss           | 2.06e+05     |
------------------------------------------
Eval num_timesteps=1304000, episode_reward=-2978.59 +/- 9.86
Episode length: 189.80 +/- 13.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 190          |
|    mean_reward          | -2.98e+03    |
| time/                   |              |
|    total_timesteps      | 1304000      |
| train/                  |              |
|    approx_kl            | 0.0004017134 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.00044     |
|    std                  | 2.33         |
|    value_loss           | 2.07e+05     |
------------------------------------------
Eval num_timesteps=1306000, episode_reward=-2971.74 +/- 30.70
Episode length: 201.40 +/- 28.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | -2.97e+03    |
| time/                   |              |
|    total_timesteps      | 1306000      |
| train/                  |              |
|    approx_kl            | 0.0018468638 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 6370         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 2.33         |
|    value_loss           | 2.06e+05     |
------------------------------------------
Eval num_timesteps=1308000, episode_reward=-2961.20 +/- 12.95
Episode length: 199.20 +/- 16.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -2.96e+03    |
| time/                   |              |
|    total_timesteps      | 1308000      |
| train/                  |              |
|    approx_kl            | 0.0028722247 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.99        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+05     |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 2.33         |
|    value_loss           | 2.06e+05     |
------------------------------------------
Eval num_timesteps=1310000, episode_reward=-2968.87 +/- 10.62
Episode length: 183.20 +/- 26.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 183           |
|    mean_reward          | -2.97e+03     |
| time/                   |               |
|    total_timesteps      | 1310000       |
| train/                  |               |
|    approx_kl            | 0.00039295925 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.99         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.32e+04      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000389     |
|    std                  | 2.33          |
|    value_loss           | 1.87e+05      |
-------------------------------------------
Eval num_timesteps=1312000, episode_reward=-2955.58 +/- 20.40
Episode length: 214.60 +/- 35.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 215          |
|    mean_reward          | -2.96e+03    |
| time/                   |              |
|    total_timesteps      | 1312000      |
| train/                  |              |
|    approx_kl            | 0.0011188379 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.32e+04     |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.0006      |
|    std                  | 2.34         |
|    value_loss           | 1.86e+05     |
------------------------------------------
Eval num_timesteps=1314000, episode_reward=-2916.40 +/- 37.60
Episode length: 231.60 +/- 40.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | -2.92e+03    |
| time/                   |              |
|    total_timesteps      | 1314000      |
| train/                  |              |
|    approx_kl            | 0.0045071375 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 6410         |
|    policy_gradient_loss | -0.00202     |
|    std                  | 2.34         |
|    value_loss           | 2.05e+05     |
------------------------------------------
Eval num_timesteps=1316000, episode_reward=-2940.13 +/- 45.16
Episode length: 218.60 +/- 65.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | -2.94e+03    |
| time/                   |              |
|    total_timesteps      | 1316000      |
| train/                  |              |
|    approx_kl            | 0.0039538606 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+05     |
|    n_updates            | 6420         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 2.34         |
|    value_loss           | 2.05e+05     |
------------------------------------------
Eval num_timesteps=1318000, episode_reward=-2931.81 +/- 12.53
Episode length: 200.60 +/- 10.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 201           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 1318000       |
| train/                  |               |
|    approx_kl            | 0.00056292134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9            |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+05      |
|    n_updates            | 6430          |
|    policy_gradient_loss | -0.000414     |
|    std                  | 2.34          |
|    value_loss           | 2.22e+05      |
-------------------------------------------
Eval num_timesteps=1320000, episode_reward=-2895.83 +/- 38.94
Episode length: 237.80 +/- 43.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 238           |
|    mean_reward          | -2.9e+03      |
| time/                   |               |
|    total_timesteps      | 1320000       |
| train/                  |               |
|    approx_kl            | 0.00011150373 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9            |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+05      |
|    n_updates            | 6440          |
|    policy_gradient_loss | -0.000194     |
|    std                  | 2.34          |
|    value_loss           | 2.04e+05      |
-------------------------------------------
Eval num_timesteps=1322000, episode_reward=-2935.38 +/- 28.20
Episode length: 192.40 +/- 23.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 192           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 1322000       |
| train/                  |               |
|    approx_kl            | 0.00013208468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+05      |
|    n_updates            | 6450          |
|    policy_gradient_loss | -0.000314     |
|    std                  | 2.34          |
|    value_loss           | 2.04e+05      |
-------------------------------------------
Eval num_timesteps=1324000, episode_reward=-2953.42 +/- 24.00
Episode length: 211.60 +/- 36.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 212          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 1324000      |
| train/                  |              |
|    approx_kl            | 0.0005604909 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.01        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 9.26e+04     |
|    n_updates            | 6460         |
|    policy_gradient_loss | -0.000617    |
|    std                  | 2.34         |
|    value_loss           | 1.85e+05     |
------------------------------------------
Eval num_timesteps=1326000, episode_reward=-2937.75 +/- 25.34
Episode length: 206.40 +/- 25.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 206           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 1326000       |
| train/                  |               |
|    approx_kl            | 0.00043681546 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+05      |
|    n_updates            | 6470          |
|    policy_gradient_loss | -0.0003       |
|    std                  | 2.34          |
|    value_loss           | 2.03e+05      |
-------------------------------------------
Eval num_timesteps=1328000, episode_reward=-2910.75 +/- 26.69
Episode length: 240.80 +/- 73.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -2.91e+03    |
| time/                   |              |
|    total_timesteps      | 1328000      |
| train/                  |              |
|    approx_kl            | 0.0053202594 |
|    clip_fraction        | 0.0699       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.03        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 7.4e+04      |
|    n_updates            | 6480         |
|    policy_gradient_loss | -0.0057      |
|    std                  | 2.36         |
|    value_loss           | 1.48e+05     |
------------------------------------------
Eval num_timesteps=1330000, episode_reward=-2898.84 +/- 55.66
Episode length: 245.80 +/- 86.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | -2.9e+03    |
| time/                   |             |
|    total_timesteps      | 1330000     |
| train/                  |             |
|    approx_kl            | 0.006095986 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.06       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.21e+04    |
|    n_updates            | 6490        |
|    policy_gradient_loss | 1.2e-05     |
|    std                  | 2.38        |
|    value_loss           | 1.84e+05    |
-----------------------------------------
Eval num_timesteps=1332000, episode_reward=-2881.20 +/- 43.41
Episode length: 236.40 +/- 53.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | -2.88e+03   |
| time/                   |             |
|    total_timesteps      | 1332000     |
| train/                  |             |
|    approx_kl            | 0.003713142 |
|    clip_fraction        | 0.0062      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.3e+04     |
|    n_updates            | 6500        |
|    policy_gradient_loss | -0.000337   |
|    std                  | 2.39        |
|    value_loss           | 1.66e+05    |
-----------------------------------------
Eval num_timesteps=1334000, episode_reward=-2918.55 +/- 32.06
Episode length: 206.40 +/- 47.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 206           |
|    mean_reward          | -2.92e+03     |
| time/                   |               |
|    total_timesteps      | 1334000       |
| train/                  |               |
|    approx_kl            | 0.00039644376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.2e+04       |
|    n_updates            | 6510          |
|    policy_gradient_loss | -0.000262     |
|    std                  | 2.39          |
|    value_loss           | 1.84e+05      |
-------------------------------------------
Eval num_timesteps=1336000, episode_reward=-2884.13 +/- 61.41
Episode length: 295.00 +/- 103.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | -2.88e+03     |
| time/                   |               |
|    total_timesteps      | 1336000       |
| train/                  |               |
|    approx_kl            | 0.00032662548 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.19e+04      |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000391     |
|    std                  | 2.39          |
|    value_loss           | 1.84e+05      |
-------------------------------------------
Eval num_timesteps=1338000, episode_reward=-2947.29 +/- 20.00
Episode length: 191.60 +/- 41.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 192          |
|    mean_reward          | -2.95e+03    |
| time/                   |              |
|    total_timesteps      | 1338000      |
| train/                  |              |
|    approx_kl            | 0.0012757601 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+05     |
|    n_updates            | 6530         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.39         |
|    value_loss           | 2.02e+05     |
------------------------------------------
Eval num_timesteps=1340000, episode_reward=-2892.04 +/- 37.18
Episode length: 279.40 +/- 89.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | -2.89e+03    |
| time/                   |              |
|    total_timesteps      | 1340000      |
| train/                  |              |
|    approx_kl            | 0.0038674884 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 8.26e+04     |
|    n_updates            | 6540         |
|    policy_gradient_loss | -0.002       |
|    std                  | 2.39         |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=1342000, episode_reward=-2934.16 +/- 25.24
Episode length: 184.20 +/- 44.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 184          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 1342000      |
| train/                  |              |
|    approx_kl            | 0.0025252064 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.18e+04     |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.39         |
|    value_loss           | 1.84e+05     |
------------------------------------------
Eval num_timesteps=1344000, episode_reward=-2933.09 +/- 35.17
Episode length: 252.20 +/- 69.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | -2.93e+03    |
| time/                   |              |
|    total_timesteps      | 1344000      |
| train/                  |              |
|    approx_kl            | 0.0015536854 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.25e+04     |
|    n_updates            | 6560         |
|    policy_gradient_loss | -0.000801    |
|    std                  | 2.4          |
|    value_loss           | 1.65e+05     |
------------------------------------------
Eval num_timesteps=1346000, episode_reward=-2935.94 +/- 32.34
Episode length: 267.80 +/- 110.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 268           |
|    mean_reward          | -2.94e+03     |
| time/                   |               |
|    total_timesteps      | 1346000       |
| train/                  |               |
|    approx_kl            | 0.00087505666 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.11         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.15e+04      |
|    n_updates            | 6570          |
|    policy_gradient_loss | 0.000914      |
|    std                  | 2.4           |
|    value_loss           | 1.83e+05      |
-------------------------------------------
Eval num_timesteps=1348000, episode_reward=-2900.26 +/- 50.34
Episode length: 242.80 +/- 64.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 243          |
|    mean_reward          | -2.9e+03     |
| time/                   |              |
|    total_timesteps      | 1348000      |
| train/                  |              |
|    approx_kl            | 0.0019136755 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.12e+04     |
|    n_updates            | 6580         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 2.4          |
|    value_loss           | 1.82e+05     |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=-2894.60 +/- 49.06
Episode length: 262.20 +/- 58.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 262        |
|    mean_reward          | -2.89e+03  |
| time/                   |            |
|    total_timesteps      | 1350000    |
| train/                  |            |
|    approx_kl            | 0.10636837 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.18      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 7.32e+04   |
|    n_updates            | 6590       |
|    policy_gradient_loss | 0.0284     |
|    std                  | 2.48       |
|    value_loss           | 1.46e+05   |
----------------------------------------
Eval num_timesteps=1352000, episode_reward=-2934.66 +/- 48.06
Episode length: 238.20 +/- 55.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | -2.93e+03  |
| time/                   |            |
|    total_timesteps      | 1352000    |
| train/                  |            |
|    approx_kl            | 0.07180198 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.26      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 8.22e+04   |
|    n_updates            | 6600       |
|    policy_gradient_loss | 0.0207     |
|    std                  | 2.51       |
|    value_loss           | 1.64e+05   |
----------------------------------------
Eval num_timesteps=1354000, episode_reward=-2931.98 +/- 51.54
Episode length: 207.20 +/- 54.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | -2.93e+03   |
| time/                   |             |
|    total_timesteps      | 1354000     |
| train/                  |             |
|    approx_kl            | 0.004087159 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.3        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 9.99e+04    |
|    n_updates            | 6610        |
|    policy_gradient_loss | -4.11e-05   |
|    std                  | 2.52        |
|    value_loss           | 2e+05       |
-----------------------------------------
Eval num_timesteps=1356000, episode_reward=-2932.17 +/- 31.85
Episode length: 253.60 +/- 31.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 254           |
|    mean_reward          | -2.93e+03     |
| time/                   |               |
|    total_timesteps      | 1356000       |
| train/                  |               |
|    approx_kl            | 0.00042654152 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.31         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+05      |
|    n_updates            | 6620          |
|    policy_gradient_loss | -0.000234     |
|    std                  | 2.53          |
|    value_loss           | 2.05e+05      |
-------------------------------------------
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 678, in <module>
    sim.run_full()
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 416, in run_full
    model.learn(total_timesteps=int(args.max_steps),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 178, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\policies.py", line 645, in forward
    latent_pi, latent_vf = self.mlp_extractor(features)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\torch_layers.py", line 222, in forward
    return self.forward_actor(features), self.forward_critic(features)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\torch_layers.py", line 225, in forward_actor
    return self.policy_net(features)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\container.py", line 215, in forward
    input = module(input)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt