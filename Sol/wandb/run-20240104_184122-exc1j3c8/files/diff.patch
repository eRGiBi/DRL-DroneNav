diff --git a/Sol/Model/PBDroneEnv.py b/Sol/Model/PBDroneEnv.py
index a249810..87bd4ac 100644
--- a/Sol/Model/PBDroneEnv.py
+++ b/Sol/Model/PBDroneEnv.py
@@ -307,25 +307,28 @@ class PBDroneEnv(
 
         try:
             # reward -= distance_to_target ** 2
-            # Reward based on distance to target
-
-            # reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
-            # reward += np.exp(-distance_to_target * 5) * 50
-            # Additional reward for progressing towards the target
-            # reward += (self._prev_distance_to_target - distance_to_target) * 30
-            # self.reward += max(3.0 * self.waypoints.progress_to_target(), 0.0)
-
-            # Add a negative reward for spinning too fast
-            # reward += -np.linalg.norm(self.ang_v) / 3
-
-            # # Penalize large actions to avoid erratic behavior
-            # reward -= 0.01 * np.linalg.norm(self._last_action)
 
             if self._current_target_index > 0:
                 # Additional reward for progressing towards the target
                 reward += self.calculate_progress_reward(self._current_position, self._last_position,
                                                          self._target_points[self._current_target_index - 1],
                                                          self._target_points[self._current_target_index]) * 1000
+            else:
+
+                # Reward based on distance to target
+
+                reward += (1 / distance_to_target)  # * self._discount ** self._steps/10
+                reward += np.exp(-distance_to_target * 5) * 50
+                # Additional reward for progressing towards the target
+                reward += (self._prev_distance_to_target - distance_to_target) * 30
+
+                # self.reward += max(3.0 * self.waypoints.progress_to_target(), 0.0)
+
+                # Add a negative reward for spinning too fast
+                reward += -np.linalg.norm(self.ang_v) / 3
+
+                # Penalize large actions to avoid erratic behavior
+                reward -= 0.01 * np.linalg.norm(self._last_action)
 
         except ZeroDivisionError:
             # Give a high reward if the drone is at the target (avoiding division by zero)
diff --git a/Sol/Model/SBActorCritic.py b/Sol/Model/SBActorCritic.py
index 5ec7ae6..377ec52 100644
--- a/Sol/Model/SBActorCritic.py
+++ b/Sol/Model/SBActorCritic.py
@@ -27,7 +27,8 @@ class CustomFeatureExtractor(BaseFeaturesExtractor):
     """
 
     def __init__(self, observation_space, net_arch, activation_fn=nn.ReLU):
-        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=net_arch[-1]['shared'])
+        super(CustomFeatureExtractor, self).__init__(observation_space,
+                                                     features_dim=net_arch)
 
         # Assuming observation space is a Box space
         self.flatten = nn.Flatten()
@@ -37,7 +38,9 @@ class CustomFeatureExtractor(BaseFeaturesExtractor):
             nn.Linear(int(observation_space.shape[0]), net_arch[0]),
             activation_fn(),
             nn.Linear(net_arch[0], net_arch[1]),
-            activation_fn()
+            activation_fn(),
+            nn.Linear(net_arch[1], net_arch[2]['vf'][0]),
+            activation_fn(),
         )
 
     def forward(self, x):
@@ -57,13 +60,13 @@ class CustomActorCriticPolicy(ActorCriticPolicy):
 
         super(CustomActorCriticPolicy, self).__init__(observation_space, action_space,
                                                       lr_schedule,
+                                                      net_arch,
                                                       activation_fn=activation_fn,
                                                       *args, **kwargs)
 
         # Disable orthogonal initialization
         kwargs["ortho_init"] = False
 
-
         # Create the custom feature extractor
         self.features_extractor = CustomFeatureExtractor(observation_space, net_arch, activation_fn)
 
@@ -96,4 +99,4 @@ class CustomActorCriticPolicy(ActorCriticPolicy):
         return action, value
 
     def _build_mlp_extractor(self) -> None:
-        self.mlp_extractor = CustomActorCriticPolicy(self.features_dim)
\ No newline at end of file
+        self.mlp_extractor = CustomFeatureExtractor(self.observation_space, self.net_arch, self.activation_fn)
diff --git a/Sol/Model/SoftActorCritic.py b/Sol/Model/SoftActorCritic.py
index 68938ba..061833a 100644
--- a/Sol/Model/SoftActorCritic.py
+++ b/Sol/Model/SoftActorCritic.py
@@ -8,6 +8,7 @@ from tf_agents.environments import suite_pybullet
 from Sol.Model.PBDroneEnv import PBDroneEnv
 from Sol.PyBullet.enums import Physics
 
+
 class SoftAcorCritic():
 
     def __init__(self):
@@ -16,9 +17,7 @@ class SoftAcorCritic():
     def set_up_training(self, env):
         tempdir = tempfile.gettempdir()
 
-        # Use "num_iterations = 1e6" for better results (2 hrs)
-        # 1e5 is just so this doesn't take too long (1 hr)
-        num_iterations = 100000  # @param {type:"integer"}
+        num_iterations = 1e6  # @param {type:"integer"}
 
         initial_collect_steps = 10000  # @param {type:"integer"}
         collect_steps_per_iteration = 1  # @param {type:"integer"}
@@ -56,15 +55,13 @@ class SoftAcorCritic():
 
 
 if __name__ == '__main__':
-    drone_environment = PBDroneEnv(race_track=None,
-                                   target_points=[np.array([0.0, 0.0, 1.0]),
+    drone_environment = PBDroneEnv(target_points=[np.array([0.0, 0.0, 1.0]),
                                                   np.array([8.59735, -3.3286, -6.07256]),
                                                   np.array([1.5974, -5.0786, -4.32256]),
                                                   np.array([3.2474, 3.32137, -2.5725]),
                                                   np.array([1.3474, 1.6714, -2.07256]), ],
                                    threshold=1,
                                    discount=1,
-                                   drone=None,
                                    physics=Physics.PYB,
                                    gui=True,
                                    initial_xyzs=np.array([[0, 0, 0]])
diff --git a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc
index 75c6ef1..8fc2cde 100644
Binary files a/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc and b/Sol/Model/__pycache__/PBDroneEnv.cpython-38.pyc differ
diff --git a/Sol/Model/pybullet_drone_simulator.py b/Sol/Model/pybullet_drone_simulator.py
index bbe4ae1..8092cf8 100644
--- a/Sol/Model/pybullet_drone_simulator.py
+++ b/Sol/Model/pybullet_drone_simulator.py
@@ -42,6 +42,7 @@ import Sol.Model.Waypoints as Waypoints
 
 from Sol.Utilities.Plotter import plot_learning_curve, plot_metrics, plot_3d_targets
 import Sol.Utilities.Callbacks as Callbacks
+from Sol.Model.SBActorCritic import CustomActorCriticPolicy
 
 # from tf_agents.environments import py_environment
 
@@ -187,9 +188,9 @@ class PBDroneSimulator:
     def test_saved(self):
         drone_environment = self.make_env(gui=True, aviary_dim=np.array([-2, -2, 0, 2, 2, 2]))
 
-        model = SAC.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.29.2023_11.55.59/best_model.zip")
-        # model = PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\sa/best_model.zip",
-        #                  env=drone_environment)
+        # model = SAC.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\/best_model.zip")
+        model = PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-01.03.2024_21.46.55/best_model.zip",
+                         env=drone_environment)
         # model = PPO.load(os.curdir + "\model_chkpts\success_model.zip")
         # model = SAC.load(os.curdir + "\model_chkpts\success_model.zip")
 
@@ -210,7 +211,7 @@ class PBDroneSimulator:
 
         for i in range(self.max_steps):
             action, _states = model.predict(obs,
-                                            deterministic=True
+                                            deterministic=False
                                             )
             obs, reward, terminated, truncated, info = drone_environment.step(action)
             print(i)
@@ -235,6 +236,44 @@ class PBDroneSimulator:
 
             time.sleep(1. / 240.)
 
+    def test_learning(self):
+
+        train_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i,
+                                                 aviary_dim=np.array([-2, -2, 0, 2, 2, 2])) for i in
+                                   range(1)])
+
+        # custom_policy = CustomActorCriticPolicy(train_env.observation_space, train_env.action_space,
+        #                                         net_arch=[512, 512, dict(vf=[256, 128],
+        #                                                                  pi=[256, 128])],
+        #                                         lr_schedule=linear_schedule(1e-3),
+        #                                         activation_fn=th.nn.Tanh)
+
+        onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
+
+
+        model = PPO("MlpPolicy",
+                    train_env,
+                    verbose=1,
+                    n_steps=2048,
+                    batch_size=49,
+                    device="auto",
+                    policy_kwargs=onpolicy_kwargs
+                    # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+                    )
+        print(model.policy)
+
+        model.learn(total_timesteps=int(5e2),)
+
+        print("#############################################")
+
+        # model = PPO(custom_policy,
+        #             train_env,
+        #             verbose=1,
+        #             n_steps=2048,
+        #             batch_size=49
+        #             # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+        #             )
+
     def run_full(self, args):
         start = time.perf_counter()
 
@@ -249,8 +288,8 @@ class PBDroneSimulator:
                                                  aviary_dim=np.array([-2, -2, 0, 2, 2, 2])) for i in
                                    range(self.num_cpu)])
         # train_env = VecCheckNan(train_env)
-        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
-                                 clip_obs=1)
+        # train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True,
+        #                          clip_obs=1)
 
         # eval_env = make_env(multi=False, gui=False, rank=0)
         #
@@ -258,14 +297,20 @@ class PBDroneSimulator:
                                                 aviary_dim=np.array([-2, -2, 0, 2, 2, 2])), ])
         # eval_env = SubprocVecEnv([self.make_env(multi=True, gui=False, rank=i) for i in range(self.num_cpu)])
         # eval_env = VecCheckNan(eval_env)
-        eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
-                                clip_obs=1)
+        # eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True,
+        #                         clip_obs=1)
 
-        # onpolicy_kwargs = dict(activation_fn=th.nn.ReLU,
-        #                        net_arch=dict(vf=[512, 512, 256, 128],
-        #                                      pi=[512, 512, 256, 128]))
+        onpolicy_kwargs = dict(activation_fn=th.nn.Tanh,
+                               net_arch=dict(vf=[512, 512, 256, 128],
+                                             pi=[512, 512, 256, 128]))
         # onpolicy_kwargs = dict(net_arch=[512, 512, dict(vf=[256, 128], pi=[256, 128])])
 
+        # custom_policy = CustomActorCriticPolicy(train_env.observation_space, train_env.action_space,
+        #                                     net_arch=[512, 512, dict(vf=[256, 128],
+        #                                                              pi=[256, 128])],
+        #                                     lr_schedule=linear_schedule(1e-3),
+        #                                     activation_fn=th.nn.Tanh)
+
         model = PPO("MlpPolicy",
                     train_env,
                     verbose=1,
@@ -278,8 +323,8 @@ class PBDroneSimulator:
                     learning_rate=1e-3,
                     tensorboard_log="./logs/ppo_tensorboard/",
                     device="auto",
-                    policy_kwargs=  # onpolicy_kwargs
-                    dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
+                    policy_kwargs= onpolicy_kwargs
+                    # dict(net_arch=[256, 256, 256], activation_fn=th.nn.GELU, ),
                     )
 
         # tensorboard --logdir ./logs/ppo_tensorboard/
@@ -327,11 +372,8 @@ class PBDroneSimulator:
         # vec_env = make_vec_env([make_env(gui=False, rank=i) for i in range(num_cpu)], n_envs=4, seed=0)
         # model = SAC("MlpPolicy", vec_env, train_freq=1, gradient_steps=2, verbose=1)
 
-        # train_env = stable_baselines3.common.monitor.Monitor(train_env)
-        # eval_env = stable_baselines3.common.monitor.Monitor(eval_env)
+        callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=100_000, verbose=1)
 
-        callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=100_000,
-                                                         verbose=1)
         stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)
 
         found_tar_callback = Callbacks.FoundTargetsCallback(log_dir=filename + '/')
@@ -362,6 +404,8 @@ class PBDroneSimulator:
         stats_path = os.path.join(filename, "vec_normalize.pkl")
         eval_env.save(stats_path)
 
+        wandb.finish()
+
         # Test the model #########################################
 
         test_env = self.make_env(multi=False)
@@ -441,23 +485,26 @@ def parse_args():
     parser.add_argument('--env-kwargs', type=str, default='{}')
     parser.add_argument('--log-dir', type=str, default='logs')
     parser.add_argument('--seed', '-s', type=int, default=1)
-    parser.add_argument('--cuda', action='store_true', default=False)
-    parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI (default: True)',
-                        metavar='')
+    parser.add_argument('--cuda', action='store_true', default=True)
+    parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI for the eval env'
+                                                           '(default: True)')
     parser.add_argument('--save-buffer', action='store_true', default=False)
     parser.add_argument('--save-model', action='store_true', default=True)
-    parser.add_argument('--save-obs', action='store_true', default=False)
-    parser.add_argument('--save-video', action='store_true', default=False)
     parser.add_argument('--save-dir', type=str, default='')
     parser.add_argument('--checkpoint-freq', type=int, default=100)
-    parser.add_argument('--checkpoint-at-end', action='store_true', default=False)
+
     parser.add_argument('--restore-agent', action='store_true', default=False)
 
+    # Wrapper specific arguments
+    parser.add_argument('--vec_check_nan', default=False, type=lambda x: bool(strtobool(x)))
+    parser.add_argument('--vec_normalize', default=False, type=lambda x: bool(strtobool(x)))
+    parser.add_argument('--ve_check_env', default=False, type=lambda x: bool(strtobool(x)))
 
     parser.add_argument('--num-cpus', type=int, default=1)
-    parser.add_argument('--num-workers', type=int, default=0)
-    parser.add_argument('--num_envs', type=int, default=1)
+
     parser.add_argument('--max_steps', type=int, default=5e6)
+
+    # RL Algorithm specific arguments
     parser.add_argument('--agent', type=str, default='PPO')
     parser.add_argument('--agent-config', type=str, default='default')
     parser.add_argument('--policy', type=str, default='default')
@@ -466,10 +513,11 @@ def parse_args():
     parser.add_argument('--threshold', type=int, default=0.3)
     parser.add_argument('--batch-size', type=int, default=2048)
     parser.add_argument('--num-steps', type=int, default=2048)
-    parser.add_argument('--ent_coef', type=int, default=0)
     parser.add_argument('--learning-rate', type=int, default=1e-3)
-    parser.add_argument('--clip_range', type=int, default=0.2)
 
+    # PPO specific
+    parser.add_argument('--clip_range', type=int, default=0.2)
+    parser.add_argument('--ent_coef', type=int, default=0)
 
     parser.add_argument('--eval-criterion', type=str, default='default')
     parser.add_argument('--eval-criterion-config', type=str, default='default')
@@ -480,27 +528,16 @@ def parse_args():
     parser.add_argument('--criterion', type=str, default='default')
     parser.add_argument('--criterion-config', type=str, default='default')
 
+    # Wandb specific arguments
     parser.add_argument('--wandb', type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,)
-
-    parser.add_argument("--wandb-project-name", type=str, default="ppo-implementation-details",
-                        help="the wandb's project name")
     parser.add_argument("--wandb-entity", type=str, default=None,
                         help="the entity (team) of wandb's project")
+
     parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
                         help="weather to capture videos of the agent performances (check out `videos` folder)")
 
-    # #### Define and parse (optional) arguments for the script ##
-    # parser = argparse.ArgumentParser(description='Single agent reinforcement learning example script using HoverAviary')
-    # parser.add_argument('--gui', default=DEFAULT_GUI, help='Whether to use PyBullet GUI (default: True)',
-    #                     metavar='')
-    # parser.add_argument('--record_video', default=DEFAULT_RECORD_VIDEO, type=str2bool,
-    #                     help='Whether to record a video (default: False)', metavar='')
-    # parser.add_argument('--output_folder', default=DEFAULT_OUTPUT_FOLDER, type=str,
-    #                     help='Folder where to save logs (default: "results")', metavar='')
-    # parser.add_argument('--colab', default=DEFAULT_COLAB, type=bool,
-    #                     help='Whether example is being run by a notebook (default: "False")', metavar='')
-    #
-    # run(**vars(ARGS))
+    parser.add_argument('--output_folder', default=DEFAULT_OUTPUT_FOLDER, type=str,
+                        help='Folder where to save logs (default: "results")', metavar='')
 
     args = parser.parse_args()
 
@@ -531,9 +568,11 @@ def init_wandb(args):
         project="rl",
         config=args,
         name=run_name,
+        tensorboard=True,
         sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
         monitor_gym=True,  # auto-upload the videos of agents playing the game
         save_code=True,  # optional
+
     )
     writer = SummaryWriter(f"runs/{run_name}")
     writer.add_text(
@@ -542,41 +581,6 @@ def init_wandb(args):
     )
 
 
-if __name__ == "__main__":
-
-    args = parse_args()
-    print(args)
-    #
-
-    # ## seeding
-    # seed = args.seed
-    # random.seed(seed)
-    # np.random.seed(seed)
-    # th.manual_seed(seed)
-    # th.backends.cudnn.deterministic = False
-    #
-    # device = th.device("cuda" if th.cuda.is_available() and args.cuda else "cpu")
-
-    # targets = Waypoints.up_circle()
-    targets = Waypoints.rnd()
-
-    sim = PBDroneSimulator(targets, target_factor=0)
-
-    init_wandb(args)
-
-    sim.run_full(args)
-    #
-    # sim.run_test()
-
-    # sim.test_saved()
-    #
-
-    # video_recorder.record_video(
-    #     model=PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.04.2023_22.26.05/best_model.zip",
-    #                    video_folder="C:\Files\Egyetem\Szakdolgozat\RL\Sol/results/videos",
-    #                    ))
-
-
 def linear_schedule(initial_value: float) -> Callable[[float], float]:
     """
     Linear learning rate schedule.
@@ -605,26 +609,6 @@ def linear_schedule(initial_value: float) -> Callable[[float], float]:
 # loaded_model.load_replay_buffer("sac_replay_buffer")
 
 
-# import imageio
-# import numpy as np
-#
-# from stable_baselines3 import A2C
-#
-# model = A2C("MlpPolicy", "LunarLander-v2").learn(100_000)
-#
-# images = []
-# obs = model.env.reset()
-# img = model.env.render(mode="rgb_array")
-# for i in range(350):
-#     images.append(img)
-#     action, _ = model.predict(obs)
-#     obs, _, _ ,_ = model.env.step(action)
-#     img = model.env.render(mode="rgb_array")
-#
-# imageio.mimsave("lander_a2c.gif", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)
-
-
-
 def manual_pb_env():
     # Connect to the PyBullet physics server
     # physicsClient = p.connect(p.GUI)
@@ -647,3 +631,38 @@ def manual_pb_env():
     # print('time_step_spec.step_type:', tf_env.time_step_spec().step_type)
     # print('time_step_spec.discount:', tf_env.time_step_spec().discount)
     # print('time_step_spec.reward:', tf_env.time_step_spec().reward)
+
+
+if __name__ == "__main__":
+    args = parse_args()
+    print(args)
+    #
+
+    # ## seeding
+    # seed = args.seed
+    # random.seed(seed)
+    # np.random.seed(seed)
+    # th.manual_seed(seed)
+    # th.backends.cudnn.deterministic = False
+    #
+    # device = th.device("cuda" if th.cuda.is_available() and args.cuda else "cpu")
+
+    # targets = Waypoints.up_circle()
+    targets = Waypoints.rnd()
+
+    sim = PBDroneSimulator(targets, target_factor=0)
+
+    init_wandb(args)
+
+    # sim.test_learning()
+    sim.run_full(args)
+    #
+    # sim.run_test()
+
+    # sim.test_saved()
+    #
+
+    # video_recorder.record_video(
+    #     model=PPO.load("C:\Files\Egyetem\Szakdolgozat\RL\Sol\model_chkpts\save-12.04.2023_22.26.05/best_model.zip",
+    #                    video_folder="C:\Files\Egyetem\Szakdolgozat\RL\Sol/results/videos",
+    #                    ))
diff --git a/Sol/Utilities/Callbacks.py b/Sol/Utilities/Callbacks.py
index 80d63b8..6caffd4 100644
--- a/Sol/Utilities/Callbacks.py
+++ b/Sol/Utilities/Callbacks.py
@@ -1,10 +1,12 @@
 import matplotlib.pyplot as plt
 import numpy as np
 from stable_baselines3.common.callbacks import BaseCallback
-from stable_baselines3.common.logger import HParam
+from stable_baselines3.common.logger import HParam, TensorBoardOutputFormat
 from stable_baselines3.common.monitor import load_results
 from stable_baselines3.common.results_plotter import ts2xy
 import wandb
+from torch.utils.tensorboard import SummaryWriter
+
 
 class FoundTargetsCallback(BaseCallback):
     """
@@ -38,61 +40,47 @@ class HParamCallback(BaseCallback):
     """
     Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.
     """
-
     def _on_training_start(self) -> None:
-        hparam_dict = {
-            "algorithm": self.model.__class__.__name__,
-            "learning rate": self.model.learning_rate,
-            "gamma": self.model.gamma,
-            "batch_size": self.model.batch_size,
-            "ent_coef": self.model.ent_coef,
-            "clip_range": self.model.clip_range,
-            "n_epochs": self.model.n_epochs,
-            "n_steps": self.model.n_steps,
-            "vf_coef": self.model.vf_coef,
-            "max_grad_norm": self.model.max_grad_norm,
-            "gae_lambda": self.model.gae_lambda,
-            "policy_kwargs": self.model.policy_kwargs,
-            "policy": self.model.policy,
-            "n_envs": self.model.n_envs,
-
-        }
-        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag
-        # Tensorbaord will find & display metrics from the `SCALARS` tab
-        metric_dict = {
-            "rollout/ep_len_mean": 0,
-            "train/value_loss": 0.0,
-            "train/entropy_loss": 0.0,
-            "train/policy_loss": 0.0,
-            "train/approx_kl": 0.0,
-            "train/clip_fraction": 0.0,
-            "train/clip_range": 0.0,
-            "train/n_updates_total": 0,
-            "train/learning_rate": 0.0,
-            "train/found_targets": 0.0,
-            "train/ep_rew_mean": 0.0,
-            "train/ep_rew_std": 0.0,
-            "train/ep_len_mean": 0.0,
-            "train/ep_len_std": 0.0,
-            "train/success_rate": 0.0,
-            "train/success_rate_std": 0.0,
-            "train/success_rate_mean": 0.0,
-            "train/episodes": 0.0,
-            "train/time_elapsed": 0.0,
-            "train/total_timesteps": 0.0,
-            "train/total_updates": 0.0,
-            "train/explained_variance": 0.0,
-            "train/n_updates": 0.0,
-            "train/serial_timesteps": 0.0,
-            "train/serial_episodes": 0.0,
-            "train/ep_rew_max": 0.0,
-            "train/ep_rew_min": 0.0,
-        }
-        self.logger.record(
-            "hparams",
-            HParam(hparam_dict, metric_dict),
-            exclude=("stdout", "log", "json", "csv"),
-        )
+        self.hparams = self.model.get_parameters()
+
+        # Create a TensorBoard writer
+        log_dir = self.model.tensorboard_log
+        self.writer = SummaryWriter(log_dir)
+
+        # Log hyperparameters
+        for key, value in self.hparams.items():
+            self.writer.add_text("hyperparameters", f"{key}: {value}")
 
     def _on_step(self) -> bool:
-        return True
\ No newline at end of file
+        return True
+
+    def _on_training_end(self) -> None:
+        """
+        This method is called when the training ends.
+        It closes the TensorBoard writer.
+        """
+        if self.writer is not None:
+            self.writer.close()
+
+
+class SummaryWriterCallback(BaseCallback):
+
+    def _on_training_start(self):
+        self._log_freq = 1000  # log every 1000 calls
+
+        output_formats = self.logger.output_formats
+        # Save reference to tensorboard formatter object
+        # note: the failure case (not formatter found) is not handled here, should be done with try/except.
+        self.tb_formatter = next(formatter for formatter in output_formats if isinstance(formatter, TensorBoardOutputFormat))
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self._log_freq == 0:
+            # You can have access to info from the env using self.locals.
+            # for instance, when using one env (index 0 of locals["infos"]):
+            # lap_count = self.locals["infos"][0]["lap_count"]
+            # self.tb_formatter.writer.add_scalar("train/lap_count", lap_count, self.num_timesteps)
+
+            self.tb_formatter.writer.add_text("direct_access", "this is a value", self.num_timesteps)
+            self.tb_formatter.writer.flush()
+
+        return True
diff --git a/Sol/Utilities/HParamCallback.py b/Sol/Utilities/HParamCallback.py
deleted file mode 100644
index 2c22178..0000000
--- a/Sol/Utilities/HParamCallback.py
+++ /dev/null
@@ -1,30 +0,0 @@
-from stable_baselines3 import A2C
-from stable_baselines3.common.callbacks import BaseCallback
-from stable_baselines3.common.logger import HParam
-
-
-class HParamCallback(BaseCallback):
-    """
-    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.
-    """
-
-    def _on_training_start(self) -> None:
-        hparam_dict = {
-            "algorithm": self.model.__class__.__name__,
-            "learning rate": self.model.learning_rate,
-            "gamma": self.model.gamma,
-        }
-        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag
-        # Tensorbaord will find & display metrics from the `SCALARS` tab
-        metric_dict = {
-            "rollout/ep_len_mean": 0,
-            "train/value_loss": 0.0,
-        }
-        self.logger.record(
-            "hparams",
-            HParam(hparam_dict, metric_dict),
-            exclude=("stdout", "log", "json", "csv"),
-        )
-
-    def _on_step(self) -> bool:
-        return True
diff --git a/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc b/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc
index 0f8fc75..0efe45d 100644
Binary files a/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc and b/Sol/Utilities/__pycache__/Callbacks.cpython-38.pyc differ
