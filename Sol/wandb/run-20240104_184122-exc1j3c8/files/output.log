AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO_114
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-190.08 +/- 54.63
Episode length: 221.80 +/- 71.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-220.42 +/- 23.28
Episode length: 190.60 +/- 28.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-203.66 +/- 23.20
Episode length: 217.00 +/- 39.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
Eval num_timesteps=7968, episode_reward=-210.34 +/- 48.33
Episode length: 214.20 +/- 60.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-193.98 +/- 25.53
Episode length: 229.80 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-204.85 +/- 37.95
Episode length: 202.40 +/- 40.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-198.32 +/- 34.32
Episode length: 221.60 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Traceback (most recent call last):
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] The pipe has been ended
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 658, in <module>
    sim.run_full(args)
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 393, in run_full
    model.learn(total_timesteps=int(5e6),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 200, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 219, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 460, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\evaluation.py", line 84, in evaluate_policy
    observations = env.reset()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 137, in reset
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 137, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 321, in _recv_bytes
    raise EOFError
EOFError