AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py:155: UserWarning: You have specified a mini-batch size of 49152, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=2048 and n_envs=1)
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO 01.08.2024_11.47.55_1
Eval num_timesteps=2000, episode_reward=-112.84 +/- 56.78
Episode length: 246.60 +/- 70.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-140.09 +/- 51.36
Episode length: 215.40 +/- 43.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 215       |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 4000      |
| train/                  |           |
|    approx_kl            | 0.0085294 |
|    clip_fraction        | 0.0397    |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.68     |
|    explained_variance   | -2.63e-05 |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+03  |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.00697  |
|    std                  | 1         |
|    value_loss           | 3.26e+03  |
---------------------------------------
Eval num_timesteps=6000, episode_reward=-125.34 +/- 43.88
Episode length: 230.40 +/- 41.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 6000         |
| train/                  |              |
|    approx_kl            | 0.0028840117 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0563       |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+03      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00203     |
|    std                  | 1            |
|    value_loss           | 3.44e+03     |
------------------------------------------
Eval num_timesteps=8000, episode_reward=-120.92 +/- 27.06
Episode length: 246.20 +/- 31.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 8000         |
| train/                  |              |
|    approx_kl            | 0.0033121682 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0316       |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1            |
|    value_loss           | 2.79e+03     |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-118.83 +/- 21.22
Episode length: 246.00 +/- 30.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.004625364 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.001       |
|    loss                 | 1.7e+03     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00323    |
|    std                  | 0.999       |
|    value_loss           | 3.42e+03    |
-----------------------------------------
Eval num_timesteps=12000, episode_reward=-106.67 +/- 19.19
Episode length: 305.20 +/- 30.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | -107        |
| time/                   |             |
|    total_timesteps      | 12000       |
| train/                  |             |
|    approx_kl            | 0.002628095 |
|    clip_fraction        | 0.00278     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.0706      |
|    learning_rate        | 0.001       |
|    loss                 | 1.66e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000919   |
|    std                  | 1           |
|    value_loss           | 3.35e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=14000, episode_reward=-119.79 +/- 40.50
Episode length: 269.80 +/- 89.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 270        |
|    mean_reward          | -120       |
| time/                   |            |
|    total_timesteps      | 14000      |
| train/                  |            |
|    approx_kl            | 0.00367097 |
|    clip_fraction        | 0.00571    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.68      |
|    explained_variance   | 0.102      |
|    learning_rate        | 0.001      |
|    loss                 | 1.52e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00285   |
|    std                  | 1          |
|    value_loss           | 3.11e+03   |
----------------------------------------
Eval num_timesteps=16000, episode_reward=-129.81 +/- 55.11
Episode length: 249.40 +/- 79.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 16000        |
| train/                  |              |
|    approx_kl            | 0.0014394587 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000972    |
|    std                  | 1            |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=-106.61 +/- 23.13
Episode length: 281.00 +/- 38.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -107         |
| time/                   |              |
|    total_timesteps      | 18000        |
| train/                  |              |
|    approx_kl            | 0.0008402229 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.01         |
|    value_loss           | 2.45e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-130.84 +/- 53.84
Episode length: 237.80 +/- 56.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0012924211 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000803    |
|    std                  | 1.01         |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=22000, episode_reward=-114.21 +/- 24.13
Episode length: 241.20 +/- 21.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 241           |
|    mean_reward          | -114          |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 0.00078787265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.71         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+03      |
|    n_updates            | 100           |
|    policy_gradient_loss | -0.00115      |
|    std                  | 1.01          |
|    value_loss           | 2.64e+03      |
-------------------------------------------
Eval num_timesteps=24000, episode_reward=-123.88 +/- 13.96
Episode length: 241.20 +/- 27.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 0.0012771292 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.01         |
|    value_loss           | 2.29e+03     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-71.97 +/- 42.83
Episode length: 324.80 +/- 44.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | -72          |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 0.0006382183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000461    |
|    std                  | 1.01         |
|    value_loss           | 2.38e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=28000, episode_reward=-98.71 +/- 48.44
Episode length: 291.20 +/- 55.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | -98.7        |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 0.0009354926 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.001        |
|    loss                 | 951          |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.01         |
|    value_loss           | 1.97e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-80.09 +/- 72.29
Episode length: 298.80 +/- 83.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | -80.1        |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0022884274 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.01         |
|    value_loss           | 2.55e+03     |
------------------------------------------
Eval num_timesteps=32000, episode_reward=-136.84 +/- 39.02
Episode length: 245.60 +/- 66.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 32000       |
| train/                  |             |
|    approx_kl            | 0.002465032 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.73       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00159    |
|    std                  | 1.02        |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=34000, episode_reward=-135.41 +/- 40.29
Episode length: 236.40 +/- 67.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 0.0012685757 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.74        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000543    |
|    std                  | 1.02         |
|    value_loss           | 2.54e+03     |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-158.54 +/- 7.12
Episode length: 206.80 +/- 15.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 207           |
|    mean_reward          | -159          |
| time/                   |               |
|    total_timesteps      | 36000         |
| train/                  |               |
|    approx_kl            | 0.00019058707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.74         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+03      |
|    n_updates            | 170           |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.02          |
|    value_loss           | 2.97e+03      |
-------------------------------------------
Eval num_timesteps=38000, episode_reward=-129.72 +/- 61.64
Episode length: 229.80 +/- 61.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 230           |
|    mean_reward          | -130          |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 0.00072516676 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.75         |
|    explained_variance   | 0.446         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000736     |
|    std                  | 1.02          |
|    value_loss           | 2.33e+03      |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-140.93 +/- 49.46
Episode length: 217.60 +/- 33.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | -141         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0022774832 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.76        |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.02         |
|    value_loss           | 2.61e+03     |
------------------------------------------
Eval num_timesteps=42000, episode_reward=29.62 +/- 120.61
Episode length: 400.20 +/- 128.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 29.6         |
| time/                   |              |
|    total_timesteps      | 42000        |
| train/                  |              |
|    approx_kl            | 0.0025566095 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.001        |
|    loss                 | 989          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.03         |
|    value_loss           | 2.01e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=44000, episode_reward=-45.50 +/- 128.27
Episode length: 415.80 +/- 191.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | -45.5        |
| time/                   |              |
|    total_timesteps      | 44000        |
| train/                  |              |
|    approx_kl            | 0.0026601045 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.001        |
|    loss                 | 704          |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.03         |
|    value_loss           | 1.42e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-31.43 +/- 145.40
Episode length: 355.40 +/- 130.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 355          |
|    mean_reward          | -31.4        |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0033045304 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.03         |
|    value_loss           | 2.17e+03     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-58.25 +/- 55.81
Episode length: 372.60 +/- 86.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | -58.3        |
| time/                   |              |
|    total_timesteps      | 48000        |
| train/                  |              |
|    approx_kl            | 0.0049638026 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.82        |
|    explained_variance   | 0.489        |
|    learning_rate        | 0.001        |
|    loss                 | 883          |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00375     |
|    std                  | 1.04         |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-57.49 +/- 59.58
Episode length: 364.20 +/- 75.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | -57.5         |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00069019385 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.84         |
|    explained_variance   | 0.53          |
|    learning_rate        | 0.001         |
|    loss                 | 675           |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.04          |
|    value_loss           | 1.38e+03      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=-28.48 +/- 75.87
Episode length: 345.80 +/- 66.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | -28.5        |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0011702944 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.001        |
|    loss                 | 810          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00161     |
|    std                  | 1.05         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-49.72 +/- 37.95
Episode length: 389.40 +/- 87.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | -49.7        |
| time/                   |              |
|    total_timesteps      | 54000        |
| train/                  |              |
|    approx_kl            | 0.0028426987 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.001        |
|    loss                 | 792          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00397     |
|    std                  | 1.05         |
|    value_loss           | 1.6e+03      |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-12.61 +/- 39.88
Episode length: 451.60 +/- 91.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | -12.6        |
| time/                   |              |
|    total_timesteps      | 56000        |
| train/                  |              |
|    approx_kl            | 0.0039555775 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.001        |
|    loss                 | 543          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.06         |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-41.10 +/- 49.23
Episode length: 405.20 +/- 82.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | -41.1        |
| time/                   |              |
|    total_timesteps      | 58000        |
| train/                  |              |
|    approx_kl            | 0.0015487091 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.91        |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.001        |
|    loss                 | 664          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00109     |
|    std                  | 1.06         |
|    value_loss           | 1.36e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-8.38 +/- 41.08
Episode length: 533.80 +/- 57.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 534          |
|    mean_reward          | -8.38        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0023949652 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.635        |
|    learning_rate        | 0.001        |
|    loss                 | 658          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.06         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-27.21 +/- 21.33
Episode length: 451.60 +/- 70.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | -27.2        |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0025894663 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.66         |
|    learning_rate        | 0.001        |
|    loss                 | 528          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.07         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-75.76 +/- 33.81
Episode length: 379.20 +/- 102.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | -75.8        |
| time/                   |              |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 0.0031362975 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.001        |
|    loss                 | 656          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.07         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=66000, episode_reward=-38.74 +/- 56.16
Episode length: 466.20 +/- 127.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | -38.7        |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0107475985 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 390          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00385     |
|    std                  | 1.07         |
|    value_loss           | 789          |
------------------------------------------
Eval num_timesteps=68000, episode_reward=-97.40 +/- 18.33
Episode length: 327.00 +/- 55.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 327          |
|    mean_reward          | -97.4        |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0025528031 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.645        |
|    learning_rate        | 0.001        |
|    loss                 | 749          |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.07         |
|    value_loss           | 1.52e+03     |
------------------------------------------
Eval num_timesteps=70000, episode_reward=-116.28 +/- 14.82
Episode length: 289.80 +/- 26.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 290         |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.004330232 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.001       |
|    loss                 | 835         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00237    |
|    std                  | 1.07        |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=72000, episode_reward=-137.05 +/- 26.53
Episode length: 251.60 +/- 50.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.003921655 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.001       |
|    loss                 | 674         |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00149    |
|    std                  | 1.07        |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=74000, episode_reward=-95.26 +/- 47.59
Episode length: 281.40 +/- 61.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | -95.3       |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.005012417 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.001       |
|    loss                 | 608         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00182    |
|    std                  | 1.07        |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=76000, episode_reward=-121.93 +/- 12.90
Episode length: 268.60 +/- 20.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 269           |
|    mean_reward          | -122          |
| time/                   |               |
|    total_timesteps      | 76000         |
| train/                  |               |
|    approx_kl            | 0.00097033265 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 946           |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.00108      |
|    std                  | 1.07          |
|    value_loss           | 1.94e+03      |
-------------------------------------------
Eval num_timesteps=78000, episode_reward=-120.56 +/- 35.32
Episode length: 253.40 +/- 55.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | -121        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.012942723 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.001       |
|    loss                 | 714         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.07        |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=-142.12 +/- 41.91
Episode length: 233.20 +/- 69.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0065776156 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.693        |
|    learning_rate        | 0.001        |
|    loss                 | 596          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0032      |
|    std                  | 1.07         |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=82000, episode_reward=-110.35 +/- 47.75
Episode length: 298.00 +/- 97.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | -110         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0012628009 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.664        |
|    learning_rate        | 0.001        |
|    loss                 | 826          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000992    |
|    std                  | 1.07         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=84000, episode_reward=-95.43 +/- 50.63
Episode length: 304.40 +/- 83.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | -95.4        |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0016812116 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.678        |
|    learning_rate        | 0.001        |
|    loss                 | 814          |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.07         |
|    value_loss           | 1.67e+03     |
------------------------------------------
Eval num_timesteps=86000, episode_reward=-114.72 +/- 27.16
Episode length: 296.40 +/- 45.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-90.01 +/- 49.57
Episode length: 314.00 +/- 83.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 314           |
|    mean_reward          | -90           |
| time/                   |               |
|    total_timesteps      | 88000         |
| train/                  |               |
|    approx_kl            | 0.00059005385 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.662         |
|    learning_rate        | 0.001         |
|    loss                 | 967           |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000596     |
|    std                  | 1.07          |
|    value_loss           | 1.98e+03      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-117.07 +/- 64.64
Episode length: 277.20 +/- 107.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 277           |
|    mean_reward          | -117          |
| time/                   |               |
|    total_timesteps      | 90000         |
| train/                  |               |
|    approx_kl            | 0.00047045044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.703         |
|    learning_rate        | 0.001         |
|    loss                 | 707           |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.00124      |
|    std                  | 1.07          |
|    value_loss           | 1.44e+03      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=-143.67 +/- 37.96
Episode length: 247.00 +/- 69.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 92000        |
| train/                  |              |
|    approx_kl            | 0.0030211576 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.96        |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 817          |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.07         |
|    value_loss           | 1.66e+03     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-110.30 +/- 18.17
Episode length: 288.80 +/- 30.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | -110        |
| time/                   |             |
|    total_timesteps      | 94000       |
| train/                  |             |
|    approx_kl            | 0.006502066 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.001       |
|    loss                 | 704         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00432    |
|    std                  | 1.08        |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=-82.24 +/- 70.45
Episode length: 316.00 +/- 95.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | -82.2         |
| time/                   |               |
|    total_timesteps      | 96000         |
| train/                  |               |
|    approx_kl            | 0.00083652127 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.98         |
|    explained_variance   | 0.656         |
|    learning_rate        | 0.001         |
|    loss                 | 917           |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000803     |
|    std                  | 1.08          |
|    value_loss           | 1.85e+03      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=-120.19 +/- 51.15
Episode length: 259.20 +/- 61.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 259           |
|    mean_reward          | -120          |
| time/                   |               |
|    total_timesteps      | 98000         |
| train/                  |               |
|    approx_kl            | 0.00033029553 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.98         |
|    explained_variance   | 0.682         |
|    learning_rate        | 0.001         |
|    loss                 | 923           |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000321     |
|    std                  | 1.08          |
|    value_loss           | 1.86e+03      |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-146.36 +/- 35.03
Episode length: 227.40 +/- 54.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | -146        |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.001138022 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.001       |
|    loss                 | 915         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00151    |
|    std                  | 1.08        |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=102000, episode_reward=-137.14 +/- 29.11
Episode length: 251.20 +/- 61.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 102000       |
| train/                  |              |
|    approx_kl            | 0.0023559912 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.656        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00223     |
|    std                  | 1.08         |
|    value_loss           | 2.02e+03     |
------------------------------------------
Eval num_timesteps=104000, episode_reward=-89.19 +/- 51.13
Episode length: 307.80 +/- 74.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -89.2        |
| time/                   |              |
|    total_timesteps      | 104000       |
| train/                  |              |
|    approx_kl            | 0.0009777483 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 771          |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000763    |
|    std                  | 1.08         |
|    value_loss           | 1.57e+03     |
------------------------------------------
Eval num_timesteps=106000, episode_reward=-87.59 +/- 86.94
Episode length: 296.00 +/- 111.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | -87.6        |
| time/                   |              |
|    total_timesteps      | 106000       |
| train/                  |              |
|    approx_kl            | 0.0021851258 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.001        |
|    loss                 | 892          |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.08         |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=108000, episode_reward=-71.33 +/- 66.22
Episode length: 341.20 +/- 102.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | -71.3        |
| time/                   |              |
|    total_timesteps      | 108000       |
| train/                  |              |
|    approx_kl            | 0.0016708465 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 779          |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000538    |
|    std                  | 1.08         |
|    value_loss           | 1.6e+03      |
------------------------------------------
Eval num_timesteps=110000, episode_reward=-65.05 +/- 48.81
Episode length: 350.60 +/- 83.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 351          |
|    mean_reward          | -65.1        |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0010936917 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6           |
|    explained_variance   | 0.713        |
|    learning_rate        | 0.001        |
|    loss                 | 774          |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.08         |
|    value_loss           | 1.56e+03     |
------------------------------------------
Eval num_timesteps=112000, episode_reward=-106.60 +/- 50.39
Episode length: 287.20 +/- 79.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | -107        |
| time/                   |             |
|    total_timesteps      | 112000      |
| train/                  |             |
|    approx_kl            | 0.000921373 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.001       |
|    loss                 | 664         |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00131    |
|    std                  | 1.09        |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=114000, episode_reward=-122.81 +/- 66.02
Episode length: 253.20 +/- 85.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | -123         |
| time/                   |              |
|    total_timesteps      | 114000       |
| train/                  |              |
|    approx_kl            | 0.0018262492 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.001        |
|    loss                 | 763          |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.09         |
|    value_loss           | 1.55e+03     |
------------------------------------------
Eval num_timesteps=116000, episode_reward=-127.11 +/- 54.70
Episode length: 277.20 +/- 100.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 116000       |
| train/                  |              |
|    approx_kl            | 0.0018787023 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.001        |
|    loss                 | 760          |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.09         |
|    value_loss           | 1.54e+03     |
------------------------------------------
Eval num_timesteps=118000, episode_reward=-171.53 +/- 7.32
Episode length: 186.20 +/- 9.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 186          |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 118000       |
| train/                  |              |
|    approx_kl            | 0.0029605983 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.734        |
|    learning_rate        | 0.001        |
|    loss                 | 657          |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.09         |
|    value_loss           | 1.33e+03     |
------------------------------------------
Eval num_timesteps=120000, episode_reward=-95.70 +/- 61.36
Episode length: 295.00 +/- 87.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | -95.7        |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0011118986 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.001        |
|    loss                 | 856          |
|    n_updates            | 580          |
|    policy_gradient_loss | 0.000197     |
|    std                  | 1.09         |
|    value_loss           | 1.76e+03     |
------------------------------------------
Eval num_timesteps=122000, episode_reward=-97.96 +/- 81.87
Episode length: 306.80 +/- 110.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | -98          |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0031865796 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.001        |
|    loss                 | 438          |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00304     |
|    std                  | 1.09         |
|    value_loss           | 889          |
------------------------------------------
Eval num_timesteps=124000, episode_reward=-114.90 +/- 45.11
Episode length: 291.60 +/- 85.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 292          |
|    mean_reward          | -115         |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 0.0041866875 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.02        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 689          |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00193     |
|    std                  | 1.09         |
|    value_loss           | 1.46e+03     |
------------------------------------------
Eval num_timesteps=126000, episode_reward=-56.82 +/- 66.79
Episode length: 373.80 +/- 88.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | -56.8       |
| time/                   |             |
|    total_timesteps      | 126000      |
| train/                  |             |
|    approx_kl            | 0.002130798 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.001       |
|    loss                 | 528         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00197    |
|    std                  | 1.09        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=128000, episode_reward=-70.35 +/- 62.05
Episode length: 374.40 +/- 117.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | -70.3        |
| time/                   |              |
|    total_timesteps      | 128000       |
| train/                  |              |
|    approx_kl            | 0.0018474897 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 742          |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000305    |
|    std                  | 1.09         |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-98.06 +/- 53.54
Episode length: 314.00 +/- 86.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | -98.1        |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0023838647 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.03        |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 748          |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.09         |
|    value_loss           | 1.51e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-68.10 +/- 49.38
Episode length: 321.00 +/- 59.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | -68.1       |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.004478093 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.001       |
|    loss                 | 626         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00247    |
|    std                  | 1.1         |
|    value_loss           | 1.27e+03    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-98.43 +/- 51.51
Episode length: 308.00 +/- 67.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -98.4        |
| time/                   |              |
|    total_timesteps      | 134000       |
| train/                  |              |
|    approx_kl            | 0.0017550603 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.776        |
|    learning_rate        | 0.001        |
|    loss                 | 622          |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.000315    |
|    std                  | 1.1          |
|    value_loss           | 1.27e+03     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-113.99 +/- 29.67
Episode length: 281.00 +/- 51.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | -114         |
| time/                   |              |
|    total_timesteps      | 136000       |
| train/                  |              |
|    approx_kl            | 0.0021363136 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.704        |
|    learning_rate        | 0.001        |
|    loss                 | 832          |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.1          |
|    value_loss           | 1.73e+03     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-80.42 +/- 79.49
Episode length: 329.20 +/- 133.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | -80.4        |
| time/                   |              |
|    total_timesteps      | 138000       |
| train/                  |              |
|    approx_kl            | 0.0014091064 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.758        |
|    learning_rate        | 0.001        |
|    loss                 | 750          |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.1          |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-86.37 +/- 33.77
Episode length: 307.60 +/- 58.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | -86.4        |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0012493368 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.001        |
|    loss                 | 729          |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.1          |
|    value_loss           | 1.47e+03     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-138.76 +/- 39.21
Episode length: 238.40 +/- 61.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 238           |
|    mean_reward          | -139          |
| time/                   |               |
|    total_timesteps      | 142000        |
| train/                  |               |
|    approx_kl            | 0.00065209717 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.06         |
|    explained_variance   | 0.768         |
|    learning_rate        | 0.001         |
|    loss                 | 675           |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000995     |
|    std                  | 1.1           |
|    value_loss           | 1.38e+03      |
-------------------------------------------
Eval num_timesteps=144000, episode_reward=-128.48 +/- 22.70
Episode length: 266.00 +/- 51.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 266           |
|    mean_reward          | -128          |
| time/                   |               |
|    total_timesteps      | 144000        |
| train/                  |               |
|    approx_kl            | 0.00069751195 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.75          |
|    learning_rate        | 0.001         |
|    loss                 | 830           |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000239     |
|    std                  | 1.1           |
|    value_loss           | 1.69e+03      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=-49.33 +/- 47.23
Episode length: 378.60 +/- 80.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | -49.3        |
| time/                   |              |
|    total_timesteps      | 146000       |
| train/                  |              |
|    approx_kl            | 0.0012283282 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 609          |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.1          |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-99.79 +/- 55.78
Episode length: 298.60 +/- 97.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | -99.8         |
| time/                   |               |
|    total_timesteps      | 148000        |
| train/                  |               |
|    approx_kl            | 0.00063365465 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.07         |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.001         |
|    loss                 | 713           |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000416     |
|    std                  | 1.11          |
|    value_loss           | 1.45e+03      |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=12.09 +/- 86.25
Episode length: 429.80 +/- 99.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0016270678 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.08        |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 634          |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00192     |
|    std                  | 1.11         |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=152000, episode_reward=-47.34 +/- 37.12
Episode length: 423.00 +/- 100.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | -47.3       |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.004483111 |
|    clip_fraction        | 0.00903     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.001       |
|    loss                 | 510         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.11        |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=154000, episode_reward=98.54 +/- 205.32
Episode length: 511.00 +/- 182.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | 98.5         |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0032039504 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 526          |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.11         |
|    value_loss           | 1.07e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=156000, episode_reward=48.33 +/- 56.61
Episode length: 607.00 +/- 94.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 48.3         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0063198535 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 416          |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00537     |
|    std                  | 1.11         |
|    value_loss           | 883          |
------------------------------------------
Eval num_timesteps=158000, episode_reward=190.14 +/- 99.82
Episode length: 770.40 +/- 137.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 770          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0029511722 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.001        |
|    loss                 | 395          |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0029      |
|    std                  | 1.11         |
|    value_loss           | 827          |
------------------------------------------
New best mean reward!
Eval num_timesteps=160000, episode_reward=360.22 +/- 495.47
Episode length: 677.00 +/- 132.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 677          |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0034135468 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.001        |
|    loss                 | 508          |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00284     |
|    std                  | 1.11         |
|    value_loss           | 1.07e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=162000, episode_reward=182.14 +/- 152.65
Episode length: 727.60 +/- 310.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 728        |
|    mean_reward          | 182        |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.00132553 |
|    clip_fraction        | 0.000928   |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.11      |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.001      |
|    loss                 | 312        |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.00182   |
|    std                  | 1.11       |
|    value_loss           | 808        |
----------------------------------------
Eval num_timesteps=164000, episode_reward=278.38 +/- 207.28
Episode length: 770.00 +/- 143.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 770          |
|    mean_reward          | 278          |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0015673733 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.702        |
|    learning_rate        | 0.001        |
|    loss                 | 555          |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000727    |
|    std                  | 1.12         |
|    value_loss           | 1.15e+03     |
------------------------------------------
Eval num_timesteps=166000, episode_reward=165.77 +/- 98.40
Episode length: 646.60 +/- 93.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 647         |
|    mean_reward          | 166         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.002042999 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.001       |
|    loss                 | 418         |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00176    |
|    std                  | 1.12        |
|    value_loss           | 862         |
-----------------------------------------
Eval num_timesteps=168000, episode_reward=155.80 +/- 250.10
Episode length: 668.20 +/- 140.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 668          |
|    mean_reward          | 156          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0043501537 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.12        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 392          |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00296     |
|    std                  | 1.12         |
|    value_loss           | 844          |
------------------------------------------
Eval num_timesteps=170000, episode_reward=125.50 +/- 168.89
Episode length: 722.40 +/- 192.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 722          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0043889424 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 457          |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00271     |
|    std                  | 1.12         |
|    value_loss           | 972          |
------------------------------------------
Eval num_timesteps=172000, episode_reward=95.27 +/- 96.54
Episode length: 676.40 +/- 159.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 95.3     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=174000, episode_reward=608.90 +/- 900.96
Episode length: 846.00 +/- 292.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 846         |
|    mean_reward          | 609         |
| time/                   |             |
|    total_timesteps      | 174000      |
| train/                  |             |
|    approx_kl            | 0.004640513 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.14       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.001       |
|    loss                 | 296         |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00279    |
|    std                  | 1.13        |
|    value_loss           | 688         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=176000, episode_reward=58.24 +/- 96.05
Episode length: 624.80 +/- 152.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 625          |
|    mean_reward          | 58.2         |
| time/                   |              |
|    total_timesteps      | 176000       |
| train/                  |              |
|    approx_kl            | 0.0044545773 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.15        |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.001        |
|    loss                 | 561          |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.13         |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=94.09 +/- 66.49
Episode length: 742.40 +/- 108.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 742         |
|    mean_reward          | 94.1        |
| time/                   |             |
|    total_timesteps      | 178000      |
| train/                  |             |
|    approx_kl            | 0.005310307 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.001       |
|    loss                 | 362         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0033     |
|    std                  | 1.13        |
|    value_loss           | 806         |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=58.14 +/- 69.86
Episode length: 702.80 +/- 141.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 703         |
|    mean_reward          | 58.1        |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.009105243 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 310         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00469    |
|    std                  | 1.13        |
|    value_loss           | 635         |
-----------------------------------------
Eval num_timesteps=182000, episode_reward=120.07 +/- 118.21
Episode length: 728.60 +/- 143.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 729          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 182000       |
| train/                  |              |
|    approx_kl            | 0.0056299316 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.19        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 405          |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.14         |
|    value_loss           | 865          |
------------------------------------------
Eval num_timesteps=184000, episode_reward=94.13 +/- 107.05
Episode length: 657.40 +/- 167.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 657         |
|    mean_reward          | 94.1        |
| time/                   |             |
|    total_timesteps      | 184000      |
| train/                  |             |
|    approx_kl            | 0.010272846 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.001       |
|    loss                 | 336         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00377    |
|    std                  | 1.14        |
|    value_loss           | 724         |
-----------------------------------------
Eval num_timesteps=186000, episode_reward=579.42 +/- 980.91
Episode length: 844.40 +/- 316.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 844          |
|    mean_reward          | 579          |
| time/                   |              |
|    total_timesteps      | 186000       |
| train/                  |              |
|    approx_kl            | 0.0012811199 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.738        |
|    learning_rate        | 0.001        |
|    loss                 | 428          |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.000767    |
|    std                  | 1.15         |
|    value_loss           | 890          |
------------------------------------------
Eval num_timesteps=188000, episode_reward=32.47 +/- 60.26
Episode length: 644.60 +/- 157.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 645        |
|    mean_reward          | 32.5       |
| time/                   |            |
|    total_timesteps      | 188000     |
| train/                  |            |
|    approx_kl            | 0.01383828 |
|    clip_fraction        | 0.0529     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.001      |
|    loss                 | 342        |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.00651   |
|    std                  | 1.15       |
|    value_loss           | 725        |
----------------------------------------
Eval num_timesteps=190000, episode_reward=158.25 +/- 94.59
Episode length: 744.80 +/- 109.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 745          |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0033675423 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.23        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 307          |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00231     |
|    std                  | 1.15         |
|    value_loss           | 650          |
------------------------------------------
Eval num_timesteps=192000, episode_reward=164.05 +/- 135.77
Episode length: 625.00 +/- 105.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 192000      |
| train/                  |             |
|    approx_kl            | 0.005965256 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.001       |
|    loss                 | 329         |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.16        |
|    value_loss           | 834         |
-----------------------------------------
Eval num_timesteps=194000, episode_reward=203.85 +/- 125.41
Episode length: 724.20 +/- 125.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 724          |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 194000       |
| train/                  |              |
|    approx_kl            | 0.0014380219 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.27        |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 409          |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.16         |
|    value_loss           | 900          |
------------------------------------------
Eval num_timesteps=196000, episode_reward=166.46 +/- 270.38
Episode length: 660.60 +/- 175.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 661          |
|    mean_reward          | 166          |
| time/                   |              |
|    total_timesteps      | 196000       |
| train/                  |              |
|    approx_kl            | 0.0051178937 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.001        |
|    loss                 | 466          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.0032      |
|    std                  | 1.17         |
|    value_loss           | 976          |
------------------------------------------
Eval num_timesteps=198000, episode_reward=162.30 +/- 198.97
Episode length: 657.40 +/- 229.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 657        |
|    mean_reward          | 162        |
| time/                   |            |
|    total_timesteps      | 198000     |
| train/                  |            |
|    approx_kl            | 0.00457043 |
|    clip_fraction        | 0.0109     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.31      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.001      |
|    loss                 | 299        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00242   |
|    std                  | 1.17       |
|    value_loss           | 659        |
----------------------------------------
Eval num_timesteps=200000, episode_reward=152.72 +/- 120.67
Episode length: 807.40 +/- 193.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.004387034 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.33       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00238    |
|    std                  | 1.18        |
|    value_loss           | 576         |
-----------------------------------------
Eval num_timesteps=202000, episode_reward=631.56 +/- 779.86
Episode length: 940.40 +/- 358.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 940         |
|    mean_reward          | 632         |
| time/                   |             |
|    total_timesteps      | 202000      |
| train/                  |             |
|    approx_kl            | 0.010043975 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 280         |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00554    |
|    std                  | 1.19        |
|    value_loss           | 590         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=204000, episode_reward=241.33 +/- 218.86
Episode length: 857.20 +/- 309.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 857          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 204000       |
| train/                  |              |
|    approx_kl            | 0.0040481137 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.36        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.19         |
|    value_loss           | 429          |
------------------------------------------
Eval num_timesteps=206000, episode_reward=164.29 +/- 229.54
Episode length: 801.00 +/- 390.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 801         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 206000      |
| train/                  |             |
|    approx_kl            | 0.002990792 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 222         |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 1.2         |
|    value_loss           | 491         |
-----------------------------------------
Eval num_timesteps=208000, episode_reward=279.12 +/- 240.48
Episode length: 949.20 +/- 252.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 949          |
|    mean_reward          | 279          |
| time/                   |              |
|    total_timesteps      | 208000       |
| train/                  |              |
|    approx_kl            | 0.0023562987 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.4         |
|    explained_variance   | 0.614        |
|    learning_rate        | 0.001        |
|    loss                 | 343          |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 1.2          |
|    value_loss           | 828          |
------------------------------------------
Eval num_timesteps=210000, episode_reward=173.27 +/- 116.82
Episode length: 820.00 +/- 229.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 820          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0039888285 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.41        |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.001        |
|    loss                 | 328          |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.2          |
|    value_loss           | 688          |
------------------------------------------
Eval num_timesteps=212000, episode_reward=216.89 +/- 143.20
Episode length: 856.40 +/- 168.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 856         |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 212000      |
| train/                  |             |
|    approx_kl            | 0.008780886 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.001       |
|    loss                 | 415         |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00378    |
|    std                  | 1.21        |
|    value_loss           | 855         |
-----------------------------------------
Eval num_timesteps=214000, episode_reward=531.82 +/- 444.78
Episode length: 925.20 +/- 142.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 925         |
|    mean_reward          | 532         |
| time/                   |             |
|    total_timesteps      | 214000      |
| train/                  |             |
|    approx_kl            | 0.027356297 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.001       |
|    loss                 | 311         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0077     |
|    std                  | 1.22        |
|    value_loss           | 736         |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=366.83 +/- 249.32
Episode length: 876.80 +/- 164.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 877         |
|    mean_reward          | 367         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.003736406 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 315         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00118    |
|    std                  | 1.22        |
|    value_loss           | 725         |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=276.67 +/- 265.85
Episode length: 743.80 +/- 158.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 744          |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 218000       |
| train/                  |              |
|    approx_kl            | 0.0012992494 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.001        |
|    loss                 | 3.45e+03     |
|    n_updates            | 1060         |
|    policy_gradient_loss | 9.96e-05     |
|    std                  | 1.22         |
|    value_loss           | 7.63e+03     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=331.89 +/- 183.68
Episode length: 927.60 +/- 247.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 928          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0048916643 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 204          |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 1.22         |
|    value_loss           | 439          |
------------------------------------------
Eval num_timesteps=222000, episode_reward=99.57 +/- 85.67
Episode length: 650.80 +/- 73.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 651         |
|    mean_reward          | 99.6        |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.010192127 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.001       |
|    loss                 | 528         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.000726   |
|    std                  | 1.22        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=77.36 +/- 102.66
Episode length: 630.60 +/- 131.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 631          |
|    mean_reward          | 77.4         |
| time/                   |              |
|    total_timesteps      | 224000       |
| train/                  |              |
|    approx_kl            | 0.0027833816 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 330          |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 1.22         |
|    value_loss           | 706          |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-9.77 +/- 37.86
Episode length: 493.00 +/- 91.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | -9.77       |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.006933964 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.001       |
|    loss                 | 418         |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00254    |
|    std                  | 1.23        |
|    value_loss           | 884         |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=129.84 +/- 137.84
Episode length: 680.40 +/- 180.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 680         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.003958728 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.001       |
|    loss                 | 348         |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 1.23        |
|    value_loss           | 718         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-16.89 +/- 16.35
Episode length: 522.20 +/- 24.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | -16.9        |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0067954143 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 440          |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00471     |
|    std                  | 1.23         |
|    value_loss           | 915          |
------------------------------------------
Eval num_timesteps=232000, episode_reward=4.13 +/- 69.93
Episode length: 542.00 +/- 102.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 542          |
|    mean_reward          | 4.13         |
| time/                   |              |
|    total_timesteps      | 232000       |
| train/                  |              |
|    approx_kl            | 0.0060774893 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 460          |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 1.23         |
|    value_loss           | 933          |
------------------------------------------
Eval num_timesteps=234000, episode_reward=45.54 +/- 33.77
Episode length: 605.00 +/- 79.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 605        |
|    mean_reward          | 45.5       |
| time/                   |            |
|    total_timesteps      | 234000     |
| train/                  |            |
|    approx_kl            | 0.00413334 |
|    clip_fraction        | 0.0062     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.51      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.001      |
|    loss                 | 304        |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.00164   |
|    std                  | 1.23       |
|    value_loss           | 646        |
----------------------------------------
Eval num_timesteps=236000, episode_reward=35.65 +/- 165.91
Episode length: 515.20 +/- 263.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 35.6        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.012301223 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 425         |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0045     |
|    std                  | 1.24        |
|    value_loss           | 875         |
-----------------------------------------
Eval num_timesteps=238000, episode_reward=242.16 +/- 236.61
Episode length: 625.60 +/- 228.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 626         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.013862609 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.001       |
|    loss                 | 300         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00428    |
|    std                  | 1.25        |
|    value_loss           | 616         |
-----------------------------------------
Eval num_timesteps=240000, episode_reward=272.90 +/- 248.04
Episode length: 553.40 +/- 136.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 553         |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.011151483 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.56       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.001       |
|    loss                 | 396         |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00393    |
|    std                  | 1.25        |
|    value_loss           | 817         |
-----------------------------------------
Eval num_timesteps=242000, episode_reward=423.39 +/- 317.62
Episode length: 583.20 +/- 64.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 583         |
|    mean_reward          | 423         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.013611348 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.57       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.001       |
|    loss                 | 500         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 1.26        |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=244000, episode_reward=567.19 +/- 328.86
Episode length: 797.20 +/- 116.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 797         |
|    mean_reward          | 567         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.007647325 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.59       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.001       |
|    loss                 | 2.56e+03    |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 1.26        |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=246000, episode_reward=261.90 +/- 216.17
Episode length: 679.60 +/- 229.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 680          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0020854976 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 441          |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 1.26         |
|    value_loss           | 906          |
------------------------------------------
Eval num_timesteps=248000, episode_reward=212.57 +/- 288.20
Episode length: 642.20 +/- 258.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 642         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.020255642 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 312         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00503    |
|    std                  | 1.26        |
|    value_loss           | 671         |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=177.89 +/- 183.57
Episode length: 697.80 +/- 195.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 698         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.008731096 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 227         |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 1.27        |
|    value_loss           | 487         |
-----------------------------------------
Eval num_timesteps=252000, episode_reward=131.64 +/- 147.12
Episode length: 679.00 +/- 234.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 132         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.012941811 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.62       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.001       |
|    loss                 | 306         |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00587    |
|    std                  | 1.27        |
|    value_loss           | 638         |
-----------------------------------------
Eval num_timesteps=254000, episode_reward=240.36 +/- 85.15
Episode length: 838.40 +/- 57.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 838          |
|    mean_reward          | 240          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0065697716 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.61        |
|    explained_variance   | 0.726        |
|    learning_rate        | 0.001        |
|    loss                 | 444          |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00332     |
|    std                  | 1.27         |
|    value_loss           | 951          |
------------------------------------------
Eval num_timesteps=256000, episode_reward=55.90 +/- 84.29
Episode length: 585.80 +/- 179.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 586      |
|    mean_reward     | 55.9     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=258000, episode_reward=188.75 +/- 141.97
Episode length: 686.20 +/- 130.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 686         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 258000      |
| train/                  |             |
|    approx_kl            | 0.015926696 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.001       |
|    loss                 | 313         |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00588    |
|    std                  | 1.27        |
|    value_loss           | 654         |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=137.47 +/- 157.36
Episode length: 630.20 +/- 145.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 630          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0050754766 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.63        |
|    explained_variance   | 0.67         |
|    learning_rate        | 0.001        |
|    loss                 | 492          |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.28         |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=262000, episode_reward=-40.95 +/- 103.97
Episode length: 381.20 +/- 163.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | -41         |
| time/                   |             |
|    total_timesteps      | 262000      |
| train/                  |             |
|    approx_kl            | 0.009216495 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.001       |
|    loss                 | 486         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00458    |
|    std                  | 1.28        |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=264000, episode_reward=-100.53 +/- 58.17
Episode length: 298.40 +/- 115.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 298         |
|    mean_reward          | -101        |
| time/                   |             |
|    total_timesteps      | 264000      |
| train/                  |             |
|    approx_kl            | 0.012498142 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.67       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 381         |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 1.29        |
|    value_loss           | 784         |
-----------------------------------------
Eval num_timesteps=266000, episode_reward=-142.70 +/- 28.64
Episode length: 222.20 +/- 60.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 266000      |
| train/                  |             |
|    approx_kl            | 0.009212378 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.68       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.001       |
|    loss                 | 673         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00349    |
|    std                  | 1.29        |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=268000, episode_reward=-74.36 +/- 42.47
Episode length: 323.20 +/- 67.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | -74.4       |
| time/                   |             |
|    total_timesteps      | 268000      |
| train/                  |             |
|    approx_kl            | 0.006751897 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 701         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 1.29        |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=0.74 +/- 112.13
Episode length: 492.00 +/- 254.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 0.74         |
| time/                   |              |
|    total_timesteps      | 270000       |
| train/                  |              |
|    approx_kl            | 0.0040245666 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.669        |
|    learning_rate        | 0.001        |
|    loss                 | 548          |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1.3          |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=272000, episode_reward=-22.86 +/- 60.96
Episode length: 438.80 +/- 107.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | -22.9        |
| time/                   |              |
|    total_timesteps      | 272000       |
| train/                  |              |
|    approx_kl            | 0.0006322592 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.001        |
|    loss                 | 702          |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000168    |
|    std                  | 1.3          |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=274000, episode_reward=-76.66 +/- 19.36
Episode length: 330.40 +/- 43.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 330         |
|    mean_reward          | -76.7       |
| time/                   |             |
|    total_timesteps      | 274000      |
| train/                  |             |
|    approx_kl            | 0.004944398 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | 598         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.3         |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=-23.75 +/- 25.18
Episode length: 410.80 +/- 34.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | -23.8        |
| time/                   |              |
|    total_timesteps      | 276000       |
| train/                  |              |
|    approx_kl            | 0.0073527824 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 272          |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1.3          |
|    value_loss           | 566          |
------------------------------------------
Eval num_timesteps=278000, episode_reward=69.80 +/- 94.34
Episode length: 601.60 +/- 201.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 602         |
|    mean_reward          | 69.8        |
| time/                   |             |
|    total_timesteps      | 278000      |
| train/                  |             |
|    approx_kl            | 0.007956503 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.74       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.001       |
|    loss                 | 507         |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 1.31        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=159.50 +/- 87.27
Episode length: 766.20 +/- 134.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 766        |
|    mean_reward          | 160        |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.00246539 |
|    clip_fraction        | 0.002      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.75      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.001      |
|    loss                 | 314        |
|    n_updates            | 1360       |
|    policy_gradient_loss | 0.000566   |
|    std                  | 1.31       |
|    value_loss           | 685        |
----------------------------------------
Eval num_timesteps=282000, episode_reward=136.88 +/- 116.86
Episode length: 672.20 +/- 154.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 672         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 282000      |
| train/                  |             |
|    approx_kl            | 0.012567889 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.76       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 320         |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00392    |
|    std                  | 1.32        |
|    value_loss           | 681         |
-----------------------------------------
Eval num_timesteps=284000, episode_reward=316.97 +/- 294.56
Episode length: 767.20 +/- 263.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 767          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 0.0054043597 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.78        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 227          |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00251     |
|    std                  | 1.32         |
|    value_loss           | 539          |
------------------------------------------
Eval num_timesteps=286000, episode_reward=284.07 +/- 299.95
Episode length: 610.00 +/- 201.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 610         |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 286000      |
| train/                  |             |
|    approx_kl            | 0.009829946 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 244         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00624    |
|    std                  | 1.33        |
|    value_loss           | 551         |
-----------------------------------------
Eval num_timesteps=288000, episode_reward=306.18 +/- 427.19
Episode length: 649.80 +/- 220.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 650         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 288000      |
| train/                  |             |
|    approx_kl            | 0.007249461 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.81       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.001       |
|    loss                 | 380         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 1.33        |
|    value_loss           | 781         |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=77.84 +/- 116.23
Episode length: 628.00 +/- 183.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 628         |
|    mean_reward          | 77.8        |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.013684451 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.001       |
|    loss                 | 389         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00461    |
|    std                  | 1.34        |
|    value_loss           | 798         |
-----------------------------------------
Eval num_timesteps=292000, episode_reward=131.29 +/- 132.30
Episode length: 633.40 +/- 153.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 633         |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 292000      |
| train/                  |             |
|    approx_kl            | 0.006160868 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 321         |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 1.35        |
|    value_loss           | 701         |
-----------------------------------------
Eval num_timesteps=294000, episode_reward=176.47 +/- 82.18
Episode length: 649.80 +/- 52.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 650         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 294000      |
| train/                  |             |
|    approx_kl            | 0.008899244 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 266         |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 1.35        |
|    value_loss           | 612         |
-----------------------------------------
Eval num_timesteps=296000, episode_reward=59.66 +/- 72.92
Episode length: 590.00 +/- 72.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 590        |
|    mean_reward          | 59.7       |
| time/                   |            |
|    total_timesteps      | 296000     |
| train/                  |            |
|    approx_kl            | 0.01551274 |
|    clip_fraction        | 0.0789     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.89      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.001      |
|    loss                 | 299        |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.00308   |
|    std                  | 1.36       |
|    value_loss           | 587        |
----------------------------------------
Eval num_timesteps=298000, episode_reward=146.80 +/- 77.10
Episode length: 633.40 +/- 46.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 633           |
|    mean_reward          | 147           |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 0.00093005423 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.9          |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 413           |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000193     |
|    std                  | 1.36          |
|    value_loss           | 869           |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=70.15 +/- 63.10
Episode length: 616.80 +/- 26.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 617         |
|    mean_reward          | 70.2        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.002697747 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.001       |
|    loss                 | 291         |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.36        |
|    value_loss           | 646         |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=56.77 +/- 88.92
Episode length: 579.80 +/- 60.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 580         |
|    mean_reward          | 56.8        |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.008650675 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 317         |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00421    |
|    std                  | 1.36        |
|    value_loss           | 700         |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=134.79 +/- 136.26
Episode length: 628.80 +/- 120.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 629         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.007556814 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 254         |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 1.36        |
|    value_loss           | 537         |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=96.65 +/- 79.38
Episode length: 639.20 +/- 64.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 639          |
|    mean_reward          | 96.7         |
| time/                   |              |
|    total_timesteps      | 306000       |
| train/                  |              |
|    approx_kl            | 0.0015308829 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.92        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 244          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.000229    |
|    std                  | 1.37         |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=308000, episode_reward=16.20 +/- 29.29
Episode length: 564.20 +/- 46.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 564         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.014878204 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 315         |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00408    |
|    std                  | 1.37        |
|    value_loss           | 647         |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=96.60 +/- 151.19
Episode length: 595.20 +/- 85.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 595         |
|    mean_reward          | 96.6        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.006509998 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 309         |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 1.38        |
|    value_loss           | 633         |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=49.50 +/- 75.35
Episode length: 517.60 +/- 89.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 49.5        |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.006070341 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 244         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0033     |
|    std                  | 1.38        |
|    value_loss           | 496         |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=-4.41 +/- 66.98
Episode length: 528.20 +/- 105.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 528          |
|    mean_reward          | -4.41        |
| time/                   |              |
|    total_timesteps      | 314000       |
| train/                  |              |
|    approx_kl            | 0.0045157317 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 311          |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00256     |
|    std                  | 1.38         |
|    value_loss           | 642          |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-20.40 +/- 23.82
Episode length: 507.40 +/- 43.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | -20.4       |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.008154616 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 312         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 1.39        |
|    value_loss           | 640         |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=-5.62 +/- 71.02
Episode length: 491.60 +/- 65.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 492         |
|    mean_reward          | -5.62       |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.002017221 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 295         |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.000693   |
|    std                  | 1.39        |
|    value_loss           | 605         |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-22.68 +/- 65.37
Episode length: 494.40 +/- 75.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -22.7        |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0039674304 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 293          |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00228     |
|    std                  | 1.4          |
|    value_loss           | 599          |
------------------------------------------
Eval num_timesteps=322000, episode_reward=-6.16 +/- 68.80
Episode length: 498.20 +/- 85.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | -6.16        |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0051369355 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.03        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 287          |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 1.41         |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=324000, episode_reward=9.48 +/- 44.15
Episode length: 551.20 +/- 35.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 551         |
|    mean_reward          | 9.48        |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.016081616 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 284         |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 1.41        |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=326000, episode_reward=-17.08 +/- 63.68
Episode length: 499.80 +/- 82.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -17.1       |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.006167679 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 282         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 1.41        |
|    value_loss           | 585         |
-----------------------------------------
Eval num_timesteps=328000, episode_reward=7.12 +/- 56.61
Episode length: 535.00 +/- 60.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 535          |
|    mean_reward          | 7.12         |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0056390855 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 275          |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 1.42         |
|    value_loss           | 570          |
------------------------------------------
Eval num_timesteps=330000, episode_reward=33.38 +/- 69.16
Episode length: 557.80 +/- 107.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 558          |
|    mean_reward          | 33.4         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0066908547 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.07        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 266          |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.43         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=332000, episode_reward=22.30 +/- 85.15
Episode length: 543.40 +/- 87.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 543         |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.008175997 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00595    |
|    std                  | 1.44        |
|    value_loss           | 404         |
-----------------------------------------
Eval num_timesteps=334000, episode_reward=-17.66 +/- 27.91
Episode length: 515.60 +/- 34.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 516        |
|    mean_reward          | -17.7      |
| time/                   |            |
|    total_timesteps      | 334000     |
| train/                  |            |
|    approx_kl            | 0.01731756 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.14      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.001      |
|    loss                 | 257        |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.00582   |
|    std                  | 1.45       |
|    value_loss           | 524        |
----------------------------------------
Eval num_timesteps=336000, episode_reward=-29.29 +/- 49.98
Episode length: 472.00 +/- 79.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | -29.3        |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0067068404 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 328          |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 1.46         |
|    value_loss           | 724          |
------------------------------------------
Eval num_timesteps=338000, episode_reward=-31.25 +/- 23.04
Episode length: 515.40 +/- 30.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -31.2        |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0075062653 |
|    clip_fraction        | 0.0957       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 315          |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.47         |
|    value_loss           | 665          |
------------------------------------------
Eval num_timesteps=340000, episode_reward=-55.70 +/- 35.55
Episode length: 442.00 +/- 58.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 442        |
|    mean_reward          | -55.7      |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.00978043 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.23      |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.001      |
|    loss                 | 334        |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.00221   |
|    std                  | 1.48       |
|    value_loss           | 756        |
----------------------------------------
Eval num_timesteps=342000, episode_reward=-36.97 +/- 46.98
Episode length: 481.40 +/- 82.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-42.80 +/- 26.09
Episode length: 454.20 +/- 66.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | -42.8       |
| time/                   |             |
|    total_timesteps      | 344000      |
| train/                  |             |
|    approx_kl            | 0.008063167 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 311         |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.49        |
|    value_loss           | 642         |
-----------------------------------------
Eval num_timesteps=346000, episode_reward=-8.96 +/- 30.69
Episode length: 499.40 +/- 24.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 499         |
|    mean_reward          | -8.96       |
| time/                   |             |
|    total_timesteps      | 346000      |
| train/                  |             |
|    approx_kl            | 0.005747608 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 247         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 1.49        |
|    value_loss           | 508         |
-----------------------------------------
Eval num_timesteps=348000, episode_reward=-53.92 +/- 76.04
Episode length: 426.60 +/- 129.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 427        |
|    mean_reward          | -53.9      |
| time/                   |            |
|    total_timesteps      | 348000     |
| train/                  |            |
|    approx_kl            | 0.00773879 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.27      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.001      |
|    loss                 | 265        |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0025    |
|    std                  | 1.5        |
|    value_loss           | 540        |
----------------------------------------
Eval num_timesteps=350000, episode_reward=-61.43 +/- 63.99
Episode length: 447.20 +/- 85.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | -61.4        |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0065829596 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 294          |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.0049      |
|    std                  | 1.5          |
|    value_loss           | 602          |
------------------------------------------
Eval num_timesteps=352000, episode_reward=-93.24 +/- 18.03
Episode length: 402.40 +/- 34.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | -93.2       |
| time/                   |             |
|    total_timesteps      | 352000      |
| train/                  |             |
|    approx_kl            | 0.011503174 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 292         |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00352    |
|    std                  | 1.5         |
|    value_loss           | 605         |
-----------------------------------------
Eval num_timesteps=354000, episode_reward=-125.03 +/- 16.44
Episode length: 331.40 +/- 61.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 331         |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 354000      |
| train/                  |             |
|    approx_kl            | 0.014751496 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 280         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00603    |
|    std                  | 1.51        |
|    value_loss           | 572         |
-----------------------------------------
Eval num_timesteps=356000, episode_reward=-113.75 +/- 26.66
Episode length: 347.20 +/- 46.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | -114        |
| time/                   |             |
|    total_timesteps      | 356000      |
| train/                  |             |
|    approx_kl            | 0.004530143 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 274         |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.52        |
|    value_loss           | 561         |
-----------------------------------------
Eval num_timesteps=358000, episode_reward=-92.26 +/- 29.04
Episode length: 391.80 +/- 49.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | -92.3        |
| time/                   |              |
|    total_timesteps      | 358000       |
| train/                  |              |
|    approx_kl            | 0.0042505115 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 267          |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 1.53         |
|    value_loss           | 547          |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-86.30 +/- 24.08
Episode length: 413.20 +/- 31.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | -86.3        |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0051825168 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.38        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 268          |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 1.54         |
|    value_loss           | 540          |
------------------------------------------
Eval num_timesteps=362000, episode_reward=-78.48 +/- 41.90
Episode length: 415.80 +/- 90.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | -78.5       |
| time/                   |             |
|    total_timesteps      | 362000      |
| train/                  |             |
|    approx_kl            | 0.008575588 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 259         |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0031     |
|    std                  | 1.56        |
|    value_loss           | 528         |
-----------------------------------------
Eval num_timesteps=364000, episode_reward=-102.62 +/- 21.15
Episode length: 381.20 +/- 43.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | -103        |
| time/                   |             |
|    total_timesteps      | 364000      |
| train/                  |             |
|    approx_kl            | 0.003229017 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.001       |
|    loss                 | 301         |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.56        |
|    value_loss           | 614         |
-----------------------------------------
Eval num_timesteps=366000, episode_reward=-109.64 +/- 10.93
Episode length: 376.00 +/- 28.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | -110         |
| time/                   |              |
|    total_timesteps      | 366000       |
| train/                  |              |
|    approx_kl            | 0.0073598498 |
|    clip_fraction        | 0.0512       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 247          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00332     |
|    std                  | 1.57         |
|    value_loss           | 502          |
------------------------------------------
Eval num_timesteps=368000, episode_reward=-127.38 +/- 47.54
Episode length: 333.20 +/- 94.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | -127        |
| time/                   |             |
|    total_timesteps      | 368000      |
| train/                  |             |
|    approx_kl            | 0.012045311 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 307         |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00252    |
|    std                  | 1.58        |
|    value_loss           | 632         |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=-144.63 +/- 20.87
Episode length: 288.40 +/- 50.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0034281923 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 332          |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000861    |
|    std                  | 1.59         |
|    value_loss           | 683          |
------------------------------------------
Eval num_timesteps=372000, episode_reward=-148.75 +/- 24.42
Episode length: 270.60 +/- 38.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 372000      |
| train/                  |             |
|    approx_kl            | 0.008826354 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 279         |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00367    |
|    std                  | 1.59        |
|    value_loss           | 573         |
-----------------------------------------
Eval num_timesteps=374000, episode_reward=-125.78 +/- 20.84
Episode length: 315.60 +/- 45.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | -126         |
| time/                   |              |
|    total_timesteps      | 374000       |
| train/                  |              |
|    approx_kl            | 0.0062328614 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 322          |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.6          |
|    value_loss           | 652          |
------------------------------------------
Eval num_timesteps=376000, episode_reward=-126.08 +/- 26.15
Episode length: 293.60 +/- 67.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 294         |
|    mean_reward          | -126        |
| time/                   |             |
|    total_timesteps      | 376000      |
| train/                  |             |
|    approx_kl            | 0.005119621 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.55       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 308         |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00256    |
|    std                  | 1.6         |
|    value_loss           | 637         |
-----------------------------------------
Eval num_timesteps=378000, episode_reward=-118.69 +/- 28.33
Episode length: 303.20 +/- 52.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | -119         |
| time/                   |              |
|    total_timesteps      | 378000       |
| train/                  |              |
|    approx_kl            | 0.0033348282 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 303          |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.000724    |
|    std                  | 1.61         |
|    value_loss           | 624          |
------------------------------------------
Eval num_timesteps=380000, episode_reward=-124.46 +/- 34.16
Episode length: 301.40 +/- 66.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0071001956 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00307     |
|    std                  | 1.62         |
|    value_loss           | 623          |
------------------------------------------
Eval num_timesteps=382000, episode_reward=-113.19 +/- 20.66
Episode length: 356.40 +/- 66.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | -113        |
| time/                   |             |
|    total_timesteps      | 382000      |
| train/                  |             |
|    approx_kl            | 0.006069214 |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 295         |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00132    |
|    std                  | 1.63        |
|    value_loss           | 604         |
-----------------------------------------
Eval num_timesteps=384000, episode_reward=-77.28 +/- 50.20
Episode length: 409.20 +/- 69.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | -77.3       |
| time/                   |             |
|    total_timesteps      | 384000      |
| train/                  |             |
|    approx_kl            | 0.011762994 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | 212         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00754    |
|    std                  | 1.63        |
|    value_loss           | 433         |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=-84.70 +/- 46.68
Episode length: 361.60 +/- 77.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | -84.7       |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.006814392 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 247         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00554    |
|    std                  | 1.65        |
|    value_loss           | 522         |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=-13.75 +/- 48.30
Episode length: 480.80 +/- 58.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | -13.7       |
| time/                   |             |
|    total_timesteps      | 388000      |
| train/                  |             |
|    approx_kl            | 0.008903231 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 243         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00465    |
|    std                  | 1.66        |
|    value_loss           | 504         |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=101.99 +/- 118.70
Episode length: 563.40 +/- 71.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 563          |
|    mean_reward          | 102          |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0068614893 |
|    clip_fraction        | 0.0829       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 164          |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.000563    |
|    std                  | 1.66         |
|    value_loss           | 343          |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-4.02 +/- 38.27
Episode length: 531.60 +/- 73.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | -4.02       |
| time/                   |             |
|    total_timesteps      | 392000      |
| train/                  |             |
|    approx_kl            | 0.010185816 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.001       |
|    loss                 | 226         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00361    |
|    std                  | 1.67        |
|    value_loss           | 525         |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=-94.33 +/- 25.56
Episode length: 376.40 +/- 62.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | -94.3        |
| time/                   |              |
|    total_timesteps      | 394000       |
| train/                  |              |
|    approx_kl            | 0.0047223438 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 165          |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.67         |
|    value_loss           | 352          |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-13.39 +/- 80.86
Episode length: 446.60 +/- 119.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | -13.4        |
| time/                   |              |
|    total_timesteps      | 396000       |
| train/                  |              |
|    approx_kl            | 0.0054111215 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 1.68         |
|    value_loss           | 406          |
------------------------------------------
Eval num_timesteps=398000, episode_reward=18.55 +/- 131.31
Episode length: 469.40 +/- 134.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 398000       |
| train/                  |              |
|    approx_kl            | 0.0061368793 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 206          |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 1.68         |
|    value_loss           | 427          |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-37.09 +/- 73.12
Episode length: 432.80 +/- 105.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 433         |
|    mean_reward          | -37.1       |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.004939537 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.001       |
|    loss                 | 350         |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00413    |
|    std                  | 1.68        |
|    value_loss           | 944         |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-10.53 +/- 41.66
Episode length: 537.00 +/- 87.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 537         |
|    mean_reward          | -10.5       |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.010836059 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 253         |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00444    |
|    std                  | 1.69        |
|    value_loss           | 593         |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=-53.88 +/- 81.87
Episode length: 397.60 +/- 98.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | -53.9       |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.008758506 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.001       |
|    loss                 | 152         |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00427    |
|    std                  | 1.69        |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=406000, episode_reward=149.13 +/- 401.88
Episode length: 494.60 +/- 152.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.003654842 |
|    clip_fraction        | 0.00522     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.001       |
|    loss                 | 629         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 1.7         |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=408000, episode_reward=-63.74 +/- 40.80
Episode length: 409.60 +/- 71.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | -63.7        |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0024116517 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 256          |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.7          |
|    value_loss           | 748          |
------------------------------------------
Eval num_timesteps=410000, episode_reward=-120.36 +/- 22.89
Episode length: 361.80 +/- 79.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | -120         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0027117375 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 257          |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.7          |
|    value_loss           | 654          |
------------------------------------------
Eval num_timesteps=412000, episode_reward=-103.69 +/- 31.30
Episode length: 352.80 +/- 88.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 353          |
|    mean_reward          | -104         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0022200185 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 284          |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 1.7          |
|    value_loss           | 664          |
------------------------------------------
Eval num_timesteps=414000, episode_reward=-148.23 +/- 22.78
Episode length: 309.00 +/- 78.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 309          |
|    mean_reward          | -148         |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0039787646 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 361          |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000616    |
|    std                  | 1.7          |
|    value_loss           | 836          |
------------------------------------------
Eval num_timesteps=416000, episode_reward=-150.27 +/- 31.51
Episode length: 263.80 +/- 80.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0014207696 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 309          |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.71         |
|    value_loss           | 735          |
------------------------------------------
Eval num_timesteps=418000, episode_reward=-118.30 +/- 12.06
Episode length: 346.40 +/- 16.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 346          |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0013034423 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.81        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 311          |
|    n_updates            | 2040         |
|    policy_gradient_loss | 0.000462     |
|    std                  | 1.71         |
|    value_loss           | 679          |
------------------------------------------
Eval num_timesteps=420000, episode_reward=-62.38 +/- 62.45
Episode length: 423.20 +/- 98.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | -62.4        |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0018985379 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.81        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 382          |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 1.71         |
|    value_loss           | 824          |
------------------------------------------
Eval num_timesteps=422000, episode_reward=-80.66 +/- 66.09
Episode length: 356.60 +/- 90.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | -80.7        |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0038311894 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.82        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 326          |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.71         |
|    value_loss           | 673          |
------------------------------------------
Eval num_timesteps=424000, episode_reward=-70.32 +/- 70.65
Episode length: 392.60 +/- 79.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | -70.3       |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.010093602 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 272         |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 1.72        |
|    value_loss           | 584         |
-----------------------------------------
Eval num_timesteps=426000, episode_reward=-41.51 +/- 82.31
Episode length: 460.40 +/- 104.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | -41.5       |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.007333617 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.83       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 150         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0029     |
|    std                  | 1.72        |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=428000, episode_reward=-31.03 +/- 55.65
Episode length: 475.60 +/- 41.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -31      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=32.21 +/- 53.41
Episode length: 568.80 +/- 88.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 32.2         |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0041282573 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.85        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | 189          |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 1.73         |
|    value_loss           | 454          |
------------------------------------------
Eval num_timesteps=432000, episode_reward=-7.33 +/- 25.30
Episode length: 495.20 +/- 29.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -7.33       |
| time/                   |             |
|    total_timesteps      | 432000      |
| train/                  |             |
|    approx_kl            | 0.011948531 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 162         |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 1.74        |
|    value_loss           | 373         |
-----------------------------------------
Eval num_timesteps=434000, episode_reward=5.19 +/- 97.00
Episode length: 498.20 +/- 117.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 498         |
|    mean_reward          | 5.19        |
| time/                   |             |
|    total_timesteps      | 434000      |
| train/                  |             |
|    approx_kl            | 0.010297652 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 167         |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.75        |
|    value_loss           | 360         |
-----------------------------------------
Eval num_timesteps=436000, episode_reward=-99.24 +/- 20.10
Episode length: 352.00 +/- 59.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | -99.2       |
| time/                   |             |
|    total_timesteps      | 436000      |
| train/                  |             |
|    approx_kl            | 0.005807123 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.91       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 183         |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00414    |
|    std                  | 1.75        |
|    value_loss           | 390         |
-----------------------------------------
Eval num_timesteps=438000, episode_reward=-23.65 +/- 49.38
Episode length: 455.20 +/- 78.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | -23.6        |
| time/                   |              |
|    total_timesteps      | 438000       |
| train/                  |              |
|    approx_kl            | 0.0042253425 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 1.76         |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-89.63 +/- 30.66
Episode length: 364.60 +/- 38.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | -89.6        |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0068626455 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 215          |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.76         |
|    value_loss           | 469          |
------------------------------------------
Eval num_timesteps=442000, episode_reward=-44.80 +/- 53.62
Episode length: 419.00 +/- 80.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | -44.8        |
| time/                   |              |
|    total_timesteps      | 442000       |
| train/                  |              |
|    approx_kl            | 0.0063398574 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.92        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 188          |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.0033      |
|    std                  | 1.76         |
|    value_loss           | 393          |
------------------------------------------
Eval num_timesteps=444000, episode_reward=-17.86 +/- 51.01
Episode length: 421.20 +/- 57.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | -17.9        |
| time/                   |              |
|    total_timesteps      | 444000       |
| train/                  |              |
|    approx_kl            | 0.0032777386 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.94        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 185          |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.000622    |
|    std                  | 1.76         |
|    value_loss           | 383          |
------------------------------------------
Eval num_timesteps=446000, episode_reward=49.04 +/- 86.87
Episode length: 463.80 +/- 66.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 49          |
| time/                   |             |
|    total_timesteps      | 446000      |
| train/                  |             |
|    approx_kl            | 0.011109788 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.94       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 246         |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 1.76        |
|    value_loss           | 599         |
-----------------------------------------
Eval num_timesteps=448000, episode_reward=66.34 +/- 102.87
Episode length: 545.80 +/- 160.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 66.3         |
| time/                   |              |
|    total_timesteps      | 448000       |
| train/                  |              |
|    approx_kl            | 0.0024839311 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.94        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.77         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=450000, episode_reward=172.26 +/- 112.03
Episode length: 632.60 +/- 112.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 633          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 0.0034914156 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.95        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 184          |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.77         |
|    value_loss           | 482          |
------------------------------------------
Eval num_timesteps=452000, episode_reward=222.38 +/- 170.91
Episode length: 574.60 +/- 78.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 575         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 452000      |
| train/                  |             |
|    approx_kl            | 0.002911511 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.95       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 1.77        |
|    value_loss           | 228         |
-----------------------------------------
Eval num_timesteps=454000, episode_reward=48.60 +/- 126.49
Episode length: 553.20 +/- 88.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 48.6         |
| time/                   |              |
|    total_timesteps      | 454000       |
| train/                  |              |
|    approx_kl            | 0.0068612727 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.96        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 485          |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.77         |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=456000, episode_reward=127.15 +/- 138.40
Episode length: 628.00 +/- 75.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 628          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 456000       |
| train/                  |              |
|    approx_kl            | 0.0077281776 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.96        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.77         |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=458000, episode_reward=271.63 +/- 489.21
Episode length: 553.20 +/- 144.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 553          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 458000       |
| train/                  |              |
|    approx_kl            | 0.0026793564 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.95        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 246          |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000855    |
|    std                  | 1.77         |
|    value_loss           | 640          |
------------------------------------------
Eval num_timesteps=460000, episode_reward=126.21 +/- 83.71
Episode length: 564.60 +/- 106.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 565        |
|    mean_reward          | 126        |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.00799432 |
|    clip_fraction        | 0.0761     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.96      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.001      |
|    loss                 | 172        |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.00474   |
|    std                  | 1.78       |
|    value_loss           | 395        |
----------------------------------------
Eval num_timesteps=462000, episode_reward=32.06 +/- 128.40
Episode length: 401.80 +/- 95.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 32.1        |
| time/                   |             |
|    total_timesteps      | 462000      |
| train/                  |             |
|    approx_kl            | 0.012618063 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.000921   |
|    std                  | 1.78        |
|    value_loss           | 439         |
-----------------------------------------
Eval num_timesteps=464000, episode_reward=200.99 +/- 43.14
Episode length: 610.20 +/- 59.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 610         |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 464000      |
| train/                  |             |
|    approx_kl            | 0.012088574 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 323         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 1.79        |
|    value_loss           | 794         |
-----------------------------------------
Eval num_timesteps=466000, episode_reward=297.21 +/- 178.78
Episode length: 693.80 +/- 178.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 694          |
|    mean_reward          | 297          |
| time/                   |              |
|    total_timesteps      | 466000       |
| train/                  |              |
|    approx_kl            | 0.0014879412 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 201          |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.000895    |
|    std                  | 1.79         |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=468000, episode_reward=268.50 +/- 169.12
Episode length: 654.80 +/- 101.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 655          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 468000       |
| train/                  |              |
|    approx_kl            | 0.0039197905 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.98        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 219          |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.79         |
|    value_loss           | 558          |
------------------------------------------
Eval num_timesteps=470000, episode_reward=478.66 +/- 478.68
Episode length: 646.80 +/- 43.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 647         |
|    mean_reward          | 479         |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.005278821 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 246         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 1.79        |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=133.42 +/- 176.66
Episode length: 539.00 +/- 151.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 539         |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 472000      |
| train/                  |             |
|    approx_kl            | 0.003681664 |
|    clip_fraction        | 0.00405     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.001       |
|    loss                 | 634         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 1.79        |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=342.59 +/- 180.78
Episode length: 720.00 +/- 79.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 720          |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 474000       |
| train/                  |              |
|    approx_kl            | 0.0019524075 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.0594       |
|    learning_rate        | 0.001        |
|    loss                 | 8.19e+03     |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.000775    |
|    std                  | 1.79         |
|    value_loss           | 1.79e+04     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=675.75 +/- 698.35
Episode length: 643.80 +/- 128.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 644          |
|    mean_reward          | 676          |
| time/                   |              |
|    total_timesteps      | 476000       |
| train/                  |              |
|    approx_kl            | 0.0019150861 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 364          |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.79         |
|    value_loss           | 929          |
------------------------------------------
New best mean reward!
Eval num_timesteps=478000, episode_reward=474.45 +/- 503.57
Episode length: 671.00 +/- 70.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 671          |
|    mean_reward          | 474          |
| time/                   |              |
|    total_timesteps      | 478000       |
| train/                  |              |
|    approx_kl            | 0.0014382673 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8           |
|    explained_variance   | 0.468        |
|    learning_rate        | 0.001        |
|    loss                 | 830          |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.79         |
|    value_loss           | 2.12e+03     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=2.81 +/- 202.62
Episode length: 615.00 +/- 96.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 615         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.006783168 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 143         |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00248    |
|    std                  | 1.79        |
|    value_loss           | 395         |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=316.91 +/- 332.57
Episode length: 575.20 +/- 99.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 575         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 482000      |
| train/                  |             |
|    approx_kl            | 0.004942823 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 209         |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00241    |
|    std                  | 1.79        |
|    value_loss           | 521         |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=96.60 +/- 166.20
Episode length: 552.20 +/- 116.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 552         |
|    mean_reward          | 96.6        |
| time/                   |             |
|    total_timesteps      | 484000      |
| train/                  |             |
|    approx_kl            | 0.003326456 |
|    clip_fraction        | 0.00449     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 192         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.000551   |
|    std                  | 1.79        |
|    value_loss           | 432         |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=436.32 +/- 411.49
Episode length: 637.00 +/- 57.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 436          |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0043016486 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.99        |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 481          |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.79         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=576.57 +/- 237.19
Episode length: 702.20 +/- 88.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 702         |
|    mean_reward          | 577         |
| time/                   |             |
|    total_timesteps      | 488000      |
| train/                  |             |
|    approx_kl            | 0.004211604 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.001       |
|    loss                 | 253         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00173    |
|    std                  | 1.79        |
|    value_loss           | 580         |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=240.48 +/- 232.39
Episode length: 602.80 +/- 171.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 603         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.010642988 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 191         |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 1.8         |
|    value_loss           | 417         |
-----------------------------------------
Eval num_timesteps=492000, episode_reward=734.92 +/- 376.57
Episode length: 678.20 +/- 61.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 678          |
|    mean_reward          | 735          |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0028179777 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 428          |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.8          |
|    value_loss           | 965          |
------------------------------------------
New best mean reward!
Eval num_timesteps=494000, episode_reward=915.60 +/- 459.97
Episode length: 695.60 +/- 28.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 696          |
|    mean_reward          | 916          |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0017777025 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 378          |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.8          |
|    value_loss           | 921          |
------------------------------------------
New best mean reward!
Eval num_timesteps=496000, episode_reward=269.35 +/- 236.73
Episode length: 637.80 +/- 82.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 638         |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.008063516 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.001       |
|    loss                 | 582         |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00303    |
|    std                  | 1.81        |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=498000, episode_reward=286.30 +/- 262.71
Episode length: 680.00 +/- 114.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 680         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.001007284 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.001       |
|    loss                 | 3.92e+03    |
|    n_updates            | 2430        |
|    policy_gradient_loss | 0.00016     |
|    std                  | 1.81        |
|    value_loss           | 8.36e+03    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=476.27 +/- 170.89
Episode length: 719.40 +/- 97.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 719          |
|    mean_reward          | 476          |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0004943004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.001        |
|    loss                 | 3.64e+03     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.000316    |
|    std                  | 1.81         |
|    value_loss           | 7.6e+03      |
------------------------------------------
Eval num_timesteps=502000, episode_reward=596.68 +/- 499.62
Episode length: 631.40 +/- 113.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 631           |
|    mean_reward          | 597           |
| time/                   |               |
|    total_timesteps      | 502000        |
| train/                  |               |
|    approx_kl            | 0.00068079575 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.562         |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+03      |
|    n_updates            | 2450          |
|    policy_gradient_loss | -0.00058      |
|    std                  | 1.81          |
|    value_loss           | 5e+03         |
-------------------------------------------
Eval num_timesteps=504000, episode_reward=183.30 +/- 111.83
Episode length: 597.80 +/- 105.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 598           |
|    mean_reward          | 183           |
| time/                   |               |
|    total_timesteps      | 504000        |
| train/                  |               |
|    approx_kl            | 0.00042923383 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.708         |
|    learning_rate        | 0.001         |
|    loss                 | 897           |
|    n_updates            | 2460          |
|    policy_gradient_loss | -0.000467     |
|    std                  | 1.81          |
|    value_loss           | 2.56e+03      |
-------------------------------------------
Eval num_timesteps=506000, episode_reward=458.88 +/- 378.37
Episode length: 647.00 +/- 87.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 647           |
|    mean_reward          | 459           |
| time/                   |               |
|    total_timesteps      | 506000        |
| train/                  |               |
|    approx_kl            | 0.00034074805 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.518         |
|    learning_rate        | 0.001         |
|    loss                 | 5.29e+03      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -0.000648     |
|    std                  | 1.81          |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=508000, episode_reward=100.26 +/- 350.73
Episode length: 619.60 +/- 100.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 620           |
|    mean_reward          | 100           |
| time/                   |               |
|    total_timesteps      | 508000        |
| train/                  |               |
|    approx_kl            | 4.3430045e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.04         |
|    explained_variance   | 0.741         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+03      |
|    n_updates            | 2480          |
|    policy_gradient_loss | -1.16e-05     |
|    std                  | 1.81          |
|    value_loss           | 2.74e+03      |
-------------------------------------------
Eval num_timesteps=510000, episode_reward=269.34 +/- 417.01
Episode length: 681.20 +/- 93.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 681         |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.000446417 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 158         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 1.81        |
|    value_loss           | 480         |
-----------------------------------------
Eval num_timesteps=512000, episode_reward=535.89 +/- 263.73
Episode length: 673.60 +/- 113.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=514000, episode_reward=344.68 +/- 228.37
Episode length: 633.80 +/- 149.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 634          |
|    mean_reward          | 345          |
| time/                   |              |
|    total_timesteps      | 514000       |
| train/                  |              |
|    approx_kl            | 0.0031397324 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 177          |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 1.81         |
|    value_loss           | 414          |
------------------------------------------
Eval num_timesteps=516000, episode_reward=-54.00 +/- 74.10
Episode length: 343.80 +/- 114.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | -54          |
| time/                   |              |
|    total_timesteps      | 516000       |
| train/                  |              |
|    approx_kl            | 0.0068606194 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.03        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 155          |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.8          |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=518000, episode_reward=409.49 +/- 573.72
Episode length: 637.20 +/- 144.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 637          |
|    mean_reward          | 409          |
| time/                   |              |
|    total_timesteps      | 518000       |
| train/                  |              |
|    approx_kl            | 0.0024371075 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 618          |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.8          |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=520000, episode_reward=0.67 +/- 101.15
Episode length: 399.20 +/- 107.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 0.674        |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0013983226 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.01        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 401          |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.8          |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=522000, episode_reward=-44.84 +/- 58.29
Episode length: 356.40 +/- 76.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | -44.8       |
| time/                   |             |
|    total_timesteps      | 522000      |
| train/                  |             |
|    approx_kl            | 0.003550226 |
|    clip_fraction        | 0.00366     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.001       |
|    loss                 | 427         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00186    |
|    std                  | 1.8         |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=524000, episode_reward=106.03 +/- 258.15
Episode length: 479.80 +/- 224.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 524000       |
| train/                  |              |
|    approx_kl            | 0.0040931683 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 468          |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 1.8          |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=526000, episode_reward=-65.33 +/- 56.58
Episode length: 338.60 +/- 89.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | -65.3        |
| time/                   |              |
|    total_timesteps      | 526000       |
| train/                  |              |
|    approx_kl            | 0.0033338347 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.02        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 443          |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.8          |
|    value_loss           | 1.12e+03     |
------------------------------------------
Eval num_timesteps=528000, episode_reward=-33.66 +/- 53.38
Episode length: 385.00 +/- 82.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 385         |
|    mean_reward          | -33.7       |
| time/                   |             |
|    total_timesteps      | 528000      |
| train/                  |             |
|    approx_kl            | 0.001829276 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.001       |
|    loss                 | 296         |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 1.81        |
|    value_loss           | 721         |
-----------------------------------------
Eval num_timesteps=530000, episode_reward=-103.86 +/- 70.56
Episode length: 270.60 +/- 95.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | -104         |
| time/                   |              |
|    total_timesteps      | 530000       |
| train/                  |              |
|    approx_kl            | 0.0012130304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.001        |
|    loss                 | 396          |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000642    |
|    std                  | 1.81         |
|    value_loss           | 882          |
------------------------------------------
Eval num_timesteps=532000, episode_reward=42.51 +/- 161.16
Episode length: 419.80 +/- 94.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | 42.5        |
| time/                   |             |
|    total_timesteps      | 532000      |
| train/                  |             |
|    approx_kl            | 0.001040162 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 278         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00087    |
|    std                  | 1.81        |
|    value_loss           | 641         |
-----------------------------------------
Eval num_timesteps=534000, episode_reward=-36.25 +/- 43.45
Episode length: 380.00 +/- 41.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | -36.2        |
| time/                   |              |
|    total_timesteps      | 534000       |
| train/                  |              |
|    approx_kl            | 0.0019743743 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.04        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 251          |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.81         |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=536000, episode_reward=12.31 +/- 60.75
Episode length: 415.60 +/- 53.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 536000      |
| train/                  |             |
|    approx_kl            | 0.006758389 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 204         |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00215    |
|    std                  | 1.81        |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=538000, episode_reward=-13.76 +/- 45.64
Episode length: 408.40 +/- 43.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | -13.8        |
| time/                   |              |
|    total_timesteps      | 538000       |
| train/                  |              |
|    approx_kl            | 0.0050049657 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.05        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1.81         |
|    value_loss           | 295          |
------------------------------------------
Eval num_timesteps=540000, episode_reward=-78.95 +/- 69.55
Episode length: 313.40 +/- 97.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 313        |
|    mean_reward          | -78.9      |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.01016189 |
|    clip_fraction        | 0.0586     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.04      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 160        |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.00444   |
|    std                  | 1.81       |
|    value_loss           | 349        |
----------------------------------------
Eval num_timesteps=542000, episode_reward=-39.35 +/- 64.52
Episode length: 358.00 +/- 72.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | -39.4       |
| time/                   |             |
|    total_timesteps      | 542000      |
| train/                  |             |
|    approx_kl            | 0.007578903 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000503   |
|    std                  | 1.82        |
|    value_loss           | 338         |
-----------------------------------------
Eval num_timesteps=544000, episode_reward=142.53 +/- 101.81
Episode length: 531.60 +/- 87.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 544000      |
| train/                  |             |
|    approx_kl            | 0.008684956 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 162         |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 1.82        |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=546000, episode_reward=153.92 +/- 210.17
Episode length: 487.20 +/- 112.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 154         |
| time/                   |             |
|    total_timesteps      | 546000      |
| train/                  |             |
|    approx_kl            | 0.009711161 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 1.82        |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=548000, episode_reward=331.57 +/- 466.64
Episode length: 522.20 +/- 131.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 522          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 548000       |
| train/                  |              |
|    approx_kl            | 0.0076920814 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.06        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 1.82         |
|    value_loss           | 247          |
------------------------------------------
Eval num_timesteps=550000, episode_reward=61.85 +/- 94.76
Episode length: 508.60 +/- 161.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 61.8        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.009668846 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 133         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00373    |
|    std                  | 1.82        |
|    value_loss           | 286         |
-----------------------------------------
Eval num_timesteps=552000, episode_reward=204.27 +/- 131.10
Episode length: 648.00 +/- 34.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 648         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 552000      |
| train/                  |             |
|    approx_kl            | 0.016063731 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00597    |
|    std                  | 1.84        |
|    value_loss           | 277         |
-----------------------------------------
Eval num_timesteps=554000, episode_reward=268.88 +/- 133.18
Episode length: 679.60 +/- 7.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 680          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 554000       |
| train/                  |              |
|    approx_kl            | 0.0059873397 |
|    clip_fraction        | 0.0936       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.12        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 183          |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.86         |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=556000, episode_reward=103.53 +/- 57.69
Episode length: 660.20 +/- 47.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 660          |
|    mean_reward          | 104          |
| time/                   |              |
|    total_timesteps      | 556000       |
| train/                  |              |
|    approx_kl            | 0.0053086258 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.14        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.001        |
|    loss                 | 153          |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.87         |
|    value_loss           | 417          |
------------------------------------------
Eval num_timesteps=558000, episode_reward=220.19 +/- 144.51
Episode length: 652.40 +/- 111.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 652          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 558000       |
| train/                  |              |
|    approx_kl            | 0.0013382568 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 156          |
|    n_updates            | 2720         |
|    policy_gradient_loss | 2.72e-07     |
|    std                  | 1.87         |
|    value_loss           | 428          |
------------------------------------------
Eval num_timesteps=560000, episode_reward=70.20 +/- 39.33
Episode length: 623.20 +/- 38.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 623         |
|    mean_reward          | 70.2        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.006309867 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00181    |
|    std                  | 1.87        |
|    value_loss           | 288         |
-----------------------------------------
Eval num_timesteps=562000, episode_reward=75.80 +/- 31.60
Episode length: 628.80 +/- 32.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 629          |
|    mean_reward          | 75.8         |
| time/                   |              |
|    total_timesteps      | 562000       |
| train/                  |              |
|    approx_kl            | 0.0058764336 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.16        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 1.87         |
|    value_loss           | 282          |
------------------------------------------
Eval num_timesteps=564000, episode_reward=97.05 +/- 77.04
Episode length: 623.20 +/- 64.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 623          |
|    mean_reward          | 97.1         |
| time/                   |              |
|    total_timesteps      | 564000       |
| train/                  |              |
|    approx_kl            | 0.0050144424 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.16        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 1.87         |
|    value_loss           | 320          |
------------------------------------------
Eval num_timesteps=566000, episode_reward=62.48 +/- 48.19
Episode length: 588.40 +/- 37.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 588          |
|    mean_reward          | 62.5         |
| time/                   |              |
|    total_timesteps      | 566000       |
| train/                  |              |
|    approx_kl            | 0.0056087547 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.15        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00233     |
|    std                  | 1.87         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=568000, episode_reward=25.13 +/- 22.00
Episode length: 555.60 +/- 30.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 556         |
|    mean_reward          | 25.1        |
| time/                   |             |
|    total_timesteps      | 568000      |
| train/                  |             |
|    approx_kl            | 0.005612639 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00227    |
|    std                  | 1.88        |
|    value_loss           | 322         |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=26.80 +/- 48.52
Episode length: 572.40 +/- 30.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 572          |
|    mean_reward          | 26.8         |
| time/                   |              |
|    total_timesteps      | 570000       |
| train/                  |              |
|    approx_kl            | 0.0013839116 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.18        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.000737    |
|    std                  | 1.88         |
|    value_loss           | 451          |
------------------------------------------
Eval num_timesteps=572000, episode_reward=30.35 +/- 62.39
Episode length: 539.80 +/- 63.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 540          |
|    mean_reward          | 30.4         |
| time/                   |              |
|    total_timesteps      | 572000       |
| train/                  |              |
|    approx_kl            | 0.0050883293 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.19        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 96.6         |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.88         |
|    value_loss           | 224          |
------------------------------------------
Eval num_timesteps=574000, episode_reward=21.46 +/- 45.70
Episode length: 533.00 +/- 51.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 533         |
|    mean_reward          | 21.5        |
| time/                   |             |
|    total_timesteps      | 574000      |
| train/                  |             |
|    approx_kl            | 0.013718279 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 160         |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0038     |
|    std                  | 1.89        |
|    value_loss           | 386         |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=-10.99 +/- 18.66
Episode length: 489.20 +/- 36.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 489         |
|    mean_reward          | -11         |
| time/                   |             |
|    total_timesteps      | 576000      |
| train/                  |             |
|    approx_kl            | 0.006125408 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.0021     |
|    std                  | 1.89        |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=578000, episode_reward=9.58 +/- 44.04
Episode length: 519.80 +/- 51.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | 9.58         |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0050737173 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.21        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 89.2         |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.9          |
|    value_loss           | 207          |
------------------------------------------
Eval num_timesteps=580000, episode_reward=-28.23 +/- 47.81
Episode length: 502.20 +/- 50.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -28.2       |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.007932762 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 1.9         |
|    value_loss           | 339         |
-----------------------------------------
Eval num_timesteps=582000, episode_reward=-0.18 +/- 30.19
Episode length: 516.40 +/- 22.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.177      |
| time/                   |             |
|    total_timesteps      | 582000      |
| train/                  |             |
|    approx_kl            | 0.004435688 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.00412    |
|    std                  | 1.91        |
|    value_loss           | 256         |
-----------------------------------------
Eval num_timesteps=584000, episode_reward=20.70 +/- 34.57
Episode length: 542.00 +/- 35.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 542        |
|    mean_reward          | 20.7       |
| time/                   |            |
|    total_timesteps      | 584000     |
| train/                  |            |
|    approx_kl            | 0.00567424 |
|    clip_fraction        | 0.0386     |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.25      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.001      |
|    loss                 | 123        |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.000935  |
|    std                  | 1.92       |
|    value_loss           | 273        |
----------------------------------------
Eval num_timesteps=586000, episode_reward=-1.05 +/- 23.43
Episode length: 527.80 +/- 23.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 528         |
|    mean_reward          | -1.05       |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.002761866 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.93        |
|    value_loss           | 239         |
-----------------------------------------
Eval num_timesteps=588000, episode_reward=51.57 +/- 32.18
Episode length: 571.20 +/- 32.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 571         |
|    mean_reward          | 51.6        |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.005299105 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.31       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 118         |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.00404    |
|    std                  | 1.95        |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=71.39 +/- 111.51
Episode length: 572.60 +/- 76.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 573         |
|    mean_reward          | 71.4        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.004448466 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 86.2        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.95        |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=592000, episode_reward=17.53 +/- 55.91
Episode length: 530.80 +/- 60.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 531         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.008793309 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.34       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 97.2        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 1.96        |
|    value_loss           | 208         |
-----------------------------------------
Eval num_timesteps=594000, episode_reward=-48.68 +/- 18.39
Episode length: 461.00 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | -48.7       |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.014465181 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.005      |
|    std                  | 1.97        |
|    value_loss           | 249         |
-----------------------------------------
Eval num_timesteps=596000, episode_reward=-45.81 +/- 20.54
Episode length: 457.80 +/- 18.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | -45.8       |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.008725813 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 131         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 1.97        |
|    value_loss           | 280         |
-----------------------------------------
Eval num_timesteps=598000, episode_reward=-44.11 +/- 45.00
Episode length: 453.80 +/- 50.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 454      |
|    mean_reward     | -44.1    |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-52.52 +/- 32.12
Episode length: 454.00 +/- 39.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | -52.5       |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.008337881 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 95.7        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00502    |
|    std                  | 1.97        |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=602000, episode_reward=-2.15 +/- 24.44
Episode length: 494.80 +/- 19.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -2.15       |
| time/                   |             |
|    total_timesteps      | 602000      |
| train/                  |             |
|    approx_kl            | 0.005660245 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.38       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00114    |
|    std                  | 1.98        |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=604000, episode_reward=-31.68 +/- 34.35
Episode length: 483.80 +/- 33.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | -31.7       |
| time/                   |             |
|    total_timesteps      | 604000      |
| train/                  |             |
|    approx_kl            | 0.012361802 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.4        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 119         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00577    |
|    std                  | 1.99        |
|    value_loss           | 244         |
-----------------------------------------
Eval num_timesteps=606000, episode_reward=-21.93 +/- 18.89
Episode length: 483.80 +/- 21.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | -21.9        |
| time/                   |              |
|    total_timesteps      | 606000       |
| train/                  |              |
|    approx_kl            | 0.0038514803 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.42        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 94.5         |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2            |
|    value_loss           | 194          |
------------------------------------------
Eval num_timesteps=608000, episode_reward=-29.84 +/- 41.90
Episode length: 471.20 +/- 40.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | -29.8       |
| time/                   |             |
|    total_timesteps      | 608000      |
| train/                  |             |
|    approx_kl            | 0.011687456 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.44       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 87.6        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.00409    |
|    std                  | 2.01        |
|    value_loss           | 187         |
-----------------------------------------
Eval num_timesteps=610000, episode_reward=4.75 +/- 32.68
Episode length: 506.00 +/- 35.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 4.75        |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.008176929 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.46       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.00259    |
|    std                  | 2.02        |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=-9.12 +/- 20.48
Episode length: 494.80 +/- 21.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -9.12       |
| time/                   |             |
|    total_timesteps      | 612000      |
| train/                  |             |
|    approx_kl            | 0.019158553 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.48       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 83.8        |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0039     |
|    std                  | 2.04        |
|    value_loss           | 174         |
-----------------------------------------
Eval num_timesteps=614000, episode_reward=-20.45 +/- 37.19
Episode length: 478.80 +/- 40.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | -20.5       |
| time/                   |             |
|    total_timesteps      | 614000      |
| train/                  |             |
|    approx_kl            | 0.009659045 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.51       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 2990        |
|    policy_gradient_loss | 0.000558    |
|    std                  | 2.05        |
|    value_loss           | 227         |
-----------------------------------------
Eval num_timesteps=616000, episode_reward=-6.42 +/- 15.84
Episode length: 502.00 +/- 21.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | -6.42       |
| time/                   |             |
|    total_timesteps      | 616000      |
| train/                  |             |
|    approx_kl            | 0.008341516 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 81          |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00643    |
|    std                  | 2.06        |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=618000, episode_reward=-28.43 +/- 31.00
Episode length: 479.80 +/- 30.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | -28.4       |
| time/                   |             |
|    total_timesteps      | 618000      |
| train/                  |             |
|    approx_kl            | 0.015128855 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 77.6        |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.000271   |
|    std                  | 2.06        |
|    value_loss           | 162         |
-----------------------------------------
Eval num_timesteps=620000, episode_reward=-24.37 +/- 28.86
Episode length: 478.20 +/- 37.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | -24.4        |
| time/                   |              |
|    total_timesteps      | 620000       |
| train/                  |              |
|    approx_kl            | 0.0061393916 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.53        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.00291     |
|    std                  | 2.06         |
|    value_loss           | 166          |
------------------------------------------
Eval num_timesteps=622000, episode_reward=30.00 +/- 63.65
Episode length: 527.80 +/- 52.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 528          |
|    mean_reward          | 30           |
| time/                   |              |
|    total_timesteps      | 622000       |
| train/                  |              |
|    approx_kl            | 0.0039992775 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.54        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 91.1         |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 2.07         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=624000, episode_reward=-8.29 +/- 15.04
Episode length: 517.80 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | -8.29       |
| time/                   |             |
|    total_timesteps      | 624000      |
| train/                  |             |
|    approx_kl            | 0.010744985 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.55       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 71.4        |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 2.08        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=626000, episode_reward=47.09 +/- 43.56
Episode length: 555.00 +/- 38.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 555         |
|    mean_reward          | 47.1        |
| time/                   |             |
|    total_timesteps      | 626000      |
| train/                  |             |
|    approx_kl            | 0.007962052 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 70.1        |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00249    |
|    std                  | 2.08        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=628000, episode_reward=17.18 +/- 17.60
Episode length: 524.20 +/- 35.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 524          |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 628000       |
| train/                  |              |
|    approx_kl            | 0.0043002972 |
|    clip_fraction        | 0.0643       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 61.3         |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 2.09         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=630000, episode_reward=22.78 +/- 30.90
Episode length: 535.00 +/- 34.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 535         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.008759819 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 66          |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00379    |
|    std                  | 2.09        |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=632000, episode_reward=-13.42 +/- 22.12
Episode length: 489.40 +/- 33.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | -13.4        |
| time/                   |              |
|    total_timesteps      | 632000       |
| train/                  |              |
|    approx_kl            | 0.0077921716 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.58        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 63.6         |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00315     |
|    std                  | 2.09         |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=634000, episode_reward=18.06 +/- 20.20
Episode length: 538.40 +/- 26.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 634000       |
| train/                  |              |
|    approx_kl            | 0.0044492166 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.59        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 63.9         |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 2.1          |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=636000, episode_reward=37.53 +/- 17.41
Episode length: 548.40 +/- 20.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 548         |
|    mean_reward          | 37.5        |
| time/                   |             |
|    total_timesteps      | 636000      |
| train/                  |             |
|    approx_kl            | 0.006193174 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.61       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 58.6        |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 2.11        |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=638000, episode_reward=-0.21 +/- 50.19
Episode length: 505.00 +/- 51.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | -0.207      |
| time/                   |             |
|    total_timesteps      | 638000      |
| train/                  |             |
|    approx_kl            | 0.015062636 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.62       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 68.3        |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 2.12        |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=54.96 +/- 17.51
Episode length: 557.00 +/- 23.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 557         |
|    mean_reward          | 55          |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.010308532 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.65       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 90.3        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.000984   |
|    std                  | 2.13        |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=-21.62 +/- 48.48
Episode length: 478.60 +/- 67.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | -21.6        |
| time/                   |              |
|    total_timesteps      | 642000       |
| train/                  |              |
|    approx_kl            | 0.0075516463 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.66        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 77.3         |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 2.14         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=644000, episode_reward=-1.01 +/- 33.57
Episode length: 506.20 +/- 48.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | -1.01       |
| time/                   |             |
|    total_timesteps      | 644000      |
| train/                  |             |
|    approx_kl            | 0.007958618 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.68       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 96.8        |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 2.15        |
|    value_loss           | 231         |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=6.97 +/- 68.61
Episode length: 505.60 +/- 73.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 6.97         |
| time/                   |              |
|    total_timesteps      | 646000       |
| train/                  |              |
|    approx_kl            | 0.0063837976 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.69        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 75.7         |
|    n_updates            | 3150         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.16         |
|    value_loss           | 160          |
------------------------------------------
Eval num_timesteps=648000, episode_reward=43.08 +/- 41.31
Episode length: 558.00 +/- 34.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 558         |
|    mean_reward          | 43.1        |
| time/                   |             |
|    total_timesteps      | 648000      |
| train/                  |             |
|    approx_kl            | 0.005689059 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.71       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 66          |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 2.17        |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=56.08 +/- 21.14
Episode length: 569.40 +/- 19.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 56.1         |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0040121065 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 69.2         |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 2.17         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=652000, episode_reward=19.57 +/- 32.34
Episode length: 529.20 +/- 61.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 529         |
|    mean_reward          | 19.6        |
| time/                   |             |
|    total_timesteps      | 652000      |
| train/                  |             |
|    approx_kl            | 0.009924701 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.74       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 77.4        |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.00458    |
|    std                  | 2.18        |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=59.22 +/- 50.14
Episode length: 551.20 +/- 40.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 551          |
|    mean_reward          | 59.2         |
| time/                   |              |
|    total_timesteps      | 654000       |
| train/                  |              |
|    approx_kl            | 0.0074616065 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.74        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 39.8         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00365     |
|    std                  | 2.18         |
|    value_loss           | 83.1         |
------------------------------------------
Eval num_timesteps=656000, episode_reward=87.65 +/- 32.46
Episode length: 569.40 +/- 34.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 569          |
|    mean_reward          | 87.6         |
| time/                   |              |
|    total_timesteps      | 656000       |
| train/                  |              |
|    approx_kl            | 0.0040004095 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.72        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 53           |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00396     |
|    std                  | 2.18         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=658000, episode_reward=79.68 +/- 22.04
Episode length: 588.20 +/- 22.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 588         |
|    mean_reward          | 79.7        |
| time/                   |             |
|    total_timesteps      | 658000      |
| train/                  |             |
|    approx_kl            | 0.005291974 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.73       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 60.9        |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 2.18        |
|    value_loss           | 139         |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=127.40 +/- 65.63
Episode length: 605.00 +/- 27.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 605          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0053078635 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.74        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 50           |
|    n_updates            | 3220         |
|    policy_gradient_loss | -0.000514    |
|    std                  | 2.19         |
|    value_loss           | 99.6         |
------------------------------------------
Eval num_timesteps=662000, episode_reward=159.43 +/- 30.17
Episode length: 607.40 +/- 42.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 607          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0043517193 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.75        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 38.3         |
|    n_updates            | 3230         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 2.2          |
|    value_loss           | 80.3         |
------------------------------------------
Eval num_timesteps=664000, episode_reward=219.28 +/- 102.28
Episode length: 600.80 +/- 24.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 601          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0030413703 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.76        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 48.8         |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 2.2          |
|    value_loss           | 117          |
------------------------------------------
Eval num_timesteps=666000, episode_reward=439.11 +/- 301.25
Episode length: 595.20 +/- 14.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 595          |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0032561761 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.77        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 52.4         |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.0041      |
|    std                  | 2.2          |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=668000, episode_reward=245.37 +/- 116.74
Episode length: 596.40 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 596          |
|    mean_reward          | 245          |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0053978525 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.78        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 79.9         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 2.21         |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=670000, episode_reward=156.64 +/- 60.38
Episode length: 619.00 +/- 19.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 619         |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.003921192 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.8        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 67.6        |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00107    |
|    std                  | 2.23        |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=672000, episode_reward=298.62 +/- 174.24
Episode length: 607.00 +/- 22.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 607         |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.008457333 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.83       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00075    |
|    std                  | 2.24        |
|    value_loss           | 365         |
-----------------------------------------
Eval num_timesteps=674000, episode_reward=373.72 +/- 216.23
Episode length: 621.40 +/- 35.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 621          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0034413806 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.85        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 64.4         |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.000851    |
|    std                  | 2.25         |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=676000, episode_reward=458.76 +/- 200.11
Episode length: 579.60 +/- 35.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 580          |
|    mean_reward          | 459          |
| time/                   |              |
|    total_timesteps      | 676000       |
| train/                  |              |
|    approx_kl            | 0.0018181759 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.86        |
|    explained_variance   | 0.631        |
|    learning_rate        | 0.001        |
|    loss                 | 632          |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.000323    |
|    std                  | 2.25         |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=678000, episode_reward=468.11 +/- 200.77
Episode length: 618.40 +/- 33.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | 468          |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0007017135 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.86        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 77.1         |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.000582    |
|    std                  | 2.25         |
|    value_loss           | 194          |
------------------------------------------
Eval num_timesteps=680000, episode_reward=482.27 +/- 282.67
Episode length: 563.60 +/- 44.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 564          |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0014020847 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 395          |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00068     |
|    std                  | 2.26         |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=682000, episode_reward=761.43 +/- 349.99
Episode length: 579.00 +/- 16.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 579          |
|    mean_reward          | 761          |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0015538123 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.87        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 436          |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.000121    |
|    std                  | 2.26         |
|    value_loss           | 1.46e+03     |
------------------------------------------
Eval num_timesteps=684000, episode_reward=550.19 +/- 283.35
Episode length: 556.20 +/- 72.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 556      |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=686000, episode_reward=387.25 +/- 251.95
Episode length: 576.40 +/- 60.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 576           |
|    mean_reward          | 387           |
| time/                   |               |
|    total_timesteps      | 686000        |
| train/                  |               |
|    approx_kl            | 0.00016879622 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.88         |
|    explained_variance   | 0.749         |
|    learning_rate        | 0.001         |
|    loss                 | 2.08e+03      |
|    n_updates            | 3340          |
|    policy_gradient_loss | -3.06e-05     |
|    std                  | 2.26          |
|    value_loss           | 4.76e+03      |
-------------------------------------------
Eval num_timesteps=688000, episode_reward=550.20 +/- 419.19
Episode length: 568.00 +/- 59.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 568           |
|    mean_reward          | 550           |
| time/                   |               |
|    total_timesteps      | 688000        |
| train/                  |               |
|    approx_kl            | 0.00071145303 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.88         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 296           |
|    n_updates            | 3350          |
|    policy_gradient_loss | -0.00133      |
|    std                  | 2.26          |
|    value_loss           | 926           |
-------------------------------------------
Eval num_timesteps=690000, episode_reward=407.22 +/- 375.97
Episode length: 516.60 +/- 66.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 690000       |
| train/                  |              |
|    approx_kl            | 0.0047485786 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 160          |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 2.26         |
|    value_loss           | 541          |
------------------------------------------
Eval num_timesteps=692000, episode_reward=369.11 +/- 386.99
Episode length: 443.20 +/- 83.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 369          |
| time/                   |              |
|    total_timesteps      | 692000       |
| train/                  |              |
|    approx_kl            | 0.0043530897 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 96.8         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 2.26         |
|    value_loss           | 239          |
------------------------------------------
Eval num_timesteps=694000, episode_reward=181.15 +/- 85.26
Episode length: 466.00 +/- 51.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 694000       |
| train/                  |              |
|    approx_kl            | 0.0030846104 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.88        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.000109    |
|    std                  | 2.26         |
|    value_loss           | 284          |
------------------------------------------
Eval num_timesteps=696000, episode_reward=285.81 +/- 96.25
Episode length: 516.60 +/- 53.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 696000       |
| train/                  |              |
|    approx_kl            | 0.0005008285 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.89        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 85.6         |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000288    |
|    std                  | 2.27         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=698000, episode_reward=226.17 +/- 89.03
Episode length: 518.60 +/- 82.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 519         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 698000      |
| train/                  |             |
|    approx_kl            | 0.002805062 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 98.2        |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 2.27        |
|    value_loss           | 296         |
-----------------------------------------
Eval num_timesteps=700000, episode_reward=261.29 +/- 119.08
Episode length: 573.40 +/- 41.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 573         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.007265646 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.89       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 137         |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00176    |
|    std                  | 2.27        |
|    value_loss           | 388         |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=267.48 +/- 167.26
Episode length: 582.00 +/- 66.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 582          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 702000       |
| train/                  |              |
|    approx_kl            | 0.0016255572 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 142          |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.000801    |
|    std                  | 2.27         |
|    value_loss           | 417          |
------------------------------------------
Eval num_timesteps=704000, episode_reward=409.24 +/- 293.85
Episode length: 620.20 +/- 27.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 620         |
|    mean_reward          | 409         |
| time/                   |             |
|    total_timesteps      | 704000      |
| train/                  |             |
|    approx_kl            | 0.004316341 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.9        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 53.1        |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 2.27        |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=706000, episode_reward=244.11 +/- 270.91
Episode length: 599.20 +/- 88.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 599          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 706000       |
| train/                  |              |
|    approx_kl            | 0.0033670417 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.9         |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+03     |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 2.28         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=708000, episode_reward=379.06 +/- 187.57
Episode length: 557.60 +/- 58.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 558           |
|    mean_reward          | 379           |
| time/                   |               |
|    total_timesteps      | 708000        |
| train/                  |               |
|    approx_kl            | 0.00048362525 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.91         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 167           |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.00135      |
|    std                  | 2.28          |
|    value_loss           | 529           |
-------------------------------------------
Eval num_timesteps=710000, episode_reward=650.02 +/- 434.05
Episode length: 627.60 +/- 88.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 628          |
|    mean_reward          | 650          |
| time/                   |              |
|    total_timesteps      | 710000       |
| train/                  |              |
|    approx_kl            | 0.0015447731 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.543        |
|    learning_rate        | 0.001        |
|    loss                 | 2.89e+03     |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 2.28         |
|    value_loss           | 7.23e+03     |
------------------------------------------
Eval num_timesteps=712000, episode_reward=257.91 +/- 124.06
Episode length: 571.80 +/- 55.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 572           |
|    mean_reward          | 258           |
| time/                   |               |
|    total_timesteps      | 712000        |
| train/                  |               |
|    approx_kl            | 0.00036868855 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.91         |
|    explained_variance   | 0.617         |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+03      |
|    n_updates            | 3470          |
|    policy_gradient_loss | -0.000423     |
|    std                  | 2.28          |
|    value_loss           | 9.27e+03      |
-------------------------------------------
Eval num_timesteps=714000, episode_reward=789.72 +/- 830.01
Episode length: 566.20 +/- 35.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 566          |
|    mean_reward          | 790          |
| time/                   |              |
|    total_timesteps      | 714000       |
| train/                  |              |
|    approx_kl            | 0.0037324373 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 2.28         |
|    value_loss           | 577          |
------------------------------------------
Eval num_timesteps=716000, episode_reward=1093.51 +/- 547.93
Episode length: 537.20 +/- 52.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 537          |
|    mean_reward          | 1.09e+03     |
| time/                   |              |
|    total_timesteps      | 716000       |
| train/                  |              |
|    approx_kl            | 0.0068007438 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 302          |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 2.28         |
|    value_loss           | 767          |
------------------------------------------
New best mean reward!
Eval num_timesteps=718000, episode_reward=360.83 +/- 168.19
Episode length: 560.20 +/- 80.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 560          |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 718000       |
| train/                  |              |
|    approx_kl            | 0.0010463747 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.91        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+03     |
|    n_updates            | 3500         |
|    policy_gradient_loss | 0.000699     |
|    std                  | 2.28         |
|    value_loss           | 4.02e+03     |
------------------------------------------
Eval num_timesteps=720000, episode_reward=581.22 +/- 367.16
Episode length: 541.60 +/- 76.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 542         |
|    mean_reward          | 581         |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.002492595 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.91       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 186         |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 2.28        |
|    value_loss           | 548         |
-----------------------------------------
Eval num_timesteps=722000, episode_reward=533.65 +/- 431.62
Episode length: 479.00 +/- 86.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 534         |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.007115058 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.92       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 99.6        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 2.28        |
|    value_loss           | 250         |
-----------------------------------------
Eval num_timesteps=724000, episode_reward=590.28 +/- 619.95
Episode length: 512.00 +/- 91.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 512          |
|    mean_reward          | 590          |
| time/                   |              |
|    total_timesteps      | 724000       |
| train/                  |              |
|    approx_kl            | 0.0047853626 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 2.28         |
|    value_loss           | 4.08e+03     |
------------------------------------------
Eval num_timesteps=726000, episode_reward=326.58 +/- 297.67
Episode length: 538.20 +/- 103.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 327          |
| time/                   |              |
|    total_timesteps      | 726000       |
| train/                  |              |
|    approx_kl            | 0.0016919188 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.92        |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+03     |
|    n_updates            | 3540         |
|    policy_gradient_loss | -0.000456    |
|    std                  | 2.29         |
|    value_loss           | 7.04e+03     |
------------------------------------------
Eval num_timesteps=728000, episode_reward=462.04 +/- 483.51
Episode length: 493.20 +/- 40.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 493           |
|    mean_reward          | 462           |
| time/                   |               |
|    total_timesteps      | 728000        |
| train/                  |               |
|    approx_kl            | 0.00027815378 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.92         |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.001         |
|    loss                 | 334           |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.000328     |
|    std                  | 2.29          |
|    value_loss           | 941           |
-------------------------------------------
Eval num_timesteps=730000, episode_reward=381.61 +/- 304.72
Episode length: 457.80 +/- 73.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 730000       |
| train/                  |              |
|    approx_kl            | 0.0008532781 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 467          |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000918    |
|    std                  | 2.29         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=732000, episode_reward=445.99 +/- 127.00
Episode length: 442.00 +/- 27.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 732000        |
| train/                  |               |
|    approx_kl            | 0.00044800676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0.66          |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+03      |
|    n_updates            | 3570          |
|    policy_gradient_loss | -2.28e-05     |
|    std                  | 2.29          |
|    value_loss           | 3.93e+03      |
-------------------------------------------
Eval num_timesteps=734000, episode_reward=465.01 +/- 346.14
Episode length: 470.20 +/- 28.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 465           |
| time/                   |               |
|    total_timesteps      | 734000        |
| train/                  |               |
|    approx_kl            | 0.00078343914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | -0.325        |
|    learning_rate        | 0.001         |
|    loss                 | 8.88e+03      |
|    n_updates            | 3580          |
|    policy_gradient_loss | -0.00106      |
|    std                  | 2.29          |
|    value_loss           | 1.99e+04      |
-------------------------------------------
Eval num_timesteps=736000, episode_reward=707.21 +/- 1076.52
Episode length: 497.80 +/- 56.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | 707           |
| time/                   |               |
|    total_timesteps      | 736000        |
| train/                  |               |
|    approx_kl            | 0.00034871104 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.93         |
|    explained_variance   | 0.516         |
|    learning_rate        | 0.001         |
|    loss                 | 3.41e+03      |
|    n_updates            | 3590          |
|    policy_gradient_loss | -2.56e-05     |
|    std                  | 2.29          |
|    value_loss           | 8.18e+03      |
-------------------------------------------
Eval num_timesteps=738000, episode_reward=320.60 +/- 192.53
Episode length: 501.00 +/- 55.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 738000      |
| train/                  |             |
|    approx_kl            | 0.000493317 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.93       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.001       |
|    loss                 | 436         |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.000647   |
|    std                  | 2.29        |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=414.50 +/- 234.82
Episode length: 508.60 +/- 57.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 415         |
| time/                   |             |
|    total_timesteps      | 740000      |
| train/                  |             |
|    approx_kl            | 0.002392271 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.93       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.001       |
|    loss                 | 502         |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.000994   |
|    std                  | 2.29        |
|    value_loss           | 1.38e+03    |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=416.41 +/- 379.07
Episode length: 405.20 +/- 54.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 742000       |
| train/                  |              |
|    approx_kl            | 0.0016918562 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.93        |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.001        |
|    loss                 | 3.58e+03     |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 2.29         |
|    value_loss           | 8.36e+03     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=416.51 +/- 390.18
Episode length: 457.80 +/- 50.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 744000       |
| train/                  |              |
|    approx_kl            | 0.0011185729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.94        |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.000328    |
|    std                  | 2.3          |
|    value_loss           | 5.98e+03     |
------------------------------------------
Eval num_timesteps=746000, episode_reward=441.62 +/- 239.04
Episode length: 506.40 +/- 103.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 442          |
| time/                   |              |
|    total_timesteps      | 746000       |
| train/                  |              |
|    approx_kl            | 0.0014282303 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.94        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.001        |
|    loss                 | 494          |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.000618    |
|    std                  | 2.3          |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=748000, episode_reward=707.00 +/- 868.27
Episode length: 424.40 +/- 50.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 707          |
| time/                   |              |
|    total_timesteps      | 748000       |
| train/                  |              |
|    approx_kl            | 0.0016717697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.0814       |
|    learning_rate        | 0.001        |
|    loss                 | 6.61e+03     |
|    n_updates            | 3650         |
|    policy_gradient_loss | 0.000482     |
|    std                  | 2.3          |
|    value_loss           | 1.49e+04     |
------------------------------------------
Eval num_timesteps=750000, episode_reward=401.44 +/- 381.61
Episode length: 455.00 +/- 51.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0007440458 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.622        |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+03     |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000247    |
|    std                  | 2.3          |
|    value_loss           | 5.77e+03     |
------------------------------------------
Eval num_timesteps=752000, episode_reward=351.91 +/- 211.58
Episode length: 399.00 +/- 47.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0035941657 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.95        |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 2.31         |
|    value_loss           | 4.4e+03      |
------------------------------------------
Eval num_timesteps=754000, episode_reward=834.81 +/- 842.81
Episode length: 517.20 +/- 97.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 835          |
| time/                   |              |
|    total_timesteps      | 754000       |
| train/                  |              |
|    approx_kl            | 0.0023085373 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 570          |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.000632    |
|    std                  | 2.31         |
|    value_loss           | 1.49e+03     |
------------------------------------------
Eval num_timesteps=756000, episode_reward=495.53 +/- 208.66
Episode length: 506.00 +/- 40.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 496          |
| time/                   |              |
|    total_timesteps      | 756000       |
| train/                  |              |
|    approx_kl            | 0.0012330093 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96        |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.000386    |
|    std                  | 2.31         |
|    value_loss           | 4.35e+03     |
------------------------------------------
Eval num_timesteps=758000, episode_reward=304.23 +/- 198.54
Episode length: 467.00 +/- 70.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 304           |
| time/                   |               |
|    total_timesteps      | 758000        |
| train/                  |               |
|    approx_kl            | 0.00026889145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.97         |
|    explained_variance   | 0.464         |
|    learning_rate        | 0.001         |
|    loss                 | 8.3e+03       |
|    n_updates            | 3700          |
|    policy_gradient_loss | 0.000133      |
|    std                  | 2.31          |
|    value_loss           | 1.69e+04      |
-------------------------------------------
Eval num_timesteps=760000, episode_reward=440.73 +/- 293.32
Episode length: 508.40 +/- 107.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 441          |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0001099357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.001        |
|    loss                 | 4.01e+03     |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.000382    |
|    std                  | 2.31         |
|    value_loss           | 8.81e+03     |
------------------------------------------
Eval num_timesteps=762000, episode_reward=1035.47 +/- 1110.81
Episode length: 541.00 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 541          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0010551186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 719          |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 2.32         |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=764000, episode_reward=268.54 +/- 155.96
Episode length: 425.60 +/- 53.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0026793564 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.455        |
|    learning_rate        | 0.001        |
|    loss                 | 2.94e+03     |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 2.32         |
|    value_loss           | 7.19e+03     |
------------------------------------------
Eval num_timesteps=766000, episode_reward=335.11 +/- 189.87
Episode length: 413.80 +/- 23.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 335          |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0015691428 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.001        |
|    loss                 | 5.86e+03     |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000529    |
|    std                  | 2.32         |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=768000, episode_reward=491.67 +/- 688.30
Episode length: 420.00 +/- 95.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=55.69 +/- 55.20
Episode length: 353.80 +/- 25.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 55.7         |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 0.0024014905 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.001        |
|    loss                 | 632          |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 2.32         |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=772000, episode_reward=168.52 +/- 283.98
Episode length: 346.60 +/- 62.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 772000      |
| train/                  |             |
|    approx_kl            | 0.007167035 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 316         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 2.32        |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=774000, episode_reward=474.99 +/- 563.67
Episode length: 414.40 +/- 69.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 475         |
| time/                   |             |
|    total_timesteps      | 774000      |
| train/                  |             |
|    approx_kl            | 0.011215764 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.97       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 234         |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.32        |
|    value_loss           | 549         |
-----------------------------------------
Eval num_timesteps=776000, episode_reward=58.84 +/- 21.67
Episode length: 379.20 +/- 24.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 58.8         |
| time/                   |              |
|    total_timesteps      | 776000       |
| train/                  |              |
|    approx_kl            | 0.0014034373 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 3780         |
|    policy_gradient_loss | 3.9e-05      |
|    std                  | 2.32         |
|    value_loss           | 3.92e+03     |
------------------------------------------
Eval num_timesteps=778000, episode_reward=93.34 +/- 76.56
Episode length: 370.60 +/- 44.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 93.3        |
| time/                   |             |
|    total_timesteps      | 778000      |
| train/                  |             |
|    approx_kl            | 0.002943565 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.98       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 258         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00098    |
|    std                  | 2.32        |
|    value_loss           | 595         |
-----------------------------------------
Eval num_timesteps=780000, episode_reward=38.73 +/- 61.19
Episode length: 359.00 +/- 26.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 38.7         |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0019932627 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+03     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000112    |
|    std                  | 2.31         |
|    value_loss           | 4.86e+03     |
------------------------------------------
Eval num_timesteps=782000, episode_reward=226.97 +/- 236.55
Episode length: 428.60 +/- 81.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 782000       |
| train/                  |              |
|    approx_kl            | 0.0006862045 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.97        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 221          |
|    n_updates            | 3810         |
|    policy_gradient_loss | -4.51e-05    |
|    std                  | 2.32         |
|    value_loss           | 576          |
------------------------------------------
Eval num_timesteps=784000, episode_reward=227.13 +/- 148.54
Episode length: 421.20 +/- 42.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 784000       |
| train/                  |              |
|    approx_kl            | 0.0045359535 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.98        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 195          |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.32         |
|    value_loss           | 444          |
------------------------------------------
Eval num_timesteps=786000, episode_reward=195.39 +/- 192.50
Episode length: 391.20 +/- 44.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 195           |
| time/                   |               |
|    total_timesteps      | 786000        |
| train/                  |               |
|    approx_kl            | 0.00087308115 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.98         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 144           |
|    n_updates            | 3830          |
|    policy_gradient_loss | -0.000506     |
|    std                  | 2.32          |
|    value_loss           | 332           |
-------------------------------------------
Eval num_timesteps=788000, episode_reward=135.05 +/- 135.19
Episode length: 398.80 +/- 31.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 788000       |
| train/                  |              |
|    approx_kl            | 0.0043467805 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9           |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 146          |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 2.33         |
|    value_loss           | 318          |
------------------------------------------
Eval num_timesteps=790000, episode_reward=303.89 +/- 158.44
Episode length: 422.80 +/- 25.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 790000      |
| train/                  |             |
|    approx_kl            | 0.006500548 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.01       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 146         |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 2.35        |
|    value_loss           | 347         |
-----------------------------------------
Eval num_timesteps=792000, episode_reward=387.19 +/- 403.34
Episode length: 457.00 +/- 60.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | 387          |
| time/                   |              |
|    total_timesteps      | 792000       |
| train/                  |              |
|    approx_kl            | 0.0014232666 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.04        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.0001      |
|    std                  | 2.36         |
|    value_loss           | 314          |
------------------------------------------
Eval num_timesteps=794000, episode_reward=2390.91 +/- 1761.64
Episode length: 575.60 +/- 72.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 576         |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 794000      |
| train/                  |             |
|    approx_kl            | 0.008816303 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.06       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.001       |
|    loss                 | 6.59e+03    |
|    n_updates            | 3870        |
|    policy_gradient_loss | 0.000285    |
|    std                  | 2.37        |
|    value_loss           | 1.38e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=796000, episode_reward=674.92 +/- 377.33
Episode length: 548.80 +/- 82.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 549          |
|    mean_reward          | 675          |
| time/                   |              |
|    total_timesteps      | 796000       |
| train/                  |              |
|    approx_kl            | 0.0069330917 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.06        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 422          |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 2.37         |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=798000, episode_reward=598.70 +/- 188.54
Episode length: 544.60 +/- 143.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 545         |
|    mean_reward          | 599         |
| time/                   |             |
|    total_timesteps      | 798000      |
| train/                  |             |
|    approx_kl            | 0.003682834 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 391         |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.000614   |
|    std                  | 2.37        |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=108.87 +/- 133.97
Episode length: 392.40 +/- 38.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.010118268 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 390         |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 2.37        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=802000, episode_reward=418.72 +/- 348.51
Episode length: 490.20 +/- 22.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 802000       |
| train/                  |              |
|    approx_kl            | 0.0014638766 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.609        |
|    learning_rate        | 0.001        |
|    loss                 | 2.36e+03     |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.000595    |
|    std                  | 2.37         |
|    value_loss           | 5.51e+03     |
------------------------------------------
Eval num_timesteps=804000, episode_reward=449.26 +/- 374.88
Episode length: 579.20 +/- 127.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 579           |
|    mean_reward          | 449           |
| time/                   |               |
|    total_timesteps      | 804000        |
| train/                  |               |
|    approx_kl            | 0.00076768774 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.07         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.001         |
|    loss                 | 2.26e+03      |
|    n_updates            | 3920          |
|    policy_gradient_loss | -0.000637     |
|    std                  | 2.38          |
|    value_loss           | 7.13e+03      |
-------------------------------------------
Eval num_timesteps=806000, episode_reward=418.51 +/- 333.86
Episode length: 487.60 +/- 100.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 419         |
| time/                   |             |
|    total_timesteps      | 806000      |
| train/                  |             |
|    approx_kl            | 0.002863845 |
|    clip_fraction        | 0.0021      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 383         |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 2.37        |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=808000, episode_reward=557.81 +/- 524.58
Episode length: 523.80 +/- 94.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 524         |
|    mean_reward          | 558         |
| time/                   |             |
|    total_timesteps      | 808000      |
| train/                  |             |
|    approx_kl            | 0.004392659 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.07       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 261         |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 2.37        |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=810000, episode_reward=99.53 +/- 154.05
Episode length: 471.20 +/- 22.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 99.5         |
| time/                   |              |
|    total_timesteps      | 810000       |
| train/                  |              |
|    approx_kl            | 0.0030059966 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.001        |
|    loss                 | 212          |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.37         |
|    value_loss           | 689          |
------------------------------------------
Eval num_timesteps=812000, episode_reward=140.58 +/- 174.16
Episode length: 519.20 +/- 61.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 519          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 812000       |
| train/                  |              |
|    approx_kl            | 0.0024145725 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.07        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 166          |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.37         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=814000, episode_reward=96.50 +/- 266.92
Episode length: 457.40 +/- 78.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 96.5          |
| time/                   |               |
|    total_timesteps      | 814000        |
| train/                  |               |
|    approx_kl            | 0.00090654363 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.07         |
|    explained_variance   | 0.798         |
|    learning_rate        | 0.001         |
|    loss                 | 330           |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.000787     |
|    std                  | 2.38          |
|    value_loss           | 1.06e+03      |
-------------------------------------------
Eval num_timesteps=816000, episode_reward=223.77 +/- 228.34
Episode length: 510.80 +/- 41.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | 224          |
| time/                   |              |
|    total_timesteps      | 816000       |
| train/                  |              |
|    approx_kl            | 0.0031898736 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 635          |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 2.38         |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=818000, episode_reward=267.37 +/- 325.28
Episode length: 543.60 +/- 117.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 818000       |
| train/                  |              |
|    approx_kl            | 0.0012067112 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.691        |
|    learning_rate        | 0.001        |
|    loss                 | 720          |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.000469    |
|    std                  | 2.38         |
|    value_loss           | 2.04e+03     |
------------------------------------------
Eval num_timesteps=820000, episode_reward=-110.67 +/- 32.71
Episode length: 396.60 +/- 37.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | -111        |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.002342917 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 217         |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 2.38        |
|    value_loss           | 620         |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=-17.79 +/- 145.64
Episode length: 416.60 +/- 70.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | -17.8        |
| time/                   |              |
|    total_timesteps      | 822000       |
| train/                  |              |
|    approx_kl            | 0.0050715236 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.08        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 197          |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 2.38         |
|    value_loss           | 561          |
------------------------------------------
Eval num_timesteps=824000, episode_reward=-85.20 +/- 45.18
Episode length: 382.60 +/- 34.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | -85.2       |
| time/                   |             |
|    total_timesteps      | 824000      |
| train/                  |             |
|    approx_kl            | 0.009297089 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 144         |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 2.38        |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=826000, episode_reward=-89.29 +/- 88.66
Episode length: 344.60 +/- 32.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 345         |
|    mean_reward          | -89.3       |
| time/                   |             |
|    total_timesteps      | 826000      |
| train/                  |             |
|    approx_kl            | 0.001788704 |
|    clip_fraction        | 0.00303     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 132         |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 2.38        |
|    value_loss           | 371         |
-----------------------------------------
Eval num_timesteps=828000, episode_reward=-86.98 +/- 51.80
Episode length: 376.40 +/- 31.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | -87         |
| time/                   |             |
|    total_timesteps      | 828000      |
| train/                  |             |
|    approx_kl            | 0.005920974 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.08       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 101         |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00185    |
|    std                  | 2.39        |
|    value_loss           | 266         |
-----------------------------------------
Eval num_timesteps=830000, episode_reward=-112.16 +/- 22.49
Episode length: 368.80 +/- 15.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0101801595 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.09        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.00329     |
|    std                  | 2.39         |
|    value_loss           | 292          |
------------------------------------------
Eval num_timesteps=832000, episode_reward=-112.01 +/- 30.30
Episode length: 354.80 +/- 40.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 832000      |
| train/                  |             |
|    approx_kl            | 0.004418454 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.11       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 86.3        |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00136    |
|    std                  | 2.4         |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=834000, episode_reward=-124.87 +/- 13.68
Episode length: 326.00 +/- 13.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0068337824 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.12        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 88.7         |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00362     |
|    std                  | 2.41         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=836000, episode_reward=-57.19 +/- 46.76
Episode length: 388.20 +/- 33.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | -57.2       |
| time/                   |             |
|    total_timesteps      | 836000      |
| train/                  |             |
|    approx_kl            | 0.004293788 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.13       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 86.2        |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.000774   |
|    std                  | 2.42        |
|    value_loss           | 212         |
-----------------------------------------
Eval num_timesteps=838000, episode_reward=-70.57 +/- 63.00
Episode length: 384.60 +/- 65.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 385        |
|    mean_reward          | -70.6      |
| time/                   |            |
|    total_timesteps      | 838000     |
| train/                  |            |
|    approx_kl            | 0.00686601 |
|    clip_fraction        | 0.0314     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.14      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.001      |
|    loss                 | 108        |
|    n_updates            | 4090       |
|    policy_gradient_loss | -0.000789  |
|    std                  | 2.43       |
|    value_loss           | 277        |
----------------------------------------
Eval num_timesteps=840000, episode_reward=-14.93 +/- 68.64
Episode length: 422.60 +/- 31.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | -14.9        |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0066224886 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.16        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 68.6         |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00323     |
|    std                  | 2.44         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=842000, episode_reward=-102.38 +/- 43.53
Episode length: 324.60 +/- 61.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | -102        |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.008087521 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.18       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 161         |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00205    |
|    std                  | 2.45        |
|    value_loss           | 420         |
-----------------------------------------
Eval num_timesteps=844000, episode_reward=133.16 +/- 476.63
Episode length: 389.60 +/- 73.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0056716637 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.2         |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 89.5         |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.46         |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=846000, episode_reward=-50.08 +/- 18.56
Episode length: 408.00 +/- 36.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | -50.1        |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 0.0071701095 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.23        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 2.48         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=848000, episode_reward=769.51 +/- 1140.82
Episode length: 493.80 +/- 195.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | 770         |
| time/                   |             |
|    total_timesteps      | 848000      |
| train/                  |             |
|    approx_kl            | 0.008069694 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.24       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.001       |
|    loss                 | 353         |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 2.48        |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=850000, episode_reward=99.21 +/- 220.86
Episode length: 471.20 +/- 159.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 99.2        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.001147801 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.25       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.001       |
|    loss                 | 628         |
|    n_updates            | 4150        |
|    policy_gradient_loss | 0.000123    |
|    std                  | 2.49        |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=852000, episode_reward=145.82 +/- 229.57
Episode length: 428.40 +/- 62.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 0.0052534644 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.26        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 331          |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 2.49         |
|    value_loss           | 897          |
------------------------------------------
Eval num_timesteps=854000, episode_reward=596.54 +/- 912.78
Episode length: 464.60 +/- 113.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 465      |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
Eval num_timesteps=856000, episode_reward=228.10 +/- 89.77
Episode length: 394.80 +/- 44.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 856000      |
| train/                  |             |
|    approx_kl            | 0.011616828 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.27       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.001       |
|    loss                 | 464         |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 2.5         |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=858000, episode_reward=85.96 +/- 64.34
Episode length: 337.20 +/- 25.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | 86           |
| time/                   |              |
|    total_timesteps      | 858000       |
| train/                  |              |
|    approx_kl            | 0.0034951584 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.001        |
|    loss                 | 199          |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 2.5          |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=860000, episode_reward=93.32 +/- 64.48
Episode length: 352.00 +/- 35.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 93.3         |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 0.0060339607 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 239          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 2.5          |
|    value_loss           | 704          |
------------------------------------------
Eval num_timesteps=862000, episode_reward=108.20 +/- 91.07
Episode length: 337.60 +/- 46.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 108          |
| time/                   |              |
|    total_timesteps      | 862000       |
| train/                  |              |
|    approx_kl            | 0.0070371768 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 190          |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 2.5          |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=864000, episode_reward=58.91 +/- 51.72
Episode length: 315.60 +/- 34.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | 58.9         |
| time/                   |              |
|    total_timesteps      | 864000       |
| train/                  |              |
|    approx_kl            | 0.0026367393 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.27        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 220          |
|    n_updates            | 4210         |
|    policy_gradient_loss | 0.000121     |
|    std                  | 2.5          |
|    value_loss           | 544          |
------------------------------------------
Eval num_timesteps=866000, episode_reward=143.59 +/- 77.94
Episode length: 363.40 +/- 43.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 363         |
|    mean_reward          | 144         |
| time/                   |             |
|    total_timesteps      | 866000      |
| train/                  |             |
|    approx_kl            | 0.002627975 |
|    clip_fraction        | 0.000635    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.28       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 151         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 2.51        |
|    value_loss           | 423         |
-----------------------------------------
Eval num_timesteps=868000, episode_reward=86.47 +/- 73.48
Episode length: 326.60 +/- 59.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 327         |
|    mean_reward          | 86.5        |
| time/                   |             |
|    total_timesteps      | 868000      |
| train/                  |             |
|    approx_kl            | 0.005863006 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.29       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 2.51        |
|    value_loss           | 428         |
-----------------------------------------
Eval num_timesteps=870000, episode_reward=13.31 +/- 31.83
Episode length: 287.40 +/- 23.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 870000       |
| train/                  |              |
|    approx_kl            | 0.0059602903 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.3         |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 140          |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.000375    |
|    std                  | 2.52         |
|    value_loss           | 367          |
------------------------------------------
Eval num_timesteps=872000, episode_reward=22.65 +/- 77.78
Episode length: 292.40 +/- 49.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 872000      |
| train/                  |             |
|    approx_kl            | 0.007470903 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.32       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 97.7        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00366    |
|    std                  | 2.53        |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=874000, episode_reward=20.63 +/- 77.47
Episode length: 284.80 +/- 40.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 285        |
|    mean_reward          | 20.6       |
| time/                   |            |
|    total_timesteps      | 874000     |
| train/                  |            |
|    approx_kl            | 0.00458988 |
|    clip_fraction        | 0.00962    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.001      |
|    loss                 | 85.1       |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.000348  |
|    std                  | 2.54       |
|    value_loss           | 201        |
----------------------------------------
Eval num_timesteps=876000, episode_reward=168.17 +/- 52.04
Episode length: 359.00 +/- 15.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 876000      |
| train/                  |             |
|    approx_kl            | 0.004763577 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.34       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 100         |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 2.55        |
|    value_loss           | 210         |
-----------------------------------------
Eval num_timesteps=878000, episode_reward=205.61 +/- 171.63
Episode length: 368.20 +/- 53.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 878000      |
| train/                  |             |
|    approx_kl            | 0.002889333 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.35       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.001       |
|    loss                 | 286         |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00204    |
|    std                  | 2.55        |
|    value_loss           | 1.34e+03    |
-----------------------------------------
Eval num_timesteps=880000, episode_reward=1.73 +/- 46.55
Episode length: 301.40 +/- 43.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | 1.73        |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.003400403 |
|    clip_fraction        | 0.00347     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.36       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 87.1        |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 2.56        |
|    value_loss           | 260         |
-----------------------------------------
Eval num_timesteps=882000, episode_reward=100.54 +/- 175.37
Episode length: 321.80 +/- 67.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 101          |
| time/                   |              |
|    total_timesteps      | 882000       |
| train/                  |              |
|    approx_kl            | 0.0069014933 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.36        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 96.6         |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 2.56         |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=884000, episode_reward=99.29 +/- 57.52
Episode length: 329.20 +/- 23.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 329        |
|    mean_reward          | 99.3       |
| time/                   |            |
|    total_timesteps      | 884000     |
| train/                  |            |
|    approx_kl            | 0.00825037 |
|    clip_fraction        | 0.0222     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.36      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.001      |
|    loss                 | 70.8       |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.00271   |
|    std                  | 2.56       |
|    value_loss           | 165        |
----------------------------------------
Eval num_timesteps=886000, episode_reward=235.84 +/- 159.82
Episode length: 391.80 +/- 40.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 886000      |
| train/                  |             |
|    approx_kl            | 0.009806545 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.36       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 70.5        |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 2.56        |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=888000, episode_reward=138.89 +/- 102.13
Episode length: 362.60 +/- 32.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 363          |
|    mean_reward          | 139          |
| time/                   |              |
|    total_timesteps      | 888000       |
| train/                  |              |
|    approx_kl            | 0.0053100754 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.37        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 51.4         |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.000926    |
|    std                  | 2.56         |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=890000, episode_reward=110.02 +/- 96.77
Episode length: 350.60 +/- 57.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 110         |
| time/                   |             |
|    total_timesteps      | 890000      |
| train/                  |             |
|    approx_kl            | 0.004986956 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.38       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 63.3        |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00131    |
|    std                  | 2.57        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=892000, episode_reward=237.38 +/- 268.69
Episode length: 370.40 +/- 40.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 892000      |
| train/                  |             |
|    approx_kl            | 0.006931443 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.38       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 57.1        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0043     |
|    std                  | 2.57        |
|    value_loss           | 128         |
-----------------------------------------
Eval num_timesteps=894000, episode_reward=188.34 +/- 176.06
Episode length: 376.60 +/- 62.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 188          |
| time/                   |              |
|    total_timesteps      | 894000       |
| train/                  |              |
|    approx_kl            | 0.0071611586 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.4         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 49           |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 2.58         |
|    value_loss           | 116          |
------------------------------------------
Eval num_timesteps=896000, episode_reward=119.05 +/- 240.87
Episode length: 343.60 +/- 66.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 896000       |
| train/                  |              |
|    approx_kl            | 0.0055734445 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 80.9         |
|    n_updates            | 4370         |
|    policy_gradient_loss | 0.000104     |
|    std                  | 2.59         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=898000, episode_reward=173.62 +/- 252.07
Episode length: 389.20 +/- 95.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 174          |
| time/                   |              |
|    total_timesteps      | 898000       |
| train/                  |              |
|    approx_kl            | 0.0077408706 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 64           |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 2.58         |
|    value_loss           | 171          |
------------------------------------------
Eval num_timesteps=900000, episode_reward=83.75 +/- 139.21
Episode length: 351.80 +/- 46.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 352          |
|    mean_reward          | 83.7         |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0063280985 |
|    clip_fraction        | 0.07         |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 66.7         |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 2.58         |
|    value_loss           | 174          |
------------------------------------------
Eval num_timesteps=902000, episode_reward=8.67 +/- 45.21
Episode length: 367.60 +/- 43.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 8.67         |
| time/                   |              |
|    total_timesteps      | 902000       |
| train/                  |              |
|    approx_kl            | 0.0049991403 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 93.9         |
|    n_updates            | 4400         |
|    policy_gradient_loss | 0.000835     |
|    std                  | 2.58         |
|    value_loss           | 208          |
------------------------------------------
Eval num_timesteps=904000, episode_reward=34.81 +/- 138.32
Episode length: 359.80 +/- 61.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 360          |
|    mean_reward          | 34.8         |
| time/                   |              |
|    total_timesteps      | 904000       |
| train/                  |              |
|    approx_kl            | 0.0033306691 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 73.4         |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.000595    |
|    std                  | 2.58         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=906000, episode_reward=311.40 +/- 351.37
Episode length: 423.60 +/- 63.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 906000       |
| train/                  |              |
|    approx_kl            | 0.0075006364 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.41        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00298     |
|    std                  | 2.59         |
|    value_loss           | 359          |
------------------------------------------
Eval num_timesteps=908000, episode_reward=1281.76 +/- 2173.11
Episode length: 480.00 +/- 115.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 908000      |
| train/                  |             |
|    approx_kl            | 0.012449624 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 140         |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 2.59        |
|    value_loss           | 370         |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=227.51 +/- 341.14
Episode length: 400.00 +/- 61.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0018356111 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 313          |
|    n_updates            | 4440         |
|    policy_gradient_loss | 4.42e-05     |
|    std                  | 2.59         |
|    value_loss           | 730          |
------------------------------------------
Eval num_timesteps=912000, episode_reward=96.84 +/- 61.90
Episode length: 404.60 +/- 31.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 96.8        |
| time/                   |             |
|    total_timesteps      | 912000      |
| train/                  |             |
|    approx_kl            | 0.007874457 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 403         |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 2.59        |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=914000, episode_reward=13.38 +/- 31.72
Episode length: 377.00 +/- 23.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 914000       |
| train/                  |              |
|    approx_kl            | 0.0012498097 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 438          |
|    n_updates            | 4460         |
|    policy_gradient_loss | 3.35e-06     |
|    std                  | 2.59         |
|    value_loss           | 1.02e+03     |
------------------------------------------
Eval num_timesteps=916000, episode_reward=-16.42 +/- 28.69
Episode length: 389.80 +/- 34.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | -16.4        |
| time/                   |              |
|    total_timesteps      | 916000       |
| train/                  |              |
|    approx_kl            | 0.0049859416 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 52.9         |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.000779    |
|    std                  | 2.59         |
|    value_loss           | 191          |
------------------------------------------
Eval num_timesteps=918000, episode_reward=14.59 +/- 55.46
Episode length: 418.60 +/- 66.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 918000      |
| train/                  |             |
|    approx_kl            | 0.005149808 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.42       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 54.2        |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00058    |
|    std                  | 2.59        |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=920000, episode_reward=9.55 +/- 88.27
Episode length: 476.60 +/- 132.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 9.55         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0050894003 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.42        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 2.59         |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=922000, episode_reward=36.75 +/- 32.67
Episode length: 532.20 +/- 110.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 532          |
|    mean_reward          | 36.7         |
| time/                   |              |
|    total_timesteps      | 922000       |
| train/                  |              |
|    approx_kl            | 0.0034370702 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.43        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 2.6          |
|    value_loss           | 484          |
------------------------------------------
Eval num_timesteps=924000, episode_reward=-27.45 +/- 29.00
Episode length: 435.40 +/- 47.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | -27.5        |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0010916447 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.43        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000107    |
|    std                  | 2.6          |
|    value_loss           | 377          |
------------------------------------------
Eval num_timesteps=926000, episode_reward=21.59 +/- 55.30
Episode length: 470.80 +/- 55.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 21.6         |
| time/                   |              |
|    total_timesteps      | 926000       |
| train/                  |              |
|    approx_kl            | 0.0049911314 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.44        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 87.4         |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 2.6          |
|    value_loss           | 326          |
------------------------------------------
Eval num_timesteps=928000, episode_reward=6.34 +/- 50.04
Episode length: 522.80 +/- 79.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 523         |
|    mean_reward          | 6.34        |
| time/                   |             |
|    total_timesteps      | 928000      |
| train/                  |             |
|    approx_kl            | 0.010316268 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.44       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 71.4        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 2.6         |
|    value_loss           | 290         |
-----------------------------------------
Eval num_timesteps=930000, episode_reward=-35.44 +/- 28.76
Episode length: 500.60 +/- 42.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | -35.4       |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.002653364 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.44       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 92.6        |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00112    |
|    std                  | 2.61        |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=932000, episode_reward=-73.85 +/- 42.80
Episode length: 448.80 +/- 91.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | -73.8        |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 0.0051375655 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 2.61         |
|    value_loss           | 288          |
------------------------------------------
Eval num_timesteps=934000, episode_reward=-79.03 +/- 19.32
Episode length: 386.40 +/- 30.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | -79          |
| time/                   |              |
|    total_timesteps      | 934000       |
| train/                  |              |
|    approx_kl            | 0.0026362329 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 70           |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.62         |
|    value_loss           | 190          |
------------------------------------------
Eval num_timesteps=936000, episode_reward=-68.78 +/- 8.31
Episode length: 441.20 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | -68.8        |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0051661115 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 74.5         |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 2.61         |
|    value_loss           | 198          |
------------------------------------------
Eval num_timesteps=938000, episode_reward=-112.23 +/- 9.19
Episode length: 413.60 +/- 40.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 938000       |
| train/                  |              |
|    approx_kl            | 0.0049475683 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 67.1         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 2.61         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=940000, episode_reward=-92.80 +/- 32.07
Episode length: 407.80 +/- 42.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -92.8    |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=942000, episode_reward=-87.03 +/- 25.64
Episode length: 394.00 +/- 32.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 394        |
|    mean_reward          | -87        |
| time/                   |            |
|    total_timesteps      | 942000     |
| train/                  |            |
|    approx_kl            | 0.00219108 |
|    clip_fraction        | 0.00234    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.001      |
|    loss                 | 85.7       |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.000538  |
|    std                  | 2.61       |
|    value_loss           | 198        |
----------------------------------------
Eval num_timesteps=944000, episode_reward=-85.49 +/- 33.40
Episode length: 385.40 +/- 74.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | -85.5        |
| time/                   |              |
|    total_timesteps      | 944000       |
| train/                  |              |
|    approx_kl            | 0.0052355225 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 67.5         |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 2.62         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=946000, episode_reward=-59.01 +/- 18.99
Episode length: 411.00 +/- 38.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | -59          |
| time/                   |              |
|    total_timesteps      | 946000       |
| train/                  |              |
|    approx_kl            | 0.0041834963 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.45        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 54.7         |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 2.62         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=948000, episode_reward=-22.88 +/- 38.17
Episode length: 409.20 +/- 58.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | -22.9        |
| time/                   |              |
|    total_timesteps      | 948000       |
| train/                  |              |
|    approx_kl            | 0.0029817384 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.46        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 69.6         |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.000474    |
|    std                  | 2.62         |
|    value_loss           | 182          |
------------------------------------------
Eval num_timesteps=950000, episode_reward=-92.17 +/- 10.94
Episode length: 401.60 +/- 44.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | -92.2        |
| time/                   |              |
|    total_timesteps      | 950000       |
| train/                  |              |
|    approx_kl            | 0.0051535703 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.46        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 2.62         |
|    value_loss           | 308          |
------------------------------------------
Eval num_timesteps=952000, episode_reward=-70.24 +/- 24.02
Episode length: 427.20 +/- 33.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | -70.2       |
| time/                   |             |
|    total_timesteps      | 952000      |
| train/                  |             |
|    approx_kl            | 0.004368231 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.47       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 55.1        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 2.63        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=954000, episode_reward=-75.50 +/- 7.06
Episode length: 415.80 +/- 19.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | -75.5       |
| time/                   |             |
|    total_timesteps      | 954000      |
| train/                  |             |
|    approx_kl            | 0.009254493 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.48       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 76.5        |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00498    |
|    std                  | 2.64        |
|    value_loss           | 202         |
-----------------------------------------
Eval num_timesteps=956000, episode_reward=-15.98 +/- 72.52
Episode length: 496.20 +/- 57.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -16          |
| time/                   |              |
|    total_timesteps      | 956000       |
| train/                  |              |
|    approx_kl            | 0.0071984483 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.48        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 56.6         |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 2.64         |
|    value_loss           | 196          |
------------------------------------------
Eval num_timesteps=958000, episode_reward=-63.80 +/- 40.11
Episode length: 423.80 +/- 32.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | -63.8       |
| time/                   |             |
|    total_timesteps      | 958000      |
| train/                  |             |
|    approx_kl            | 0.008205177 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.49       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 37.7        |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00307    |
|    std                  | 2.65        |
|    value_loss           | 99.8        |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=-112.19 +/- 44.94
Episode length: 428.00 +/- 71.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.010703154 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.49       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 75.8        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.00345    |
|    std                  | 2.65        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=962000, episode_reward=-65.86 +/- 35.41
Episode length: 448.20 +/- 67.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | -65.9        |
| time/                   |              |
|    total_timesteps      | 962000       |
| train/                  |              |
|    approx_kl            | 0.0063028196 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.5         |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 52.2         |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 2.66         |
|    value_loss           | 127          |
------------------------------------------
Eval num_timesteps=964000, episode_reward=-42.99 +/- 60.86
Episode length: 475.60 +/- 57.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | -43         |
| time/                   |             |
|    total_timesteps      | 964000      |
| train/                  |             |
|    approx_kl            | 0.004717756 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.52       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 46          |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00148    |
|    std                  | 2.67        |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=966000, episode_reward=-64.14 +/- 33.69
Episode length: 457.20 +/- 39.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | -64.1        |
| time/                   |              |
|    total_timesteps      | 966000       |
| train/                  |              |
|    approx_kl            | 0.0050114766 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 63.7         |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 2.67         |
|    value_loss           | 185          |
------------------------------------------
Eval num_timesteps=968000, episode_reward=42.35 +/- 139.12
Episode length: 481.00 +/- 103.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 42.4         |
| time/                   |              |
|    total_timesteps      | 968000       |
| train/                  |              |
|    approx_kl            | 0.0077830874 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.53        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 38.9         |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00419     |
|    std                  | 2.67         |
|    value_loss           | 84.8         |
------------------------------------------
Eval num_timesteps=970000, episode_reward=-43.84 +/- 70.45
Episode length: 416.60 +/- 63.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 417        |
|    mean_reward          | -43.8      |
| time/                   |            |
|    total_timesteps      | 970000     |
| train/                  |            |
|    approx_kl            | 0.00967334 |
|    clip_fraction        | 0.0796     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.53      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.001      |
|    loss                 | 35         |
|    n_updates            | 4730       |
|    policy_gradient_loss | -0.00491   |
|    std                  | 2.66       |
|    value_loss           | 97.9       |
----------------------------------------
Eval num_timesteps=972000, episode_reward=-35.97 +/- 33.13
Episode length: 442.00 +/- 32.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | -36         |
| time/                   |             |
|    total_timesteps      | 972000      |
| train/                  |             |
|    approx_kl            | 0.013831923 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.53       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 44.7        |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.67        |
|    value_loss           | 113         |
-----------------------------------------
Eval num_timesteps=974000, episode_reward=218.11 +/- 240.61
Episode length: 510.20 +/- 16.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 974000      |
| train/                  |             |
|    approx_kl            | 0.008835798 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.55       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 47.9        |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 2.67        |
|    value_loss           | 147         |
-----------------------------------------
Eval num_timesteps=976000, episode_reward=146.19 +/- 123.32
Episode length: 525.60 +/- 16.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 976000       |
| train/                  |              |
|    approx_kl            | 0.0075565996 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.55        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 66.8         |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 2.68         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=978000, episode_reward=124.81 +/- 115.64
Episode length: 504.00 +/- 15.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 978000       |
| train/                  |              |
|    approx_kl            | 0.0054656356 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.56        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 61.4         |
|    n_updates            | 4770         |
|    policy_gradient_loss | -3.79e-05    |
|    std                  | 2.68         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=980000, episode_reward=212.99 +/- 182.36
Episode length: 486.60 +/- 62.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 213           |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 0.00076838856 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.56         |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.001         |
|    loss                 | 39.9          |
|    n_updates            | 4780          |
|    policy_gradient_loss | -0.00061      |
|    std                  | 2.68          |
|    value_loss           | 102           |
-------------------------------------------
Eval num_timesteps=982000, episode_reward=72.05 +/- 94.62
Episode length: 476.40 +/- 53.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 72           |
| time/                   |              |
|    total_timesteps      | 982000       |
| train/                  |              |
|    approx_kl            | 0.0020367599 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.57        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 79.3         |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.68         |
|    value_loss           | 277          |
------------------------------------------
Eval num_timesteps=984000, episode_reward=19.82 +/- 50.63
Episode length: 510.00 +/- 24.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 19.8        |
| time/                   |             |
|    total_timesteps      | 984000      |
| train/                  |             |
|    approx_kl            | 0.004063602 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.57       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 94.1        |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.000969   |
|    std                  | 2.69        |
|    value_loss           | 327         |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=62.43 +/- 158.17
Episode length: 462.80 +/- 74.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 62.4         |
| time/                   |              |
|    total_timesteps      | 986000       |
| train/                  |              |
|    approx_kl            | 0.0020295247 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 46           |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.69         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=988000, episode_reward=116.41 +/- 109.79
Episode length: 490.80 +/- 60.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 116         |
| time/                   |             |
|    total_timesteps      | 988000      |
| train/                  |             |
|    approx_kl            | 0.008343512 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 62.5        |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 2.7         |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=373.69 +/- 544.73
Episode length: 488.80 +/- 74.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 0.0005477442 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.59        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 90           |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000151    |
|    std                  | 2.7          |
|    value_loss           | 323          |
------------------------------------------
Eval num_timesteps=992000, episode_reward=192.67 +/- 85.82
Episode length: 528.60 +/- 17.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 529         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 992000      |
| train/                  |             |
|    approx_kl            | 0.008195194 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 35.5        |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00282    |
|    std                  | 2.7         |
|    value_loss           | 93.8        |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=568.96 +/- 429.62
Episode length: 530.40 +/- 18.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 530         |
|    mean_reward          | 569         |
| time/                   |             |
|    total_timesteps      | 994000      |
| train/                  |             |
|    approx_kl            | 0.002276261 |
|    clip_fraction        | 0.004       |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 37          |
|    n_updates            | 4850        |
|    policy_gradient_loss | -2.4e-05    |
|    std                  | 2.7         |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=315.77 +/- 145.93
Episode length: 514.80 +/- 37.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 996000      |
| train/                  |             |
|    approx_kl            | 0.010445779 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.59       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 58.8        |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 2.71        |
|    value_loss           | 196         |
-----------------------------------------
Eval num_timesteps=998000, episode_reward=254.40 +/- 225.35
Episode length: 437.20 +/- 80.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 998000      |
| train/                  |             |
|    approx_kl            | 0.004462145 |
|    clip_fraction        | 0.00659     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.6        |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 88.7        |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 2.71        |
|    value_loss           | 328         |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=285.88 +/- 175.74
Episode length: 417.00 +/- 33.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0023715359 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+03      |
|    n_updates            | 4880         |
|    policy_gradient_loss | 0.000203     |
|    std                  | 2.71         |
|    value_loss           | 8.24e+03     |
------------------------------------------
Eval num_timesteps=1002000, episode_reward=30.28 +/- 174.44
Episode length: 440.40 +/- 54.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 30.3         |
| time/                   |              |
|    total_timesteps      | 1002000      |
| train/                  |              |
|    approx_kl            | 0.0019197343 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 84.8         |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 2.71         |
|    value_loss           | 293          |
------------------------------------------
Eval num_timesteps=1004000, episode_reward=466.09 +/- 373.06
Episode length: 423.80 +/- 62.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 1004000      |
| train/                  |              |
|    approx_kl            | 0.0062180934 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 64.8         |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 2.72         |
|    value_loss           | 189          |
------------------------------------------
Eval num_timesteps=1006000, episode_reward=356.96 +/- 260.54
Episode length: 421.60 +/- 28.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 357         |
| time/                   |             |
|    total_timesteps      | 1006000     |
| train/                  |             |
|    approx_kl            | 0.009460546 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.62       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 430         |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 2.72        |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=1008000, episode_reward=194.33 +/- 112.72
Episode length: 409.00 +/- 52.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 1008000      |
| train/                  |              |
|    approx_kl            | 0.0053957067 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.001        |
|    loss                 | 5.33e+03     |
|    n_updates            | 4920         |
|    policy_gradient_loss | 0.00158      |
|    std                  | 2.72         |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=1010000, episode_reward=216.22 +/- 147.48
Episode length: 410.20 +/- 27.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 0.0006057468 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+03     |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.000232    |
|    std                  | 2.72         |
|    value_loss           | 5.42e+03     |
------------------------------------------
Eval num_timesteps=1012000, episode_reward=367.28 +/- 203.21
Episode length: 391.40 +/- 34.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 1012000      |
| train/                  |              |
|    approx_kl            | 6.491263e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 3.37e+03     |
|    n_updates            | 4940         |
|    policy_gradient_loss | -1.73e-05    |
|    std                  | 2.72         |
|    value_loss           | 7.74e+03     |
------------------------------------------
Eval num_timesteps=1014000, episode_reward=118.57 +/- 110.24
Episode length: 365.20 +/- 24.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 1014000      |
| train/                  |              |
|    approx_kl            | 0.0044367155 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 142          |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 2.72         |
|    value_loss           | 663          |
------------------------------------------
Eval num_timesteps=1016000, episode_reward=393.60 +/- 284.85
Episode length: 413.80 +/- 25.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 394         |
| time/                   |             |
|    total_timesteps      | 1016000     |
| train/                  |             |
|    approx_kl            | 0.010004212 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 63.5        |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.00366    |
|    std                  | 2.72        |
|    value_loss           | 241         |
-----------------------------------------
Eval num_timesteps=1018000, episode_reward=396.11 +/- 214.96
Episode length: 414.00 +/- 19.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 0.0026761927 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 1.76e+03     |
|    n_updates            | 4970         |
|    policy_gradient_loss | -7.8e-05     |
|    std                  | 2.71         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=586.17 +/- 234.31
Episode length: 419.80 +/- 72.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 586          |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0007461169 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.736        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+03     |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 2.71         |
|    value_loss           | 7.49e+03     |
------------------------------------------
Eval num_timesteps=1022000, episode_reward=265.50 +/- 318.83
Episode length: 411.60 +/- 43.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 1022000      |
| train/                  |              |
|    approx_kl            | 0.0001196003 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 2.54e+03     |
|    n_updates            | 4990         |
|    policy_gradient_loss | 0.000293     |
|    std                  | 2.71         |
|    value_loss           | 5.24e+03     |
------------------------------------------
Eval num_timesteps=1024000, episode_reward=229.72 +/- 317.14
Episode length: 408.80 +/- 83.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 409      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1026000, episode_reward=386.45 +/- 94.02
Episode length: 386.80 +/- 17.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 1026000     |
| train/                  |             |
|    approx_kl            | 3.62813e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.6        |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.001       |
|    loss                 | 5.09e+03    |
|    n_updates            | 5000        |
|    policy_gradient_loss | -7.6e-05    |
|    std                  | 2.71        |
|    value_loss           | 1.05e+04    |
-----------------------------------------
Eval num_timesteps=1028000, episode_reward=571.86 +/- 328.49
Episode length: 418.00 +/- 26.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 572          |
| time/                   |              |
|    total_timesteps      | 1028000      |
| train/                  |              |
|    approx_kl            | 7.111393e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.000401    |
|    std                  | 2.72         |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=1030000, episode_reward=534.60 +/- 450.54
Episode length: 454.40 +/- 52.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 535           |
| time/                   |               |
|    total_timesteps      | 1030000       |
| train/                  |               |
|    approx_kl            | 0.00031462757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.6          |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 136           |
|    n_updates            | 5020          |
|    policy_gradient_loss | -0.000103     |
|    std                  | 2.72          |
|    value_loss           | 610           |
-------------------------------------------
Eval num_timesteps=1032000, episode_reward=170.36 +/- 254.18
Episode length: 404.80 +/- 23.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 1032000      |
| train/                  |              |
|    approx_kl            | 0.0001792731 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 5.79e+03     |
|    n_updates            | 5030         |
|    policy_gradient_loss | 0.000314     |
|    std                  | 2.71         |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=1034000, episode_reward=591.69 +/- 441.40
Episode length: 439.40 +/- 59.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 592          |
| time/                   |              |
|    total_timesteps      | 1034000      |
| train/                  |              |
|    approx_kl            | 0.0002455939 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 249          |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.000271    |
|    std                  | 2.72         |
|    value_loss           | 768          |
------------------------------------------
Eval num_timesteps=1036000, episode_reward=481.57 +/- 299.38
Episode length: 416.00 +/- 44.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 1036000      |
| train/                  |              |
|    approx_kl            | 0.0005557415 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.6         |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.001        |
|    loss                 | 2.26e+03     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 2.72         |
|    value_loss           | 4.9e+03      |
------------------------------------------
Eval num_timesteps=1038000, episode_reward=511.89 +/- 248.16
Episode length: 393.20 +/- 30.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | 512         |
| time/                   |             |
|    total_timesteps      | 1038000     |
| train/                  |             |
|    approx_kl            | 0.000365106 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.001       |
|    loss                 | 3.75e+03    |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.000417   |
|    std                  | 2.72        |
|    value_loss           | 7.74e+03    |
-----------------------------------------
Eval num_timesteps=1040000, episode_reward=430.48 +/- 309.02
Episode length: 415.60 +/- 29.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 1040000       |
| train/                  |               |
|    approx_kl            | 0.00081597304 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.61         |
|    explained_variance   | 0.709         |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+03      |
|    n_updates            | 5070          |
|    policy_gradient_loss | -0.00118      |
|    std                  | 2.72          |
|    value_loss           | 9.77e+03      |
-------------------------------------------
Eval num_timesteps=1042000, episode_reward=646.47 +/- 211.96
Episode length: 397.20 +/- 39.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 646          |
| time/                   |              |
|    total_timesteps      | 1042000      |
| train/                  |              |
|    approx_kl            | 0.0005203659 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.61        |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+03     |
|    n_updates            | 5080         |
|    policy_gradient_loss | -8.83e-05    |
|    std                  | 2.72         |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=1044000, episode_reward=497.64 +/- 373.01
Episode length: 416.60 +/- 50.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 498           |
| time/                   |               |
|    total_timesteps      | 1044000       |
| train/                  |               |
|    approx_kl            | 0.00010497696 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.61         |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 111           |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000116     |
|    std                  | 2.72          |
|    value_loss           | 326           |
-------------------------------------------
Eval num_timesteps=1046000, episode_reward=479.97 +/- 282.88
Episode length: 392.00 +/- 29.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 480         |
| time/                   |             |
|    total_timesteps      | 1046000     |
| train/                  |             |
|    approx_kl            | 0.007131762 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.001       |
|    loss                 | 220         |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.00204    |
|    std                  | 2.72        |
|    value_loss           | 881         |
-----------------------------------------
Eval num_timesteps=1048000, episode_reward=180.78 +/- 279.30
Episode length: 354.60 +/- 45.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 1048000     |
| train/                  |             |
|    approx_kl            | 0.005217372 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.61       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 164         |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.00124    |
|    std                  | 2.72        |
|    value_loss           | 527         |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=522.87 +/- 168.06
Episode length: 373.20 +/- 28.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 523          |
| time/                   |              |
|    total_timesteps      | 1050000      |
| train/                  |              |
|    approx_kl            | 0.0007183843 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.62        |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 5120         |
|    policy_gradient_loss | 0.000261     |
|    std                  | 2.73         |
|    value_loss           | 5.63e+03     |
------------------------------------------
Eval num_timesteps=1052000, episode_reward=279.34 +/- 193.24
Episode length: 338.60 +/- 40.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 339           |
|    mean_reward          | 279           |
| time/                   |               |
|    total_timesteps      | 1052000       |
| train/                  |               |
|    approx_kl            | 0.00071063853 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.62         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 104           |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 2.73          |
|    value_loss           | 319           |
-------------------------------------------
Eval num_timesteps=1054000, episode_reward=555.67 +/- 204.61
Episode length: 377.60 +/- 10.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 1054000      |
| train/                  |              |
|    approx_kl            | 0.0046258816 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.63        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2.74         |
|    value_loss           | 233          |
------------------------------------------
Eval num_timesteps=1056000, episode_reward=398.39 +/- 173.66
Episode length: 381.60 +/- 28.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 398          |
| time/                   |              |
|    total_timesteps      | 1056000      |
| train/                  |              |
|    approx_kl            | 0.0065069287 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.64        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 2.73e+03     |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.000333    |
|    std                  | 2.75         |
|    value_loss           | 6.45e+03     |
------------------------------------------
Eval num_timesteps=1058000, episode_reward=399.29 +/- 332.48
Episode length: 384.80 +/- 27.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 385           |
|    mean_reward          | 399           |
| time/                   |               |
|    total_timesteps      | 1058000       |
| train/                  |               |
|    approx_kl            | 0.00074662676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.683         |
|    learning_rate        | 0.001         |
|    loss                 | 5.54e+03      |
|    n_updates            | 5160          |
|    policy_gradient_loss | 0.000121      |
|    std                  | 2.75          |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=1060000, episode_reward=467.99 +/- 417.43
Episode length: 436.00 +/- 58.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 468           |
| time/                   |               |
|    total_timesteps      | 1060000       |
| train/                  |               |
|    approx_kl            | 6.3854124e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66         |
|    explained_variance   | 0.762         |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+03      |
|    n_updates            | 5170          |
|    policy_gradient_loss | -3.48e-05     |
|    std                  | 2.76          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1062000, episode_reward=612.89 +/- 472.73
Episode length: 454.60 +/- 40.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 613          |
| time/                   |              |
|    total_timesteps      | 1062000      |
| train/                  |              |
|    approx_kl            | 8.246969e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.66        |
|    explained_variance   | 0.695        |
|    learning_rate        | 0.001        |
|    loss                 | 5.41e+03     |
|    n_updates            | 5180         |
|    policy_gradient_loss | -4.97e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=1064000, episode_reward=137.32 +/- 248.48
Episode length: 440.60 +/- 28.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 1064000      |
| train/                  |              |
|    approx_kl            | 4.991045e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.66        |
|    explained_variance   | 0.717        |
|    learning_rate        | 0.001        |
|    loss                 | 5.63e+03     |
|    n_updates            | 5190         |
|    policy_gradient_loss | -8.29e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=1066000, episode_reward=342.28 +/- 303.73
Episode length: 407.20 +/- 38.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 342           |
| time/                   |               |
|    total_timesteps      | 1066000       |
| train/                  |               |
|    approx_kl            | 0.00017830715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.736         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 5200          |
|    policy_gradient_loss | -0.000269     |
|    std                  | 2.76          |
|    value_loss           | 8.42e+03      |
-------------------------------------------
Eval num_timesteps=1068000, episode_reward=502.75 +/- 413.70
Episode length: 449.80 +/- 45.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 1068000      |
| train/                  |              |
|    approx_kl            | 7.995157e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 5.34e+03     |
|    n_updates            | 5210         |
|    policy_gradient_loss | -5.52e-05    |
|    std                  | 2.76         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=1070000, episode_reward=-176.45 +/- 213.88
Episode length: 469.60 +/- 19.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | -176          |
| time/                   |               |
|    total_timesteps      | 1070000       |
| train/                  |               |
|    approx_kl            | 0.00012068558 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.773         |
|    learning_rate        | 0.001         |
|    loss                 | 5.98e+03      |
|    n_updates            | 5220          |
|    policy_gradient_loss | -0.000276     |
|    std                  | 2.76          |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=1072000, episode_reward=218.71 +/- 304.61
Episode length: 428.80 +/- 45.94
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 429            |
|    mean_reward          | 219            |
| time/                   |                |
|    total_timesteps      | 1072000        |
| train/                  |                |
|    approx_kl            | 0.000110044755 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.67          |
|    explained_variance   | 0.743          |
|    learning_rate        | 0.001          |
|    loss                 | 6.29e+03       |
|    n_updates            | 5230           |
|    policy_gradient_loss | -2.36e-05      |
|    std                  | 2.76           |
|    value_loss           | 1.33e+04       |
--------------------------------------------
Eval num_timesteps=1074000, episode_reward=455.71 +/- 342.75
Episode length: 466.00 +/- 41.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 456           |
| time/                   |               |
|    total_timesteps      | 1074000       |
| train/                  |               |
|    approx_kl            | 0.00010889501 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 204           |
|    n_updates            | 5240          |
|    policy_gradient_loss | -0.000221     |
|    std                  | 2.76          |
|    value_loss           | 584           |
-------------------------------------------
Eval num_timesteps=1076000, episode_reward=45.01 +/- 236.16
Episode length: 429.00 +/- 80.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 45            |
| time/                   |               |
|    total_timesteps      | 1076000       |
| train/                  |               |
|    approx_kl            | 0.00075032533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.783         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 5250          |
|    policy_gradient_loss | -0.00105      |
|    std                  | 2.76          |
|    value_loss           | 6.29e+03      |
-------------------------------------------
Eval num_timesteps=1078000, episode_reward=17.89 +/- 266.54
Episode length: 406.40 +/- 40.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 1078000      |
| train/                  |              |
|    approx_kl            | 0.0015863214 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 173          |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000336    |
|    std                  | 2.76         |
|    value_loss           | 781          |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=235.17 +/- 410.96
Episode length: 406.00 +/- 31.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 0.0002659746 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 138          |
|    n_updates            | 5270         |
|    policy_gradient_loss | 6.75e-05     |
|    std                  | 2.76         |
|    value_loss           | 508          |
------------------------------------------
Eval num_timesteps=1082000, episode_reward=178.22 +/- 270.82
Episode length: 402.60 +/- 61.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 403           |
|    mean_reward          | 178           |
| time/                   |               |
|    total_timesteps      | 1082000       |
| train/                  |               |
|    approx_kl            | 0.00035865913 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.822         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+03      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000597     |
|    std                  | 2.76          |
|    value_loss           | 3.72e+03      |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=227.90 +/- 354.56
Episode length: 391.80 +/- 36.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 1084000      |
| train/                  |              |
|    approx_kl            | 0.0004800302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+03     |
|    n_updates            | 5290         |
|    policy_gradient_loss | -6.09e-05    |
|    std                  | 2.76         |
|    value_loss           | 5.2e+03      |
------------------------------------------
Eval num_timesteps=1086000, episode_reward=53.41 +/- 366.31
Episode length: 431.20 +/- 23.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 53.4          |
| time/                   |               |
|    total_timesteps      | 1086000       |
| train/                  |               |
|    approx_kl            | 0.00018707328 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.792         |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+03      |
|    n_updates            | 5300          |
|    policy_gradient_loss | 4.55e-05      |
|    std                  | 2.76          |
|    value_loss           | 6.31e+03      |
-------------------------------------------
Eval num_timesteps=1088000, episode_reward=376.07 +/- 388.60
Episode length: 419.60 +/- 38.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 1088000      |
| train/                  |              |
|    approx_kl            | 0.0014932523 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 533          |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.76         |
|    value_loss           | 1.61e+03     |
------------------------------------------
Eval num_timesteps=1090000, episode_reward=323.82 +/- 340.07
Episode length: 496.20 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 324          |
| time/                   |              |
|    total_timesteps      | 1090000      |
| train/                  |              |
|    approx_kl            | 0.0023664564 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.000747    |
|    std                  | 2.76         |
|    value_loss           | 4.57e+03     |
------------------------------------------
Eval num_timesteps=1092000, episode_reward=358.67 +/- 285.66
Episode length: 480.60 +/- 45.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 1092000       |
| train/                  |               |
|    approx_kl            | 0.00074998365 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.607         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+04      |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.00126      |
|    std                  | 2.76          |
|    value_loss           | 2.49e+04      |
-------------------------------------------
Eval num_timesteps=1094000, episode_reward=61.90 +/- 506.62
Episode length: 456.60 +/- 22.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 61.9          |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 0.00019555789 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.779         |
|    learning_rate        | 0.001         |
|    loss                 | 4.74e+03      |
|    n_updates            | 5340          |
|    policy_gradient_loss | 2.58e-05      |
|    std                  | 2.76          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1096000, episode_reward=431.41 +/- 280.88
Episode length: 494.40 +/- 49.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 494           |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 1096000       |
| train/                  |               |
|    approx_kl            | 2.7150178e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.67         |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+03      |
|    n_updates            | 5350          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.76          |
|    value_loss           | 6.64e+03      |
-------------------------------------------
Eval num_timesteps=1098000, episode_reward=580.09 +/- 298.40
Episode length: 496.40 +/- 31.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 580          |
| time/                   |              |
|    total_timesteps      | 1098000      |
| train/                  |              |
|    approx_kl            | 0.0001429372 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.67        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+03     |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.000446    |
|    std                  | 2.77         |
|    value_loss           | 6.37e+03     |
------------------------------------------
Eval num_timesteps=1100000, episode_reward=250.32 +/- 361.19
Episode length: 444.40 +/- 57.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 250           |
| time/                   |               |
|    total_timesteps      | 1100000       |
| train/                  |               |
|    approx_kl            | 0.00023322768 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.81          |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+03      |
|    n_updates            | 5370          |
|    policy_gradient_loss | -0.000443     |
|    std                  | 2.77          |
|    value_loss           | 4.37e+03      |
-------------------------------------------
Eval num_timesteps=1102000, episode_reward=437.11 +/- 256.20
Episode length: 458.60 +/- 39.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 437           |
| time/                   |               |
|    total_timesteps      | 1102000       |
| train/                  |               |
|    approx_kl            | 0.00074634934 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.001         |
|    loss                 | 2.6e+03       |
|    n_updates            | 5380          |
|    policy_gradient_loss | -0.000946     |
|    std                  | 2.77          |
|    value_loss           | 6.01e+03      |
-------------------------------------------
Eval num_timesteps=1104000, episode_reward=-85.59 +/- 224.86
Episode length: 459.00 +/- 63.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | -85.6         |
| time/                   |               |
|    total_timesteps      | 1104000       |
| train/                  |               |
|    approx_kl            | 0.00072457804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.752         |
|    learning_rate        | 0.001         |
|    loss                 | 2.97e+03      |
|    n_updates            | 5390          |
|    policy_gradient_loss | -1.93e-05     |
|    std                  | 2.77          |
|    value_loss           | 6.96e+03      |
-------------------------------------------
Eval num_timesteps=1106000, episode_reward=351.74 +/- 200.60
Episode length: 482.60 +/- 51.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 352           |
| time/                   |               |
|    total_timesteps      | 1106000       |
| train/                  |               |
|    approx_kl            | 0.00022668517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.839         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 5400          |
|    policy_gradient_loss | 0.00011       |
|    std                  | 2.77          |
|    value_loss           | 4.5e+03       |
-------------------------------------------
Eval num_timesteps=1108000, episode_reward=81.45 +/- 398.05
Episode length: 482.20 +/- 48.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 81.5         |
| time/                   |              |
|    total_timesteps      | 1108000      |
| train/                  |              |
|    approx_kl            | 2.121643e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.745        |
|    learning_rate        | 0.001        |
|    loss                 | 3.14e+03     |
|    n_updates            | 5410         |
|    policy_gradient_loss | -1.64e-05    |
|    std                  | 2.77         |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=1110000, episode_reward=228.15 +/- 353.92
Episode length: 458.80 +/- 32.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
Eval num_timesteps=1112000, episode_reward=140.71 +/- 276.34
Episode length: 489.60 +/- 26.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | 141           |
| time/                   |               |
|    total_timesteps      | 1112000       |
| train/                  |               |
|    approx_kl            | 0.00010469614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.742         |
|    learning_rate        | 0.001         |
|    loss                 | 6.66e+03      |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000295     |
|    std                  | 2.77          |
|    value_loss           | 1.47e+04      |
-------------------------------------------
Eval num_timesteps=1114000, episode_reward=283.74 +/- 388.57
Episode length: 496.00 +/- 36.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 1114000      |
| train/                  |              |
|    approx_kl            | 0.0001864188 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+03     |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.000348    |
|    std                  | 2.77         |
|    value_loss           | 4.29e+03     |
------------------------------------------
Eval num_timesteps=1116000, episode_reward=472.11 +/- 237.07
Episode length: 440.40 +/- 65.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 472          |
| time/                   |              |
|    total_timesteps      | 1116000      |
| train/                  |              |
|    approx_kl            | 0.0007480604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 250          |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.000493    |
|    std                  | 2.77         |
|    value_loss           | 666          |
------------------------------------------
Eval num_timesteps=1118000, episode_reward=271.27 +/- 116.44
Episode length: 438.60 +/- 30.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 271           |
| time/                   |               |
|    total_timesteps      | 1118000       |
| train/                  |               |
|    approx_kl            | 0.00042650214 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 266           |
|    n_updates            | 5450          |
|    policy_gradient_loss | -8.1e-05      |
|    std                  | 2.77          |
|    value_loss           | 857           |
-------------------------------------------
Eval num_timesteps=1120000, episode_reward=36.43 +/- 98.49
Episode length: 458.40 +/- 58.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 36.4          |
| time/                   |               |
|    total_timesteps      | 1120000       |
| train/                  |               |
|    approx_kl            | 0.00064291456 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68         |
|    explained_variance   | 0.791         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 5460          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 2.77          |
|    value_loss           | 5.97e+03      |
-------------------------------------------
Eval num_timesteps=1122000, episode_reward=299.00 +/- 230.80
Episode length: 505.80 +/- 46.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 299          |
| time/                   |              |
|    total_timesteps      | 1122000      |
| train/                  |              |
|    approx_kl            | 0.0014300132 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.68        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 309          |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.001       |
|    std                  | 2.77         |
|    value_loss           | 905          |
------------------------------------------
Eval num_timesteps=1124000, episode_reward=678.54 +/- 497.30
Episode length: 518.00 +/- 20.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | 679          |
| time/                   |              |
|    total_timesteps      | 1124000      |
| train/                  |              |
|    approx_kl            | 0.0026846044 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.69        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 62           |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.000948    |
|    std                  | 2.78         |
|    value_loss           | 202          |
------------------------------------------
Eval num_timesteps=1126000, episode_reward=181.02 +/- 133.03
Episode length: 473.80 +/- 68.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 1126000      |
| train/                  |              |
|    approx_kl            | 0.0038319496 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.7         |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 52.3         |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.000817    |
|    std                  | 2.79         |
|    value_loss           | 163          |
------------------------------------------
Eval num_timesteps=1128000, episode_reward=107.74 +/- 104.94
Episode length: 446.80 +/- 45.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 108          |
| time/                   |              |
|    total_timesteps      | 1128000      |
| train/                  |              |
|    approx_kl            | 0.0059802076 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 144          |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 2.79         |
|    value_loss           | 402          |
------------------------------------------
Eval num_timesteps=1130000, episode_reward=308.40 +/- 217.68
Episode length: 419.60 +/- 41.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 1130000      |
| train/                  |              |
|    approx_kl            | 0.0027752027 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 61.8         |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00068     |
|    std                  | 2.79         |
|    value_loss           | 187          |
------------------------------------------
Eval num_timesteps=1132000, episode_reward=216.72 +/- 321.12
Episode length: 466.60 +/- 47.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | 217          |
| time/                   |              |
|    total_timesteps      | 1132000      |
| train/                  |              |
|    approx_kl            | 0.0067759603 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 41           |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.79         |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=1134000, episode_reward=179.50 +/- 122.50
Episode length: 462.40 +/- 59.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 1134000      |
| train/                  |              |
|    approx_kl            | 0.0020540007 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.71        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 46.2         |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.79         |
|    value_loss           | 152          |
------------------------------------------
Eval num_timesteps=1136000, episode_reward=297.17 +/- 255.83
Episode length: 475.40 +/- 29.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 1136000     |
| train/                  |             |
|    approx_kl            | 0.007625806 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.71       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 54.9        |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0023     |
|    std                  | 2.79        |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=1138000, episode_reward=700.49 +/- 314.62
Episode length: 495.20 +/- 26.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 700          |
| time/                   |              |
|    total_timesteps      | 1138000      |
| train/                  |              |
|    approx_kl            | 0.0027589605 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.72        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 28.8         |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000789    |
|    std                  | 2.8          |
|    value_loss           | 99.3         |
------------------------------------------
Eval num_timesteps=1140000, episode_reward=314.55 +/- 174.37
Episode length: 473.40 +/- 48.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0016390863 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.73        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+03     |
|    n_updates            | 5560         |
|    policy_gradient_loss | -9.11e-06    |
|    std                  | 2.8          |
|    value_loss           | 4.47e+03     |
------------------------------------------
Eval num_timesteps=1142000, episode_reward=74.99 +/- 213.53
Episode length: 490.60 +/- 33.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 75          |
| time/                   |             |
|    total_timesteps      | 1142000     |
| train/                  |             |
|    approx_kl            | 0.000376558 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.001       |
|    loss                 | 2.31e+03    |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.000386   |
|    std                  | 2.8         |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=1144000, episode_reward=233.18 +/- 185.53
Episode length: 516.60 +/- 20.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 1144000     |
| train/                  |             |
|    approx_kl            | 0.011782505 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 30.2        |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 2.8         |
|    value_loss           | 86.8        |
-----------------------------------------
Eval num_timesteps=1146000, episode_reward=278.35 +/- 282.76
Episode length: 480.60 +/- 22.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 481        |
|    mean_reward          | 278        |
| time/                   |            |
|    total_timesteps      | 1146000    |
| train/                  |            |
|    approx_kl            | 0.00396178 |
|    clip_fraction        | 0.00371    |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.72      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.001      |
|    loss                 | 119        |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.00146   |
|    std                  | 2.79       |
|    value_loss           | 334        |
----------------------------------------
Eval num_timesteps=1148000, episode_reward=234.72 +/- 152.42
Episode length: 495.80 +/- 29.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1148000      |
| train/                  |              |
|    approx_kl            | 0.0069391737 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.72        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 53.6         |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 2.8          |
|    value_loss           | 188          |
------------------------------------------
Eval num_timesteps=1150000, episode_reward=169.52 +/- 114.84
Episode length: 468.60 +/- 63.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.007075223 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.73       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 38.7        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 2.8         |
|    value_loss           | 140         |
-----------------------------------------
Eval num_timesteps=1152000, episode_reward=152.50 +/- 78.33
Episode length: 468.40 +/- 52.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 152          |
| time/                   |              |
|    total_timesteps      | 1152000      |
| train/                  |              |
|    approx_kl            | 0.0028471705 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 44.2         |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 2.8          |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=1154000, episode_reward=284.39 +/- 185.30
Episode length: 496.20 +/- 28.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 1154000      |
| train/                  |              |
|    approx_kl            | 0.0039370353 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.74        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 65.9         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.81         |
|    value_loss           | 203          |
------------------------------------------
Eval num_timesteps=1156000, episode_reward=429.50 +/- 203.78
Episode length: 502.80 +/- 36.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 429          |
| time/                   |              |
|    total_timesteps      | 1156000      |
| train/                  |              |
|    approx_kl            | 0.0042452635 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.75        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 93.1         |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.82         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=1158000, episode_reward=396.16 +/- 213.99
Episode length: 504.00 +/- 27.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 1158000     |
| train/                  |             |
|    approx_kl            | 0.004601136 |
|    clip_fraction        | 0.00908     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.76       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 76          |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 2.82        |
|    value_loss           | 220         |
-----------------------------------------
Eval num_timesteps=1160000, episode_reward=708.70 +/- 239.95
Episode length: 485.60 +/- 25.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 709         |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.004411298 |
|    clip_fraction        | 0.00513     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.77       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 2.83        |
|    value_loss           | 308         |
-----------------------------------------
Eval num_timesteps=1162000, episode_reward=782.54 +/- 134.34
Episode length: 478.20 +/- 33.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 1162000     |
| train/                  |             |
|    approx_kl            | 0.003906991 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.77       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 172         |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.000273   |
|    std                  | 2.83        |
|    value_loss           | 573         |
-----------------------------------------
Eval num_timesteps=1164000, episode_reward=467.13 +/- 431.48
Episode length: 470.40 +/- 46.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 467           |
| time/                   |               |
|    total_timesteps      | 1164000       |
| train/                  |               |
|    approx_kl            | 0.00046858503 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.77         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 196           |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.000292     |
|    std                  | 2.83          |
|    value_loss           | 619           |
-------------------------------------------
Eval num_timesteps=1166000, episode_reward=572.52 +/- 469.43
Episode length: 468.00 +/- 35.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 573          |
| time/                   |              |
|    total_timesteps      | 1166000      |
| train/                  |              |
|    approx_kl            | 0.0006105879 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.77        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.001        |
|    loss                 | 3.51e+03     |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.000134    |
|    std                  | 2.83         |
|    value_loss           | 8.35e+03     |
------------------------------------------
Eval num_timesteps=1168000, episode_reward=267.70 +/- 473.99
Episode length: 449.40 +/- 19.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 1168000      |
| train/                  |              |
|    approx_kl            | 0.0010448743 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 81.8         |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.000579    |
|    std                  | 2.84         |
|    value_loss           | 299          |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=393.35 +/- 462.19
Episode length: 466.40 +/- 42.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 393          |
| time/                   |              |
|    total_timesteps      | 1170000      |
| train/                  |              |
|    approx_kl            | 0.0005818042 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 243          |
|    n_updates            | 5710         |
|    policy_gradient_loss | 0.000201     |
|    std                  | 2.84         |
|    value_loss           | 839          |
------------------------------------------
Eval num_timesteps=1172000, episode_reward=142.02 +/- 410.53
Episode length: 495.00 +/- 32.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 142          |
| time/                   |              |
|    total_timesteps      | 1172000      |
| train/                  |              |
|    approx_kl            | 0.0020818184 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.78        |
|    explained_variance   | 0.723        |
|    learning_rate        | 0.001        |
|    loss                 | 3.4e+03      |
|    n_updates            | 5720         |
|    policy_gradient_loss | 0.000453     |
|    std                  | 2.84         |
|    value_loss           | 8.83e+03     |
------------------------------------------
Eval num_timesteps=1174000, episode_reward=107.82 +/- 422.63
Episode length: 484.60 +/- 49.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 108           |
| time/                   |               |
|    total_timesteps      | 1174000       |
| train/                  |               |
|    approx_kl            | 0.00063969404 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.704         |
|    learning_rate        | 0.001         |
|    loss                 | 5.45e+03      |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000696     |
|    std                  | 2.84          |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=1176000, episode_reward=229.22 +/- 312.32
Episode length: 486.00 +/- 25.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 229           |
| time/                   |               |
|    total_timesteps      | 1176000       |
| train/                  |               |
|    approx_kl            | 0.00015915645 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.791         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+03      |
|    n_updates            | 5740          |
|    policy_gradient_loss | 0.000203      |
|    std                  | 2.84          |
|    value_loss           | 8.83e+03      |
-------------------------------------------
Eval num_timesteps=1178000, episode_reward=568.24 +/- 509.05
Episode length: 489.60 +/- 22.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | 568           |
| time/                   |               |
|    total_timesteps      | 1178000       |
| train/                  |               |
|    approx_kl            | 0.00040179677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 106           |
|    n_updates            | 5750          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 2.84          |
|    value_loss           | 352           |
-------------------------------------------
Eval num_timesteps=1180000, episode_reward=602.64 +/- 339.55
Episode length: 470.40 +/- 63.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 470         |
|    mean_reward          | 603         |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.004323437 |
|    clip_fraction        | 0.00513     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.79       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 128         |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 2.84        |
|    value_loss           | 531         |
-----------------------------------------
Eval num_timesteps=1182000, episode_reward=168.38 +/- 476.56
Episode length: 430.00 +/- 35.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 1182000      |
| train/                  |              |
|    approx_kl            | 0.0060846154 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.79        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.001        |
|    loss                 | 5.05e+03     |
|    n_updates            | 5770         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.85         |
|    value_loss           | 1.13e+04     |
------------------------------------------
Eval num_timesteps=1184000, episode_reward=115.91 +/- 346.42
Episode length: 438.20 +/- 48.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 116           |
| time/                   |               |
|    total_timesteps      | 1184000       |
| train/                  |               |
|    approx_kl            | 0.00095577084 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.79         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 335           |
|    n_updates            | 5780          |
|    policy_gradient_loss | -0.000143     |
|    std                  | 2.85          |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=1186000, episode_reward=200.97 +/- 320.53
Episode length: 474.00 +/- 35.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 1186000       |
| train/                  |               |
|    approx_kl            | 0.00014671634 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.8          |
|    explained_variance   | 0.691         |
|    learning_rate        | 0.001         |
|    loss                 | 8.11e+03      |
|    n_updates            | 5790          |
|    policy_gradient_loss | 0.000125      |
|    std                  | 2.85          |
|    value_loss           | 1.78e+04      |
-------------------------------------------
Eval num_timesteps=1188000, episode_reward=354.44 +/- 419.16
Episode length: 459.00 +/- 40.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 1188000     |
| train/                  |             |
|    approx_kl            | 0.000251792 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.8        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 216         |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.000172   |
|    std                  | 2.85        |
|    value_loss           | 802         |
-----------------------------------------
Eval num_timesteps=1190000, episode_reward=598.86 +/- 515.32
Episode length: 444.20 +/- 25.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 599           |
| time/                   |               |
|    total_timesteps      | 1190000       |
| train/                  |               |
|    approx_kl            | 0.00040137424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.8          |
|    explained_variance   | 0.748         |
|    learning_rate        | 0.001         |
|    loss                 | 3.12e+03      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.000115     |
|    std                  | 2.85          |
|    value_loss           | 7.45e+03      |
-------------------------------------------
Eval num_timesteps=1192000, episode_reward=267.60 +/- 397.41
Episode length: 464.20 +/- 15.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 1192000      |
| train/                  |              |
|    approx_kl            | 0.0001900449 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.001        |
|    loss                 | 2.59e+03     |
|    n_updates            | 5820         |
|    policy_gradient_loss | -0.000314    |
|    std                  | 2.85         |
|    value_loss           | 6.34e+03     |
------------------------------------------
Eval num_timesteps=1194000, episode_reward=202.86 +/- 520.63
Episode length: 456.40 +/- 35.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 1194000      |
| train/                  |              |
|    approx_kl            | 0.0014410439 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.000758    |
|    std                  | 2.86         |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=1196000, episode_reward=290.56 +/- 425.58
Episode length: 421.00 +/- 69.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 1196000  |
---------------------------------
Eval num_timesteps=1198000, episode_reward=132.22 +/- 364.70
Episode length: 400.00 +/- 24.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 132          |
| time/                   |              |
|    total_timesteps      | 1198000      |
| train/                  |              |
|    approx_kl            | 0.0032739239 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 39           |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.000728    |
|    std                  | 2.85         |
|    value_loss           | 131          |
------------------------------------------
Eval num_timesteps=1200000, episode_reward=89.06 +/- 367.29
Episode length: 409.20 +/- 12.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 89.1         |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0064218896 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.8         |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 39.2         |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 2.85         |
|    value_loss           | 172          |
------------------------------------------
Eval num_timesteps=1202000, episode_reward=397.39 +/- 344.90
Episode length: 413.20 +/- 20.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 1202000     |
| train/                  |             |
|    approx_kl            | 0.009645417 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.81       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0021     |
|    std                  | 2.86        |
|    value_loss           | 478         |
-----------------------------------------
Eval num_timesteps=1204000, episode_reward=267.49 +/- 232.91
Episode length: 389.80 +/- 33.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 1204000      |
| train/                  |              |
|    approx_kl            | 0.0049791476 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 2.11e+03     |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.000484    |
|    std                  | 2.87         |
|    value_loss           | 4.76e+03     |
------------------------------------------
Eval num_timesteps=1206000, episode_reward=99.86 +/- 372.25
Episode length: 371.40 +/- 23.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 99.9        |
| time/                   |             |
|    total_timesteps      | 1206000     |
| train/                  |             |
|    approx_kl            | 0.010188799 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.82       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 38.6        |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 2.88        |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=1208000, episode_reward=255.24 +/- 92.65
Episode length: 358.80 +/- 20.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 359         |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 1208000     |
| train/                  |             |
|    approx_kl            | 0.005035518 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 49          |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.000701   |
|    std                  | 2.88        |
|    value_loss           | 204         |
-----------------------------------------
Eval num_timesteps=1210000, episode_reward=270.54 +/- 266.87
Episode length: 368.60 +/- 20.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 369         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 1210000     |
| train/                  |             |
|    approx_kl            | 0.008887691 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.84       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 31.3        |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 2.88        |
|    value_loss           | 144         |
-----------------------------------------
Eval num_timesteps=1212000, episode_reward=337.96 +/- 321.53
Episode length: 369.20 +/- 17.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | 338          |
| time/                   |              |
|    total_timesteps      | 1212000      |
| train/                  |              |
|    approx_kl            | 0.0016845313 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.84        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 28.6         |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.000401    |
|    std                  | 2.88         |
|    value_loss           | 81.9         |
------------------------------------------
Eval num_timesteps=1214000, episode_reward=162.31 +/- 318.05
Episode length: 355.20 +/- 33.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 1214000     |
| train/                  |             |
|    approx_kl            | 0.004704209 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.001       |
|    loss                 | 3.37e+03    |
|    n_updates            | 5920        |
|    policy_gradient_loss | 0.000912    |
|    std                  | 2.88        |
|    value_loss           | 7.96e+03    |
-----------------------------------------
Eval num_timesteps=1216000, episode_reward=169.55 +/- 287.49
Episode length: 354.40 +/- 44.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 1216000      |
| train/                  |              |
|    approx_kl            | 0.0031114686 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 640          |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 2.88         |
|    value_loss           | 1.5e+03      |
------------------------------------------
Eval num_timesteps=1218000, episode_reward=66.40 +/- 307.21
Episode length: 360.80 +/- 58.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 361          |
|    mean_reward          | 66.4         |
| time/                   |              |
|    total_timesteps      | 1218000      |
| train/                  |              |
|    approx_kl            | 0.0016789439 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 2.32e+03     |
|    n_updates            | 5940         |
|    policy_gradient_loss | 0.00113      |
|    std                  | 2.88         |
|    value_loss           | 5.37e+03     |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=309.41 +/- 84.20
Episode length: 340.40 +/- 19.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 1220000      |
| train/                  |              |
|    approx_kl            | 0.0025107893 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 88.5         |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000941    |
|    std                  | 2.87         |
|    value_loss           | 344          |
------------------------------------------
Eval num_timesteps=1222000, episode_reward=300.87 +/- 201.08
Episode length: 328.40 +/- 36.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 328          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 1222000      |
| train/                  |              |
|    approx_kl            | 0.0040261056 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+03     |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00066     |
|    std                  | 2.87         |
|    value_loss           | 5.47e+03     |
------------------------------------------
Eval num_timesteps=1224000, episode_reward=200.67 +/- 119.09
Episode length: 349.60 +/- 21.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 1224000      |
| train/                  |              |
|    approx_kl            | 0.0028146124 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 81.2         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.87         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=1226000, episode_reward=211.95 +/- 232.73
Episode length: 359.20 +/- 20.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 359          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 1226000      |
| train/                  |              |
|    approx_kl            | 0.0012608338 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.82        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 51.8         |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.0005      |
|    std                  | 2.87         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=1228000, episode_reward=281.52 +/- 199.86
Episode length: 334.60 +/- 22.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 1228000     |
| train/                  |             |
|    approx_kl            | 0.002593807 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.83       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.001       |
|    loss                 | 1.48e+03    |
|    n_updates            | 5990        |
|    policy_gradient_loss | 0.000311    |
|    std                  | 2.88        |
|    value_loss           | 3.7e+03     |
-----------------------------------------
Eval num_timesteps=1230000, episode_reward=212.36 +/- 238.47
Episode length: 343.00 +/- 30.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 343          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 1230000      |
| train/                  |              |
|    approx_kl            | 0.0009122004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.83        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 104          |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.000598    |
|    std                  | 2.88         |
|    value_loss           | 383          |
------------------------------------------
Eval num_timesteps=1232000, episode_reward=177.74 +/- 110.12
Episode length: 319.80 +/- 20.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 178           |
| time/                   |               |
|    total_timesteps      | 1232000       |
| train/                  |               |
|    approx_kl            | 0.00072673906 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.84         |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.001         |
|    loss                 | 26.5          |
|    n_updates            | 6010          |
|    policy_gradient_loss | -0.000203     |
|    std                  | 2.89          |
|    value_loss           | 98.4          |
-------------------------------------------
Eval num_timesteps=1234000, episode_reward=267.43 +/- 233.69
Episode length: 347.20 +/- 21.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 1234000      |
| train/                  |              |
|    approx_kl            | 0.0066038594 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.85        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 27           |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.9          |
|    value_loss           | 137          |
------------------------------------------
Eval num_timesteps=1236000, episode_reward=188.10 +/- 65.98
Episode length: 335.40 +/- 6.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 335        |
|    mean_reward          | 188        |
| time/                   |            |
|    total_timesteps      | 1236000    |
| train/                  |            |
|    approx_kl            | 0.01004136 |
|    clip_fraction        | 0.0471     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.87      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.001      |
|    loss                 | 25.7       |
|    n_updates            | 6030       |
|    policy_gradient_loss | -0.00249   |
|    std                  | 2.91       |
|    value_loss           | 114        |
----------------------------------------
Eval num_timesteps=1238000, episode_reward=348.38 +/- 65.45
Episode length: 358.00 +/- 21.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | 348         |
| time/                   |             |
|    total_timesteps      | 1238000     |
| train/                  |             |
|    approx_kl            | 0.008648017 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.87       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 32.8        |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.00316    |
|    std                  | 2.91        |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=1240000, episode_reward=275.10 +/- 345.92
Episode length: 379.00 +/- 36.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 275          |
| time/                   |              |
|    total_timesteps      | 1240000      |
| train/                  |              |
|    approx_kl            | 0.0012206855 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.87        |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+03      |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 2.91         |
|    value_loss           | 5.74e+03     |
------------------------------------------
Eval num_timesteps=1242000, episode_reward=119.29 +/- 391.76
Episode length: 386.20 +/- 30.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 1242000       |
| train/                  |               |
|    approx_kl            | 0.00032932888 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.87         |
|    explained_variance   | 0.838         |
|    learning_rate        | 0.001         |
|    loss                 | 2.35e+03      |
|    n_updates            | 6060          |
|    policy_gradient_loss | 3.89e-05      |
|    std                  | 2.91          |
|    value_loss           | 5.04e+03      |
-------------------------------------------
Eval num_timesteps=1244000, episode_reward=296.36 +/- 329.68
Episode length: 393.60 +/- 12.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 296           |
| time/                   |               |
|    total_timesteps      | 1244000       |
| train/                  |               |
|    approx_kl            | 0.00011167521 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.87         |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+03      |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 2.91          |
|    value_loss           | 4.84e+03      |
-------------------------------------------
Eval num_timesteps=1246000, episode_reward=394.84 +/- 239.20
Episode length: 363.40 +/- 18.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 363          |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 1246000      |
| train/                  |              |
|    approx_kl            | 0.0002844049 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.87        |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.001        |
|    loss                 | 3.31e+03     |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.000471    |
|    std                  | 2.91         |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=1248000, episode_reward=474.09 +/- 111.91
Episode length: 381.20 +/- 13.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 474           |
| time/                   |               |
|    total_timesteps      | 1248000       |
| train/                  |               |
|    approx_kl            | 0.00030121216 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.88         |
|    explained_variance   | 0.788         |
|    learning_rate        | 0.001         |
|    loss                 | 3.53e+03      |
|    n_updates            | 6090          |
|    policy_gradient_loss | -1.05e-06     |
|    std                  | 2.91          |
|    value_loss           | 7.88e+03      |
-------------------------------------------
Eval num_timesteps=1250000, episode_reward=97.04 +/- 268.30
Episode length: 383.80 +/- 29.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | 97            |
| time/                   |               |
|    total_timesteps      | 1250000       |
| train/                  |               |
|    approx_kl            | 0.00017625684 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.88         |
|    explained_variance   | 0.862         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+03      |
|    n_updates            | 6100          |
|    policy_gradient_loss | -0.000509     |
|    std                  | 2.92          |
|    value_loss           | 4.79e+03      |
-------------------------------------------
Eval num_timesteps=1252000, episode_reward=238.54 +/- 89.02
Episode length: 357.00 +/- 20.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 357          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 1252000      |
| train/                  |              |
|    approx_kl            | 9.499525e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 3.33e+03     |
|    n_updates            | 6110         |
|    policy_gradient_loss | 1.67e-06     |
|    std                  | 2.92         |
|    value_loss           | 7.81e+03     |
------------------------------------------
Eval num_timesteps=1254000, episode_reward=77.65 +/- 266.53
Episode length: 380.40 +/- 43.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 77.7         |
| time/                   |              |
|    total_timesteps      | 1254000      |
| train/                  |              |
|    approx_kl            | 0.0034023037 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 48.9         |
|    n_updates            | 6120         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.92         |
|    value_loss           | 306          |
------------------------------------------
Eval num_timesteps=1256000, episode_reward=452.16 +/- 403.71
Episode length: 383.80 +/- 23.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 1256000     |
| train/                  |             |
|    approx_kl            | 0.004355624 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 1.58e+03    |
|    n_updates            | 6130        |
|    policy_gradient_loss | 0.0021      |
|    std                  | 2.92        |
|    value_loss           | 3.38e+03    |
-----------------------------------------
Eval num_timesteps=1258000, episode_reward=312.54 +/- 183.27
Episode length: 339.20 +/- 27.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 339         |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 1258000     |
| train/                  |             |
|    approx_kl            | 0.009572455 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 24.2        |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 2.92        |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=1260000, episode_reward=267.19 +/- 271.66
Episode length: 323.20 +/- 31.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.005161074 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.88       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 52.8        |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.000478   |
|    std                  | 2.92        |
|    value_loss           | 223         |
-----------------------------------------
Eval num_timesteps=1262000, episode_reward=348.92 +/- 224.98
Episode length: 343.80 +/- 12.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 344          |
|    mean_reward          | 349          |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0056347875 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.88        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 18           |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 2.92         |
|    value_loss           | 74.5         |
------------------------------------------
Eval num_timesteps=1264000, episode_reward=416.90 +/- 200.61
Episode length: 373.00 +/- 6.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 0.0031175036 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.9         |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 54.5         |
|    n_updates            | 6170         |
|    policy_gradient_loss | 0.000863     |
|    std                  | 2.93         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=1266000, episode_reward=403.82 +/- 313.70
Episode length: 381.40 +/- 28.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 404          |
| time/                   |              |
|    total_timesteps      | 1266000      |
| train/                  |              |
|    approx_kl            | 0.0032712636 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+03     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.00075     |
|    std                  | 2.94         |
|    value_loss           | 5.64e+03     |
------------------------------------------
Eval num_timesteps=1268000, episode_reward=353.51 +/- 178.69
Episode length: 393.20 +/- 29.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 1268000      |
| train/                  |              |
|    approx_kl            | 0.0006754121 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 3.73e+03     |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.00085     |
|    std                  | 2.94         |
|    value_loss           | 8.22e+03     |
------------------------------------------
Eval num_timesteps=1270000, episode_reward=316.52 +/- 102.04
Episode length: 408.00 +/- 45.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0048284065 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 57.6         |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 2.94         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=1272000, episode_reward=250.48 +/- 318.11
Episode length: 422.00 +/- 39.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 1272000     |
| train/                  |             |
|    approx_kl            | 0.002751325 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.91       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 2.33e+03    |
|    n_updates            | 6210        |
|    policy_gradient_loss | 0.000911    |
|    std                  | 2.93        |
|    value_loss           | 5.11e+03    |
-----------------------------------------
Eval num_timesteps=1274000, episode_reward=718.81 +/- 278.58
Episode length: 444.60 +/- 41.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 719          |
| time/                   |              |
|    total_timesteps      | 1274000      |
| train/                  |              |
|    approx_kl            | 0.0038598336 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.93         |
|    value_loss           | 1.02e+03     |
------------------------------------------
Eval num_timesteps=1276000, episode_reward=354.57 +/- 282.08
Episode length: 451.60 +/- 33.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 1276000      |
| train/                  |              |
|    approx_kl            | 0.0032524471 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 3.95e+03     |
|    n_updates            | 6230         |
|    policy_gradient_loss | -0.000639    |
|    std                  | 2.94         |
|    value_loss           | 9.31e+03     |
------------------------------------------
Eval num_timesteps=1278000, episode_reward=552.52 +/- 239.86
Episode length: 427.40 +/- 69.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 553          |
| time/                   |              |
|    total_timesteps      | 1278000      |
| train/                  |              |
|    approx_kl            | 0.0006188059 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 4.25e+03     |
|    n_updates            | 6240         |
|    policy_gradient_loss | -2.92e-05    |
|    std                  | 2.94         |
|    value_loss           | 9.3e+03      |
------------------------------------------
Eval num_timesteps=1280000, episode_reward=58.80 +/- 389.96
Episode length: 426.60 +/- 24.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 58.8     |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1282000, episode_reward=678.60 +/- 288.64
Episode length: 452.40 +/- 49.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 679           |
| time/                   |               |
|    total_timesteps      | 1282000       |
| train/                  |               |
|    approx_kl            | 0.00066540076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 110           |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 2.94          |
|    value_loss           | 548           |
-------------------------------------------
Eval num_timesteps=1284000, episode_reward=180.74 +/- 311.58
Episode length: 458.00 +/- 35.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 181           |
| time/                   |               |
|    total_timesteps      | 1284000       |
| train/                  |               |
|    approx_kl            | 0.00079743005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 6260          |
|    policy_gradient_loss | 3.08e-05      |
|    std                  | 2.94          |
|    value_loss           | 4.26e+03      |
-------------------------------------------
Eval num_timesteps=1286000, episode_reward=115.01 +/- 389.64
Episode length: 475.20 +/- 25.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 115           |
| time/                   |               |
|    total_timesteps      | 1286000       |
| train/                  |               |
|    approx_kl            | 0.00014405232 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.000106     |
|    std                  | 2.94          |
|    value_loss           | 3.71e+03      |
-------------------------------------------
Eval num_timesteps=1288000, episode_reward=738.23 +/- 394.28
Episode length: 502.40 +/- 16.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 502           |
|    mean_reward          | 738           |
| time/                   |               |
|    total_timesteps      | 1288000       |
| train/                  |               |
|    approx_kl            | 0.00011758492 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+03      |
|    n_updates            | 6280          |
|    policy_gradient_loss | -5.23e-05     |
|    std                  | 2.94          |
|    value_loss           | 8.27e+03      |
-------------------------------------------
Eval num_timesteps=1290000, episode_reward=18.65 +/- 317.48
Episode length: 447.00 +/- 19.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 18.7          |
| time/                   |               |
|    total_timesteps      | 1290000       |
| train/                  |               |
|    approx_kl            | 0.00030888632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.91         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 2.94          |
|    value_loss           | 3.03e+03      |
-------------------------------------------
Eval num_timesteps=1292000, episode_reward=292.13 +/- 575.86
Episode length: 490.40 +/- 28.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 1292000      |
| train/                  |              |
|    approx_kl            | 0.0045175934 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.91        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 231          |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 2.94         |
|    value_loss           | 584          |
------------------------------------------
Eval num_timesteps=1294000, episode_reward=280.73 +/- 267.57
Episode length: 464.40 +/- 28.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 281          |
| time/                   |              |
|    total_timesteps      | 1294000      |
| train/                  |              |
|    approx_kl            | 0.0023155375 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 1.86e+03     |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.000435    |
|    std                  | 2.94         |
|    value_loss           | 4.59e+03     |
------------------------------------------
Eval num_timesteps=1296000, episode_reward=378.59 +/- 138.66
Episode length: 438.40 +/- 27.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 379          |
| time/                   |              |
|    total_timesteps      | 1296000      |
| train/                  |              |
|    approx_kl            | 0.0006983265 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 3.06e+03     |
|    n_updates            | 6320         |
|    policy_gradient_loss | 1.52e-05     |
|    std                  | 2.94         |
|    value_loss           | 6.95e+03     |
------------------------------------------
Eval num_timesteps=1298000, episode_reward=310.41 +/- 199.46
Episode length: 444.80 +/- 61.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 445         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 1298000     |
| train/                  |             |
|    approx_kl            | 0.000713235 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.92       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 3.09e+03    |
|    n_updates            | 6330        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 2.94        |
|    value_loss           | 6.7e+03     |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=478.11 +/- 339.52
Episode length: 465.80 +/- 61.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0038021589 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 80.2         |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 2.95         |
|    value_loss           | 390          |
------------------------------------------
Eval num_timesteps=1302000, episode_reward=525.69 +/- 200.07
Episode length: 427.80 +/- 60.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 1302000      |
| train/                  |              |
|    approx_kl            | 0.0016892175 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.92        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.001        |
|    loss                 | 3.23e+03     |
|    n_updates            | 6350         |
|    policy_gradient_loss | 0.00104      |
|    std                  | 2.95         |
|    value_loss           | 7.77e+03     |
------------------------------------------
Eval num_timesteps=1304000, episode_reward=397.32 +/- 251.48
Episode length: 394.20 +/- 36.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 397           |
| time/                   |               |
|    total_timesteps      | 1304000       |
| train/                  |               |
|    approx_kl            | 0.00011641989 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 4.76e+03      |
|    n_updates            | 6360          |
|    policy_gradient_loss | -0.000138     |
|    std                  | 2.95          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1306000, episode_reward=405.90 +/- 585.16
Episode length: 421.00 +/- 49.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 1306000      |
| train/                  |              |
|    approx_kl            | 0.0001342746 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.93        |
|    explained_variance   | 0.781        |
|    learning_rate        | 0.001        |
|    loss                 | 4.77e+03     |
|    n_updates            | 6370         |
|    policy_gradient_loss | -0.000289    |
|    std                  | 2.95         |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=1308000, episode_reward=598.26 +/- 460.50
Episode length: 419.40 +/- 33.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 598           |
| time/                   |               |
|    total_timesteps      | 1308000       |
| train/                  |               |
|    approx_kl            | 0.00012002335 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.816         |
|    learning_rate        | 0.001         |
|    loss                 | 3.51e+03      |
|    n_updates            | 6380          |
|    policy_gradient_loss | 0.000218      |
|    std                  | 2.95          |
|    value_loss           | 8.57e+03      |
-------------------------------------------
Eval num_timesteps=1310000, episode_reward=139.05 +/- 462.93
Episode length: 428.00 +/- 22.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 428           |
|    mean_reward          | 139           |
| time/                   |               |
|    total_timesteps      | 1310000       |
| train/                  |               |
|    approx_kl            | 0.00024271192 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.817         |
|    learning_rate        | 0.001         |
|    loss                 | 5.91e+03      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000675     |
|    std                  | 2.95          |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Eval num_timesteps=1312000, episode_reward=275.97 +/- 296.50
Episode length: 483.80 +/- 51.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 484           |
|    mean_reward          | 276           |
| time/                   |               |
|    total_timesteps      | 1312000       |
| train/                  |               |
|    approx_kl            | 0.00020965809 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 3.23e+03      |
|    n_updates            | 6400          |
|    policy_gradient_loss | -0.000432     |
|    std                  | 2.95          |
|    value_loss           | 7.14e+03      |
-------------------------------------------
Eval num_timesteps=1314000, episode_reward=276.37 +/- 319.91
Episode length: 422.00 +/- 43.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 276           |
| time/                   |               |
|    total_timesteps      | 1314000       |
| train/                  |               |
|    approx_kl            | 0.00021523342 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 2.01e+03      |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000388     |
|    std                  | 2.95          |
|    value_loss           | 4.2e+03       |
-------------------------------------------
Eval num_timesteps=1316000, episode_reward=87.17 +/- 285.85
Episode length: 399.60 +/- 30.07
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 400            |
|    mean_reward          | 87.2           |
| time/                   |                |
|    total_timesteps      | 1316000        |
| train/                  |                |
|    approx_kl            | 0.000120655954 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.93          |
|    explained_variance   | 0.812          |
|    learning_rate        | 0.001          |
|    loss                 | 3.13e+03       |
|    n_updates            | 6420           |
|    policy_gradient_loss | -0.000142      |
|    std                  | 2.95           |
|    value_loss           | 6.65e+03       |
--------------------------------------------
Eval num_timesteps=1318000, episode_reward=636.74 +/- 306.95
Episode length: 436.60 +/- 62.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 437           |
|    mean_reward          | 637           |
| time/                   |               |
|    total_timesteps      | 1318000       |
| train/                  |               |
|    approx_kl            | 0.00014086414 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.93         |
|    explained_variance   | 0.84          |
|    learning_rate        | 0.001         |
|    loss                 | 3.49e+03      |
|    n_updates            | 6430          |
|    policy_gradient_loss | -0.000244     |
|    std                  | 2.95          |
|    value_loss           | 8.04e+03      |
-------------------------------------------
Eval num_timesteps=1320000, episode_reward=185.69 +/- 364.62
Episode length: 461.40 +/- 32.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 1320000     |
| train/                  |             |
|    approx_kl            | 0.000857997 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.94       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 239         |
|    n_updates            | 6440        |
|    policy_gradient_loss | -0.000585   |
|    std                  | 2.96        |
|    value_loss           | 996         |
-----------------------------------------
Eval num_timesteps=1322000, episode_reward=736.12 +/- 299.60
Episode length: 487.20 +/- 38.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 736          |
| time/                   |              |
|    total_timesteps      | 1322000      |
| train/                  |              |
|    approx_kl            | 0.0013475365 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.94        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 3.99e+03     |
|    n_updates            | 6450         |
|    policy_gradient_loss | 0.00016      |
|    std                  | 2.96         |
|    value_loss           | 8.91e+03     |
------------------------------------------
Eval num_timesteps=1324000, episode_reward=339.52 +/- 685.82
Episode length: 458.60 +/- 31.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 340           |
| time/                   |               |
|    total_timesteps      | 1324000       |
| train/                  |               |
|    approx_kl            | 3.6451267e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.94         |
|    explained_variance   | 0.766         |
|    learning_rate        | 0.001         |
|    loss                 | 6.2e+03       |
|    n_updates            | 6460          |
|    policy_gradient_loss | 0.00011       |
|    std                  | 2.96          |
|    value_loss           | 1.39e+04      |
-------------------------------------------
Eval num_timesteps=1326000, episode_reward=125.06 +/- 364.61
Episode length: 473.20 +/- 31.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 473           |
|    mean_reward          | 125           |
| time/                   |               |
|    total_timesteps      | 1326000       |
| train/                  |               |
|    approx_kl            | 0.00020256633 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.94         |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+03      |
|    n_updates            | 6470          |
|    policy_gradient_loss | -0.000394     |
|    std                  | 2.96          |
|    value_loss           | 5.21e+03      |
-------------------------------------------
Eval num_timesteps=1328000, episode_reward=48.64 +/- 376.37
Episode length: 427.00 +/- 52.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 48.6          |
| time/                   |               |
|    total_timesteps      | 1328000       |
| train/                  |               |
|    approx_kl            | 0.00018310445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 6480          |
|    policy_gradient_loss | -0.000244     |
|    std                  | 2.96          |
|    value_loss           | 4.67e+03      |
-------------------------------------------
Eval num_timesteps=1330000, episode_reward=192.69 +/- 217.73
Episode length: 416.20 +/- 51.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 193           |
| time/                   |               |
|    total_timesteps      | 1330000       |
| train/                  |               |
|    approx_kl            | 0.00025223978 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 4.48e+03      |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000275     |
|    std                  | 2.96          |
|    value_loss           | 9.68e+03      |
-------------------------------------------
Eval num_timesteps=1332000, episode_reward=-106.05 +/- 200.75
Episode length: 405.00 +/- 35.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | -106          |
| time/                   |               |
|    total_timesteps      | 1332000       |
| train/                  |               |
|    approx_kl            | 0.00037477684 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.774         |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+03      |
|    n_updates            | 6500          |
|    policy_gradient_loss | -0.000582     |
|    std                  | 2.97          |
|    value_loss           | 9.28e+03      |
-------------------------------------------
Eval num_timesteps=1334000, episode_reward=-86.96 +/- 271.91
Episode length: 411.80 +/- 7.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 412           |
|    mean_reward          | -87           |
| time/                   |               |
|    total_timesteps      | 1334000       |
| train/                  |               |
|    approx_kl            | 0.00039407983 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 6510          |
|    policy_gradient_loss | -0.000267     |
|    std                  | 2.97          |
|    value_loss           | 5.56e+03      |
-------------------------------------------
Eval num_timesteps=1336000, episode_reward=135.45 +/- 206.92
Episode length: 370.20 +/- 30.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 1336000      |
| train/                  |              |
|    approx_kl            | 0.0002611094 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+03     |
|    n_updates            | 6520         |
|    policy_gradient_loss | -0.000533    |
|    std                  | 2.97         |
|    value_loss           | 5.92e+03     |
------------------------------------------
Eval num_timesteps=1338000, episode_reward=52.23 +/- 494.85
Episode length: 444.60 +/- 50.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 445           |
|    mean_reward          | 52.2          |
| time/                   |               |
|    total_timesteps      | 1338000       |
| train/                  |               |
|    approx_kl            | 0.00047499058 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.807         |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+03      |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 2.97          |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=1340000, episode_reward=385.17 +/- 139.45
Episode length: 372.20 +/- 13.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | 385           |
| time/                   |               |
|    total_timesteps      | 1340000       |
| train/                  |               |
|    approx_kl            | 0.00036714305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+03      |
|    n_updates            | 6540          |
|    policy_gradient_loss | -0.00033      |
|    std                  | 2.97          |
|    value_loss           | 6.35e+03      |
-------------------------------------------
Eval num_timesteps=1342000, episode_reward=196.07 +/- 395.32
Episode length: 424.20 +/- 36.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 1342000      |
| train/                  |              |
|    approx_kl            | 0.0004699978 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 2.57e+03     |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.000409    |
|    std                  | 2.97         |
|    value_loss           | 5.63e+03     |
------------------------------------------
Eval num_timesteps=1344000, episode_reward=160.76 +/- 269.01
Episode length: 386.60 +/- 33.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 387           |
|    mean_reward          | 161           |
| time/                   |               |
|    total_timesteps      | 1344000       |
| train/                  |               |
|    approx_kl            | 0.00042241035 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.738         |
|    learning_rate        | 0.001         |
|    loss                 | 5.23e+03      |
|    n_updates            | 6560          |
|    policy_gradient_loss | -0.000499     |
|    std                  | 2.97          |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=1346000, episode_reward=200.74 +/- 359.61
Episode length: 424.60 +/- 26.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 1346000       |
| train/                  |               |
|    approx_kl            | 7.9302816e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 3.89e+03      |
|    n_updates            | 6570          |
|    policy_gradient_loss | 6.89e-05      |
|    std                  | 2.97          |
|    value_loss           | 8.24e+03      |
-------------------------------------------
Eval num_timesteps=1348000, episode_reward=286.44 +/- 78.27
Episode length: 339.00 +/- 5.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 1348000      |
| train/                  |              |
|    approx_kl            | 9.207288e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.95        |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 6580         |
|    policy_gradient_loss | -0.000121    |
|    std                  | 2.97         |
|    value_loss           | 4.58e+03     |
------------------------------------------
Eval num_timesteps=1350000, episode_reward=-175.89 +/- 182.78
Episode length: 411.00 +/- 31.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | -176          |
| time/                   |               |
|    total_timesteps      | 1350000       |
| train/                  |               |
|    approx_kl            | 0.00015165505 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.834         |
|    learning_rate        | 0.001         |
|    loss                 | 2.7e+03       |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.000108     |
|    std                  | 2.97          |
|    value_loss           | 6.06e+03      |
-------------------------------------------
Eval num_timesteps=1352000, episode_reward=184.08 +/- 319.53
Episode length: 376.60 +/- 23.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 377           |
|    mean_reward          | 184           |
| time/                   |               |
|    total_timesteps      | 1352000       |
| train/                  |               |
|    approx_kl            | 0.00019709097 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.95         |
|    explained_variance   | 0.801         |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+03      |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.000577     |
|    std                  | 2.97          |
|    value_loss           | 8.44e+03      |
-------------------------------------------
Eval num_timesteps=1354000, episode_reward=429.54 +/- 479.17
Episode length: 388.00 +/- 15.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 1354000       |
| train/                  |               |
|    approx_kl            | 0.00013096517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.96         |
|    explained_variance   | 0.83          |
|    learning_rate        | 0.001         |
|    loss                 | 2.22e+03      |
|    n_updates            | 6610          |
|    policy_gradient_loss | 7.62e-07      |
|    std                  | 2.97          |
|    value_loss           | 4.79e+03      |
-------------------------------------------
Eval num_timesteps=1356000, episode_reward=48.23 +/- 229.69
Episode length: 390.40 +/- 52.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 390           |
|    mean_reward          | 48.2          |
| time/                   |               |
|    total_timesteps      | 1356000       |
| train/                  |               |
|    approx_kl            | 0.00013146471 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.96         |
|    explained_variance   | 0.855         |
|    learning_rate        | 0.001         |
|    loss                 | 3.2e+03       |
|    n_updates            | 6620          |
|    policy_gradient_loss | -0.000407     |
|    std                  | 2.97          |
|    value_loss           | 7.08e+03      |
-------------------------------------------
Eval num_timesteps=1358000, episode_reward=194.18 +/- 342.73
Episode length: 406.40 +/- 27.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 1358000      |
| train/                  |              |
|    approx_kl            | 0.0002226023 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.96        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 2.84e+03     |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.000251    |
|    std                  | 2.97         |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=1360000, episode_reward=272.96 +/- 193.23
Episode length: 371.20 +/- 20.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 371           |
|    mean_reward          | 273           |
| time/                   |               |
|    total_timesteps      | 1360000       |
| train/                  |               |
|    approx_kl            | 0.00012748686 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.96         |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+03      |
|    n_updates            | 6640          |
|    policy_gradient_loss | -0.00028      |
|    std                  | 2.97          |
|    value_loss           | 6.14e+03      |
-------------------------------------------
Eval num_timesteps=1362000, episode_reward=293.37 +/- 409.33
Episode length: 403.20 +/- 36.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 403           |
|    mean_reward          | 293           |
| time/                   |               |
|    total_timesteps      | 1362000       |
| train/                  |               |
|    approx_kl            | 0.00081865606 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.96         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 112           |
|    n_updates            | 6650          |
|    policy_gradient_loss | -0.000551     |
|    std                  | 2.97          |
|    value_loss           | 584           |
-------------------------------------------
Eval num_timesteps=1364000, episode_reward=513.90 +/- 419.68
Episode length: 402.20 +/- 25.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 514          |
| time/                   |              |
|    total_timesteps      | 1364000      |
| train/                  |              |
|    approx_kl            | 0.0013975448 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.96        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 51.4         |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.00034     |
|    std                  | 2.98         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=1366000, episode_reward=463.10 +/- 303.33
Episode length: 399.40 +/- 30.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 1366000  |
---------------------------------
Eval num_timesteps=1368000, episode_reward=542.60 +/- 148.51
Episode length: 383.20 +/- 23.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 543          |
| time/                   |              |
|    total_timesteps      | 1368000      |
| train/                  |              |
|    approx_kl            | 0.0024893377 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.97        |
|    explained_variance   | 0.683        |
|    learning_rate        | 0.001        |
|    loss                 | 5.56e+03     |
|    n_updates            | 6670         |
|    policy_gradient_loss | 0.000865     |
|    std                  | 2.99         |
|    value_loss           | 1.47e+04     |
------------------------------------------
Eval num_timesteps=1370000, episode_reward=204.89 +/- 471.34
Episode length: 408.20 +/- 31.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 205           |
| time/                   |               |
|    total_timesteps      | 1370000       |
| train/                  |               |
|    approx_kl            | 0.00027171033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.98         |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.001         |
|    loss                 | 2.01e+03      |
|    n_updates            | 6680          |
|    policy_gradient_loss | -0.000371     |
|    std                  | 2.99          |
|    value_loss           | 4.92e+03      |
-------------------------------------------
Eval num_timesteps=1372000, episode_reward=525.88 +/- 290.66
Episode length: 377.20 +/- 39.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 1372000      |
| train/                  |              |
|    approx_kl            | 0.0007338935 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.98        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 156          |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00064     |
|    std                  | 3            |
|    value_loss           | 683          |
------------------------------------------
Eval num_timesteps=1374000, episode_reward=423.68 +/- 397.28
Episode length: 381.20 +/- 27.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 424           |
| time/                   |               |
|    total_timesteps      | 1374000       |
| train/                  |               |
|    approx_kl            | 0.00090109033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.99         |
|    explained_variance   | 0.78          |
|    learning_rate        | 0.001         |
|    loss                 | 3.76e+03      |
|    n_updates            | 6700          |
|    policy_gradient_loss | -0.000324     |
|    std                  | 3             |
|    value_loss           | 8.9e+03       |
-------------------------------------------
Eval num_timesteps=1376000, episode_reward=269.62 +/- 365.78
Episode length: 381.60 +/- 31.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 1376000      |
| train/                  |              |
|    approx_kl            | 0.0008310546 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.99        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 97.6         |
|    n_updates            | 6710         |
|    policy_gradient_loss | -0.000506    |
|    std                  | 3            |
|    value_loss           | 316          |
------------------------------------------
Eval num_timesteps=1378000, episode_reward=268.95 +/- 270.79
Episode length: 371.00 +/- 9.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 1378000      |
| train/                  |              |
|    approx_kl            | 0.0005723941 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.99        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+03     |
|    n_updates            | 6720         |
|    policy_gradient_loss | -0.000151    |
|    std                  | 3            |
|    value_loss           | 6.01e+03     |
------------------------------------------
Eval num_timesteps=1380000, episode_reward=574.61 +/- 287.54
Episode length: 380.20 +/- 35.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 380         |
|    mean_reward          | 575         |
| time/                   |             |
|    total_timesteps      | 1380000     |
| train/                  |             |
|    approx_kl            | 0.000172439 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.99       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 1.49e+03    |
|    n_updates            | 6730        |
|    policy_gradient_loss | -0.000119   |
|    std                  | 3           |
|    value_loss           | 3.36e+03    |
-----------------------------------------
Eval num_timesteps=1382000, episode_reward=463.67 +/- 302.34
Episode length: 381.40 +/- 24.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 464          |
| time/                   |              |
|    total_timesteps      | 1382000      |
| train/                  |              |
|    approx_kl            | 3.156811e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.99        |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.001        |
|    loss                 | 1.82e+03     |
|    n_updates            | 6740         |
|    policy_gradient_loss | -4.92e-05    |
|    std                  | 3            |
|    value_loss           | 4.35e+03     |
------------------------------------------
Eval num_timesteps=1384000, episode_reward=82.45 +/- 357.01
Episode length: 383.80 +/- 40.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 82.4         |
| time/                   |              |
|    total_timesteps      | 1384000      |
| train/                  |              |
|    approx_kl            | 0.0012726352 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.99        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 55.8         |
|    n_updates            | 6750         |
|    policy_gradient_loss | -0.0007      |
|    std                  | 2.99         |
|    value_loss           | 305          |
------------------------------------------
Eval num_timesteps=1386000, episode_reward=440.52 +/- 276.15
Episode length: 379.60 +/- 25.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 441          |
| time/                   |              |
|    total_timesteps      | 1386000      |
| train/                  |              |
|    approx_kl            | 0.0045179464 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.98        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 2.99         |
|    value_loss           | 4.05e+03     |
------------------------------------------
Eval num_timesteps=1388000, episode_reward=117.03 +/- 262.31
Episode length: 395.40 +/- 40.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 117          |
| time/                   |              |
|    total_timesteps      | 1388000      |
| train/                  |              |
|    approx_kl            | 0.0010031222 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.98        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+03     |
|    n_updates            | 6770         |
|    policy_gradient_loss | 1.5e-05      |
|    std                  | 2.99         |
|    value_loss           | 4.32e+03     |
------------------------------------------
Eval num_timesteps=1390000, episode_reward=192.12 +/- 421.25
Episode length: 397.00 +/- 16.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 397            |
|    mean_reward          | 192            |
| time/                   |                |
|    total_timesteps      | 1390000        |
| train/                  |                |
|    approx_kl            | 0.000120482175 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.97          |
|    explained_variance   | 0.861          |
|    learning_rate        | 0.001          |
|    loss                 | 1.97e+03       |
|    n_updates            | 6780           |
|    policy_gradient_loss | -9.82e-05      |
|    std                  | 2.99           |
|    value_loss           | 4.52e+03       |
--------------------------------------------
Eval num_timesteps=1392000, episode_reward=503.76 +/- 367.63
Episode length: 376.60 +/- 55.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 377           |
|    mean_reward          | 504           |
| time/                   |               |
|    total_timesteps      | 1392000       |
| train/                  |               |
|    approx_kl            | 0.00016516136 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.97         |
|    explained_variance   | 0.824         |
|    learning_rate        | 0.001         |
|    loss                 | 2.71e+03      |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000502     |
|    std                  | 2.99          |
|    value_loss           | 6.42e+03      |
-------------------------------------------
Eval num_timesteps=1394000, episode_reward=188.56 +/- 327.95
Episode length: 388.20 +/- 38.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 189           |
| time/                   |               |
|    total_timesteps      | 1394000       |
| train/                  |               |
|    approx_kl            | 0.00020007938 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.98         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 2.72e+03      |
|    n_updates            | 6800          |
|    policy_gradient_loss | -8.1e-05      |
|    std                  | 2.99          |
|    value_loss           | 5.78e+03      |
-------------------------------------------
Eval num_timesteps=1396000, episode_reward=62.63 +/- 349.76
Episode length: 408.00 +/- 52.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 62.6          |
| time/                   |               |
|    total_timesteps      | 1396000       |
| train/                  |               |
|    approx_kl            | 0.00041848607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.98         |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+03       |
|    n_updates            | 6810          |
|    policy_gradient_loss | -0.00104      |
|    std                  | 2.99          |
|    value_loss           | 6.4e+03       |
-------------------------------------------
Eval num_timesteps=1398000, episode_reward=396.68 +/- 398.20
Episode length: 418.20 +/- 13.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 418        |
|    mean_reward          | 397        |
| time/                   |            |
|    total_timesteps      | 1398000    |
| train/                  |            |
|    approx_kl            | 0.00726318 |
|    clip_fraction        | 0.0224     |
|    clip_range           | 0.2        |
|    entropy_loss         | -9.98      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.001      |
|    loss                 | 81.6       |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.00234   |
|    std                  | 2.99       |
|    value_loss           | 466        |
----------------------------------------
Eval num_timesteps=1400000, episode_reward=158.62 +/- 231.86
Episode length: 375.60 +/- 31.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 159         |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.003408328 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.99       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.001       |
|    loss                 | 6.37e+03    |
|    n_updates            | 6830        |
|    policy_gradient_loss | 0.000689    |
|    std                  | 2.99        |
|    value_loss           | 1.39e+04    |
-----------------------------------------
Eval num_timesteps=1402000, episode_reward=327.37 +/- 495.32
Episode length: 399.40 +/- 28.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 399           |
|    mean_reward          | 327           |
| time/                   |               |
|    total_timesteps      | 1402000       |
| train/                  |               |
|    approx_kl            | 0.00034288576 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.99         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 3.38e+03      |
|    n_updates            | 6840          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 3             |
|    value_loss           | 7.61e+03      |
-------------------------------------------
Eval num_timesteps=1404000, episode_reward=584.05 +/- 303.21
Episode length: 383.80 +/- 40.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | 584           |
| time/                   |               |
|    total_timesteps      | 1404000       |
| train/                  |               |
|    approx_kl            | 0.00018178063 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.99         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 5.52e+03      |
|    n_updates            | 6850          |
|    policy_gradient_loss | 6.99e-05      |
|    std                  | 3             |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=1406000, episode_reward=240.26 +/- 313.15
Episode length: 407.20 +/- 16.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 240           |
| time/                   |               |
|    total_timesteps      | 1406000       |
| train/                  |               |
|    approx_kl            | 0.00047900947 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.99         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 79.7          |
|    n_updates            | 6860          |
|    policy_gradient_loss | -0.000723     |
|    std                  | 3             |
|    value_loss           | 372           |
-------------------------------------------
Eval num_timesteps=1408000, episode_reward=279.51 +/- 431.46
Episode length: 417.00 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 280          |
| time/                   |              |
|    total_timesteps      | 1408000      |
| train/                  |              |
|    approx_kl            | 0.0023921342 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+03     |
|    n_updates            | 6870         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 3.01         |
|    value_loss           | 4.07e+03     |
------------------------------------------
Eval num_timesteps=1410000, episode_reward=395.68 +/- 199.99
Episode length: 391.40 +/- 46.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 1410000      |
| train/                  |              |
|    approx_kl            | 0.0015214318 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 1.9e+03      |
|    n_updates            | 6880         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 3.01         |
|    value_loss           | 4.25e+03     |
------------------------------------------
Eval num_timesteps=1412000, episode_reward=244.00 +/- 345.80
Episode length: 370.20 +/- 76.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 370          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 1412000      |
| train/                  |              |
|    approx_kl            | 0.0006633925 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+03     |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.000504    |
|    std                  | 3.01         |
|    value_loss           | 3.85e+03     |
------------------------------------------
Eval num_timesteps=1414000, episode_reward=139.58 +/- 388.35
Episode length: 404.20 +/- 32.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 140          |
| time/                   |              |
|    total_timesteps      | 1414000      |
| train/                  |              |
|    approx_kl            | 0.0010462343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 56.2         |
|    n_updates            | 6900         |
|    policy_gradient_loss | -0.000737    |
|    std                  | 3.01         |
|    value_loss           | 286          |
------------------------------------------
Eval num_timesteps=1416000, episode_reward=161.69 +/- 403.99
Episode length: 424.40 +/- 32.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 1416000     |
| train/                  |             |
|    approx_kl            | 0.001725565 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10         |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.001       |
|    loss                 | 2.15e+03    |
|    n_updates            | 6910        |
|    policy_gradient_loss | 0.000833    |
|    std                  | 3.01        |
|    value_loss           | 4.31e+03    |
-----------------------------------------
Eval num_timesteps=1418000, episode_reward=416.83 +/- 473.38
Episode length: 420.20 +/- 23.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 417           |
| time/                   |               |
|    total_timesteps      | 1418000       |
| train/                  |               |
|    approx_kl            | 0.00016856386 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10           |
|    explained_variance   | 0.836         |
|    learning_rate        | 0.001         |
|    loss                 | 3.32e+03      |
|    n_updates            | 6920          |
|    policy_gradient_loss | -0.000114     |
|    std                  | 3.02          |
|    value_loss           | 7.24e+03      |
-------------------------------------------
Eval num_timesteps=1420000, episode_reward=283.17 +/- 474.19
Episode length: 439.00 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 283          |
| time/                   |              |
|    total_timesteps      | 1420000      |
| train/                  |              |
|    approx_kl            | 9.552567e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 2.99e+03     |
|    n_updates            | 6930         |
|    policy_gradient_loss | -0.000253    |
|    std                  | 3.02         |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=1422000, episode_reward=346.03 +/- 296.21
Episode length: 416.60 +/- 30.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 346           |
| time/                   |               |
|    total_timesteps      | 1422000       |
| train/                  |               |
|    approx_kl            | 0.00012625178 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10           |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 6940          |
|    policy_gradient_loss | -0.000342     |
|    std                  | 3.02          |
|    value_loss           | 3.95e+03      |
-------------------------------------------
Eval num_timesteps=1424000, episode_reward=463.39 +/- 283.85
Episode length: 396.20 +/- 30.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 463          |
| time/                   |              |
|    total_timesteps      | 1424000      |
| train/                  |              |
|    approx_kl            | 7.417501e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.802        |
|    learning_rate        | 0.001        |
|    loss                 | 5.67e+03     |
|    n_updates            | 6950         |
|    policy_gradient_loss | -1.62e-05    |
|    std                  | 3.02         |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=1426000, episode_reward=325.27 +/- 404.04
Episode length: 418.80 +/- 57.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 1426000      |
| train/                  |              |
|    approx_kl            | 7.286444e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+03     |
|    n_updates            | 6960         |
|    policy_gradient_loss | -0.000339    |
|    std                  | 3.02         |
|    value_loss           | 3.84e+03     |
------------------------------------------
Eval num_timesteps=1428000, episode_reward=308.10 +/- 393.11
Episode length: 415.40 +/- 32.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 1428000      |
| train/                  |              |
|    approx_kl            | 0.0005924733 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.000788    |
|    std                  | 3.02         |
|    value_loss           | 858          |
------------------------------------------
Eval num_timesteps=1430000, episode_reward=275.89 +/- 291.37
Episode length: 387.80 +/- 39.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 276          |
| time/                   |              |
|    total_timesteps      | 1430000      |
| train/                  |              |
|    approx_kl            | 0.0038307062 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 44.3         |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 3.02         |
|    value_loss           | 239          |
------------------------------------------
Eval num_timesteps=1432000, episode_reward=445.90 +/- 233.42
Episode length: 404.20 +/- 19.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 1432000      |
| train/                  |              |
|    approx_kl            | 0.0016027694 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 2.03e+03     |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.000196    |
|    std                  | 3.03         |
|    value_loss           | 4.42e+03     |
------------------------------------------
Eval num_timesteps=1434000, episode_reward=290.84 +/- 124.58
Episode length: 370.60 +/- 19.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 1434000      |
| train/                  |              |
|    approx_kl            | 0.0026901034 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 33.6         |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00161     |
|    std                  | 3.03         |
|    value_loss           | 320          |
------------------------------------------
Eval num_timesteps=1436000, episode_reward=349.69 +/- 312.04
Episode length: 408.60 +/- 45.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 1436000      |
| train/                  |              |
|    approx_kl            | 0.0075475704 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.803        |
|    learning_rate        | 0.001        |
|    loss                 | 2.15e+03     |
|    n_updates            | 7010         |
|    policy_gradient_loss | -0.000775    |
|    std                  | 3.04         |
|    value_loss           | 5.29e+03     |
------------------------------------------
Eval num_timesteps=1438000, episode_reward=184.61 +/- 265.12
Episode length: 396.60 +/- 25.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 1438000      |
| train/                  |              |
|    approx_kl            | 0.0005456564 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10          |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 34.7         |
|    n_updates            | 7020         |
|    policy_gradient_loss | -0.000344    |
|    std                  | 3.05         |
|    value_loss           | 119          |
------------------------------------------
Eval num_timesteps=1440000, episode_reward=165.12 +/- 329.83
Episode length: 394.00 +/- 13.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 394         |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.004367362 |
|    clip_fraction        | 0.00547     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 42.9        |
|    n_updates            | 7030        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 3.05        |
|    value_loss           | 201         |
-----------------------------------------
Eval num_timesteps=1442000, episode_reward=208.75 +/- 368.72
Episode length: 423.00 +/- 42.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 1442000     |
| train/                  |             |
|    approx_kl            | 0.004318681 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 7040        |
|    policy_gradient_loss | -0.000847   |
|    std                  | 3.05        |
|    value_loss           | 3.22e+03    |
-----------------------------------------
Eval num_timesteps=1444000, episode_reward=304.30 +/- 391.45
Episode length: 430.80 +/- 38.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 1444000      |
| train/                  |              |
|    approx_kl            | 0.0024632928 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.838        |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+03     |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 3.05         |
|    value_loss           | 6.1e+03      |
------------------------------------------
Eval num_timesteps=1446000, episode_reward=565.77 +/- 458.81
Episode length: 417.20 +/- 39.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 566           |
| time/                   |               |
|    total_timesteps      | 1446000       |
| train/                  |               |
|    approx_kl            | 0.00037983412 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 5.8e+03       |
|    n_updates            | 7060          |
|    policy_gradient_loss | 0.000395      |
|    std                  | 3.05          |
|    value_loss           | 1.34e+04      |
-------------------------------------------
Eval num_timesteps=1448000, episode_reward=252.74 +/- 276.47
Episode length: 422.00 +/- 27.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 1448000      |
| train/                  |              |
|    approx_kl            | 8.387066e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.001        |
|    loss                 | 6e+03        |
|    n_updates            | 7070         |
|    policy_gradient_loss | -0.000205    |
|    std                  | 3.06         |
|    value_loss           | 1.28e+04     |
------------------------------------------
Eval num_timesteps=1450000, episode_reward=172.60 +/- 569.04
Episode length: 439.40 +/- 44.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 1450000      |
| train/                  |              |
|    approx_kl            | 8.480961e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 3.65e+03     |
|    n_updates            | 7080         |
|    policy_gradient_loss | -8.87e-05    |
|    std                  | 3.06         |
|    value_loss           | 8.06e+03     |
------------------------------------------
Eval num_timesteps=1452000, episode_reward=204.61 +/- 283.63
Episode length: 408.00 +/- 43.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 1452000  |
---------------------------------
Eval num_timesteps=1454000, episode_reward=734.60 +/- 241.12
Episode length: 434.00 +/- 41.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 735           |
| time/                   |               |
|    total_timesteps      | 1454000       |
| train/                  |               |
|    approx_kl            | 5.7482597e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.848         |
|    learning_rate        | 0.001         |
|    loss                 | 4.66e+03      |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 3.06          |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=1456000, episode_reward=13.03 +/- 305.43
Episode length: 421.00 +/- 14.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 1456000      |
| train/                  |              |
|    approx_kl            | 5.258707e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 2.91e+03     |
|    n_updates            | 7100         |
|    policy_gradient_loss | -0.000142    |
|    std                  | 3.06         |
|    value_loss           | 6.21e+03     |
------------------------------------------
Eval num_timesteps=1458000, episode_reward=619.98 +/- 393.58
Episode length: 409.40 +/- 24.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 620          |
| time/                   |              |
|    total_timesteps      | 1458000      |
| train/                  |              |
|    approx_kl            | 6.113411e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 5.12e+03     |
|    n_updates            | 7110         |
|    policy_gradient_loss | -6.49e-05    |
|    std                  | 3.06         |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=1460000, episode_reward=-157.71 +/- 261.52
Episode length: 417.20 +/- 50.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 417         |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 1460000     |
| train/                  |             |
|    approx_kl            | 3.44227e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.001       |
|    loss                 | 5.18e+03    |
|    n_updates            | 7120        |
|    policy_gradient_loss | -5.28e-05   |
|    std                  | 3.06        |
|    value_loss           | 1.13e+04    |
-----------------------------------------
Eval num_timesteps=1462000, episode_reward=182.82 +/- 335.08
Episode length: 401.20 +/- 9.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 1462000     |
| train/                  |             |
|    approx_kl            | 0.003782839 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 342         |
|    n_updates            | 7130        |
|    policy_gradient_loss | -0.00175    |
|    std                  | 3.06        |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=1464000, episode_reward=215.54 +/- 528.15
Episode length: 438.60 +/- 41.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 1464000      |
| train/                  |              |
|    approx_kl            | 0.0029370612 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 2.08e+03     |
|    n_updates            | 7140         |
|    policy_gradient_loss | -0.000646    |
|    std                  | 3.07         |
|    value_loss           | 4.79e+03     |
------------------------------------------
Eval num_timesteps=1466000, episode_reward=230.56 +/- 677.38
Episode length: 474.40 +/- 43.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 474           |
|    mean_reward          | 231           |
| time/                   |               |
|    total_timesteps      | 1466000       |
| train/                  |               |
|    approx_kl            | 0.00051573425 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.807         |
|    learning_rate        | 0.001         |
|    loss                 | 7.07e+03      |
|    n_updates            | 7150          |
|    policy_gradient_loss | -0.000658     |
|    std                  | 3.07          |
|    value_loss           | 1.53e+04      |
-------------------------------------------
Eval num_timesteps=1468000, episode_reward=353.17 +/- 213.56
Episode length: 463.00 +/- 53.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 353          |
| time/                   |              |
|    total_timesteps      | 1468000      |
| train/                  |              |
|    approx_kl            | 0.0012639499 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+03     |
|    n_updates            | 7160         |
|    policy_gradient_loss | -0.000901    |
|    std                  | 3.07         |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=1470000, episode_reward=494.20 +/- 505.97
Episode length: 482.80 +/- 25.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 483          |
|    mean_reward          | 494          |
| time/                   |              |
|    total_timesteps      | 1470000      |
| train/                  |              |
|    approx_kl            | 0.0009320023 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 7170         |
|    policy_gradient_loss | 8.52e-06     |
|    std                  | 3.08         |
|    value_loss           | 3.96e+03     |
------------------------------------------
Eval num_timesteps=1472000, episode_reward=299.93 +/- 229.96
Episode length: 497.00 +/- 34.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 300          |
| time/                   |              |
|    total_timesteps      | 1472000      |
| train/                  |              |
|    approx_kl            | 0.0035751278 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 363          |
|    n_updates            | 7180         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 3.08         |
|    value_loss           | 1.22e+03     |
------------------------------------------
Eval num_timesteps=1474000, episode_reward=503.34 +/- 197.86
Episode length: 419.80 +/- 26.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 1474000      |
| train/                  |              |
|    approx_kl            | 0.0037110613 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 190          |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000237    |
|    std                  | 3.09         |
|    value_loss           | 996          |
------------------------------------------
Eval num_timesteps=1476000, episode_reward=186.41 +/- 300.11
Episode length: 390.00 +/- 17.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 1476000      |
| train/                  |              |
|    approx_kl            | 0.0006468141 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 7200         |
|    policy_gradient_loss | -0.000329    |
|    std                  | 3.1          |
|    value_loss           | 4.8e+03      |
------------------------------------------
Eval num_timesteps=1478000, episode_reward=241.61 +/- 446.98
Episode length: 399.40 +/- 21.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 242          |
| time/                   |              |
|    total_timesteps      | 1478000      |
| train/                  |              |
|    approx_kl            | 0.0002985025 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.001        |
|    loss                 | 5.05e+03     |
|    n_updates            | 7210         |
|    policy_gradient_loss | 0.000118     |
|    std                  | 3.1          |
|    value_loss           | 1.12e+04     |
------------------------------------------
Eval num_timesteps=1480000, episode_reward=477.67 +/- 418.26
Episode length: 447.60 +/- 34.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 448           |
|    mean_reward          | 478           |
| time/                   |               |
|    total_timesteps      | 1480000       |
| train/                  |               |
|    approx_kl            | 0.00015237878 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+03      |
|    n_updates            | 7220          |
|    policy_gradient_loss | -0.000237     |
|    std                  | 3.1           |
|    value_loss           | 6e+03         |
-------------------------------------------
Eval num_timesteps=1482000, episode_reward=45.50 +/- 424.73
Episode length: 400.20 +/- 33.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 45.5          |
| time/                   |               |
|    total_timesteps      | 1482000       |
| train/                  |               |
|    approx_kl            | 0.00016157041 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.87          |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+03      |
|    n_updates            | 7230          |
|    policy_gradient_loss | -0.000188     |
|    std                  | 3.1           |
|    value_loss           | 6.8e+03       |
-------------------------------------------
Eval num_timesteps=1484000, episode_reward=490.03 +/- 219.18
Episode length: 375.40 +/- 27.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 490          |
| time/                   |              |
|    total_timesteps      | 1484000      |
| train/                  |              |
|    approx_kl            | 0.0058973753 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 53.6         |
|    n_updates            | 7240         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 3.1          |
|    value_loss           | 477          |
------------------------------------------
Eval num_timesteps=1486000, episode_reward=384.91 +/- 267.77
Episode length: 361.80 +/- 29.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 362          |
|    mean_reward          | 385          |
| time/                   |              |
|    total_timesteps      | 1486000      |
| train/                  |              |
|    approx_kl            | 0.0037182407 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 6.28e+03     |
|    n_updates            | 7250         |
|    policy_gradient_loss | 0.00013      |
|    std                  | 3.09         |
|    value_loss           | 1.41e+04     |
------------------------------------------
Eval num_timesteps=1488000, episode_reward=-18.93 +/- 329.39
Episode length: 407.60 +/- 12.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | -18.9         |
| time/                   |               |
|    total_timesteps      | 1488000       |
| train/                  |               |
|    approx_kl            | 0.00038257366 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+03      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.000313     |
|    std                  | 3.09          |
|    value_loss           | 3.02e+03      |
-------------------------------------------
Eval num_timesteps=1490000, episode_reward=412.79 +/- 302.48
Episode length: 375.80 +/- 23.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 376           |
|    mean_reward          | 413           |
| time/                   |               |
|    total_timesteps      | 1490000       |
| train/                  |               |
|    approx_kl            | 0.00030438864 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.812         |
|    learning_rate        | 0.001         |
|    loss                 | 5e+03         |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000385     |
|    std                  | 3.09          |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=1492000, episode_reward=63.99 +/- 323.68
Episode length: 400.80 +/- 30.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 64           |
| time/                   |              |
|    total_timesteps      | 1492000      |
| train/                  |              |
|    approx_kl            | 0.0020887721 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 219          |
|    n_updates            | 7280         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 3.09         |
|    value_loss           | 869          |
------------------------------------------
Eval num_timesteps=1494000, episode_reward=346.05 +/- 421.26
Episode length: 391.40 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 346         |
| time/                   |             |
|    total_timesteps      | 1494000     |
| train/                  |             |
|    approx_kl            | 0.010149706 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 41.6        |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.00431    |
|    std                  | 3.09        |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=1496000, episode_reward=221.15 +/- 327.48
Episode length: 384.60 +/- 8.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 1496000      |
| train/                  |              |
|    approx_kl            | 0.0030679572 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.81         |
|    learning_rate        | 0.001        |
|    loss                 | 3.77e+03     |
|    n_updates            | 7300         |
|    policy_gradient_loss | 1.08e-05     |
|    std                  | 3.09         |
|    value_loss           | 8.75e+03     |
------------------------------------------
Eval num_timesteps=1498000, episode_reward=190.07 +/- 364.17
Episode length: 375.20 +/- 34.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 1498000      |
| train/                  |              |
|    approx_kl            | 0.0005217277 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 2.74e+03     |
|    n_updates            | 7310         |
|    policy_gradient_loss | -0.000354    |
|    std                  | 3.09         |
|    value_loss           | 6.51e+03     |
------------------------------------------
Eval num_timesteps=1500000, episode_reward=494.91 +/- 401.32
Episode length: 427.40 +/- 32.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 495           |
| time/                   |               |
|    total_timesteps      | 1500000       |
| train/                  |               |
|    approx_kl            | 0.00011965429 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.877         |
|    learning_rate        | 0.001         |
|    loss                 | 1.87e+03      |
|    n_updates            | 7320          |
|    policy_gradient_loss | 0.000142      |
|    std                  | 3.08          |
|    value_loss           | 4.39e+03      |
-------------------------------------------
Eval num_timesteps=1502000, episode_reward=387.57 +/- 213.36
Episode length: 384.60 +/- 9.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 1502000      |
| train/                  |              |
|    approx_kl            | 6.734801e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 2.74e+03     |
|    n_updates            | 7330         |
|    policy_gradient_loss | -0.000184    |
|    std                  | 3.08         |
|    value_loss           | 5.82e+03     |
------------------------------------------
Eval num_timesteps=1504000, episode_reward=151.33 +/- 437.88
Episode length: 417.60 +/- 24.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 151          |
| time/                   |              |
|    total_timesteps      | 1504000      |
| train/                  |              |
|    approx_kl            | 0.0003690166 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+03     |
|    n_updates            | 7340         |
|    policy_gradient_loss | -0.000669    |
|    std                  | 3.09         |
|    value_loss           | 3.85e+03     |
------------------------------------------
Eval num_timesteps=1506000, episode_reward=248.85 +/- 227.83
Episode length: 378.60 +/- 45.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 1506000     |
| train/                  |             |
|    approx_kl            | 0.000322775 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.001       |
|    loss                 | 4.52e+03    |
|    n_updates            | 7350        |
|    policy_gradient_loss | -0.000316   |
|    std                  | 3.09        |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=1508000, episode_reward=372.27 +/- 290.40
Episode length: 371.20 +/- 34.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | 372          |
| time/                   |              |
|    total_timesteps      | 1508000      |
| train/                  |              |
|    approx_kl            | 8.842486e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+03     |
|    n_updates            | 7360         |
|    policy_gradient_loss | -0.000154    |
|    std                  | 3.09         |
|    value_loss           | 5.04e+03     |
------------------------------------------
Eval num_timesteps=1510000, episode_reward=120.78 +/- 267.61
Episode length: 379.60 +/- 25.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | 121           |
| time/                   |               |
|    total_timesteps      | 1510000       |
| train/                  |               |
|    approx_kl            | 0.00010635701 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.75          |
|    learning_rate        | 0.001         |
|    loss                 | 3.56e+03      |
|    n_updates            | 7370          |
|    policy_gradient_loss | -0.000177     |
|    std                  | 3.09          |
|    value_loss           | 9.49e+03      |
-------------------------------------------
Eval num_timesteps=1512000, episode_reward=273.53 +/- 492.44
Episode length: 423.80 +/- 22.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 274          |
| time/                   |              |
|    total_timesteps      | 1512000      |
| train/                  |              |
|    approx_kl            | 0.0004272299 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+03     |
|    n_updates            | 7380         |
|    policy_gradient_loss | -0.000957    |
|    std                  | 3.09         |
|    value_loss           | 2.95e+03     |
------------------------------------------
Eval num_timesteps=1514000, episode_reward=419.85 +/- 325.19
Episode length: 409.00 +/- 17.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | 420           |
| time/                   |               |
|    total_timesteps      | 1514000       |
| train/                  |               |
|    approx_kl            | 0.00032690412 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 3.92e+03      |
|    n_updates            | 7390          |
|    policy_gradient_loss | 0.000319      |
|    std                  | 3.09          |
|    value_loss           | 8.12e+03      |
-------------------------------------------
Eval num_timesteps=1516000, episode_reward=195.73 +/- 500.76
Episode length: 411.40 +/- 54.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 196           |
| time/                   |               |
|    total_timesteps      | 1516000       |
| train/                  |               |
|    approx_kl            | 0.00010549702 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.857         |
|    learning_rate        | 0.001         |
|    loss                 | 3.22e+03      |
|    n_updates            | 7400          |
|    policy_gradient_loss | -0.000167     |
|    std                  | 3.09          |
|    value_loss           | 7.28e+03      |
-------------------------------------------
Eval num_timesteps=1518000, episode_reward=148.12 +/- 251.05
Episode length: 368.40 +/- 44.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 368           |
|    mean_reward          | 148           |
| time/                   |               |
|    total_timesteps      | 1518000       |
| train/                  |               |
|    approx_kl            | 1.6059144e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 6.04e+03      |
|    n_updates            | 7410          |
|    policy_gradient_loss | 0.000304      |
|    std                  | 3.09          |
|    value_loss           | 1.35e+04      |
-------------------------------------------
Eval num_timesteps=1520000, episode_reward=69.33 +/- 342.95
Episode length: 398.80 +/- 23.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 69.3        |
| time/                   |             |
|    total_timesteps      | 1520000     |
| train/                  |             |
|    approx_kl            | 9.37524e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.001       |
|    loss                 | 2.71e+03    |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.000138   |
|    std                  | 3.09        |
|    value_loss           | 6.59e+03    |
-----------------------------------------
Eval num_timesteps=1522000, episode_reward=-14.69 +/- 382.90
Episode length: 424.00 +/- 22.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | -14.7         |
| time/                   |               |
|    total_timesteps      | 1522000       |
| train/                  |               |
|    approx_kl            | 0.00034172682 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+03      |
|    n_updates            | 7430          |
|    policy_gradient_loss | -0.00068      |
|    std                  | 3.09          |
|    value_loss           | 4.05e+03      |
-------------------------------------------
Eval num_timesteps=1524000, episode_reward=293.14 +/- 465.91
Episode length: 393.00 +/- 31.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 293          |
| time/                   |              |
|    total_timesteps      | 1524000      |
| train/                  |              |
|    approx_kl            | 0.0006053314 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+03     |
|    n_updates            | 7440         |
|    policy_gradient_loss | -0.000108    |
|    std                  | 3.09         |
|    value_loss           | 8.59e+03     |
------------------------------------------
Eval num_timesteps=1526000, episode_reward=137.89 +/- 286.74
Episode length: 378.40 +/- 36.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 378           |
|    mean_reward          | 138           |
| time/                   |               |
|    total_timesteps      | 1526000       |
| train/                  |               |
|    approx_kl            | 0.00016364452 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.867         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 7450          |
|    policy_gradient_loss | -0.000261     |
|    std                  | 3.09          |
|    value_loss           | 4.69e+03      |
-------------------------------------------
Eval num_timesteps=1528000, episode_reward=71.82 +/- 201.77
Episode length: 382.00 +/- 52.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 71.8         |
| time/                   |              |
|    total_timesteps      | 1528000      |
| train/                  |              |
|    approx_kl            | 6.620129e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 2.77e+03     |
|    n_updates            | 7460         |
|    policy_gradient_loss | 7.02e-05     |
|    std                  | 3.09         |
|    value_loss           | 6.08e+03     |
------------------------------------------
Eval num_timesteps=1530000, episode_reward=-65.58 +/- 390.79
Episode length: 421.60 +/- 23.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | -65.6         |
| time/                   |               |
|    total_timesteps      | 1530000       |
| train/                  |               |
|    approx_kl            | 0.00014395759 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+03      |
|    n_updates            | 7470          |
|    policy_gradient_loss | -0.000375     |
|    std                  | 3.09          |
|    value_loss           | 3e+03         |
-------------------------------------------
Eval num_timesteps=1532000, episode_reward=517.35 +/- 307.80
Episode length: 377.60 +/- 56.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 378           |
|    mean_reward          | 517           |
| time/                   |               |
|    total_timesteps      | 1532000       |
| train/                  |               |
|    approx_kl            | 0.00017599884 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+03      |
|    n_updates            | 7480          |
|    policy_gradient_loss | -0.000126     |
|    std                  | 3.1           |
|    value_loss           | 6.02e+03      |
-------------------------------------------
Eval num_timesteps=1534000, episode_reward=52.29 +/- 371.52
Episode length: 392.20 +/- 28.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 52.3          |
| time/                   |               |
|    total_timesteps      | 1534000       |
| train/                  |               |
|    approx_kl            | 0.00046918893 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 90.9          |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000615     |
|    std                  | 3.1           |
|    value_loss           | 552           |
-------------------------------------------
Eval num_timesteps=1536000, episode_reward=275.95 +/- 440.47
Episode length: 413.00 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1538000, episode_reward=138.40 +/- 243.17
Episode length: 404.40 +/- 40.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 138          |
| time/                   |              |
|    total_timesteps      | 1538000      |
| train/                  |              |
|    approx_kl            | 0.0033399195 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 39.4         |
|    n_updates            | 7500         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 3.12         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=1540000, episode_reward=113.42 +/- 282.21
Episode length: 389.40 +/- 28.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 389          |
|    mean_reward          | 113          |
| time/                   |              |
|    total_timesteps      | 1540000      |
| train/                  |              |
|    approx_kl            | 0.0038881057 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 1.81e+03     |
|    n_updates            | 7510         |
|    policy_gradient_loss | -0.000378    |
|    std                  | 3.12         |
|    value_loss           | 4.68e+03     |
------------------------------------------
Eval num_timesteps=1542000, episode_reward=489.80 +/- 344.70
Episode length: 372.80 +/- 17.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 490          |
| time/                   |              |
|    total_timesteps      | 1542000      |
| train/                  |              |
|    approx_kl            | 0.0003611682 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.1        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 4.07e+03     |
|    n_updates            | 7520         |
|    policy_gradient_loss | 0.000383     |
|    std                  | 3.13         |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=1544000, episode_reward=372.69 +/- 123.11
Episode length: 365.20 +/- 28.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 365           |
|    mean_reward          | 373           |
| time/                   |               |
|    total_timesteps      | 1544000       |
| train/                  |               |
|    approx_kl            | 0.00013005736 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.1         |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+03      |
|    n_updates            | 7530          |
|    policy_gradient_loss | -0.00021      |
|    std                  | 3.13          |
|    value_loss           | 3.82e+03      |
-------------------------------------------
Eval num_timesteps=1546000, episode_reward=531.73 +/- 233.51
Episode length: 371.80 +/- 34.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | 532           |
| time/                   |               |
|    total_timesteps      | 1546000       |
| train/                  |               |
|    approx_kl            | 0.00026466633 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.833         |
|    learning_rate        | 0.001         |
|    loss                 | 2.45e+03      |
|    n_updates            | 7540          |
|    policy_gradient_loss | -0.00034      |
|    std                  | 3.13          |
|    value_loss           | 6.37e+03      |
-------------------------------------------
Eval num_timesteps=1548000, episode_reward=610.60 +/- 103.35
Episode length: 385.00 +/- 27.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 611          |
| time/                   |              |
|    total_timesteps      | 1548000      |
| train/                  |              |
|    approx_kl            | 0.0005482084 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+03     |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 3.13         |
|    value_loss           | 5.49e+03     |
------------------------------------------
Eval num_timesteps=1550000, episode_reward=66.47 +/- 133.36
Episode length: 386.40 +/- 30.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 66.5          |
| time/                   |               |
|    total_timesteps      | 1550000       |
| train/                  |               |
|    approx_kl            | 0.00069701497 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.87          |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+03      |
|    n_updates            | 7560          |
|    policy_gradient_loss | -0.000555     |
|    std                  | 3.13          |
|    value_loss           | 2.87e+03      |
-------------------------------------------
Eval num_timesteps=1552000, episode_reward=294.32 +/- 383.06
Episode length: 401.80 +/- 30.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 294          |
| time/                   |              |
|    total_timesteps      | 1552000      |
| train/                  |              |
|    approx_kl            | 0.0008032916 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.001        |
|    loss                 | 3.04e+03     |
|    n_updates            | 7570         |
|    policy_gradient_loss | -0.000642    |
|    std                  | 3.13         |
|    value_loss           | 7.17e+03     |
------------------------------------------
Eval num_timesteps=1554000, episode_reward=464.30 +/- 87.92
Episode length: 370.00 +/- 22.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 370           |
|    mean_reward          | 464           |
| time/                   |               |
|    total_timesteps      | 1554000       |
| train/                  |               |
|    approx_kl            | 0.00089332025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.001         |
|    loss                 | 961           |
|    n_updates            | 7580          |
|    policy_gradient_loss | -0.000654     |
|    std                  | 3.14          |
|    value_loss           | 2.22e+03      |
-------------------------------------------
Eval num_timesteps=1556000, episode_reward=546.25 +/- 393.23
Episode length: 408.80 +/- 14.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | 546           |
| time/                   |               |
|    total_timesteps      | 1556000       |
| train/                  |               |
|    approx_kl            | 0.00089899043 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 122           |
|    n_updates            | 7590          |
|    policy_gradient_loss | -0.00065      |
|    std                  | 3.14          |
|    value_loss           | 605           |
-------------------------------------------
Eval num_timesteps=1558000, episode_reward=266.26 +/- 197.67
Episode length: 374.80 +/- 31.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 1558000      |
| train/                  |              |
|    approx_kl            | 0.0005600406 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 7600         |
|    policy_gradient_loss | 0.000191     |
|    std                  | 3.14         |
|    value_loss           | 5.29e+03     |
------------------------------------------
Eval num_timesteps=1560000, episode_reward=326.39 +/- 287.17
Episode length: 379.60 +/- 26.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 326          |
| time/                   |              |
|    total_timesteps      | 1560000      |
| train/                  |              |
|    approx_kl            | 7.990975e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 3.87e+03     |
|    n_updates            | 7610         |
|    policy_gradient_loss | -0.000117    |
|    std                  | 3.14         |
|    value_loss           | 8.87e+03     |
------------------------------------------
Eval num_timesteps=1562000, episode_reward=446.58 +/- 227.23
Episode length: 405.00 +/- 36.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 447           |
| time/                   |               |
|    total_timesteps      | 1562000       |
| train/                  |               |
|    approx_kl            | 0.00027808815 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+03      |
|    n_updates            | 7620          |
|    policy_gradient_loss | -0.000497     |
|    std                  | 3.13          |
|    value_loss           | 2.9e+03       |
-------------------------------------------
Eval num_timesteps=1564000, episode_reward=535.09 +/- 244.69
Episode length: 416.40 +/- 29.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 535           |
| time/                   |               |
|    total_timesteps      | 1564000       |
| train/                  |               |
|    approx_kl            | 0.00027684617 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.001         |
|    loss                 | 1e+03         |
|    n_updates            | 7630          |
|    policy_gradient_loss | -0.000424     |
|    std                  | 3.13          |
|    value_loss           | 2.47e+03      |
-------------------------------------------
Eval num_timesteps=1566000, episode_reward=200.45 +/- 340.71
Episode length: 410.40 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 1566000     |
| train/                  |             |
|    approx_kl            | 0.002287488 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 83.5        |
|    n_updates            | 7640        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 3.14        |
|    value_loss           | 519         |
-----------------------------------------
Eval num_timesteps=1568000, episode_reward=206.42 +/- 321.74
Episode length: 459.40 +/- 49.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 206          |
| time/                   |              |
|    total_timesteps      | 1568000      |
| train/                  |              |
|    approx_kl            | 0.0035217078 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.001        |
|    loss                 | 3.35e+03     |
|    n_updates            | 7650         |
|    policy_gradient_loss | 0.000908     |
|    std                  | 3.14         |
|    value_loss           | 8.77e+03     |
------------------------------------------
Eval num_timesteps=1570000, episode_reward=310.40 +/- 240.88
Episode length: 445.60 +/- 28.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 310           |
| time/                   |               |
|    total_timesteps      | 1570000       |
| train/                  |               |
|    approx_kl            | 0.00044970744 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+03      |
|    n_updates            | 7660          |
|    policy_gradient_loss | -0.000316     |
|    std                  | 3.15          |
|    value_loss           | 4.89e+03      |
-------------------------------------------
Eval num_timesteps=1572000, episode_reward=443.86 +/- 222.05
Episode length: 435.80 +/- 41.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 1572000       |
| train/                  |               |
|    approx_kl            | 0.00018657272 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.851         |
|    learning_rate        | 0.001         |
|    loss                 | 3.45e+03      |
|    n_updates            | 7670          |
|    policy_gradient_loss | -1.99e-05     |
|    std                  | 3.15          |
|    value_loss           | 7.67e+03      |
-------------------------------------------
Eval num_timesteps=1574000, episode_reward=268.24 +/- 195.29
Episode length: 420.80 +/- 31.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 421           |
|    mean_reward          | 268           |
| time/                   |               |
|    total_timesteps      | 1574000       |
| train/                  |               |
|    approx_kl            | 0.00038461058 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.2         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 125           |
|    n_updates            | 7680          |
|    policy_gradient_loss | -0.000434     |
|    std                  | 3.15          |
|    value_loss           | 821           |
-------------------------------------------
Eval num_timesteps=1576000, episode_reward=181.06 +/- 206.55
Episode length: 417.00 +/- 30.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 1576000      |
| train/                  |              |
|    approx_kl            | 0.0028829451 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 97.1         |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 3.16         |
|    value_loss           | 482          |
------------------------------------------
Eval num_timesteps=1578000, episode_reward=139.26 +/- 97.88
Episode length: 378.80 +/- 47.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 1578000     |
| train/                  |             |
|    approx_kl            | 0.005401808 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 42.5        |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00159    |
|    std                  | 3.17        |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=1580000, episode_reward=376.22 +/- 234.57
Episode length: 399.60 +/- 16.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 1580000      |
| train/                  |              |
|    approx_kl            | 0.0011809474 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 71.8         |
|    n_updates            | 7710         |
|    policy_gradient_loss | -0.000541    |
|    std                  | 3.2          |
|    value_loss           | 294          |
------------------------------------------
Eval num_timesteps=1582000, episode_reward=137.89 +/- 39.31
Episode length: 381.00 +/- 11.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 138          |
| time/                   |              |
|    total_timesteps      | 1582000      |
| train/                  |              |
|    approx_kl            | 0.0052208607 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.2        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 27.2         |
|    n_updates            | 7720         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 3.21         |
|    value_loss           | 94.4         |
------------------------------------------
Eval num_timesteps=1584000, episode_reward=310.92 +/- 157.77
Episode length: 401.20 +/- 37.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 311           |
| time/                   |               |
|    total_timesteps      | 1584000       |
| train/                  |               |
|    approx_kl            | 0.00071098155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+03      |
|    n_updates            | 7730          |
|    policy_gradient_loss | -0.000575     |
|    std                  | 3.22          |
|    value_loss           | 3.91e+03      |
-------------------------------------------
Eval num_timesteps=1586000, episode_reward=52.14 +/- 120.18
Episode length: 388.20 +/- 38.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 52.1          |
| time/                   |               |
|    total_timesteps      | 1586000       |
| train/                  |               |
|    approx_kl            | 0.00046815854 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.836         |
|    learning_rate        | 0.001         |
|    loss                 | 2.45e+03      |
|    n_updates            | 7740          |
|    policy_gradient_loss | -0.000277     |
|    std                  | 3.23          |
|    value_loss           | 6.09e+03      |
-------------------------------------------
Eval num_timesteps=1588000, episode_reward=186.62 +/- 290.60
Episode length: 366.00 +/- 35.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 1588000      |
| train/                  |              |
|    approx_kl            | 0.0004380355 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 60.1         |
|    n_updates            | 7750         |
|    policy_gradient_loss | -0.000249    |
|    std                  | 3.23         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=1590000, episode_reward=260.07 +/- 235.64
Episode length: 421.60 +/- 47.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 260          |
| time/                   |              |
|    total_timesteps      | 1590000      |
| train/                  |              |
|    approx_kl            | 0.0054227617 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 30.3         |
|    n_updates            | 7760         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 3.23         |
|    value_loss           | 91.8         |
------------------------------------------
Eval num_timesteps=1592000, episode_reward=218.82 +/- 195.79
Episode length: 379.60 +/- 25.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 380          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 1592000      |
| train/                  |              |
|    approx_kl            | 0.0061751194 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 38.6         |
|    n_updates            | 7770         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 3.24         |
|    value_loss           | 174          |
------------------------------------------
Eval num_timesteps=1594000, episode_reward=215.86 +/- 232.39
Episode length: 385.40 +/- 47.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 385         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 1594000     |
| train/                  |             |
|    approx_kl            | 0.002475716 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.001       |
|    loss                 | 1.82e+03    |
|    n_updates            | 7780        |
|    policy_gradient_loss | 0.00107     |
|    std                  | 3.24        |
|    value_loss           | 4.71e+03    |
-----------------------------------------
Eval num_timesteps=1596000, episode_reward=255.41 +/- 269.24
Episode length: 411.40 +/- 26.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 255          |
| time/                   |              |
|    total_timesteps      | 1596000      |
| train/                  |              |
|    approx_kl            | 0.0023885504 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 42.4         |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 3.24         |
|    value_loss           | 181          |
------------------------------------------
Eval num_timesteps=1598000, episode_reward=110.76 +/- 68.84
Episode length: 362.00 +/- 20.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 362         |
|    mean_reward          | 111         |
| time/                   |             |
|    total_timesteps      | 1598000     |
| train/                  |             |
|    approx_kl            | 0.003695153 |
|    clip_fraction        | 0.00728     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 56          |
|    n_updates            | 7800        |
|    policy_gradient_loss | -0.000471   |
|    std                  | 3.25        |
|    value_loss           | 195         |
-----------------------------------------
Eval num_timesteps=1600000, episode_reward=313.18 +/- 264.09
Episode length: 394.40 +/- 38.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 313          |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0015367715 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+03     |
|    n_updates            | 7810         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 3.25         |
|    value_loss           | 4.71e+03     |
------------------------------------------
Eval num_timesteps=1602000, episode_reward=320.32 +/- 202.05
Episode length: 395.80 +/- 41.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 320          |
| time/                   |              |
|    total_timesteps      | 1602000      |
| train/                  |              |
|    approx_kl            | 0.0036665993 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 22.5         |
|    n_updates            | 7820         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 3.25         |
|    value_loss           | 76           |
------------------------------------------
Eval num_timesteps=1604000, episode_reward=267.32 +/- 160.39
Episode length: 410.60 +/- 54.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 1604000      |
| train/                  |              |
|    approx_kl            | 0.0016970945 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+03     |
|    n_updates            | 7830         |
|    policy_gradient_loss | 0.000111     |
|    std                  | 3.24         |
|    value_loss           | 4.05e+03     |
------------------------------------------
Eval num_timesteps=1606000, episode_reward=721.49 +/- 143.58
Episode length: 447.40 +/- 58.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 721           |
| time/                   |               |
|    total_timesteps      | 1606000       |
| train/                  |               |
|    approx_kl            | 0.00029564573 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+03      |
|    n_updates            | 7840          |
|    policy_gradient_loss | -0.000376     |
|    std                  | 3.24          |
|    value_loss           | 5.58e+03      |
-------------------------------------------
Eval num_timesteps=1608000, episode_reward=444.60 +/- 362.94
Episode length: 434.40 +/- 37.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 445           |
| time/                   |               |
|    total_timesteps      | 1608000       |
| train/                  |               |
|    approx_kl            | 1.9725703e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 7850          |
|    policy_gradient_loss | 4.42e-05      |
|    std                  | 3.25          |
|    value_loss           | 2.48e+03      |
-------------------------------------------
Eval num_timesteps=1610000, episode_reward=249.23 +/- 242.32
Episode length: 415.80 +/- 36.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 249           |
| time/                   |               |
|    total_timesteps      | 1610000       |
| train/                  |               |
|    approx_kl            | 0.00022633991 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+03      |
|    n_updates            | 7860          |
|    policy_gradient_loss | -0.000269     |
|    std                  | 3.25          |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=1612000, episode_reward=472.86 +/- 431.67
Episode length: 446.20 +/- 28.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 1612000      |
| train/                  |              |
|    approx_kl            | 0.0009429533 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+03      |
|    n_updates            | 7870         |
|    policy_gradient_loss | -0.000496    |
|    std                  | 3.25         |
|    value_loss           | 5.82e+03     |
------------------------------------------
Eval num_timesteps=1614000, episode_reward=280.37 +/- 317.34
Episode length: 473.40 +/- 61.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 280          |
| time/                   |              |
|    total_timesteps      | 1614000      |
| train/                  |              |
|    approx_kl            | 0.0006111392 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 2.36e+03     |
|    n_updates            | 7880         |
|    policy_gradient_loss | -0.000705    |
|    std                  | 3.25         |
|    value_loss           | 5.82e+03     |
------------------------------------------
Eval num_timesteps=1616000, episode_reward=214.99 +/- 290.01
Episode length: 404.60 +/- 19.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 1616000     |
| train/                  |             |
|    approx_kl            | 0.001467126 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.001       |
|    loss                 | 1.46e+03    |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.000735   |
|    std                  | 3.25        |
|    value_loss           | 3.27e+03    |
-----------------------------------------
Eval num_timesteps=1618000, episode_reward=444.70 +/- 261.32
Episode length: 413.80 +/- 32.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 445          |
| time/                   |              |
|    total_timesteps      | 1618000      |
| train/                  |              |
|    approx_kl            | 0.0034755485 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+03     |
|    n_updates            | 7900         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 3.25         |
|    value_loss           | 5.13e+03     |
------------------------------------------
Eval num_timesteps=1620000, episode_reward=358.95 +/- 188.95
Episode length: 421.80 +/- 35.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 1620000       |
| train/                  |               |
|    approx_kl            | 0.00063519995 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.64e+03      |
|    n_updates            | 7910          |
|    policy_gradient_loss | 3.29e-06      |
|    std                  | 3.25          |
|    value_loss           | 4.03e+03      |
-------------------------------------------
Eval num_timesteps=1622000, episode_reward=352.75 +/- 354.00
Episode length: 427.60 +/- 20.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 1622000  |
---------------------------------
Eval num_timesteps=1624000, episode_reward=409.03 +/- 177.93
Episode length: 410.80 +/- 29.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 409           |
| time/                   |               |
|    total_timesteps      | 1624000       |
| train/                  |               |
|    approx_kl            | 0.00026091005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+03      |
|    n_updates            | 7920          |
|    policy_gradient_loss | -0.000516     |
|    std                  | 3.25          |
|    value_loss           | 2.38e+03      |
-------------------------------------------
Eval num_timesteps=1626000, episode_reward=234.98 +/- 289.79
Episode length: 408.40 +/- 16.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 235           |
| time/                   |               |
|    total_timesteps      | 1626000       |
| train/                  |               |
|    approx_kl            | 0.00029262513 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.001         |
|    loss                 | 2.79e+03      |
|    n_updates            | 7930          |
|    policy_gradient_loss | -0.000387     |
|    std                  | 3.25          |
|    value_loss           | 6.29e+03      |
-------------------------------------------
Eval num_timesteps=1628000, episode_reward=287.02 +/- 244.19
Episode length: 392.00 +/- 24.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 1628000      |
| train/                  |              |
|    approx_kl            | 0.0041871862 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 19.8         |
|    n_updates            | 7940         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 3.27         |
|    value_loss           | 76.4         |
------------------------------------------
Eval num_timesteps=1630000, episode_reward=252.43 +/- 121.31
Episode length: 367.80 +/- 35.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 368          |
|    mean_reward          | 252          |
| time/                   |              |
|    total_timesteps      | 1630000      |
| train/                  |              |
|    approx_kl            | 0.0032040747 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 584          |
|    n_updates            | 7950         |
|    policy_gradient_loss | 0.000628     |
|    std                  | 3.28         |
|    value_loss           | 1.71e+03     |
------------------------------------------
Eval num_timesteps=1632000, episode_reward=244.54 +/- 266.34
Episode length: 370.00 +/- 38.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 370           |
|    mean_reward          | 245           |
| time/                   |               |
|    total_timesteps      | 1632000       |
| train/                  |               |
|    approx_kl            | 0.00048029647 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+03      |
|    n_updates            | 7960          |
|    policy_gradient_loss | -0.000195     |
|    std                  | 3.29          |
|    value_loss           | 6.45e+03      |
-------------------------------------------
Eval num_timesteps=1634000, episode_reward=452.15 +/- 250.69
Episode length: 379.60 +/- 30.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 1634000       |
| train/                  |               |
|    approx_kl            | 0.00089032383 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 52.3          |
|    n_updates            | 7970          |
|    policy_gradient_loss | -0.000463     |
|    std                  | 3.29          |
|    value_loss           | 209           |
-------------------------------------------
Eval num_timesteps=1636000, episode_reward=400.77 +/- 406.35
Episode length: 391.60 +/- 28.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 1636000      |
| train/                  |              |
|    approx_kl            | 0.0033881092 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+03     |
|    n_updates            | 7980         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 3.3          |
|    value_loss           | 3.22e+03     |
------------------------------------------
Eval num_timesteps=1638000, episode_reward=370.58 +/- 354.15
Episode length: 416.60 +/- 44.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 371          |
| time/                   |              |
|    total_timesteps      | 1638000      |
| train/                  |              |
|    approx_kl            | 0.0010592768 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.001        |
|    loss                 | 2.83e+03     |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 3.3          |
|    value_loss           | 6.07e+03     |
------------------------------------------
Eval num_timesteps=1640000, episode_reward=413.87 +/- 310.03
Episode length: 432.80 +/- 81.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 433           |
|    mean_reward          | 414           |
| time/                   |               |
|    total_timesteps      | 1640000       |
| train/                  |               |
|    approx_kl            | 0.00043837805 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+03      |
|    n_updates            | 8000          |
|    policy_gradient_loss | -0.000358     |
|    std                  | 3.3           |
|    value_loss           | 4.19e+03      |
-------------------------------------------
Eval num_timesteps=1642000, episode_reward=399.07 +/- 275.63
Episode length: 400.60 +/- 69.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 399          |
| time/                   |              |
|    total_timesteps      | 1642000      |
| train/                  |              |
|    approx_kl            | 0.0003640399 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.001        |
|    loss                 | 979          |
|    n_updates            | 8010         |
|    policy_gradient_loss | -0.000449    |
|    std                  | 3.3          |
|    value_loss           | 2.79e+03     |
------------------------------------------
Eval num_timesteps=1644000, episode_reward=502.38 +/- 197.72
Episode length: 403.80 +/- 73.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 502           |
| time/                   |               |
|    total_timesteps      | 1644000       |
| train/                  |               |
|    approx_kl            | 0.00032709754 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+03      |
|    n_updates            | 8020          |
|    policy_gradient_loss | -0.000233     |
|    std                  | 3.3           |
|    value_loss           | 6.53e+03      |
-------------------------------------------
Eval num_timesteps=1646000, episode_reward=395.86 +/- 555.31
Episode length: 401.60 +/- 38.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 1646000      |
| train/                  |              |
|    approx_kl            | 0.0013564704 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 22.3         |
|    n_updates            | 8030         |
|    policy_gradient_loss | -0.000651    |
|    std                  | 3.3          |
|    value_loss           | 181          |
------------------------------------------
Eval num_timesteps=1648000, episode_reward=362.46 +/- 352.71
Episode length: 414.20 +/- 33.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 362           |
| time/                   |               |
|    total_timesteps      | 1648000       |
| train/                  |               |
|    approx_kl            | 0.00063037366 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 51            |
|    n_updates            | 8040          |
|    policy_gradient_loss | 0.000618      |
|    std                  | 3.31          |
|    value_loss           | 314           |
-------------------------------------------
Eval num_timesteps=1650000, episode_reward=218.79 +/- 265.70
Episode length: 375.80 +/- 26.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 1650000      |
| train/                  |              |
|    approx_kl            | 0.0029934717 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.803        |
|    learning_rate        | 0.001        |
|    loss                 | 3.66e+03     |
|    n_updates            | 8050         |
|    policy_gradient_loss | 0.000199     |
|    std                  | 3.31         |
|    value_loss           | 8.66e+03     |
------------------------------------------
Eval num_timesteps=1652000, episode_reward=288.02 +/- 366.28
Episode length: 397.60 +/- 30.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 288          |
| time/                   |              |
|    total_timesteps      | 1652000      |
| train/                  |              |
|    approx_kl            | 0.0013804799 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 126          |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.000985    |
|    std                  | 3.32         |
|    value_loss           | 704          |
------------------------------------------
Eval num_timesteps=1654000, episode_reward=131.36 +/- 177.81
Episode length: 409.40 +/- 39.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 131          |
| time/                   |              |
|    total_timesteps      | 1654000      |
| train/                  |              |
|    approx_kl            | 0.0015318361 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.001        |
|    loss                 | 883          |
|    n_updates            | 8070         |
|    policy_gradient_loss | -9.35e-05    |
|    std                  | 3.32         |
|    value_loss           | 2.86e+03     |
------------------------------------------
Eval num_timesteps=1656000, episode_reward=299.08 +/- 119.51
Episode length: 404.80 +/- 47.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 299           |
| time/                   |               |
|    total_timesteps      | 1656000       |
| train/                  |               |
|    approx_kl            | 0.00039997153 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.3         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 3.8e+03       |
|    n_updates            | 8080          |
|    policy_gradient_loss | -0.000619     |
|    std                  | 3.32          |
|    value_loss           | 8.83e+03      |
-------------------------------------------
Eval num_timesteps=1658000, episode_reward=417.73 +/- 299.56
Episode length: 390.00 +/- 22.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 1658000      |
| train/                  |              |
|    approx_kl            | 0.0005429265 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.3        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 2.32e+03     |
|    n_updates            | 8090         |
|    policy_gradient_loss | 2.89e-06     |
|    std                  | 3.32         |
|    value_loss           | 5.29e+03     |
------------------------------------------
Eval num_timesteps=1660000, episode_reward=451.02 +/- 398.34
Episode length: 424.20 +/- 17.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | 451         |
| time/                   |             |
|    total_timesteps      | 1660000     |
| train/                  |             |
|    approx_kl            | 0.007800081 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 35.6        |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.00327    |
|    std                  | 3.33        |
|    value_loss           | 188         |
-----------------------------------------
Eval num_timesteps=1662000, episode_reward=302.43 +/- 304.30
Episode length: 427.60 +/- 66.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 302          |
| time/                   |              |
|    total_timesteps      | 1662000      |
| train/                  |              |
|    approx_kl            | 0.0005856862 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 7.39e+03     |
|    n_updates            | 8110         |
|    policy_gradient_loss | -0.000437    |
|    std                  | 3.34         |
|    value_loss           | 1.63e+04     |
------------------------------------------
Eval num_timesteps=1664000, episode_reward=128.78 +/- 422.72
Episode length: 420.00 +/- 31.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 1664000     |
| train/                  |             |
|    approx_kl            | 7.17107e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.001       |
|    loss                 | 2.15e+03    |
|    n_updates            | 8120        |
|    policy_gradient_loss | 6.28e-05    |
|    std                  | 3.34        |
|    value_loss           | 4.68e+03    |
-----------------------------------------
Eval num_timesteps=1666000, episode_reward=238.19 +/- 512.69
Episode length: 430.40 +/- 37.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 238           |
| time/                   |               |
|    total_timesteps      | 1666000       |
| train/                  |               |
|    approx_kl            | 1.0779215e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 3.84e+03      |
|    n_updates            | 8130          |
|    policy_gradient_loss | 3.74e-05      |
|    std                  | 3.35          |
|    value_loss           | 8.81e+03      |
-------------------------------------------
Eval num_timesteps=1668000, episode_reward=166.03 +/- 401.37
Episode length: 420.20 +/- 44.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 166           |
| time/                   |               |
|    total_timesteps      | 1668000       |
| train/                  |               |
|    approx_kl            | 0.00029269955 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+03      |
|    n_updates            | 8140          |
|    policy_gradient_loss | -0.000564     |
|    std                  | 3.35          |
|    value_loss           | 3.17e+03      |
-------------------------------------------
Eval num_timesteps=1670000, episode_reward=329.46 +/- 176.90
Episode length: 477.00 +/- 29.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 477           |
|    mean_reward          | 329           |
| time/                   |               |
|    total_timesteps      | 1670000       |
| train/                  |               |
|    approx_kl            | 0.00056528614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 105           |
|    n_updates            | 8150          |
|    policy_gradient_loss | -0.000228     |
|    std                  | 3.35          |
|    value_loss           | 541           |
-------------------------------------------
Eval num_timesteps=1672000, episode_reward=343.51 +/- 335.74
Episode length: 444.00 +/- 28.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 344           |
| time/                   |               |
|    total_timesteps      | 1672000       |
| train/                  |               |
|    approx_kl            | 0.00045058824 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+03      |
|    n_updates            | 8160          |
|    policy_gradient_loss | -0.00054      |
|    std                  | 3.34          |
|    value_loss           | 2.81e+03      |
-------------------------------------------
Eval num_timesteps=1674000, episode_reward=408.50 +/- 293.88
Episode length: 467.20 +/- 12.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 409           |
| time/                   |               |
|    total_timesteps      | 1674000       |
| train/                  |               |
|    approx_kl            | 0.00024982358 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+03      |
|    n_updates            | 8170          |
|    policy_gradient_loss | -0.000369     |
|    std                  | 3.34          |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=1676000, episode_reward=466.15 +/- 292.69
Episode length: 514.00 +/- 59.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 514          |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 1676000      |
| train/                  |              |
|    approx_kl            | 0.0034100795 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 136          |
|    n_updates            | 8180         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 3.34         |
|    value_loss           | 727          |
------------------------------------------
Eval num_timesteps=1678000, episode_reward=449.66 +/- 213.13
Episode length: 499.80 +/- 45.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 450          |
| time/                   |              |
|    total_timesteps      | 1678000      |
| train/                  |              |
|    approx_kl            | 0.0064819423 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+03     |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.000491    |
|    std                  | 3.33         |
|    value_loss           | 3.78e+03     |
------------------------------------------
Eval num_timesteps=1680000, episode_reward=311.48 +/- 155.79
Episode length: 466.20 +/- 37.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0003976294 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+03      |
|    n_updates            | 8200         |
|    policy_gradient_loss | -0.000151    |
|    std                  | 3.33         |
|    value_loss           | 9.73e+03     |
------------------------------------------
Eval num_timesteps=1682000, episode_reward=383.91 +/- 509.92
Episode length: 481.60 +/- 24.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 384           |
| time/                   |               |
|    total_timesteps      | 1682000       |
| train/                  |               |
|    approx_kl            | 1.3303768e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.001         |
|    loss                 | 3.06e+03      |
|    n_updates            | 8210          |
|    policy_gradient_loss | 9.09e-05      |
|    std                  | 3.33          |
|    value_loss           | 6.59e+03      |
-------------------------------------------
Eval num_timesteps=1684000, episode_reward=394.63 +/- 292.98
Episode length: 495.00 +/- 42.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 495           |
|    mean_reward          | 395           |
| time/                   |               |
|    total_timesteps      | 1684000       |
| train/                  |               |
|    approx_kl            | 0.00012700027 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.001         |
|    loss                 | 190           |
|    n_updates            | 8220          |
|    policy_gradient_loss | -0.000148     |
|    std                  | 3.34          |
|    value_loss           | 719           |
-------------------------------------------
Eval num_timesteps=1686000, episode_reward=196.31 +/- 229.84
Episode length: 492.80 +/- 35.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 493           |
|    mean_reward          | 196           |
| time/                   |               |
|    total_timesteps      | 1686000       |
| train/                  |               |
|    approx_kl            | 5.9145037e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 1.96e+03      |
|    n_updates            | 8230          |
|    policy_gradient_loss | 4.41e-05      |
|    std                  | 3.34          |
|    value_loss           | 5.09e+03      |
-------------------------------------------
Eval num_timesteps=1688000, episode_reward=482.66 +/- 254.39
Episode length: 485.60 +/- 39.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 483          |
| time/                   |              |
|    total_timesteps      | 1688000      |
| train/                  |              |
|    approx_kl            | 0.0013712356 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.001        |
|    loss                 | 230          |
|    n_updates            | 8240         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 3.34         |
|    value_loss           | 1.26e+03     |
------------------------------------------
Eval num_timesteps=1690000, episode_reward=525.65 +/- 361.42
Episode length: 517.60 +/- 3.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 518        |
|    mean_reward          | 526        |
| time/                   |            |
|    total_timesteps      | 1690000    |
| train/                  |            |
|    approx_kl            | 0.00248285 |
|    clip_fraction        | 0.00161    |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.001      |
|    loss                 | 1.96e+03   |
|    n_updates            | 8250       |
|    policy_gradient_loss | -0.000371  |
|    std                  | 3.34       |
|    value_loss           | 5.04e+03   |
----------------------------------------
Eval num_timesteps=1692000, episode_reward=280.61 +/- 221.21
Episode length: 502.60 +/- 20.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 503           |
|    mean_reward          | 281           |
| time/                   |               |
|    total_timesteps      | 1692000       |
| train/                  |               |
|    approx_kl            | 0.00048478643 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.849         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+03      |
|    n_updates            | 8260          |
|    policy_gradient_loss | -0.000313     |
|    std                  | 3.35          |
|    value_loss           | 4.46e+03      |
-------------------------------------------
Eval num_timesteps=1694000, episode_reward=160.03 +/- 103.75
Episode length: 504.00 +/- 7.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 160          |
| time/                   |              |
|    total_timesteps      | 1694000      |
| train/                  |              |
|    approx_kl            | 0.0012173064 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 94.6         |
|    n_updates            | 8270         |
|    policy_gradient_loss | -0.000521    |
|    std                  | 3.36         |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=1696000, episode_reward=611.37 +/- 244.38
Episode length: 508.40 +/- 7.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | 611          |
| time/                   |              |
|    total_timesteps      | 1696000      |
| train/                  |              |
|    approx_kl            | 0.0020281812 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 2.25e+03     |
|    n_updates            | 8280         |
|    policy_gradient_loss | 0.000343     |
|    std                  | 3.36         |
|    value_loss           | 5.89e+03     |
------------------------------------------
Eval num_timesteps=1698000, episode_reward=227.55 +/- 569.13
Episode length: 486.40 +/- 52.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 1698000     |
| train/                  |             |
|    approx_kl            | 0.006307959 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 68          |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 3.37        |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=1700000, episode_reward=45.59 +/- 90.24
Episode length: 448.80 +/- 38.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 45.6         |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0027354325 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 366          |
|    n_updates            | 8300         |
|    policy_gradient_loss | -0.000987    |
|    std                  | 3.39         |
|    value_loss           | 1.3e+03      |
------------------------------------------
Eval num_timesteps=1702000, episode_reward=34.06 +/- 55.40
Episode length: 481.60 +/- 28.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 482         |
|    mean_reward          | 34.1        |
| time/                   |             |
|    total_timesteps      | 1702000     |
| train/                  |             |
|    approx_kl            | 0.004959458 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.4       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 93.1        |
|    n_updates            | 8310        |
|    policy_gradient_loss | -0.000904   |
|    std                  | 3.4         |
|    value_loss           | 383         |
-----------------------------------------
Eval num_timesteps=1704000, episode_reward=83.99 +/- 95.98
Episode length: 486.40 +/- 43.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 84            |
| time/                   |               |
|    total_timesteps      | 1704000       |
| train/                  |               |
|    approx_kl            | 0.00025327638 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.4         |
|    explained_variance   | 0.82          |
|    learning_rate        | 0.001         |
|    loss                 | 242           |
|    n_updates            | 8320          |
|    policy_gradient_loss | -4.59e-05     |
|    std                  | 3.4           |
|    value_loss           | 1.09e+03      |
-------------------------------------------
Eval num_timesteps=1706000, episode_reward=114.54 +/- 140.37
Episode length: 484.00 +/- 33.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | 115          |
| time/                   |              |
|    total_timesteps      | 1706000      |
| train/                  |              |
|    approx_kl            | 0.0019152905 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.4        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 8330         |
|    policy_gradient_loss | -0.000906    |
|    std                  | 3.4          |
|    value_loss           | 386          |
------------------------------------------
Eval num_timesteps=1708000, episode_reward=38.31 +/- 88.85
Episode length: 476.80 +/- 43.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 38.3     |
| time/              |          |
|    total_timesteps | 1708000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=12.91 +/- 66.14
Episode length: 453.00 +/- 56.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 1710000      |
| train/                  |              |
|    approx_kl            | 0.0025583557 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 92.2         |
|    n_updates            | 8340         |
|    policy_gradient_loss | -0.000463    |
|    std                  | 3.41         |
|    value_loss           | 415          |
------------------------------------------
Eval num_timesteps=1712000, episode_reward=-46.41 +/- 31.96
Episode length: 459.80 +/- 22.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | -46.4        |
| time/                   |              |
|    total_timesteps      | 1712000      |
| train/                  |              |
|    approx_kl            | 0.0014496078 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 134          |
|    n_updates            | 8350         |
|    policy_gradient_loss | -0.000674    |
|    std                  | 3.41         |
|    value_loss           | 454          |
------------------------------------------
Eval num_timesteps=1714000, episode_reward=-10.25 +/- 48.04
Episode length: 466.60 +/- 42.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | -10.3       |
| time/                   |             |
|    total_timesteps      | 1714000     |
| train/                  |             |
|    approx_kl            | 0.006909773 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 125         |
|    n_updates            | 8360        |
|    policy_gradient_loss | -0.00153    |
|    std                  | 3.43        |
|    value_loss           | 470         |
-----------------------------------------
Eval num_timesteps=1716000, episode_reward=-64.70 +/- 40.29
Episode length: 430.00 +/- 38.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | -64.7       |
| time/                   |             |
|    total_timesteps      | 1716000     |
| train/                  |             |
|    approx_kl            | 0.008777308 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 191         |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.00251    |
|    std                  | 3.46        |
|    value_loss           | 589         |
-----------------------------------------
Eval num_timesteps=1718000, episode_reward=-28.48 +/- 51.02
Episode length: 439.60 +/- 41.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | -28.5        |
| time/                   |              |
|    total_timesteps      | 1718000      |
| train/                  |              |
|    approx_kl            | 0.0067659495 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 59           |
|    n_updates            | 8380         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 3.46         |
|    value_loss           | 172          |
------------------------------------------
Eval num_timesteps=1720000, episode_reward=-43.03 +/- 43.79
Episode length: 450.20 +/- 49.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | -43          |
| time/                   |              |
|    total_timesteps      | 1720000      |
| train/                  |              |
|    approx_kl            | 0.0073031415 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 30.8         |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 3.46         |
|    value_loss           | 107          |
------------------------------------------
Eval num_timesteps=1722000, episode_reward=-72.72 +/- 48.80
Episode length: 451.20 +/- 87.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | -72.7        |
| time/                   |              |
|    total_timesteps      | 1722000      |
| train/                  |              |
|    approx_kl            | 0.0042412076 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 48.4         |
|    n_updates            | 8400         |
|    policy_gradient_loss | -0.00345     |
|    std                  | 3.47         |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=1724000, episode_reward=-51.83 +/- 21.31
Episode length: 452.80 +/- 32.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | -51.8        |
| time/                   |              |
|    total_timesteps      | 1724000      |
| train/                  |              |
|    approx_kl            | 0.0025786462 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 42.8         |
|    n_updates            | 8410         |
|    policy_gradient_loss | -0.000202    |
|    std                  | 3.49         |
|    value_loss           | 138          |
------------------------------------------
Eval num_timesteps=1726000, episode_reward=58.16 +/- 145.46
Episode length: 466.40 +/- 37.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 58.2        |
| time/                   |             |
|    total_timesteps      | 1726000     |
| train/                  |             |
|    approx_kl            | 0.014789832 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.001       |
|    loss                 | 28.2        |
|    n_updates            | 8420        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 3.49        |
|    value_loss           | 84.9        |
-----------------------------------------
Eval num_timesteps=1728000, episode_reward=-48.08 +/- 27.92
Episode length: 440.20 +/- 40.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | -48.1        |
| time/                   |              |
|    total_timesteps      | 1728000      |
| train/                  |              |
|    approx_kl            | 0.0044122213 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 51.4         |
|    n_updates            | 8430         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 3.47         |
|    value_loss           | 218          |
------------------------------------------
Eval num_timesteps=1730000, episode_reward=78.91 +/- 210.34
Episode length: 499.20 +/- 37.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 499          |
|    mean_reward          | 78.9         |
| time/                   |              |
|    total_timesteps      | 1730000      |
| train/                  |              |
|    approx_kl            | 0.0056713386 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 53.2         |
|    n_updates            | 8440         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 3.46         |
|    value_loss           | 179          |
------------------------------------------
Eval num_timesteps=1732000, episode_reward=-28.96 +/- 48.82
Episode length: 492.80 +/- 44.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 493          |
|    mean_reward          | -29          |
| time/                   |              |
|    total_timesteps      | 1732000      |
| train/                  |              |
|    approx_kl            | 0.0068641137 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 111          |
|    n_updates            | 8450         |
|    policy_gradient_loss | -0.000698    |
|    std                  | 3.46         |
|    value_loss           | 341          |
------------------------------------------
Eval num_timesteps=1734000, episode_reward=-5.71 +/- 53.84
Episode length: 483.80 +/- 46.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | -5.71        |
| time/                   |              |
|    total_timesteps      | 1734000      |
| train/                  |              |
|    approx_kl            | 0.0032208522 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 111          |
|    n_updates            | 8460         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 3.48         |
|    value_loss           | 495          |
------------------------------------------
Eval num_timesteps=1736000, episode_reward=-65.69 +/- 37.35
Episode length: 440.20 +/- 25.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | -65.7        |
| time/                   |              |
|    total_timesteps      | 1736000      |
| train/                  |              |
|    approx_kl            | 0.0032631038 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.5        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 75.8         |
|    n_updates            | 8470         |
|    policy_gradient_loss | -0.000746    |
|    std                  | 3.5          |
|    value_loss           | 266          |
------------------------------------------
Eval num_timesteps=1738000, episode_reward=-43.08 +/- 33.54
Episode length: 451.20 +/- 22.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | -43.1        |
| time/                   |              |
|    total_timesteps      | 1738000      |
| train/                  |              |
|    approx_kl            | 0.0069130333 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 35.2         |
|    n_updates            | 8480         |
|    policy_gradient_loss | -0.00326     |
|    std                  | 3.5          |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=1740000, episode_reward=-49.15 +/- 18.06
Episode length: 452.00 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | -49.2        |
| time/                   |              |
|    total_timesteps      | 1740000      |
| train/                  |              |
|    approx_kl            | 0.0071383533 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 33.6         |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 3.53         |
|    value_loss           | 83.5         |
------------------------------------------
Eval num_timesteps=1742000, episode_reward=-39.73 +/- 38.38
Episode length: 452.60 +/- 31.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | -39.7       |
| time/                   |             |
|    total_timesteps      | 1742000     |
| train/                  |             |
|    approx_kl            | 0.006275287 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.001       |
|    loss                 | 23          |
|    n_updates            | 8500        |
|    policy_gradient_loss | -0.00139    |
|    std                  | 3.54        |
|    value_loss           | 62.4        |
-----------------------------------------
Eval num_timesteps=1744000, episode_reward=-19.88 +/- 27.35
Episode length: 496.40 +/- 28.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -19.9        |
| time/                   |              |
|    total_timesteps      | 1744000      |
| train/                  |              |
|    approx_kl            | 0.0149549935 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 34.7         |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.00388     |
|    std                  | 3.55         |
|    value_loss           | 121          |
------------------------------------------
Eval num_timesteps=1746000, episode_reward=149.81 +/- 282.98
Episode length: 503.20 +/- 23.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 1746000     |
| train/                  |             |
|    approx_kl            | 0.008493479 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 26.7        |
|    n_updates            | 8520        |
|    policy_gradient_loss | -0.00278    |
|    std                  | 3.55        |
|    value_loss           | 80.4        |
-----------------------------------------
Eval num_timesteps=1748000, episode_reward=-30.89 +/- 59.84
Episode length: 461.60 +/- 56.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 462         |
|    mean_reward          | -30.9       |
| time/                   |             |
|    total_timesteps      | 1748000     |
| train/                  |             |
|    approx_kl            | 0.008232619 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.6       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 43          |
|    n_updates            | 8530        |
|    policy_gradient_loss | 0.00118     |
|    std                  | 3.55        |
|    value_loss           | 177         |
-----------------------------------------
Eval num_timesteps=1750000, episode_reward=27.96 +/- 85.04
Episode length: 475.60 +/- 33.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 28           |
| time/                   |              |
|    total_timesteps      | 1750000      |
| train/                  |              |
|    approx_kl            | 0.0077930116 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 45.6         |
|    n_updates            | 8540         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 3.57         |
|    value_loss           | 151          |
------------------------------------------
Eval num_timesteps=1752000, episode_reward=19.51 +/- 63.95
Episode length: 482.40 +/- 58.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 19.5         |
| time/                   |              |
|    total_timesteps      | 1752000      |
| train/                  |              |
|    approx_kl            | 0.0030383542 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+03     |
|    n_updates            | 8550         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 3.57         |
|    value_loss           | 5.97e+03     |
------------------------------------------
Eval num_timesteps=1754000, episode_reward=-0.15 +/- 53.30
Episode length: 480.80 +/- 52.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | -0.154       |
| time/                   |              |
|    total_timesteps      | 1754000      |
| train/                  |              |
|    approx_kl            | 0.0047899224 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.6        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 24.6         |
|    n_updates            | 8560         |
|    policy_gradient_loss | -0.000922    |
|    std                  | 3.59         |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=1756000, episode_reward=-5.63 +/- 39.60
Episode length: 488.20 +/- 13.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | -5.63        |
| time/                   |              |
|    total_timesteps      | 1756000      |
| train/                  |              |
|    approx_kl            | 0.0013315429 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 40           |
|    n_updates            | 8570         |
|    policy_gradient_loss | 0.000256     |
|    std                  | 3.6          |
|    value_loss           | 112          |
------------------------------------------
Eval num_timesteps=1758000, episode_reward=1.93 +/- 43.88
Episode length: 494.40 +/- 33.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 1758000     |
| train/                  |             |
|    approx_kl            | 0.010150015 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 25.8        |
|    n_updates            | 8580        |
|    policy_gradient_loss | -0.00448    |
|    std                  | 3.63        |
|    value_loss           | 82.5        |
-----------------------------------------
Eval num_timesteps=1760000, episode_reward=-38.59 +/- 30.89
Episode length: 461.60 +/- 34.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | -38.6        |
| time/                   |              |
|    total_timesteps      | 1760000      |
| train/                  |              |
|    approx_kl            | 0.0042406875 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.7        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 3.67         |
|    value_loss           | 79.2         |
------------------------------------------
Eval num_timesteps=1762000, episode_reward=-24.04 +/- 36.43
Episode length: 460.00 +/- 35.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | -24         |
| time/                   |             |
|    total_timesteps      | 1762000     |
| train/                  |             |
|    approx_kl            | 0.008469889 |
|    clip_fraction        | 0.0473      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 21.6        |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.00154    |
|    std                  | 3.69        |
|    value_loss           | 75.1        |
-----------------------------------------
Eval num_timesteps=1764000, episode_reward=82.21 +/- 72.20
Episode length: 496.20 +/- 28.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 82.2         |
| time/                   |              |
|    total_timesteps      | 1764000      |
| train/                  |              |
|    approx_kl            | 0.0066399043 |
|    clip_fraction        | 0.0817       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 27.7         |
|    n_updates            | 8610         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 3.7          |
|    value_loss           | 65.7         |
------------------------------------------
Eval num_timesteps=1766000, episode_reward=-10.54 +/- 37.39
Episode length: 474.20 +/- 43.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | -10.5        |
| time/                   |              |
|    total_timesteps      | 1766000      |
| train/                  |              |
|    approx_kl            | 0.0065081664 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 48           |
|    n_updates            | 8620         |
|    policy_gradient_loss | 0.000367     |
|    std                  | 3.7          |
|    value_loss           | 248          |
------------------------------------------
Eval num_timesteps=1768000, episode_reward=71.99 +/- 149.66
Episode length: 443.80 +/- 32.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 72           |
| time/                   |              |
|    total_timesteps      | 1768000      |
| train/                  |              |
|    approx_kl            | 0.0048836228 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 27.8         |
|    n_updates            | 8630         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 3.72         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=1770000, episode_reward=-32.56 +/- 29.42
Episode length: 449.40 +/- 50.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 449        |
|    mean_reward          | -32.6      |
| time/                   |            |
|    total_timesteps      | 1770000    |
| train/                  |            |
|    approx_kl            | 0.00833769 |
|    clip_fraction        | 0.0342     |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.001      |
|    loss                 | 31.4       |
|    n_updates            | 8640       |
|    policy_gradient_loss | -0.00181   |
|    std                  | 3.73       |
|    value_loss           | 103        |
----------------------------------------
Eval num_timesteps=1772000, episode_reward=21.76 +/- 60.21
Episode length: 477.60 +/- 41.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 21.8         |
| time/                   |              |
|    total_timesteps      | 1772000      |
| train/                  |              |
|    approx_kl            | 0.0010554967 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 36           |
|    n_updates            | 8650         |
|    policy_gradient_loss | -0.000587    |
|    std                  | 3.74         |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=1774000, episode_reward=32.30 +/- 88.74
Episode length: 459.00 +/- 59.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 32.3        |
| time/                   |             |
|    total_timesteps      | 1774000     |
| train/                  |             |
|    approx_kl            | 0.011636043 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.001       |
|    loss                 | 20.2        |
|    n_updates            | 8660        |
|    policy_gradient_loss | -0.00311    |
|    std                  | 3.74        |
|    value_loss           | 53.5        |
-----------------------------------------
Eval num_timesteps=1776000, episode_reward=123.16 +/- 202.94
Episode length: 488.40 +/- 49.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 123         |
| time/                   |             |
|    total_timesteps      | 1776000     |
| train/                  |             |
|    approx_kl            | 0.005029369 |
|    clip_fraction        | 0.00913     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 31.2        |
|    n_updates            | 8670        |
|    policy_gradient_loss | -0.00077    |
|    std                  | 3.74        |
|    value_loss           | 96.2        |
-----------------------------------------
Eval num_timesteps=1778000, episode_reward=36.75 +/- 64.50
Episode length: 454.80 +/- 40.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 36.8         |
| time/                   |              |
|    total_timesteps      | 1778000      |
| train/                  |              |
|    approx_kl            | 0.0037883325 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 25.6         |
|    n_updates            | 8680         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 3.75         |
|    value_loss           | 99.3         |
------------------------------------------
Eval num_timesteps=1780000, episode_reward=131.08 +/- 139.81
Episode length: 497.60 +/- 19.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 131          |
| time/                   |              |
|    total_timesteps      | 1780000      |
| train/                  |              |
|    approx_kl            | 0.0075964974 |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 15.7         |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 3.76         |
|    value_loss           | 57.1         |
------------------------------------------
Eval num_timesteps=1782000, episode_reward=75.09 +/- 164.74
Episode length: 516.60 +/- 29.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 75.1         |
| time/                   |              |
|    total_timesteps      | 1782000      |
| train/                  |              |
|    approx_kl            | 0.0013322503 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 13.9         |
|    n_updates            | 8700         |
|    policy_gradient_loss | -0.000278    |
|    std                  | 3.77         |
|    value_loss           | 59.2         |
------------------------------------------
Eval num_timesteps=1784000, episode_reward=95.19 +/- 140.48
Episode length: 471.40 +/- 37.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 95.2        |
| time/                   |             |
|    total_timesteps      | 1784000     |
| train/                  |             |
|    approx_kl            | 0.005742955 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 22.2        |
|    n_updates            | 8710        |
|    policy_gradient_loss | 0.000125    |
|    std                  | 3.78        |
|    value_loss           | 86.5        |
-----------------------------------------
Eval num_timesteps=1786000, episode_reward=460.12 +/- 253.13
Episode length: 501.40 +/- 13.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 460         |
| time/                   |             |
|    total_timesteps      | 1786000     |
| train/                  |             |
|    approx_kl            | 0.012566721 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.001       |
|    loss                 | 18.7        |
|    n_updates            | 8720        |
|    policy_gradient_loss | -0.00201    |
|    std                  | 3.79        |
|    value_loss           | 71.4        |
-----------------------------------------
Eval num_timesteps=1788000, episode_reward=367.69 +/- 222.29
Episode length: 509.00 +/- 26.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 368         |
| time/                   |             |
|    total_timesteps      | 1788000     |
| train/                  |             |
|    approx_kl            | 0.001565315 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.001       |
|    loss                 | 1.61e+03    |
|    n_updates            | 8730        |
|    policy_gradient_loss | 7.37e-05    |
|    std                  | 3.8         |
|    value_loss           | 4.77e+03    |
-----------------------------------------
Eval num_timesteps=1790000, episode_reward=411.71 +/- 446.31
Episode length: 485.60 +/- 52.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 412         |
| time/                   |             |
|    total_timesteps      | 1790000     |
| train/                  |             |
|    approx_kl            | 0.000745101 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.001       |
|    loss                 | 2.23e+03    |
|    n_updates            | 8740        |
|    policy_gradient_loss | -0.000743   |
|    std                  | 3.81        |
|    value_loss           | 5.53e+03    |
-----------------------------------------
Eval num_timesteps=1792000, episode_reward=234.61 +/- 159.44
Episode length: 470.00 +/- 69.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 1792000  |
---------------------------------
Eval num_timesteps=1794000, episode_reward=72.32 +/- 159.60
Episode length: 494.20 +/- 16.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 494           |
|    mean_reward          | 72.3          |
| time/                   |               |
|    total_timesteps      | 1794000       |
| train/                  |               |
|    approx_kl            | 0.00049106346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.8         |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.001         |
|    loss                 | 38.2          |
|    n_updates            | 8750          |
|    policy_gradient_loss | -0.000301     |
|    std                  | 3.81          |
|    value_loss           | 205           |
-------------------------------------------
Eval num_timesteps=1796000, episode_reward=190.38 +/- 187.97
Episode length: 497.20 +/- 8.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 1796000      |
| train/                  |              |
|    approx_kl            | 0.0071451743 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 55.4         |
|    n_updates            | 8760         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 3.8          |
|    value_loss           | 218          |
------------------------------------------
Eval num_timesteps=1798000, episode_reward=49.13 +/- 52.58
Episode length: 485.80 +/- 42.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 49.1         |
| time/                   |              |
|    total_timesteps      | 1798000      |
| train/                  |              |
|    approx_kl            | 0.0077344906 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.8        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 25           |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 3.82         |
|    value_loss           | 79.6         |
------------------------------------------
Eval num_timesteps=1800000, episode_reward=213.39 +/- 205.92
Episode length: 505.00 +/- 11.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 1800000     |
| train/                  |             |
|    approx_kl            | 0.009050005 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 20.3        |
|    n_updates            | 8780        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 3.83        |
|    value_loss           | 78.1        |
-----------------------------------------
Eval num_timesteps=1802000, episode_reward=132.89 +/- 51.55
Episode length: 481.00 +/- 40.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 1802000      |
| train/                  |              |
|    approx_kl            | 0.0006741348 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 57.1         |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.000137    |
|    std                  | 3.83         |
|    value_loss           | 330          |
------------------------------------------
Eval num_timesteps=1804000, episode_reward=106.90 +/- 202.63
Episode length: 449.40 +/- 51.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 107          |
| time/                   |              |
|    total_timesteps      | 1804000      |
| train/                  |              |
|    approx_kl            | 0.0029735216 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 85.5         |
|    n_updates            | 8800         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 3.84         |
|    value_loss           | 278          |
------------------------------------------
Eval num_timesteps=1806000, episode_reward=290.72 +/- 358.95
Episode length: 480.20 +/- 36.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 480       |
|    mean_reward          | 291       |
| time/                   |           |
|    total_timesteps      | 1806000   |
| train/                  |           |
|    approx_kl            | 0.0073366 |
|    clip_fraction        | 0.0211    |
|    clip_range           | 0.2       |
|    entropy_loss         | -10.9     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.001     |
|    loss                 | 24        |
|    n_updates            | 8810      |
|    policy_gradient_loss | -0.00249  |
|    std                  | 3.84      |
|    value_loss           | 85.2      |
---------------------------------------
Eval num_timesteps=1808000, episode_reward=317.31 +/- 265.99
Episode length: 504.40 +/- 10.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 1808000     |
| train/                  |             |
|    approx_kl            | 0.003438977 |
|    clip_fraction        | 0.00317     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 52.3        |
|    n_updates            | 8820        |
|    policy_gradient_loss | -0.000937   |
|    std                  | 3.85        |
|    value_loss           | 223         |
-----------------------------------------
Eval num_timesteps=1810000, episode_reward=221.83 +/- 170.30
Episode length: 467.80 +/- 33.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 1810000      |
| train/                  |              |
|    approx_kl            | 0.0029806118 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.001        |
|    loss                 | 2.41e+03     |
|    n_updates            | 8830         |
|    policy_gradient_loss | -0.000282    |
|    std                  | 3.86         |
|    value_loss           | 6.31e+03     |
------------------------------------------
Eval num_timesteps=1812000, episode_reward=148.69 +/- 140.00
Episode length: 480.40 +/- 22.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 480           |
|    mean_reward          | 149           |
| time/                   |               |
|    total_timesteps      | 1812000       |
| train/                  |               |
|    approx_kl            | 0.00031122286 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.9         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 99            |
|    n_updates            | 8840          |
|    policy_gradient_loss | -5.97e-05     |
|    std                  | 3.86          |
|    value_loss           | 438           |
-------------------------------------------
Eval num_timesteps=1814000, episode_reward=500.20 +/- 387.67
Episode length: 470.00 +/- 35.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | 500          |
| time/                   |              |
|    total_timesteps      | 1814000      |
| train/                  |              |
|    approx_kl            | 0.0003939896 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 68.8         |
|    n_updates            | 8850         |
|    policy_gradient_loss | -0.000308    |
|    std                  | 3.88         |
|    value_loss           | 309          |
------------------------------------------
Eval num_timesteps=1816000, episode_reward=188.67 +/- 158.10
Episode length: 440.40 +/- 53.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 1816000     |
| train/                  |             |
|    approx_kl            | 0.007008897 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 23          |
|    n_updates            | 8860        |
|    policy_gradient_loss | -0.00188    |
|    std                  | 3.87        |
|    value_loss           | 91.4        |
-----------------------------------------
Eval num_timesteps=1818000, episode_reward=287.55 +/- 404.80
Episode length: 471.60 +/- 49.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 288          |
| time/                   |              |
|    total_timesteps      | 1818000      |
| train/                  |              |
|    approx_kl            | 0.0016713804 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 25           |
|    n_updates            | 8870         |
|    policy_gradient_loss | -0.000927    |
|    std                  | 3.87         |
|    value_loss           | 76           |
------------------------------------------
Eval num_timesteps=1820000, episode_reward=234.93 +/- 439.11
Episode length: 443.60 +/- 25.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1820000      |
| train/                  |              |
|    approx_kl            | 0.0028870804 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.001        |
|    loss                 | 2.76e+03     |
|    n_updates            | 8880         |
|    policy_gradient_loss | -0.000773    |
|    std                  | 3.87         |
|    value_loss           | 6.67e+03     |
------------------------------------------
Eval num_timesteps=1822000, episode_reward=165.52 +/- 139.16
Episode length: 418.20 +/- 40.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 166          |
| time/                   |              |
|    total_timesteps      | 1822000      |
| train/                  |              |
|    approx_kl            | 0.0014824377 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 40           |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.000771    |
|    std                  | 3.87         |
|    value_loss           | 161          |
------------------------------------------
Eval num_timesteps=1824000, episode_reward=119.53 +/- 72.26
Episode length: 437.40 +/- 41.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 437        |
|    mean_reward          | 120        |
| time/                   |            |
|    total_timesteps      | 1824000    |
| train/                  |            |
|    approx_kl            | 0.00477247 |
|    clip_fraction        | 0.0109     |
|    clip_range           | 0.2        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.001      |
|    loss                 | 56.3       |
|    n_updates            | 8900       |
|    policy_gradient_loss | -0.00108   |
|    std                  | 3.88       |
|    value_loss           | 199        |
----------------------------------------
Eval num_timesteps=1826000, episode_reward=61.80 +/- 148.11
Episode length: 382.40 +/- 40.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 382          |
|    mean_reward          | 61.8         |
| time/                   |              |
|    total_timesteps      | 1826000      |
| train/                  |              |
|    approx_kl            | 0.0039714174 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 33.3         |
|    n_updates            | 8910         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 3.89         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=1828000, episode_reward=113.22 +/- 141.19
Episode length: 376.00 +/- 18.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 113          |
| time/                   |              |
|    total_timesteps      | 1828000      |
| train/                  |              |
|    approx_kl            | 0.0056062844 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 29           |
|    n_updates            | 8920         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 3.9          |
|    value_loss           | 102          |
------------------------------------------
Eval num_timesteps=1830000, episode_reward=156.89 +/- 454.75
Episode length: 406.20 +/- 34.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 157          |
| time/                   |              |
|    total_timesteps      | 1830000      |
| train/                  |              |
|    approx_kl            | 0.0030279737 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 25.1         |
|    n_updates            | 8930         |
|    policy_gradient_loss | 3.17e-05     |
|    std                  | 3.9          |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=1832000, episode_reward=160.89 +/- 84.39
Episode length: 438.40 +/- 29.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 1832000      |
| train/                  |              |
|    approx_kl            | 0.0017232916 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.737        |
|    learning_rate        | 0.001        |
|    loss                 | 2.21e+03     |
|    n_updates            | 8940         |
|    policy_gradient_loss | -0.00041     |
|    std                  | 3.9          |
|    value_loss           | 5.21e+03     |
------------------------------------------
Eval num_timesteps=1834000, episode_reward=208.17 +/- 327.04
Episode length: 446.00 +/- 68.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 208          |
| time/                   |              |
|    total_timesteps      | 1834000      |
| train/                  |              |
|    approx_kl            | 0.0027865637 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.001        |
|    loss                 | 19.7         |
|    n_updates            | 8950         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 3.9          |
|    value_loss           | 77.2         |
------------------------------------------
Eval num_timesteps=1836000, episode_reward=260.30 +/- 107.94
Episode length: 461.40 +/- 53.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 1836000     |
| train/                  |             |
|    approx_kl            | 0.002032712 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 8960        |
|    policy_gradient_loss | 0.000136    |
|    std                  | 3.9         |
|    value_loss           | 714         |
-----------------------------------------
Eval num_timesteps=1838000, episode_reward=143.87 +/- 80.47
Episode length: 455.20 +/- 35.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 144           |
| time/                   |               |
|    total_timesteps      | 1838000       |
| train/                  |               |
|    approx_kl            | 0.00033461544 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -10.9         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+03      |
|    n_updates            | 8970          |
|    policy_gradient_loss | -0.000193     |
|    std                  | 3.9           |
|    value_loss           | 4.38e+03      |
-------------------------------------------
Eval num_timesteps=1840000, episode_reward=147.09 +/- 327.62
Episode length: 458.60 +/- 34.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 147          |
| time/                   |              |
|    total_timesteps      | 1840000      |
| train/                  |              |
|    approx_kl            | 0.0016570947 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 172          |
|    n_updates            | 8980         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 3.89         |
|    value_loss           | 976          |
------------------------------------------
Eval num_timesteps=1842000, episode_reward=117.85 +/- 194.39
Episode length: 442.40 +/- 36.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 118          |
| time/                   |              |
|    total_timesteps      | 1842000      |
| train/                  |              |
|    approx_kl            | 0.0057485383 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -10.9        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 48.2         |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 3.9          |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=1844000, episode_reward=270.56 +/- 384.82
Episode length: 477.40 +/- 41.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 1844000     |
| train/                  |             |
|    approx_kl            | 0.005158493 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 12.5        |
|    n_updates            | 9000        |
|    policy_gradient_loss | -0.00149    |
|    std                  | 3.9         |
|    value_loss           | 52.7        |
-----------------------------------------
Eval num_timesteps=1846000, episode_reward=-25.75 +/- 185.77
Episode length: 490.60 +/- 58.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | -25.8       |
| time/                   |             |
|    total_timesteps      | 1846000     |
| train/                  |             |
|    approx_kl            | 0.007433946 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 42.1        |
|    n_updates            | 9010        |
|    policy_gradient_loss | -0.00122    |
|    std                  | 3.91        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=1848000, episode_reward=188.76 +/- 128.60
Episode length: 505.60 +/- 22.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 189          |
| time/                   |              |
|    total_timesteps      | 1848000      |
| train/                  |              |
|    approx_kl            | 0.0059834425 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 89           |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 3.92         |
|    value_loss           | 321          |
------------------------------------------
Eval num_timesteps=1850000, episode_reward=286.73 +/- 189.27
Episode length: 503.20 +/- 23.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.005205868 |
|    clip_fraction        | 0.00972     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 75.1        |
|    n_updates            | 9030        |
|    policy_gradient_loss | -0.00111    |
|    std                  | 3.93        |
|    value_loss           | 261         |
-----------------------------------------
Eval num_timesteps=1852000, episode_reward=202.65 +/- 116.39
Episode length: 506.40 +/- 22.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 1852000     |
| train/                  |             |
|    approx_kl            | 0.003961578 |
|    clip_fraction        | 0.00562     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 63.3        |
|    n_updates            | 9040        |
|    policy_gradient_loss | -0.000578   |
|    std                  | 3.94        |
|    value_loss           | 282         |
-----------------------------------------
Eval num_timesteps=1854000, episode_reward=167.27 +/- 111.57
Episode length: 493.00 +/- 27.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 493          |
|    mean_reward          | 167          |
| time/                   |              |
|    total_timesteps      | 1854000      |
| train/                  |              |
|    approx_kl            | 0.0010215654 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 32.2         |
|    n_updates            | 9050         |
|    policy_gradient_loss | -0.00072     |
|    std                  | 3.94         |
|    value_loss           | 89.4         |
------------------------------------------
Eval num_timesteps=1856000, episode_reward=63.05 +/- 53.71
Episode length: 417.00 +/- 34.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 417         |
|    mean_reward          | 63          |
| time/                   |             |
|    total_timesteps      | 1856000     |
| train/                  |             |
|    approx_kl            | 0.004594222 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 20.7        |
|    n_updates            | 9060        |
|    policy_gradient_loss | -0.000876   |
|    std                  | 3.95        |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=1858000, episode_reward=257.95 +/- 246.26
Episode length: 474.00 +/- 38.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 258          |
| time/                   |              |
|    total_timesteps      | 1858000      |
| train/                  |              |
|    approx_kl            | 0.0021933648 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 48.1         |
|    n_updates            | 9070         |
|    policy_gradient_loss | -0.000594    |
|    std                  | 3.97         |
|    value_loss           | 206          |
------------------------------------------
Eval num_timesteps=1860000, episode_reward=86.90 +/- 40.31
Episode length: 452.60 +/- 44.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | 86.9        |
| time/                   |             |
|    total_timesteps      | 1860000     |
| train/                  |             |
|    approx_kl            | 0.002733931 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.001       |
|    loss                 | 1.82e+03    |
|    n_updates            | 9080        |
|    policy_gradient_loss | 0.000246    |
|    std                  | 3.97        |
|    value_loss           | 4.87e+03    |
-----------------------------------------
Eval num_timesteps=1862000, episode_reward=65.67 +/- 98.79
Episode length: 442.40 +/- 47.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 65.7         |
| time/                   |              |
|    total_timesteps      | 1862000      |
| train/                  |              |
|    approx_kl            | 0.0025584372 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 19.1         |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.000787    |
|    std                  | 3.97         |
|    value_loss           | 84.8         |
------------------------------------------
Eval num_timesteps=1864000, episode_reward=279.71 +/- 143.59
Episode length: 522.80 +/- 113.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 523          |
|    mean_reward          | 280          |
| time/                   |              |
|    total_timesteps      | 1864000      |
| train/                  |              |
|    approx_kl            | 0.0019895777 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 39.6         |
|    n_updates            | 9100         |
|    policy_gradient_loss | 0.000442     |
|    std                  | 3.97         |
|    value_loss           | 149          |
------------------------------------------
Eval num_timesteps=1866000, episode_reward=237.98 +/- 155.19
Episode length: 487.80 +/- 54.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 238          |
| time/                   |              |
|    total_timesteps      | 1866000      |
| train/                  |              |
|    approx_kl            | 0.0036460073 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 9110         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 3.97         |
|    value_loss           | 466          |
------------------------------------------
Eval num_timesteps=1868000, episode_reward=130.14 +/- 100.90
Episode length: 492.60 +/- 29.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 493         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 1868000     |
| train/                  |             |
|    approx_kl            | 0.005537878 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 158         |
|    n_updates            | 9120        |
|    policy_gradient_loss | -0.00112    |
|    std                  | 3.98        |
|    value_loss           | 515         |
-----------------------------------------
Eval num_timesteps=1870000, episode_reward=644.28 +/- 359.30
Episode length: 528.60 +/- 18.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | 644          |
| time/                   |              |
|    total_timesteps      | 1870000      |
| train/                  |              |
|    approx_kl            | 0.0061436207 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 26.4         |
|    n_updates            | 9130         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 3.99         |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=1872000, episode_reward=-62.20 +/- 200.72
Episode length: 504.00 +/- 18.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -62.2        |
| time/                   |              |
|    total_timesteps      | 1872000      |
| train/                  |              |
|    approx_kl            | 0.0066897464 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11          |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 18.4         |
|    n_updates            | 9140         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 3.99         |
|    value_loss           | 69           |
------------------------------------------
Eval num_timesteps=1874000, episode_reward=294.34 +/- 151.99
Episode length: 450.00 +/- 43.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 1874000     |
| train/                  |             |
|    approx_kl            | 0.006506916 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 32.5        |
|    n_updates            | 9150        |
|    policy_gradient_loss | -0.00159    |
|    std                  | 4.01        |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=1876000, episode_reward=261.83 +/- 155.11
Episode length: 437.20 +/- 44.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 262          |
| time/                   |              |
|    total_timesteps      | 1876000      |
| train/                  |              |
|    approx_kl            | 0.0039562588 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.001        |
|    loss                 | 3.95e+03     |
|    n_updates            | 9160         |
|    policy_gradient_loss | -0.000946    |
|    std                  | 4.02         |
|    value_loss           | 9.93e+03     |
------------------------------------------
Eval num_timesteps=1878000, episode_reward=310.96 +/- 435.35
Episode length: 465.80 +/- 22.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 1878000  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=574.23 +/- 316.27
Episode length: 480.60 +/- 48.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 574          |
| time/                   |              |
|    total_timesteps      | 1880000      |
| train/                  |              |
|    approx_kl            | 0.0016397233 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 85.7         |
|    n_updates            | 9170         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 4.03         |
|    value_loss           | 242          |
------------------------------------------
Eval num_timesteps=1882000, episode_reward=233.35 +/- 169.30
Episode length: 501.40 +/- 116.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 1882000     |
| train/                  |             |
|    approx_kl            | 0.004713066 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.001       |
|    loss                 | 1.65e+03    |
|    n_updates            | 9180        |
|    policy_gradient_loss | 0.00105     |
|    std                  | 4.03        |
|    value_loss           | 4.59e+03    |
-----------------------------------------
Eval num_timesteps=1884000, episode_reward=185.00 +/- 358.82
Episode length: 400.60 +/- 40.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 1884000     |
| train/                  |             |
|    approx_kl            | 0.008213092 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 47.5        |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.00223    |
|    std                  | 4.04        |
|    value_loss           | 198         |
-----------------------------------------
Eval num_timesteps=1886000, episode_reward=431.90 +/- 460.31
Episode length: 487.00 +/- 26.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 1886000      |
| train/                  |              |
|    approx_kl            | 0.0016279265 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.807        |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+03     |
|    n_updates            | 9200         |
|    policy_gradient_loss | -0.000265    |
|    std                  | 4.05         |
|    value_loss           | 5.91e+03     |
------------------------------------------
Eval num_timesteps=1888000, episode_reward=143.64 +/- 436.69
Episode length: 467.60 +/- 29.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 144          |
| time/                   |              |
|    total_timesteps      | 1888000      |
| train/                  |              |
|    approx_kl            | 0.0002290828 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 148          |
|    n_updates            | 9210         |
|    policy_gradient_loss | -0.000234    |
|    std                  | 4.05         |
|    value_loss           | 570          |
------------------------------------------
Eval num_timesteps=1890000, episode_reward=78.29 +/- 448.72
Episode length: 445.80 +/- 54.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 78.3         |
| time/                   |              |
|    total_timesteps      | 1890000      |
| train/                  |              |
|    approx_kl            | 6.591255e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 2.29e+03     |
|    n_updates            | 9220         |
|    policy_gradient_loss | -0.000128    |
|    std                  | 4.05         |
|    value_loss           | 5.16e+03     |
------------------------------------------
Eval num_timesteps=1892000, episode_reward=435.25 +/- 155.65
Episode length: 439.60 +/- 36.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 435          |
| time/                   |              |
|    total_timesteps      | 1892000      |
| train/                  |              |
|    approx_kl            | 0.0003733792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 9230         |
|    policy_gradient_loss | -0.000798    |
|    std                  | 4.06         |
|    value_loss           | 2.6e+03      |
------------------------------------------
Eval num_timesteps=1894000, episode_reward=451.77 +/- 377.48
Episode length: 434.00 +/- 47.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 1894000       |
| train/                  |               |
|    approx_kl            | 2.6857771e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.837         |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+03      |
|    n_updates            | 9240          |
|    policy_gradient_loss | 0.000177      |
|    std                  | 4.06          |
|    value_loss           | 6.69e+03      |
-------------------------------------------
Eval num_timesteps=1896000, episode_reward=213.13 +/- 312.94
Episode length: 461.40 +/- 47.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 213          |
| time/                   |              |
|    total_timesteps      | 1896000      |
| train/                  |              |
|    approx_kl            | 0.0019015608 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 78.6         |
|    n_updates            | 9250         |
|    policy_gradient_loss | -0.000999    |
|    std                  | 4.06         |
|    value_loss           | 375          |
------------------------------------------
Eval num_timesteps=1898000, episode_reward=418.96 +/- 310.57
Episode length: 501.80 +/- 24.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 502          |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 1898000      |
| train/                  |              |
|    approx_kl            | 0.0027330902 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 145          |
|    n_updates            | 9260         |
|    policy_gradient_loss | -0.000222    |
|    std                  | 4.07         |
|    value_loss           | 561          |
------------------------------------------
Eval num_timesteps=1900000, episode_reward=235.47 +/- 106.89
Episode length: 473.80 +/- 41.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1900000      |
| train/                  |              |
|    approx_kl            | 0.0010173295 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 191          |
|    n_updates            | 9270         |
|    policy_gradient_loss | -0.000546    |
|    std                  | 4.08         |
|    value_loss           | 546          |
------------------------------------------
Eval num_timesteps=1902000, episode_reward=431.54 +/- 318.05
Episode length: 453.40 +/- 38.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 432           |
| time/                   |               |
|    total_timesteps      | 1902000       |
| train/                  |               |
|    approx_kl            | 0.00037130772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.737         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 9280          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 4.09          |
|    value_loss           | 7.61e+03      |
-------------------------------------------
Eval num_timesteps=1904000, episode_reward=401.63 +/- 270.82
Episode length: 443.80 +/- 28.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 402           |
| time/                   |               |
|    total_timesteps      | 1904000       |
| train/                  |               |
|    approx_kl            | 0.00021541008 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 146           |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000214     |
|    std                  | 4.09          |
|    value_loss           | 635           |
-------------------------------------------
Eval num_timesteps=1906000, episode_reward=78.00 +/- 157.12
Episode length: 437.60 +/- 61.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 78            |
| time/                   |               |
|    total_timesteps      | 1906000       |
| train/                  |               |
|    approx_kl            | 0.00024934098 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.001         |
|    loss                 | 3.1e+03       |
|    n_updates            | 9300          |
|    policy_gradient_loss | -2.56e-05     |
|    std                  | 4.1           |
|    value_loss           | 6.78e+03      |
-------------------------------------------
Eval num_timesteps=1908000, episode_reward=468.52 +/- 355.31
Episode length: 426.00 +/- 42.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 426          |
|    mean_reward          | 469          |
| time/                   |              |
|    total_timesteps      | 1908000      |
| train/                  |              |
|    approx_kl            | 7.305542e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 5.96e+03     |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.000224    |
|    std                  | 4.1          |
|    value_loss           | 1.41e+04     |
------------------------------------------
Eval num_timesteps=1910000, episode_reward=485.07 +/- 236.09
Episode length: 447.20 +/- 50.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 485           |
| time/                   |               |
|    total_timesteps      | 1910000       |
| train/                  |               |
|    approx_kl            | 0.00011973927 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.852         |
|    learning_rate        | 0.001         |
|    loss                 | 3.12e+03      |
|    n_updates            | 9320          |
|    policy_gradient_loss | -0.00015      |
|    std                  | 4.1           |
|    value_loss           | 6.53e+03      |
-------------------------------------------
Eval num_timesteps=1912000, episode_reward=388.18 +/- 279.49
Episode length: 442.80 +/- 59.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 1912000      |
| train/                  |              |
|    approx_kl            | 0.0003852335 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+03     |
|    n_updates            | 9330         |
|    policy_gradient_loss | -0.000725    |
|    std                  | 4.1          |
|    value_loss           | 3.7e+03      |
------------------------------------------
Eval num_timesteps=1914000, episode_reward=596.27 +/- 411.22
Episode length: 434.00 +/- 41.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 596           |
| time/                   |               |
|    total_timesteps      | 1914000       |
| train/                  |               |
|    approx_kl            | 0.00069195486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.848         |
|    learning_rate        | 0.001         |
|    loss                 | 3.62e+03      |
|    n_updates            | 9340          |
|    policy_gradient_loss | -0.000966     |
|    std                  | 4.1           |
|    value_loss           | 8.19e+03      |
-------------------------------------------
Eval num_timesteps=1916000, episode_reward=184.21 +/- 477.66
Episode length: 469.00 +/- 48.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 1916000      |
| train/                  |              |
|    approx_kl            | 0.0002631324 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+03     |
|    n_updates            | 9350         |
|    policy_gradient_loss | 0.000135     |
|    std                  | 4.1          |
|    value_loss           | 6.86e+03     |
------------------------------------------
Eval num_timesteps=1918000, episode_reward=-55.56 +/- 261.14
Episode length: 441.40 +/- 39.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | -55.6         |
| time/                   |               |
|    total_timesteps      | 1918000       |
| train/                  |               |
|    approx_kl            | 0.00032864278 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 2.43e+03      |
|    n_updates            | 9360          |
|    policy_gradient_loss | -0.000479     |
|    std                  | 4.1           |
|    value_loss           | 5.39e+03      |
-------------------------------------------
Eval num_timesteps=1920000, episode_reward=439.70 +/- 452.78
Episode length: 455.20 +/- 49.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 440           |
| time/                   |               |
|    total_timesteps      | 1920000       |
| train/                  |               |
|    approx_kl            | 5.3506898e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.1         |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.001         |
|    loss                 | 776           |
|    n_updates            | 9370          |
|    policy_gradient_loss | 0.000245      |
|    std                  | 4.1           |
|    value_loss           | 1.88e+03      |
-------------------------------------------
Eval num_timesteps=1922000, episode_reward=289.84 +/- 115.81
Episode length: 437.00 +/- 41.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 290          |
| time/                   |              |
|    total_timesteps      | 1922000      |
| train/                  |              |
|    approx_kl            | 0.0046587544 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 56.9         |
|    n_updates            | 9380         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 4.1          |
|    value_loss           | 200          |
------------------------------------------
Eval num_timesteps=1924000, episode_reward=275.22 +/- 477.90
Episode length: 452.20 +/- 52.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 452        |
|    mean_reward          | 275        |
| time/                   |            |
|    total_timesteps      | 1924000    |
| train/                  |            |
|    approx_kl            | 0.00205947 |
|    clip_fraction        | 4.88e-05   |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.001      |
|    loss                 | 1.97e+03   |
|    n_updates            | 9390       |
|    policy_gradient_loss | 0.00118    |
|    std                  | 4.1        |
|    value_loss           | 4.93e+03   |
----------------------------------------
Eval num_timesteps=1926000, episode_reward=270.21 +/- 313.16
Episode length: 466.80 +/- 41.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 1926000      |
| train/                  |              |
|    approx_kl            | 0.0016336781 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 150          |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 4.1          |
|    value_loss           | 803          |
------------------------------------------
Eval num_timesteps=1928000, episode_reward=438.09 +/- 284.46
Episode length: 447.60 +/- 50.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 438          |
| time/                   |              |
|    total_timesteps      | 1928000      |
| train/                  |              |
|    approx_kl            | 0.0010734015 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 86.1         |
|    n_updates            | 9410         |
|    policy_gradient_loss | 0.000578     |
|    std                  | 4.11         |
|    value_loss           | 490          |
------------------------------------------
Eval num_timesteps=1930000, episode_reward=345.57 +/- 259.96
Episode length: 464.40 +/- 50.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 346          |
| time/                   |              |
|    total_timesteps      | 1930000      |
| train/                  |              |
|    approx_kl            | 0.0013523346 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.1        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 82.1         |
|    n_updates            | 9420         |
|    policy_gradient_loss | -0.000855    |
|    std                  | 4.12         |
|    value_loss           | 330          |
------------------------------------------
Eval num_timesteps=1932000, episode_reward=294.29 +/- 127.77
Episode length: 507.20 +/- 8.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 1932000     |
| train/                  |             |
|    approx_kl            | 0.002355196 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 48          |
|    n_updates            | 9430        |
|    policy_gradient_loss | -0.000736   |
|    std                  | 4.13        |
|    value_loss           | 192         |
-----------------------------------------
Eval num_timesteps=1934000, episode_reward=414.40 +/- 195.62
Episode length: 484.60 +/- 49.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 485          |
|    mean_reward          | 414          |
| time/                   |              |
|    total_timesteps      | 1934000      |
| train/                  |              |
|    approx_kl            | 0.0006861546 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 103          |
|    n_updates            | 9440         |
|    policy_gradient_loss | 1.2e-05      |
|    std                  | 4.15         |
|    value_loss           | 478          |
------------------------------------------
Eval num_timesteps=1936000, episode_reward=111.87 +/- 384.68
Episode length: 481.60 +/- 26.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 482          |
|    mean_reward          | 112          |
| time/                   |              |
|    total_timesteps      | 1936000      |
| train/                  |              |
|    approx_kl            | 0.0005629192 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 96.9         |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.000433    |
|    std                  | 4.16         |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=1938000, episode_reward=588.23 +/- 80.08
Episode length: 455.20 +/- 63.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 588          |
| time/                   |              |
|    total_timesteps      | 1938000      |
| train/                  |              |
|    approx_kl            | 0.0003148665 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 41.4         |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.000237    |
|    std                  | 4.17         |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=1940000, episode_reward=352.50 +/- 121.08
Episode length: 461.20 +/- 60.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 353          |
| time/                   |              |
|    total_timesteps      | 1940000      |
| train/                  |              |
|    approx_kl            | 0.0028623363 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 9470         |
|    policy_gradient_loss | -0.000397    |
|    std                  | 4.17         |
|    value_loss           | 2.81e+03     |
------------------------------------------
Eval num_timesteps=1942000, episode_reward=357.25 +/- 166.66
Episode length: 508.20 +/- 38.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 357           |
| time/                   |               |
|    total_timesteps      | 1942000       |
| train/                  |               |
|    approx_kl            | 0.00056662585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+03      |
|    n_updates            | 9480          |
|    policy_gradient_loss | -0.000224     |
|    std                  | 4.17          |
|    value_loss           | 3.87e+03      |
-------------------------------------------
Eval num_timesteps=1944000, episode_reward=620.07 +/- 163.99
Episode length: 467.80 +/- 51.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 620          |
| time/                   |              |
|    total_timesteps      | 1944000      |
| train/                  |              |
|    approx_kl            | 0.0003814627 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 764          |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.000445    |
|    std                  | 4.17         |
|    value_loss           | 2.04e+03     |
------------------------------------------
Eval num_timesteps=1946000, episode_reward=594.02 +/- 245.13
Episode length: 466.40 +/- 44.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 594          |
| time/                   |              |
|    total_timesteps      | 1946000      |
| train/                  |              |
|    approx_kl            | 0.0007123082 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 329          |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.000506    |
|    std                  | 4.18         |
|    value_loss           | 919          |
------------------------------------------
Eval num_timesteps=1948000, episode_reward=269.30 +/- 375.03
Episode length: 465.40 +/- 37.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 1948000      |
| train/                  |              |
|    approx_kl            | 0.0013953233 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+03     |
|    n_updates            | 9510         |
|    policy_gradient_loss | -0.000883    |
|    std                  | 4.19         |
|    value_loss           | 3.32e+03     |
------------------------------------------
Eval num_timesteps=1950000, episode_reward=395.47 +/- 374.98
Episode length: 422.80 +/- 14.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 1950000      |
| train/                  |              |
|    approx_kl            | 0.0014915308 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.001        |
|    loss                 | 2.54e+03     |
|    n_updates            | 9520         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 4.19         |
|    value_loss           | 5.71e+03     |
------------------------------------------
Eval num_timesteps=1952000, episode_reward=63.21 +/- 209.64
Episode length: 443.00 +/- 42.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 63.2          |
| time/                   |               |
|    total_timesteps      | 1952000       |
| train/                  |               |
|    approx_kl            | 0.00034611375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+03      |
|    n_updates            | 9530          |
|    policy_gradient_loss | -0.000155     |
|    std                  | 4.19          |
|    value_loss           | 3.06e+03      |
-------------------------------------------
Eval num_timesteps=1954000, episode_reward=458.50 +/- 296.23
Episode length: 490.60 +/- 25.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 491          |
|    mean_reward          | 459          |
| time/                   |              |
|    total_timesteps      | 1954000      |
| train/                  |              |
|    approx_kl            | 9.134805e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 3.26e+03     |
|    n_updates            | 9540         |
|    policy_gradient_loss | -1.1e-05     |
|    std                  | 4.19         |
|    value_loss           | 7.15e+03     |
------------------------------------------
Eval num_timesteps=1956000, episode_reward=464.93 +/- 407.48
Episode length: 446.60 +/- 44.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 1956000      |
| train/                  |              |
|    approx_kl            | 0.0023442716 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 204          |
|    n_updates            | 9550         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 4.19         |
|    value_loss           | 575          |
------------------------------------------
Eval num_timesteps=1958000, episode_reward=595.90 +/- 334.57
Episode length: 472.80 +/- 28.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 596          |
| time/                   |              |
|    total_timesteps      | 1958000      |
| train/                  |              |
|    approx_kl            | 0.0033225217 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 9560         |
|    policy_gradient_loss | 0.00105      |
|    std                  | 4.2          |
|    value_loss           | 3.77e+03     |
------------------------------------------
Eval num_timesteps=1960000, episode_reward=208.54 +/- 151.01
Episode length: 458.20 +/- 48.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 209           |
| time/                   |               |
|    total_timesteps      | 1960000       |
| train/                  |               |
|    approx_kl            | 3.9992767e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+03      |
|    n_updates            | 9570          |
|    policy_gradient_loss | 9.29e-05      |
|    std                  | 4.2           |
|    value_loss           | 3.34e+03      |
-------------------------------------------
Eval num_timesteps=1962000, episode_reward=298.14 +/- 158.94
Episode length: 491.60 +/- 37.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 298          |
| time/                   |              |
|    total_timesteps      | 1962000      |
| train/                  |              |
|    approx_kl            | 0.0001009081 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+03     |
|    n_updates            | 9580         |
|    policy_gradient_loss | 1.76e-05     |
|    std                  | 4.2          |
|    value_loss           | 3.51e+03     |
------------------------------------------
Eval num_timesteps=1964000, episode_reward=277.63 +/- 61.14
Episode length: 492.60 +/- 33.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 1964000  |
---------------------------------
Eval num_timesteps=1966000, episode_reward=313.97 +/- 434.24
Episode length: 425.80 +/- 23.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 314           |
| time/                   |               |
|    total_timesteps      | 1966000       |
| train/                  |               |
|    approx_kl            | 4.6697533e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+03      |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000143     |
|    std                  | 4.2           |
|    value_loss           | 3.09e+03      |
-------------------------------------------
Eval num_timesteps=1968000, episode_reward=235.49 +/- 135.39
Episode length: 451.80 +/- 78.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 235          |
| time/                   |              |
|    total_timesteps      | 1968000      |
| train/                  |              |
|    approx_kl            | 0.0014302658 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 46.7         |
|    n_updates            | 9600         |
|    policy_gradient_loss | -0.000777    |
|    std                  | 4.2          |
|    value_loss           | 154          |
------------------------------------------
Eval num_timesteps=1970000, episode_reward=510.82 +/- 323.87
Episode length: 450.00 +/- 38.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 511         |
| time/                   |             |
|    total_timesteps      | 1970000     |
| train/                  |             |
|    approx_kl            | 0.007733415 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 88.5        |
|    n_updates            | 9610        |
|    policy_gradient_loss | -0.00249    |
|    std                  | 4.2         |
|    value_loss           | 425         |
-----------------------------------------
Eval num_timesteps=1972000, episode_reward=305.00 +/- 597.65
Episode length: 481.20 +/- 51.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 305          |
| time/                   |              |
|    total_timesteps      | 1972000      |
| train/                  |              |
|    approx_kl            | 0.0004362465 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+03     |
|    n_updates            | 9620         |
|    policy_gradient_loss | 0.000425     |
|    std                  | 4.2          |
|    value_loss           | 6.61e+03     |
------------------------------------------
Eval num_timesteps=1974000, episode_reward=433.79 +/- 480.80
Episode length: 431.80 +/- 29.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 432           |
|    mean_reward          | 434           |
| time/                   |               |
|    total_timesteps      | 1974000       |
| train/                  |               |
|    approx_kl            | 7.6920114e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+03      |
|    n_updates            | 9630          |
|    policy_gradient_loss | -9.44e-05     |
|    std                  | 4.2           |
|    value_loss           | 3.91e+03      |
-------------------------------------------
Eval num_timesteps=1976000, episode_reward=345.79 +/- 493.03
Episode length: 442.60 +/- 23.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 346          |
| time/                   |              |
|    total_timesteps      | 1976000      |
| train/                  |              |
|    approx_kl            | 0.0001345985 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 2.58e+03     |
|    n_updates            | 9640         |
|    policy_gradient_loss | -0.000441    |
|    std                  | 4.2          |
|    value_loss           | 6.39e+03     |
------------------------------------------
Eval num_timesteps=1978000, episode_reward=488.58 +/- 210.63
Episode length: 473.80 +/- 45.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 489          |
| time/                   |              |
|    total_timesteps      | 1978000      |
| train/                  |              |
|    approx_kl            | 9.703651e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 9650         |
|    policy_gradient_loss | 4.51e-05     |
|    std                  | 4.2          |
|    value_loss           | 3.35e+03     |
------------------------------------------
Eval num_timesteps=1980000, episode_reward=489.83 +/- 289.05
Episode length: 423.00 +/- 44.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 490          |
| time/                   |              |
|    total_timesteps      | 1980000      |
| train/                  |              |
|    approx_kl            | 0.0017235297 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 285          |
|    n_updates            | 9660         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 4.2          |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=1982000, episode_reward=37.43 +/- 169.74
Episode length: 405.20 +/- 21.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 37.4         |
| time/                   |              |
|    total_timesteps      | 1982000      |
| train/                  |              |
|    approx_kl            | 0.0024415888 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 48.4         |
|    n_updates            | 9670         |
|    policy_gradient_loss | -0.00036     |
|    std                  | 4.21         |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=1984000, episode_reward=101.97 +/- 255.97
Episode length: 409.00 +/- 28.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 102          |
| time/                   |              |
|    total_timesteps      | 1984000      |
| train/                  |              |
|    approx_kl            | 0.0075413426 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 44.6         |
|    n_updates            | 9680         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 4.21         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=1986000, episode_reward=312.96 +/- 393.44
Episode length: 426.80 +/- 35.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 313          |
| time/                   |              |
|    total_timesteps      | 1986000      |
| train/                  |              |
|    approx_kl            | 0.0026272123 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+03     |
|    n_updates            | 9690         |
|    policy_gradient_loss | 5.53e-05     |
|    std                  | 4.21         |
|    value_loss           | 2.76e+03     |
------------------------------------------
Eval num_timesteps=1988000, episode_reward=146.04 +/- 188.98
Episode length: 389.80 +/- 30.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 390         |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 1988000     |
| train/                  |             |
|    approx_kl            | 0.000245428 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 1.57e+03    |
|    n_updates            | 9700        |
|    policy_gradient_loss | 0.000167    |
|    std                  | 4.21        |
|    value_loss           | 3.37e+03    |
-----------------------------------------
Eval num_timesteps=1990000, episode_reward=578.13 +/- 185.48
Episode length: 389.60 +/- 25.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 578          |
| time/                   |              |
|    total_timesteps      | 1990000      |
| train/                  |              |
|    approx_kl            | 9.948888e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.001        |
|    loss                 | 1.82e+03     |
|    n_updates            | 9710         |
|    policy_gradient_loss | -0.000357    |
|    std                  | 4.21         |
|    value_loss           | 4.2e+03      |
------------------------------------------
Eval num_timesteps=1992000, episode_reward=159.61 +/- 164.34
Episode length: 405.60 +/- 60.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 160          |
| time/                   |              |
|    total_timesteps      | 1992000      |
| train/                  |              |
|    approx_kl            | 8.158997e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+03      |
|    n_updates            | 9720         |
|    policy_gradient_loss | 4.47e-05     |
|    std                  | 4.21         |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=1994000, episode_reward=396.64 +/- 357.99
Episode length: 410.40 +/- 51.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 397          |
| time/                   |              |
|    total_timesteps      | 1994000      |
| train/                  |              |
|    approx_kl            | 1.822994e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+03     |
|    n_updates            | 9730         |
|    policy_gradient_loss | -6.5e-05     |
|    std                  | 4.21         |
|    value_loss           | 5.36e+03     |
------------------------------------------
Eval num_timesteps=1996000, episode_reward=160.13 +/- 147.37
Episode length: 413.80 +/- 51.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 160          |
| time/                   |              |
|    total_timesteps      | 1996000      |
| train/                  |              |
|    approx_kl            | 0.0005831289 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 238          |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.000701    |
|    std                  | 4.21         |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=1998000, episode_reward=263.96 +/- 257.40
Episode length: 401.00 +/- 41.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 1998000     |
| train/                  |             |
|    approx_kl            | 0.004596156 |
|    clip_fraction        | 0.00503     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 45.2        |
|    n_updates            | 9750        |
|    policy_gradient_loss | -0.00183    |
|    std                  | 4.2         |
|    value_loss           | 266         |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=-39.21 +/- 101.03
Episode length: 371.40 +/- 34.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 371          |
|    mean_reward          | -39.2        |
| time/                   |              |
|    total_timesteps      | 2000000      |
| train/                  |              |
|    approx_kl            | 0.0060623675 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 31.4         |
|    n_updates            | 9760         |
|    policy_gradient_loss | -8.67e-05    |
|    std                  | 4.19         |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=2002000, episode_reward=199.78 +/- 165.87
Episode length: 390.60 +/- 30.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 2002000     |
| train/                  |             |
|    approx_kl            | 0.009773146 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 42.3        |
|    n_updates            | 9770        |
|    policy_gradient_loss | -0.00317    |
|    std                  | 4.19        |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=2004000, episode_reward=252.91 +/- 262.92
Episode length: 392.00 +/- 34.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 2004000      |
| train/                  |              |
|    approx_kl            | 0.0009132013 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+03     |
|    n_updates            | 9780         |
|    policy_gradient_loss | 0.000297     |
|    std                  | 4.19         |
|    value_loss           | 3.55e+03     |
------------------------------------------
Eval num_timesteps=2006000, episode_reward=482.82 +/- 230.14
Episode length: 418.80 +/- 48.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 483           |
| time/                   |               |
|    total_timesteps      | 2006000       |
| train/                  |               |
|    approx_kl            | 0.00018562953 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.8           |
|    learning_rate        | 0.001         |
|    loss                 | 2.97e+03      |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000203     |
|    std                  | 4.19          |
|    value_loss           | 7.99e+03      |
-------------------------------------------
Eval num_timesteps=2008000, episode_reward=240.17 +/- 77.90
Episode length: 394.40 +/- 16.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 240           |
| time/                   |               |
|    total_timesteps      | 2008000       |
| train/                  |               |
|    approx_kl            | 0.00037574285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 93.8          |
|    n_updates            | 9800          |
|    policy_gradient_loss | -0.000616     |
|    std                  | 4.19          |
|    value_loss           | 494           |
-------------------------------------------
Eval num_timesteps=2010000, episode_reward=311.35 +/- 237.31
Episode length: 441.40 +/- 33.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 441         |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 2010000     |
| train/                  |             |
|    approx_kl            | 0.001464224 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 49.7        |
|    n_updates            | 9810        |
|    policy_gradient_loss | -0.000614   |
|    std                  | 4.2         |
|    value_loss           | 173         |
-----------------------------------------
Eval num_timesteps=2012000, episode_reward=100.14 +/- 120.92
Episode length: 366.60 +/- 26.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 367         |
|    mean_reward          | 100         |
| time/                   |             |
|    total_timesteps      | 2012000     |
| train/                  |             |
|    approx_kl            | 0.001659374 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.001       |
|    loss                 | 3.52e+03    |
|    n_updates            | 9820        |
|    policy_gradient_loss | -0.000672   |
|    std                  | 4.2         |
|    value_loss           | 8.95e+03    |
-----------------------------------------
Eval num_timesteps=2014000, episode_reward=577.63 +/- 316.62
Episode length: 444.80 +/- 37.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 578          |
| time/                   |              |
|    total_timesteps      | 2014000      |
| train/                  |              |
|    approx_kl            | 0.0032434305 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 66.5         |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 4.2          |
|    value_loss           | 352          |
------------------------------------------
Eval num_timesteps=2016000, episode_reward=406.78 +/- 200.27
Episode length: 438.20 +/- 38.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 438         |
|    mean_reward          | 407         |
| time/                   |             |
|    total_timesteps      | 2016000     |
| train/                  |             |
|    approx_kl            | 0.004510887 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+03    |
|    n_updates            | 9840        |
|    policy_gradient_loss | 0.000818    |
|    std                  | 4.2         |
|    value_loss           | 3.42e+03    |
-----------------------------------------
Eval num_timesteps=2018000, episode_reward=373.63 +/- 281.75
Episode length: 468.80 +/- 43.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 2018000      |
| train/                  |              |
|    approx_kl            | 0.0004194971 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.001        |
|    loss                 | 1.79e+03     |
|    n_updates            | 9850         |
|    policy_gradient_loss | -0.000447    |
|    std                  | 4.2          |
|    value_loss           | 5.53e+03     |
------------------------------------------
Eval num_timesteps=2020000, episode_reward=296.01 +/- 259.57
Episode length: 421.60 +/- 19.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 296          |
| time/                   |              |
|    total_timesteps      | 2020000      |
| train/                  |              |
|    approx_kl            | 8.583136e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+03     |
|    n_updates            | 9860         |
|    policy_gradient_loss | 9.57e-05     |
|    std                  | 4.2          |
|    value_loss           | 7.29e+03     |
------------------------------------------
Eval num_timesteps=2022000, episode_reward=41.94 +/- 162.02
Episode length: 453.80 +/- 49.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 454        |
|    mean_reward          | 41.9       |
| time/                   |            |
|    total_timesteps      | 2022000    |
| train/                  |            |
|    approx_kl            | 0.00744579 |
|    clip_fraction        | 0.0148     |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.001      |
|    loss                 | 33.2       |
|    n_updates            | 9870       |
|    policy_gradient_loss | -0.00242   |
|    std                  | 4.2        |
|    value_loss           | 161        |
----------------------------------------
Eval num_timesteps=2024000, episode_reward=158.03 +/- 232.91
Episode length: 421.00 +/- 54.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 2024000      |
| train/                  |              |
|    approx_kl            | 0.0036316826 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 9880         |
|    policy_gradient_loss | -0.00015     |
|    std                  | 4.21         |
|    value_loss           | 3.5e+03      |
------------------------------------------
Eval num_timesteps=2026000, episode_reward=430.95 +/- 289.19
Episode length: 417.20 +/- 43.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 417         |
|    mean_reward          | 431         |
| time/                   |             |
|    total_timesteps      | 2026000     |
| train/                  |             |
|    approx_kl            | 0.000574969 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.001       |
|    loss                 | 865         |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.000542   |
|    std                  | 4.21        |
|    value_loss           | 2.49e+03    |
-----------------------------------------
Eval num_timesteps=2028000, episode_reward=230.41 +/- 320.18
Episode length: 434.00 +/- 46.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 230           |
| time/                   |               |
|    total_timesteps      | 2028000       |
| train/                  |               |
|    approx_kl            | 0.00014079202 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.001         |
|    loss                 | 901           |
|    n_updates            | 9900          |
|    policy_gradient_loss | -0.00031      |
|    std                  | 4.21          |
|    value_loss           | 2.23e+03      |
-------------------------------------------
Eval num_timesteps=2030000, episode_reward=304.77 +/- 403.93
Episode length: 409.40 +/- 21.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | 305           |
| time/                   |               |
|    total_timesteps      | 2030000       |
| train/                  |               |
|    approx_kl            | 0.00095315103 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.001         |
|    loss                 | 36            |
|    n_updates            | 9910          |
|    policy_gradient_loss | -0.000774     |
|    std                  | 4.21          |
|    value_loss           | 201           |
-------------------------------------------
Eval num_timesteps=2032000, episode_reward=42.50 +/- 182.26
Episode length: 426.20 +/- 68.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 42.5        |
| time/                   |             |
|    total_timesteps      | 2032000     |
| train/                  |             |
|    approx_kl            | 0.007381852 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 409         |
|    n_updates            | 9920        |
|    policy_gradient_loss | -0.0029     |
|    std                  | 4.2         |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=2034000, episode_reward=123.49 +/- 196.61
Episode length: 415.40 +/- 41.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 123          |
| time/                   |              |
|    total_timesteps      | 2034000      |
| train/                  |              |
|    approx_kl            | 0.0014683736 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 72.8         |
|    n_updates            | 9930         |
|    policy_gradient_loss | -0.000288    |
|    std                  | 4.19         |
|    value_loss           | 397          |
------------------------------------------
Eval num_timesteps=2036000, episode_reward=319.68 +/- 113.14
Episode length: 448.20 +/- 35.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 320          |
| time/                   |              |
|    total_timesteps      | 2036000      |
| train/                  |              |
|    approx_kl            | 0.0009776093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+03      |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.000712    |
|    std                  | 4.18         |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=2038000, episode_reward=555.62 +/- 373.23
Episode length: 446.60 +/- 54.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 556           |
| time/                   |               |
|    total_timesteps      | 2038000       |
| train/                  |               |
|    approx_kl            | 0.00046388136 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 9950          |
|    policy_gradient_loss | -0.000261     |
|    std                  | 4.18          |
|    value_loss           | 2.67e+03      |
-------------------------------------------
Eval num_timesteps=2040000, episode_reward=528.40 +/- 413.03
Episode length: 457.60 +/- 48.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 528           |
| time/                   |               |
|    total_timesteps      | 2040000       |
| train/                  |               |
|    approx_kl            | 5.8664795e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 1.92e+03      |
|    n_updates            | 9960          |
|    policy_gradient_loss | 0.00012       |
|    std                  | 4.18          |
|    value_loss           | 4.23e+03      |
-------------------------------------------
Eval num_timesteps=2042000, episode_reward=636.05 +/- 385.66
Episode length: 423.20 +/- 65.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 636          |
| time/                   |              |
|    total_timesteps      | 2042000      |
| train/                  |              |
|    approx_kl            | 6.676861e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+03      |
|    n_updates            | 9970         |
|    policy_gradient_loss | -0.000185    |
|    std                  | 4.18         |
|    value_loss           | 3.62e+03     |
------------------------------------------
Eval num_timesteps=2044000, episode_reward=440.36 +/- 319.61
Episode length: 418.80 +/- 69.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 440           |
| time/                   |               |
|    total_timesteps      | 2044000       |
| train/                  |               |
|    approx_kl            | 0.00010249243 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.822         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+03       |
|    n_updates            | 9980          |
|    policy_gradient_loss | -0.000114     |
|    std                  | 4.18          |
|    value_loss           | 3.56e+03      |
-------------------------------------------
Eval num_timesteps=2046000, episode_reward=432.01 +/- 310.95
Episode length: 442.80 +/- 45.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 432           |
| time/                   |               |
|    total_timesteps      | 2046000       |
| train/                  |               |
|    approx_kl            | 3.3453427e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+03      |
|    n_updates            | 9990          |
|    policy_gradient_loss | -6.36e-05     |
|    std                  | 4.18          |
|    value_loss           | 4.56e+03      |
-------------------------------------------
Eval num_timesteps=2048000, episode_reward=712.90 +/- 357.77
Episode length: 460.20 +/- 52.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 2048000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 1000     |
|    time_elapsed    | 10181    |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=214.07 +/- 357.82
Episode length: 433.20 +/- 49.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 214          |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 5.472917e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 97.3         |
|    n_updates            | 10000        |
|    policy_gradient_loss | -0.000172    |
|    std                  | 4.19         |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=2052000, episode_reward=651.82 +/- 184.85
Episode length: 458.00 +/- 44.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 652           |
| time/                   |               |
|    total_timesteps      | 2052000       |
| train/                  |               |
|    approx_kl            | 0.00042270237 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.001         |
|    loss                 | 2.28e+03      |
|    n_updates            | 10010         |
|    policy_gradient_loss | -0.00028      |
|    std                  | 4.19          |
|    value_loss           | 5.59e+03      |
-------------------------------------------
Eval num_timesteps=2054000, episode_reward=345.41 +/- 419.46
Episode length: 460.80 +/- 29.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 461           |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 2054000       |
| train/                  |               |
|    approx_kl            | 0.00081284554 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 393           |
|    n_updates            | 10020         |
|    policy_gradient_loss | -0.000889     |
|    std                  | 4.19          |
|    value_loss           | 1.29e+03      |
-------------------------------------------
Eval num_timesteps=2056000, episode_reward=425.37 +/- 239.81
Episode length: 441.40 +/- 80.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 441         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 2056000     |
| train/                  |             |
|    approx_kl            | 0.000834023 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 33.8        |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.000638   |
|    std                  | 4.19        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=2058000, episode_reward=445.17 +/- 417.27
Episode length: 441.40 +/- 52.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 441         |
|    mean_reward          | 445         |
| time/                   |             |
|    total_timesteps      | 2058000     |
| train/                  |             |
|    approx_kl            | 0.001021107 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.001       |
|    loss                 | 2.99e+03    |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.00077    |
|    std                  | 4.2         |
|    value_loss           | 6.99e+03    |
-----------------------------------------
Eval num_timesteps=2060000, episode_reward=477.59 +/- 218.02
Episode length: 397.20 +/- 27.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 397          |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 2060000      |
| train/                  |              |
|    approx_kl            | 0.0004597436 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 842          |
|    n_updates            | 10050        |
|    policy_gradient_loss | -0.000605    |
|    std                  | 4.2          |
|    value_loss           | 2.12e+03     |
------------------------------------------
Eval num_timesteps=2062000, episode_reward=370.40 +/- 354.14
Episode length: 394.40 +/- 30.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 370           |
| time/                   |               |
|    total_timesteps      | 2062000       |
| train/                  |               |
|    approx_kl            | 0.00062626053 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.877         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+03      |
|    n_updates            | 10060         |
|    policy_gradient_loss | -0.000411     |
|    std                  | 4.2           |
|    value_loss           | 3.73e+03      |
-------------------------------------------
Eval num_timesteps=2064000, episode_reward=354.61 +/- 257.02
Episode length: 411.40 +/- 66.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 2064000      |
| train/                  |              |
|    approx_kl            | 0.0059734983 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 15.3         |
|    n_updates            | 10070        |
|    policy_gradient_loss | -0.00183     |
|    std                  | 4.19         |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=2066000, episode_reward=581.06 +/- 429.57
Episode length: 443.60 +/- 48.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 444         |
|    mean_reward          | 581         |
| time/                   |             |
|    total_timesteps      | 2066000     |
| train/                  |             |
|    approx_kl            | 0.004409484 |
|    clip_fraction        | 0.00923     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 54.3        |
|    n_updates            | 10080       |
|    policy_gradient_loss | -0.00122    |
|    std                  | 4.19        |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=2068000, episode_reward=381.44 +/- 282.83
Episode length: 406.20 +/- 54.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | 381         |
| time/                   |             |
|    total_timesteps      | 2068000     |
| train/                  |             |
|    approx_kl            | 0.002498075 |
|    clip_fraction        | 0.00298     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.001       |
|    loss                 | 2.62e+03    |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.000244   |
|    std                  | 4.2         |
|    value_loss           | 7.07e+03    |
-----------------------------------------
Eval num_timesteps=2070000, episode_reward=508.38 +/- 423.31
Episode length: 451.20 +/- 42.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 451           |
|    mean_reward          | 508           |
| time/                   |               |
|    total_timesteps      | 2070000       |
| train/                  |               |
|    approx_kl            | 0.00048418733 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 240           |
|    n_updates            | 10100         |
|    policy_gradient_loss | -0.000498     |
|    std                  | 4.2           |
|    value_loss           | 913           |
-------------------------------------------
Eval num_timesteps=2072000, episode_reward=415.72 +/- 210.44
Episode length: 472.20 +/- 61.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 472           |
|    mean_reward          | 416           |
| time/                   |               |
|    total_timesteps      | 2072000       |
| train/                  |               |
|    approx_kl            | 0.00033274613 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.827         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+03      |
|    n_updates            | 10110         |
|    policy_gradient_loss | 0.000171      |
|    std                  | 4.2           |
|    value_loss           | 6.01e+03      |
-------------------------------------------
Eval num_timesteps=2074000, episode_reward=232.43 +/- 363.22
Episode length: 452.40 +/- 51.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 232           |
| time/                   |               |
|    total_timesteps      | 2074000       |
| train/                  |               |
|    approx_kl            | 0.00012689884 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+03       |
|    n_updates            | 10120         |
|    policy_gradient_loss | -0.000341     |
|    std                  | 4.2           |
|    value_loss           | 4.52e+03      |
-------------------------------------------
Eval num_timesteps=2076000, episode_reward=272.24 +/- 344.67
Episode length: 440.20 +/- 31.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | 272           |
| time/                   |               |
|    total_timesteps      | 2076000       |
| train/                  |               |
|    approx_kl            | 0.00023429401 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+03      |
|    n_updates            | 10130         |
|    policy_gradient_loss | -0.000228     |
|    std                  | 4.2           |
|    value_loss           | 3.21e+03      |
-------------------------------------------
Eval num_timesteps=2078000, episode_reward=396.35 +/- 165.39
Episode length: 481.60 +/- 32.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 396           |
| time/                   |               |
|    total_timesteps      | 2078000       |
| train/                  |               |
|    approx_kl            | 0.00025073014 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 662           |
|    n_updates            | 10140         |
|    policy_gradient_loss | -0.000285     |
|    std                  | 4.2           |
|    value_loss           | 2.15e+03      |
-------------------------------------------
Eval num_timesteps=2080000, episode_reward=321.26 +/- 446.04
Episode length: 481.80 +/- 54.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 482           |
|    mean_reward          | 321           |
| time/                   |               |
|    total_timesteps      | 2080000       |
| train/                  |               |
|    approx_kl            | 0.00036604356 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.001         |
|    loss                 | 819           |
|    n_updates            | 10150         |
|    policy_gradient_loss | -7.98e-05     |
|    std                  | 4.2           |
|    value_loss           | 2.07e+03      |
-------------------------------------------
Eval num_timesteps=2082000, episode_reward=-37.81 +/- 335.91
Episode length: 449.60 +/- 32.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | -37.8         |
| time/                   |               |
|    total_timesteps      | 2082000       |
| train/                  |               |
|    approx_kl            | 0.00043262247 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 82.8          |
|    n_updates            | 10160         |
|    policy_gradient_loss | -0.000438     |
|    std                  | 4.2           |
|    value_loss           | 377           |
-------------------------------------------
Eval num_timesteps=2084000, episode_reward=241.27 +/- 151.44
Episode length: 507.00 +/- 10.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 2084000     |
| train/                  |             |
|    approx_kl            | 0.002307029 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 115         |
|    n_updates            | 10170       |
|    policy_gradient_loss | -0.000684   |
|    std                  | 4.2         |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=2086000, episode_reward=256.05 +/- 163.90
Episode length: 441.80 +/- 38.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 2086000     |
| train/                  |             |
|    approx_kl            | 0.003601505 |
|    clip_fraction        | 0.00317     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.001       |
|    loss                 | 2.85e+03    |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.00169    |
|    std                  | 4.21        |
|    value_loss           | 7.15e+03    |
-----------------------------------------
Eval num_timesteps=2088000, episode_reward=472.83 +/- 307.21
Episode length: 447.00 +/- 64.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 2088000      |
| train/                  |              |
|    approx_kl            | 0.0009652518 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+03     |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.000627    |
|    std                  | 4.21         |
|    value_loss           | 3.43e+03     |
------------------------------------------
Eval num_timesteps=2090000, episode_reward=221.15 +/- 329.45
Episode length: 479.20 +/- 27.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 479           |
|    mean_reward          | 221           |
| time/                   |               |
|    total_timesteps      | 2090000       |
| train/                  |               |
|    approx_kl            | 0.00021609981 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+03      |
|    n_updates            | 10200         |
|    policy_gradient_loss | -8.42e-07     |
|    std                  | 4.21          |
|    value_loss           | 3.73e+03      |
-------------------------------------------
Eval num_timesteps=2092000, episode_reward=288.59 +/- 96.86
Episode length: 502.60 +/- 9.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 503           |
|    mean_reward          | 289           |
| time/                   |               |
|    total_timesteps      | 2092000       |
| train/                  |               |
|    approx_kl            | 5.9812242e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.771         |
|    learning_rate        | 0.001         |
|    loss                 | 2.04e+03      |
|    n_updates            | 10210         |
|    policy_gradient_loss | -4.49e-05     |
|    std                  | 4.21          |
|    value_loss           | 5.27e+03      |
-------------------------------------------
Eval num_timesteps=2094000, episode_reward=284.20 +/- 361.83
Episode length: 511.20 +/- 10.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 511          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 2094000      |
| train/                  |              |
|    approx_kl            | 2.996155e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.2        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+03     |
|    n_updates            | 10220        |
|    policy_gradient_loss | -3.58e-05    |
|    std                  | 4.21         |
|    value_loss           | 3.71e+03     |
------------------------------------------
Eval num_timesteps=2096000, episode_reward=496.24 +/- 234.08
Episode length: 446.80 +/- 60.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 2096000       |
| train/                  |               |
|    approx_kl            | 0.00023816354 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.2         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+03      |
|    n_updates            | 10230         |
|    policy_gradient_loss | -0.000333     |
|    std                  | 4.21          |
|    value_loss           | 3.74e+03      |
-------------------------------------------
Eval num_timesteps=2098000, episode_reward=556.08 +/- 313.41
Episode length: 493.80 +/- 39.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | 556         |
| time/                   |             |
|    total_timesteps      | 2098000     |
| train/                  |             |
|    approx_kl            | 0.005655956 |
|    clip_fraction        | 0.00898     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 30.2        |
|    n_updates            | 10240       |
|    policy_gradient_loss | -0.00159    |
|    std                  | 4.22        |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=2100000, episode_reward=238.99 +/- 366.37
Episode length: 396.00 +/- 41.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 2100000      |
| train/                  |              |
|    approx_kl            | 0.0042624027 |
|    clip_fraction        | 0.00635      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 43.3         |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.00079     |
|    std                  | 4.23         |
|    value_loss           | 253          |
------------------------------------------
Eval num_timesteps=2102000, episode_reward=330.66 +/- 192.94
Episode length: 499.20 +/- 7.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 499           |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 2102000       |
| train/                  |               |
|    approx_kl            | 0.00054006075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+03      |
|    n_updates            | 10260         |
|    policy_gradient_loss | 0.00016       |
|    std                  | 4.23          |
|    value_loss           | 4.84e+03      |
-------------------------------------------
Eval num_timesteps=2104000, episode_reward=175.27 +/- 430.58
Episode length: 448.40 +/- 45.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 448         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 2104000     |
| train/                  |             |
|    approx_kl            | 0.001520792 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 71.3        |
|    n_updates            | 10270       |
|    policy_gradient_loss | -0.000726   |
|    std                  | 4.24        |
|    value_loss           | 328         |
-----------------------------------------
Eval num_timesteps=2106000, episode_reward=482.03 +/- 354.65
Episode length: 460.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 2106000      |
| train/                  |              |
|    approx_kl            | 0.0045032185 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 2.52e+03     |
|    n_updates            | 10280        |
|    policy_gradient_loss | -0.000212    |
|    std                  | 4.25         |
|    value_loss           | 6.3e+03      |
------------------------------------------
Eval num_timesteps=2108000, episode_reward=265.44 +/- 254.10
Episode length: 441.00 +/- 37.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 265          |
| time/                   |              |
|    total_timesteps      | 2108000      |
| train/                  |              |
|    approx_kl            | 0.0045183096 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 49.7         |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00157     |
|    std                  | 4.25         |
|    value_loss           | 190          |
------------------------------------------
Eval num_timesteps=2110000, episode_reward=332.82 +/- 314.30
Episode length: 414.20 +/- 31.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 333         |
| time/                   |             |
|    total_timesteps      | 2110000     |
| train/                  |             |
|    approx_kl            | 0.004409491 |
|    clip_fraction        | 0.00522     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 31          |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.00136    |
|    std                  | 4.25        |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=2112000, episode_reward=212.72 +/- 552.67
Episode length: 447.20 +/- 41.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 213          |
| time/                   |              |
|    total_timesteps      | 2112000      |
| train/                  |              |
|    approx_kl            | 0.0015161496 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+03     |
|    n_updates            | 10310        |
|    policy_gradient_loss | -0.000936    |
|    std                  | 4.25         |
|    value_loss           | 2.42e+03     |
------------------------------------------
Eval num_timesteps=2114000, episode_reward=241.75 +/- 371.33
Episode length: 395.80 +/- 51.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 396           |
|    mean_reward          | 242           |
| time/                   |               |
|    total_timesteps      | 2114000       |
| train/                  |               |
|    approx_kl            | 0.00023365617 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.001         |
|    loss                 | 3e+03         |
|    n_updates            | 10320         |
|    policy_gradient_loss | 0.00025       |
|    std                  | 4.26          |
|    value_loss           | 6.6e+03       |
-------------------------------------------
Eval num_timesteps=2116000, episode_reward=139.25 +/- 227.73
Episode length: 401.40 +/- 44.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 139           |
| time/                   |               |
|    total_timesteps      | 2116000       |
| train/                  |               |
|    approx_kl            | 0.00015493025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+03      |
|    n_updates            | 10330         |
|    policy_gradient_loss | -0.000357     |
|    std                  | 4.26          |
|    value_loss           | 3.27e+03      |
-------------------------------------------
Eval num_timesteps=2118000, episode_reward=283.98 +/- 355.38
Episode length: 426.20 +/- 52.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 2118000       |
| train/                  |               |
|    approx_kl            | 0.00020543032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.001         |
|    loss                 | 3.44e+03      |
|    n_updates            | 10340         |
|    policy_gradient_loss | 7.36e-05      |
|    std                  | 4.25          |
|    value_loss           | 7.65e+03      |
-------------------------------------------
Eval num_timesteps=2120000, episode_reward=455.43 +/- 217.17
Episode length: 421.60 +/- 37.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 455           |
| time/                   |               |
|    total_timesteps      | 2120000       |
| train/                  |               |
|    approx_kl            | 3.4110155e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 7.38e+03      |
|    n_updates            | 10350         |
|    policy_gradient_loss | -0.000104     |
|    std                  | 4.25          |
|    value_loss           | 1.56e+04      |
-------------------------------------------
Eval num_timesteps=2122000, episode_reward=162.08 +/- 298.42
Episode length: 401.40 +/- 20.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 162           |
| time/                   |               |
|    total_timesteps      | 2122000       |
| train/                  |               |
|    approx_kl            | 2.6998023e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+03      |
|    n_updates            | 10360         |
|    policy_gradient_loss | -0.000143     |
|    std                  | 4.26          |
|    value_loss           | 9.67e+03      |
-------------------------------------------
Eval num_timesteps=2124000, episode_reward=33.75 +/- 282.85
Episode length: 417.60 +/- 29.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 33.8          |
| time/                   |               |
|    total_timesteps      | 2124000       |
| train/                  |               |
|    approx_kl            | 0.00017902235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 113           |
|    n_updates            | 10370         |
|    policy_gradient_loss | -0.000179     |
|    std                  | 4.26          |
|    value_loss           | 512           |
-------------------------------------------
Eval num_timesteps=2126000, episode_reward=357.77 +/- 331.79
Episode length: 390.60 +/- 29.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 358          |
| time/                   |              |
|    total_timesteps      | 2126000      |
| train/                  |              |
|    approx_kl            | 0.0013139988 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 30.6         |
|    n_updates            | 10380        |
|    policy_gradient_loss | -0.00049     |
|    std                  | 4.26         |
|    value_loss           | 169          |
------------------------------------------
Eval num_timesteps=2128000, episode_reward=249.66 +/- 331.81
Episode length: 400.40 +/- 35.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 250          |
| time/                   |              |
|    total_timesteps      | 2128000      |
| train/                  |              |
|    approx_kl            | 0.0014145381 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 10390        |
|    policy_gradient_loss | 0.000273     |
|    std                  | 4.25         |
|    value_loss           | 2.94e+03     |
------------------------------------------
Eval num_timesteps=2130000, episode_reward=323.21 +/- 314.15
Episode length: 386.60 +/- 12.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 387           |
|    mean_reward          | 323           |
| time/                   |               |
|    total_timesteps      | 2130000       |
| train/                  |               |
|    approx_kl            | 0.00042292793 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.852         |
|    learning_rate        | 0.001         |
|    loss                 | 4.02e+03      |
|    n_updates            | 10400         |
|    policy_gradient_loss | -0.000387     |
|    std                  | 4.24          |
|    value_loss           | 9.72e+03      |
-------------------------------------------
Eval num_timesteps=2132000, episode_reward=435.94 +/- 332.26
Episode length: 376.20 +/- 34.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 376           |
|    mean_reward          | 436           |
| time/                   |               |
|    total_timesteps      | 2132000       |
| train/                  |               |
|    approx_kl            | 0.00011458376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+03      |
|    n_updates            | 10410         |
|    policy_gradient_loss | 6.94e-05      |
|    std                  | 4.24          |
|    value_loss           | 5.65e+03      |
-------------------------------------------
Eval num_timesteps=2134000, episode_reward=301.58 +/- 244.99
Episode length: 374.60 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 2134000  |
---------------------------------
Eval num_timesteps=2136000, episode_reward=357.79 +/- 200.07
Episode length: 383.00 +/- 29.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 358          |
| time/                   |              |
|    total_timesteps      | 2136000      |
| train/                  |              |
|    approx_kl            | 0.0015594508 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+03     |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.00174     |
|    std                  | 4.24         |
|    value_loss           | 2.61e+03     |
------------------------------------------
Eval num_timesteps=2138000, episode_reward=135.48 +/- 110.28
Episode length: 432.20 +/- 49.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 2138000      |
| train/                  |              |
|    approx_kl            | 0.0014360219 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+03     |
|    n_updates            | 10430        |
|    policy_gradient_loss | 0.00158      |
|    std                  | 4.25         |
|    value_loss           | 3.44e+03     |
------------------------------------------
Eval num_timesteps=2140000, episode_reward=374.87 +/- 242.79
Episode length: 423.40 +/- 29.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 2140000      |
| train/                  |              |
|    approx_kl            | 4.144502e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 4.25e+03     |
|    n_updates            | 10440        |
|    policy_gradient_loss | 1.45e-05     |
|    std                  | 4.25         |
|    value_loss           | 9.81e+03     |
------------------------------------------
Eval num_timesteps=2142000, episode_reward=118.21 +/- 244.46
Episode length: 395.60 +/- 18.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 118          |
| time/                   |              |
|    total_timesteps      | 2142000      |
| train/                  |              |
|    approx_kl            | 7.063843e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 506          |
|    n_updates            | 10450        |
|    policy_gradient_loss | -0.000164    |
|    std                  | 4.25         |
|    value_loss           | 1.44e+03     |
------------------------------------------
Eval num_timesteps=2144000, episode_reward=390.00 +/- 424.52
Episode length: 419.40 +/- 61.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 2144000      |
| train/                  |              |
|    approx_kl            | 7.001552e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 728          |
|    n_updates            | 10460        |
|    policy_gradient_loss | 0.00022      |
|    std                  | 4.26         |
|    value_loss           | 2.31e+03     |
------------------------------------------
Eval num_timesteps=2146000, episode_reward=271.25 +/- 275.75
Episode length: 392.00 +/- 32.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 271           |
| time/                   |               |
|    total_timesteps      | 2146000       |
| train/                  |               |
|    approx_kl            | 0.00030120305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+03      |
|    n_updates            | 10470         |
|    policy_gradient_loss | 0.000525      |
|    std                  | 4.26          |
|    value_loss           | 5.6e+03       |
-------------------------------------------
Eval num_timesteps=2148000, episode_reward=528.55 +/- 306.84
Episode length: 401.40 +/- 50.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 529           |
| time/                   |               |
|    total_timesteps      | 2148000       |
| train/                  |               |
|    approx_kl            | 2.0188716e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 4.89e+03      |
|    n_updates            | 10480         |
|    policy_gradient_loss | 7.26e-05      |
|    std                  | 4.26          |
|    value_loss           | 1e+04         |
-------------------------------------------
Eval num_timesteps=2150000, episode_reward=477.05 +/- 387.19
Episode length: 427.20 +/- 19.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 477           |
| time/                   |               |
|    total_timesteps      | 2150000       |
| train/                  |               |
|    approx_kl            | 0.00024412989 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+03      |
|    n_updates            | 10490         |
|    policy_gradient_loss | -0.000733     |
|    std                  | 4.26          |
|    value_loss           | 3.76e+03      |
-------------------------------------------
Eval num_timesteps=2152000, episode_reward=546.66 +/- 369.46
Episode length: 440.80 +/- 42.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | 547           |
| time/                   |               |
|    total_timesteps      | 2152000       |
| train/                  |               |
|    approx_kl            | 0.00026730422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 10500         |
|    policy_gradient_loss | -0.000416     |
|    std                  | 4.26          |
|    value_loss           | 3.58e+03      |
-------------------------------------------
Eval num_timesteps=2154000, episode_reward=335.57 +/- 407.24
Episode length: 424.80 +/- 36.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 336          |
| time/                   |              |
|    total_timesteps      | 2154000      |
| train/                  |              |
|    approx_kl            | 7.473657e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 2.72e+03     |
|    n_updates            | 10510        |
|    policy_gradient_loss | -3.53e-05    |
|    std                  | 4.26         |
|    value_loss           | 6.25e+03     |
------------------------------------------
Eval num_timesteps=2156000, episode_reward=372.44 +/- 357.99
Episode length: 414.00 +/- 41.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 372           |
| time/                   |               |
|    total_timesteps      | 2156000       |
| train/                  |               |
|    approx_kl            | 2.0997686e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+03      |
|    n_updates            | 10520         |
|    policy_gradient_loss | -8.64e-05     |
|    std                  | 4.26          |
|    value_loss           | 4.17e+03      |
-------------------------------------------
Eval num_timesteps=2158000, episode_reward=351.36 +/- 485.48
Episode length: 411.40 +/- 29.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 2158000      |
| train/                  |              |
|    approx_kl            | 9.993801e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 825          |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.000179    |
|    std                  | 4.26         |
|    value_loss           | 1.85e+03     |
------------------------------------------
Eval num_timesteps=2160000, episode_reward=337.70 +/- 312.49
Episode length: 392.60 +/- 50.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 2160000       |
| train/                  |               |
|    approx_kl            | 2.9700692e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.001         |
|    loss                 | 2.96e+03      |
|    n_updates            | 10540         |
|    policy_gradient_loss | 6.69e-05      |
|    std                  | 4.27          |
|    value_loss           | 6.64e+03      |
-------------------------------------------
Eval num_timesteps=2162000, episode_reward=419.40 +/- 295.62
Episode length: 405.80 +/- 33.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 419           |
| time/                   |               |
|    total_timesteps      | 2162000       |
| train/                  |               |
|    approx_kl            | 1.1994358e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+03       |
|    n_updates            | 10550         |
|    policy_gradient_loss | -8.38e-05     |
|    std                  | 4.27          |
|    value_loss           | 4.99e+03      |
-------------------------------------------
Eval num_timesteps=2164000, episode_reward=46.34 +/- 303.18
Episode length: 409.20 +/- 49.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | 46.3          |
| time/                   |               |
|    total_timesteps      | 2164000       |
| train/                  |               |
|    approx_kl            | 3.2711017e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.001         |
|    loss                 | 4.51e+03      |
|    n_updates            | 10560         |
|    policy_gradient_loss | -0.000138     |
|    std                  | 4.27          |
|    value_loss           | 1.33e+04      |
-------------------------------------------
Eval num_timesteps=2166000, episode_reward=429.51 +/- 372.90
Episode length: 381.40 +/- 25.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 381           |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 2166000       |
| train/                  |               |
|    approx_kl            | 0.00010977895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+03      |
|    n_updates            | 10570         |
|    policy_gradient_loss | -0.00025      |
|    std                  | 4.27          |
|    value_loss           | 3.14e+03      |
-------------------------------------------
Eval num_timesteps=2168000, episode_reward=365.82 +/- 322.45
Episode length: 435.00 +/- 41.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 435           |
|    mean_reward          | 366           |
| time/                   |               |
|    total_timesteps      | 2168000       |
| train/                  |               |
|    approx_kl            | 0.00013453397 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 10580         |
|    policy_gradient_loss | -0.000233     |
|    std                  | 4.28          |
|    value_loss           | 3.05e+03      |
-------------------------------------------
Eval num_timesteps=2170000, episode_reward=155.14 +/- 285.72
Episode length: 426.00 +/- 62.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 155           |
| time/                   |               |
|    total_timesteps      | 2170000       |
| train/                  |               |
|    approx_kl            | 3.8052152e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.001         |
|    loss                 | 4.3e+03       |
|    n_updates            | 10590         |
|    policy_gradient_loss | -6.98e-05     |
|    std                  | 4.28          |
|    value_loss           | 8.66e+03      |
-------------------------------------------
Eval num_timesteps=2172000, episode_reward=176.85 +/- 288.17
Episode length: 413.40 +/- 48.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 413           |
|    mean_reward          | 177           |
| time/                   |               |
|    total_timesteps      | 2172000       |
| train/                  |               |
|    approx_kl            | 0.00018713498 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 10600         |
|    policy_gradient_loss | -0.000445     |
|    std                  | 4.28          |
|    value_loss           | 4.19e+03      |
-------------------------------------------
Eval num_timesteps=2174000, episode_reward=273.07 +/- 454.69
Episode length: 412.20 +/- 33.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 412           |
|    mean_reward          | 273           |
| time/                   |               |
|    total_timesteps      | 2174000       |
| train/                  |               |
|    approx_kl            | 0.00035880154 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+03      |
|    n_updates            | 10610         |
|    policy_gradient_loss | -0.00056      |
|    std                  | 4.28          |
|    value_loss           | 2.8e+03       |
-------------------------------------------
Eval num_timesteps=2176000, episode_reward=445.88 +/- 358.55
Episode length: 461.20 +/- 46.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 2176000      |
| train/                  |              |
|    approx_kl            | 0.0021172978 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 26.8         |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.00086     |
|    std                  | 4.29         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=2178000, episode_reward=303.69 +/- 378.50
Episode length: 400.80 +/- 27.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 2178000      |
| train/                  |              |
|    approx_kl            | 0.0013737003 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 10630        |
|    policy_gradient_loss | -1.4e-05     |
|    std                  | 4.3          |
|    value_loss           | 688          |
------------------------------------------
Eval num_timesteps=2180000, episode_reward=313.00 +/- 433.47
Episode length: 462.20 +/- 29.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 313           |
| time/                   |               |
|    total_timesteps      | 2180000       |
| train/                  |               |
|    approx_kl            | 0.00043896044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.3         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 956           |
|    n_updates            | 10640         |
|    policy_gradient_loss | -0.000265     |
|    std                  | 4.31          |
|    value_loss           | 2.39e+03      |
-------------------------------------------
Eval num_timesteps=2182000, episode_reward=266.27 +/- 333.41
Episode length: 441.20 +/- 52.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 2182000      |
| train/                  |              |
|    approx_kl            | 0.0026333875 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 31           |
|    n_updates            | 10650        |
|    policy_gradient_loss | -0.00123     |
|    std                  | 4.32         |
|    value_loss           | 180          |
------------------------------------------
Eval num_timesteps=2184000, episode_reward=267.40 +/- 371.98
Episode length: 433.00 +/- 41.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 2184000      |
| train/                  |              |
|    approx_kl            | 0.0038545486 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.3        |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+03     |
|    n_updates            | 10660        |
|    policy_gradient_loss | 0.00114      |
|    std                  | 4.33         |
|    value_loss           | 2.68e+03     |
------------------------------------------
Eval num_timesteps=2186000, episode_reward=219.89 +/- 369.68
Episode length: 457.80 +/- 40.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 220           |
| time/                   |               |
|    total_timesteps      | 2186000       |
| train/                  |               |
|    approx_kl            | 0.00028256883 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+03      |
|    n_updates            | 10670         |
|    policy_gradient_loss | 0.000289      |
|    std                  | 4.33          |
|    value_loss           | 7.32e+03      |
-------------------------------------------
Eval num_timesteps=2188000, episode_reward=498.03 +/- 492.57
Episode length: 436.00 +/- 66.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 498           |
| time/                   |               |
|    total_timesteps      | 2188000       |
| train/                  |               |
|    approx_kl            | 8.9780515e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 742           |
|    n_updates            | 10680         |
|    policy_gradient_loss | -0.000171     |
|    std                  | 4.34          |
|    value_loss           | 2.08e+03      |
-------------------------------------------
Eval num_timesteps=2190000, episode_reward=92.03 +/- 279.20
Episode length: 445.60 +/- 48.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 92           |
| time/                   |              |
|    total_timesteps      | 2190000      |
| train/                  |              |
|    approx_kl            | 0.0011435866 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 77.7         |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.000602    |
|    std                  | 4.35         |
|    value_loss           | 312          |
------------------------------------------
Eval num_timesteps=2192000, episode_reward=304.01 +/- 160.28
Episode length: 439.80 +/- 44.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 2192000      |
| train/                  |              |
|    approx_kl            | 0.0020359121 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 3.91e+03     |
|    n_updates            | 10700        |
|    policy_gradient_loss | -0.000345    |
|    std                  | 4.36         |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=2194000, episode_reward=155.01 +/- 182.74
Episode length: 398.60 +/- 33.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 399           |
|    mean_reward          | 155           |
| time/                   |               |
|    total_timesteps      | 2194000       |
| train/                  |               |
|    approx_kl            | 0.00022273787 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+03      |
|    n_updates            | 10710         |
|    policy_gradient_loss | 0.000113      |
|    std                  | 4.36          |
|    value_loss           | 2.9e+03       |
-------------------------------------------
Eval num_timesteps=2196000, episode_reward=359.67 +/- 323.74
Episode length: 429.40 +/- 31.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 360           |
| time/                   |               |
|    total_timesteps      | 2196000       |
| train/                  |               |
|    approx_kl            | 3.5769655e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 10720         |
|    policy_gradient_loss | -8.81e-05     |
|    std                  | 4.36          |
|    value_loss           | 2.65e+03      |
-------------------------------------------
Eval num_timesteps=2198000, episode_reward=214.18 +/- 133.81
Episode length: 400.80 +/- 52.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 214           |
| time/                   |               |
|    total_timesteps      | 2198000       |
| train/                  |               |
|    approx_kl            | 1.0090036e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 1.54e+03      |
|    n_updates            | 10730         |
|    policy_gradient_loss | -2.69e-05     |
|    std                  | 4.36          |
|    value_loss           | 3.64e+03      |
-------------------------------------------
Eval num_timesteps=2200000, episode_reward=28.54 +/- 265.01
Episode length: 415.40 +/- 48.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 28.5         |
| time/                   |              |
|    total_timesteps      | 2200000      |
| train/                  |              |
|    approx_kl            | 0.0003782477 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+03     |
|    n_updates            | 10740        |
|    policy_gradient_loss | -0.000813    |
|    std                  | 4.36         |
|    value_loss           | 2.75e+03     |
------------------------------------------
Eval num_timesteps=2202000, episode_reward=613.74 +/- 216.69
Episode length: 430.80 +/- 35.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 614           |
| time/                   |               |
|    total_timesteps      | 2202000       |
| train/                  |               |
|    approx_kl            | 0.00043048806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 54            |
|    n_updates            | 10750         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 4.37          |
|    value_loss           | 353           |
-------------------------------------------
Eval num_timesteps=2204000, episode_reward=599.47 +/- 339.01
Episode length: 410.20 +/- 41.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 599          |
| time/                   |              |
|    total_timesteps      | 2204000      |
| train/                  |              |
|    approx_kl            | 0.0013306357 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 2.3e+03      |
|    n_updates            | 10760        |
|    policy_gradient_loss | -0.000247    |
|    std                  | 4.37         |
|    value_loss           | 5.48e+03     |
------------------------------------------
Eval num_timesteps=2206000, episode_reward=241.34 +/- 171.24
Episode length: 454.00 +/- 56.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 241           |
| time/                   |               |
|    total_timesteps      | 2206000       |
| train/                  |               |
|    approx_kl            | 0.00014461635 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.862         |
|    learning_rate        | 0.001         |
|    loss                 | 3.48e+03      |
|    n_updates            | 10770         |
|    policy_gradient_loss | 0.000487      |
|    std                  | 4.37          |
|    value_loss           | 7.49e+03      |
-------------------------------------------
Eval num_timesteps=2208000, episode_reward=438.87 +/- 139.62
Episode length: 388.40 +/- 37.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 439           |
| time/                   |               |
|    total_timesteps      | 2208000       |
| train/                  |               |
|    approx_kl            | 4.2111846e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 294           |
|    n_updates            | 10780         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 4.37          |
|    value_loss           | 1.18e+03      |
-------------------------------------------
Eval num_timesteps=2210000, episode_reward=140.35 +/- 180.82
Episode length: 422.60 +/- 53.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 140         |
| time/                   |             |
|    total_timesteps      | 2210000     |
| train/                  |             |
|    approx_kl            | 0.008774675 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 46.4        |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.00262    |
|    std                  | 4.38        |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=2212000, episode_reward=427.47 +/- 323.06
Episode length: 409.20 +/- 19.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 427          |
| time/                   |              |
|    total_timesteps      | 2212000      |
| train/                  |              |
|    approx_kl            | 0.0038383864 |
|    clip_fraction        | 0.00669      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 812          |
|    n_updates            | 10800        |
|    policy_gradient_loss | 0.000504     |
|    std                  | 4.39         |
|    value_loss           | 1.93e+03     |
------------------------------------------
Eval num_timesteps=2214000, episode_reward=401.17 +/- 269.33
Episode length: 415.60 +/- 64.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 2214000      |
| train/                  |              |
|    approx_kl            | 0.0007463208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+03     |
|    n_updates            | 10810        |
|    policy_gradient_loss | -0.000701    |
|    std                  | 4.39         |
|    value_loss           | 3.23e+03     |
------------------------------------------
Eval num_timesteps=2216000, episode_reward=180.64 +/- 399.31
Episode length: 407.80 +/- 34.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 2216000      |
| train/                  |              |
|    approx_kl            | 0.0008083427 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 319          |
|    n_updates            | 10820        |
|    policy_gradient_loss | -0.000346    |
|    std                  | 4.39         |
|    value_loss           | 1.37e+03     |
------------------------------------------
Eval num_timesteps=2218000, episode_reward=45.97 +/- 185.73
Episode length: 433.40 +/- 32.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 433           |
|    mean_reward          | 46            |
| time/                   |               |
|    total_timesteps      | 2218000       |
| train/                  |               |
|    approx_kl            | 0.00029461097 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+03      |
|    n_updates            | 10830         |
|    policy_gradient_loss | -0.000547     |
|    std                  | 4.39          |
|    value_loss           | 4.26e+03      |
-------------------------------------------
Eval num_timesteps=2220000, episode_reward=502.02 +/- 253.56
Episode length: 421.80 +/- 59.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2222000, episode_reward=208.11 +/- 190.25
Episode length: 408.20 +/- 28.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 208           |
| time/                   |               |
|    total_timesteps      | 2222000       |
| train/                  |               |
|    approx_kl            | 0.00010325649 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 10840         |
|    policy_gradient_loss | -0.000219     |
|    std                  | 4.39          |
|    value_loss           | 3.12e+03      |
-------------------------------------------
Eval num_timesteps=2224000, episode_reward=499.25 +/- 232.96
Episode length: 397.60 +/- 46.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 499          |
| time/                   |              |
|    total_timesteps      | 2224000      |
| train/                  |              |
|    approx_kl            | 3.964544e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 2.96e+03     |
|    n_updates            | 10850        |
|    policy_gradient_loss | -8.18e-05    |
|    std                  | 4.39         |
|    value_loss           | 6.92e+03     |
------------------------------------------
Eval num_timesteps=2226000, episode_reward=501.25 +/- 117.90
Episode length: 400.80 +/- 22.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 501           |
| time/                   |               |
|    total_timesteps      | 2226000       |
| train/                  |               |
|    approx_kl            | 1.5962549e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+03      |
|    n_updates            | 10860         |
|    policy_gradient_loss | -9.95e-06     |
|    std                  | 4.39          |
|    value_loss           | 4.56e+03      |
-------------------------------------------
Eval num_timesteps=2228000, episode_reward=312.81 +/- 315.31
Episode length: 447.20 +/- 49.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 313          |
| time/                   |              |
|    total_timesteps      | 2228000      |
| train/                  |              |
|    approx_kl            | 7.902805e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 51.9         |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.000136    |
|    std                  | 4.39         |
|    value_loss           | 362          |
------------------------------------------
Eval num_timesteps=2230000, episode_reward=306.23 +/- 201.49
Episode length: 454.20 +/- 41.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 306           |
| time/                   |               |
|    total_timesteps      | 2230000       |
| train/                  |               |
|    approx_kl            | 0.00022649331 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+03      |
|    n_updates            | 10880         |
|    policy_gradient_loss | -9.64e-05     |
|    std                  | 4.38          |
|    value_loss           | 4.72e+03      |
-------------------------------------------
Eval num_timesteps=2232000, episode_reward=112.76 +/- 411.17
Episode length: 443.20 +/- 33.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 443         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 2232000     |
| train/                  |             |
|    approx_kl            | 3.91019e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.001       |
|    loss                 | 1.39e+03    |
|    n_updates            | 10890       |
|    policy_gradient_loss | -8.97e-05   |
|    std                  | 4.38        |
|    value_loss           | 3.92e+03    |
-----------------------------------------
Eval num_timesteps=2234000, episode_reward=128.58 +/- 114.63
Episode length: 453.20 +/- 47.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 129           |
| time/                   |               |
|    total_timesteps      | 2234000       |
| train/                  |               |
|    approx_kl            | 5.4523378e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.001         |
|    loss                 | 766           |
|    n_updates            | 10900         |
|    policy_gradient_loss | -4.79e-05     |
|    std                  | 4.38          |
|    value_loss           | 2.75e+03      |
-------------------------------------------
Eval num_timesteps=2236000, episode_reward=73.05 +/- 407.26
Episode length: 423.80 +/- 17.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 73.1          |
| time/                   |               |
|    total_timesteps      | 2236000       |
| train/                  |               |
|    approx_kl            | 0.00012433284 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.826         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+03       |
|    n_updates            | 10910         |
|    policy_gradient_loss | -0.000239     |
|    std                  | 4.38          |
|    value_loss           | 5.23e+03      |
-------------------------------------------
Eval num_timesteps=2238000, episode_reward=0.93 +/- 294.84
Episode length: 404.40 +/- 36.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 404          |
|    mean_reward          | 0.928        |
| time/                   |              |
|    total_timesteps      | 2238000      |
| train/                  |              |
|    approx_kl            | 0.0013045449 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 66.8         |
|    n_updates            | 10920        |
|    policy_gradient_loss | -0.000803    |
|    std                  | 4.39         |
|    value_loss           | 483          |
------------------------------------------
Eval num_timesteps=2240000, episode_reward=124.98 +/- 239.63
Episode length: 408.80 +/- 57.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 2240000      |
| train/                  |              |
|    approx_kl            | 0.0030342764 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 37.5         |
|    n_updates            | 10930        |
|    policy_gradient_loss | -0.00122     |
|    std                  | 4.4          |
|    value_loss           | 264          |
------------------------------------------
Eval num_timesteps=2242000, episode_reward=285.80 +/- 313.08
Episode length: 423.60 +/- 46.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 424        |
|    mean_reward          | 286        |
| time/                   |            |
|    total_timesteps      | 2242000    |
| train/                  |            |
|    approx_kl            | 0.00929278 |
|    clip_fraction        | 0.0414     |
|    clip_range           | 0.2        |
|    entropy_loss         | -11.4      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.001      |
|    loss                 | 3.17e+03   |
|    n_updates            | 10940      |
|    policy_gradient_loss | -0.000932  |
|    std                  | 4.4        |
|    value_loss           | 9.57e+03   |
----------------------------------------
Eval num_timesteps=2244000, episode_reward=232.97 +/- 230.72
Episode length: 434.00 +/- 48.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 233          |
| time/                   |              |
|    total_timesteps      | 2244000      |
| train/                  |              |
|    approx_kl            | 0.0010028076 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.859        |
|    learning_rate        | 0.001        |
|    loss                 | 3.35e+03     |
|    n_updates            | 10950        |
|    policy_gradient_loss | -0.000237    |
|    std                  | 4.41         |
|    value_loss           | 7.36e+03     |
------------------------------------------
Eval num_timesteps=2246000, episode_reward=191.50 +/- 288.96
Episode length: 399.60 +/- 28.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 191           |
| time/                   |               |
|    total_timesteps      | 2246000       |
| train/                  |               |
|    approx_kl            | 0.00024212565 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 761           |
|    n_updates            | 10960         |
|    policy_gradient_loss | -0.000336     |
|    std                  | 4.41          |
|    value_loss           | 2.05e+03      |
-------------------------------------------
Eval num_timesteps=2248000, episode_reward=90.66 +/- 356.53
Episode length: 391.80 +/- 30.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 90.7         |
| time/                   |              |
|    total_timesteps      | 2248000      |
| train/                  |              |
|    approx_kl            | 8.818446e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 2.22e+03     |
|    n_updates            | 10970        |
|    policy_gradient_loss | 1.78e-05     |
|    std                  | 4.41         |
|    value_loss           | 4.59e+03     |
------------------------------------------
Eval num_timesteps=2250000, episode_reward=196.87 +/- 34.08
Episode length: 366.00 +/- 29.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0023872964 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 101          |
|    n_updates            | 10980        |
|    policy_gradient_loss | -0.00159     |
|    std                  | 4.41         |
|    value_loss           | 399          |
------------------------------------------
Eval num_timesteps=2252000, episode_reward=360.87 +/- 240.41
Episode length: 379.40 +/- 29.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 379          |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 2252000      |
| train/                  |              |
|    approx_kl            | 0.0051480317 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.001        |
|    loss                 | 2.72e+03     |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.000384    |
|    std                  | 4.41         |
|    value_loss           | 7.13e+03     |
------------------------------------------
Eval num_timesteps=2254000, episode_reward=275.80 +/- 202.22
Episode length: 383.40 +/- 27.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 276           |
| time/                   |               |
|    total_timesteps      | 2254000       |
| train/                  |               |
|    approx_kl            | 0.00062449614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 815           |
|    n_updates            | 11000         |
|    policy_gradient_loss | -0.000203     |
|    std                  | 4.4           |
|    value_loss           | 1.86e+03      |
-------------------------------------------
Eval num_timesteps=2256000, episode_reward=485.06 +/- 349.56
Episode length: 393.20 +/- 43.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 485           |
| time/                   |               |
|    total_timesteps      | 2256000       |
| train/                  |               |
|    approx_kl            | 0.00036406814 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.4         |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 72.5          |
|    n_updates            | 11010         |
|    policy_gradient_loss | -0.000402     |
|    std                  | 4.4           |
|    value_loss           | 322           |
-------------------------------------------
Eval num_timesteps=2258000, episode_reward=402.76 +/- 198.26
Episode length: 390.40 +/- 40.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 403          |
| time/                   |              |
|    total_timesteps      | 2258000      |
| train/                  |              |
|    approx_kl            | 0.0050862115 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.4        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 37.1         |
|    n_updates            | 11020        |
|    policy_gradient_loss | -0.00267     |
|    std                  | 4.41         |
|    value_loss           | 175          |
------------------------------------------
Eval num_timesteps=2260000, episode_reward=319.23 +/- 248.66
Episode length: 378.80 +/- 26.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 319         |
| time/                   |             |
|    total_timesteps      | 2260000     |
| train/                  |             |
|    approx_kl            | 0.010706712 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.001       |
|    loss                 | 4.62e+03    |
|    n_updates            | 11030       |
|    policy_gradient_loss | -2.82e-05   |
|    std                  | 4.42        |
|    value_loss           | 1.14e+04    |
-----------------------------------------
Eval num_timesteps=2262000, episode_reward=449.79 +/- 264.64
Episode length: 379.40 +/- 20.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 379           |
|    mean_reward          | 450           |
| time/                   |               |
|    total_timesteps      | 2262000       |
| train/                  |               |
|    approx_kl            | 0.00055650726 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 825           |
|    n_updates            | 11040         |
|    policy_gradient_loss | 0.000297      |
|    std                  | 4.43          |
|    value_loss           | 2.27e+03      |
-------------------------------------------
Eval num_timesteps=2264000, episode_reward=496.01 +/- 335.08
Episode length: 404.20 +/- 35.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 2264000       |
| train/                  |               |
|    approx_kl            | 0.00034653553 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 11050         |
|    policy_gradient_loss | -0.000685     |
|    std                  | 4.43          |
|    value_loss           | 2.76e+03      |
-------------------------------------------
Eval num_timesteps=2266000, episode_reward=288.05 +/- 305.71
Episode length: 392.60 +/- 31.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 288           |
| time/                   |               |
|    total_timesteps      | 2266000       |
| train/                  |               |
|    approx_kl            | 0.00019468373 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+03      |
|    n_updates            | 11060         |
|    policy_gradient_loss | -3.05e-05     |
|    std                  | 4.43          |
|    value_loss           | 3e+03         |
-------------------------------------------
Eval num_timesteps=2268000, episode_reward=588.64 +/- 261.63
Episode length: 402.40 +/- 30.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 589          |
| time/                   |              |
|    total_timesteps      | 2268000      |
| train/                  |              |
|    approx_kl            | 0.0025931685 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 37           |
|    n_updates            | 11070        |
|    policy_gradient_loss | -0.00152     |
|    std                  | 4.44         |
|    value_loss           | 171          |
------------------------------------------
Eval num_timesteps=2270000, episode_reward=472.57 +/- 198.34
Episode length: 368.00 +/- 19.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 473         |
| time/                   |             |
|    total_timesteps      | 2270000     |
| train/                  |             |
|    approx_kl            | 0.003199636 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 625         |
|    n_updates            | 11080       |
|    policy_gradient_loss | 0.000547    |
|    std                  | 4.45        |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=2272000, episode_reward=244.01 +/- 218.04
Episode length: 391.20 +/- 46.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 391           |
|    mean_reward          | 244           |
| time/                   |               |
|    total_timesteps      | 2272000       |
| train/                  |               |
|    approx_kl            | 0.00092796463 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 38            |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000384     |
|    std                  | 4.45          |
|    value_loss           | 250           |
-------------------------------------------
Eval num_timesteps=2274000, episode_reward=246.06 +/- 334.51
Episode length: 391.40 +/- 52.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 246          |
| time/                   |              |
|    total_timesteps      | 2274000      |
| train/                  |              |
|    approx_kl            | 0.0027107666 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 27.9         |
|    n_updates            | 11100        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 4.45         |
|    value_loss           | 159          |
------------------------------------------
Eval num_timesteps=2276000, episode_reward=326.41 +/- 278.46
Episode length: 376.40 +/- 22.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 2276000     |
| train/                  |             |
|    approx_kl            | 0.006890758 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 24.6        |
|    n_updates            | 11110       |
|    policy_gradient_loss | -0.00114    |
|    std                  | 4.45        |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=2278000, episode_reward=246.79 +/- 321.36
Episode length: 365.60 +/- 38.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 247          |
| time/                   |              |
|    total_timesteps      | 2278000      |
| train/                  |              |
|    approx_kl            | 0.0011962006 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 804          |
|    n_updates            | 11120        |
|    policy_gradient_loss | 0.000226     |
|    std                  | 4.45         |
|    value_loss           | 2.32e+03     |
------------------------------------------
Eval num_timesteps=2280000, episode_reward=246.65 +/- 158.32
Episode length: 383.20 +/- 15.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 2280000     |
| train/                  |             |
|    approx_kl            | 0.001276713 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 35.8        |
|    n_updates            | 11130       |
|    policy_gradient_loss | -0.00101    |
|    std                  | 4.46        |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=2282000, episode_reward=418.97 +/- 314.66
Episode length: 417.00 +/- 62.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 417         |
|    mean_reward          | 419         |
| time/                   |             |
|    total_timesteps      | 2282000     |
| train/                  |             |
|    approx_kl            | 0.003089193 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 718         |
|    n_updates            | 11140       |
|    policy_gradient_loss | -0.000498   |
|    std                  | 4.47        |
|    value_loss           | 2.22e+03    |
-----------------------------------------
Eval num_timesteps=2284000, episode_reward=380.88 +/- 59.11
Episode length: 398.40 +/- 28.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 398           |
|    mean_reward          | 381           |
| time/                   |               |
|    total_timesteps      | 2284000       |
| train/                  |               |
|    approx_kl            | 0.00029241526 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.001         |
|    loss                 | 712           |
|    n_updates            | 11150         |
|    policy_gradient_loss | 0.000311      |
|    std                  | 4.48          |
|    value_loss           | 2.32e+03      |
-------------------------------------------
Eval num_timesteps=2286000, episode_reward=443.08 +/- 329.69
Episode length: 428.60 +/- 27.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 443          |
| time/                   |              |
|    total_timesteps      | 2286000      |
| train/                  |              |
|    approx_kl            | 7.629892e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+03     |
|    n_updates            | 11160        |
|    policy_gradient_loss | -0.000337    |
|    std                  | 4.48         |
|    value_loss           | 4.41e+03     |
------------------------------------------
Eval num_timesteps=2288000, episode_reward=278.23 +/- 285.80
Episode length: 383.40 +/- 23.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 278          |
| time/                   |              |
|    total_timesteps      | 2288000      |
| train/                  |              |
|    approx_kl            | 7.120037e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+03     |
|    n_updates            | 11170        |
|    policy_gradient_loss | -0.000205    |
|    std                  | 4.48         |
|    value_loss           | 2.82e+03     |
------------------------------------------
Eval num_timesteps=2290000, episode_reward=213.21 +/- 196.26
Episode length: 393.00 +/- 25.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 393           |
|    mean_reward          | 213           |
| time/                   |               |
|    total_timesteps      | 2290000       |
| train/                  |               |
|    approx_kl            | 0.00015204697 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+03      |
|    n_updates            | 11180         |
|    policy_gradient_loss | -0.00026      |
|    std                  | 4.48          |
|    value_loss           | 2.76e+03      |
-------------------------------------------
Eval num_timesteps=2292000, episode_reward=146.06 +/- 315.79
Episode length: 374.00 +/- 45.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 2292000      |
| train/                  |              |
|    approx_kl            | 0.0011250728 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 74.9         |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.000792    |
|    std                  | 4.48         |
|    value_loss           | 362          |
------------------------------------------
Eval num_timesteps=2294000, episode_reward=153.36 +/- 161.32
Episode length: 368.60 +/- 27.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | 153          |
| time/                   |              |
|    total_timesteps      | 2294000      |
| train/                  |              |
|    approx_kl            | 0.0020800051 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 58.8         |
|    n_updates            | 11200        |
|    policy_gradient_loss | -0.000311    |
|    std                  | 4.47         |
|    value_loss           | 353          |
------------------------------------------
Eval num_timesteps=2296000, episode_reward=236.62 +/- 224.20
Episode length: 395.60 +/- 47.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 237          |
| time/                   |              |
|    total_timesteps      | 2296000      |
| train/                  |              |
|    approx_kl            | 0.0018126501 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 792          |
|    n_updates            | 11210        |
|    policy_gradient_loss | 0.00105      |
|    std                  | 4.47         |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=2298000, episode_reward=150.99 +/- 129.29
Episode length: 409.20 +/- 37.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 409           |
|    mean_reward          | 151           |
| time/                   |               |
|    total_timesteps      | 2298000       |
| train/                  |               |
|    approx_kl            | 0.00025671837 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.001         |
|    loss                 | 2.63e+03      |
|    n_updates            | 11220         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 4.46          |
|    value_loss           | 6.93e+03      |
-------------------------------------------
Eval num_timesteps=2300000, episode_reward=373.20 +/- 390.57
Episode length: 441.00 +/- 50.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 373          |
| time/                   |              |
|    total_timesteps      | 2300000      |
| train/                  |              |
|    approx_kl            | 0.0004299342 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 710          |
|    n_updates            | 11230        |
|    policy_gradient_loss | -0.000521    |
|    std                  | 4.47         |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=2302000, episode_reward=344.85 +/- 251.55
Episode length: 429.20 +/- 41.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 2302000       |
| train/                  |               |
|    approx_kl            | 0.00075541844 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 11240         |
|    policy_gradient_loss | -0.000693     |
|    std                  | 4.48          |
|    value_loss           | 2.66e+03      |
-------------------------------------------
Eval num_timesteps=2304000, episode_reward=343.65 +/- 233.68
Episode length: 461.00 +/- 31.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 2304000  |
---------------------------------
Eval num_timesteps=2306000, episode_reward=154.35 +/- 272.94
Episode length: 419.40 +/- 46.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 154           |
| time/                   |               |
|    total_timesteps      | 2306000       |
| train/                  |               |
|    approx_kl            | 0.00027849013 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 810           |
|    n_updates            | 11250         |
|    policy_gradient_loss | -0.000257     |
|    std                  | 4.49          |
|    value_loss           | 1.9e+03       |
-------------------------------------------
Eval num_timesteps=2308000, episode_reward=149.28 +/- 340.51
Episode length: 415.20 +/- 61.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 2308000      |
| train/                  |              |
|    approx_kl            | 4.530442e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 551          |
|    n_updates            | 11260        |
|    policy_gradient_loss | -0.000153    |
|    std                  | 4.49         |
|    value_loss           | 1.83e+03     |
------------------------------------------
Eval num_timesteps=2310000, episode_reward=217.41 +/- 193.36
Episode length: 453.80 +/- 23.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 217           |
| time/                   |               |
|    total_timesteps      | 2310000       |
| train/                  |               |
|    approx_kl            | 0.00034506337 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 75.8          |
|    n_updates            | 11270         |
|    policy_gradient_loss | -0.000562     |
|    std                  | 4.5           |
|    value_loss           | 296           |
-------------------------------------------
Eval num_timesteps=2312000, episode_reward=355.77 +/- 460.81
Episode length: 419.20 +/- 38.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 356          |
| time/                   |              |
|    total_timesteps      | 2312000      |
| train/                  |              |
|    approx_kl            | 0.0005076258 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 493          |
|    n_updates            | 11280        |
|    policy_gradient_loss | -0.000517    |
|    std                  | 4.51         |
|    value_loss           | 1.06e+03     |
------------------------------------------
Eval num_timesteps=2314000, episode_reward=338.19 +/- 307.88
Episode length: 430.00 +/- 18.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 2314000       |
| train/                  |               |
|    approx_kl            | 0.00022480282 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+03      |
|    n_updates            | 11290         |
|    policy_gradient_loss | -8.32e-06     |
|    std                  | 4.52          |
|    value_loss           | 3.53e+03      |
-------------------------------------------
Eval num_timesteps=2316000, episode_reward=172.77 +/- 106.28
Episode length: 448.00 +/- 56.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 448           |
|    mean_reward          | 173           |
| time/                   |               |
|    total_timesteps      | 2316000       |
| train/                  |               |
|    approx_kl            | 2.6665337e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+03      |
|    n_updates            | 11300         |
|    policy_gradient_loss | -2.83e-05     |
|    std                  | 4.52          |
|    value_loss           | 4.5e+03       |
-------------------------------------------
Eval num_timesteps=2318000, episode_reward=122.20 +/- 307.20
Episode length: 432.00 +/- 54.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 122          |
| time/                   |              |
|    total_timesteps      | 2318000      |
| train/                  |              |
|    approx_kl            | 6.084834e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 11310        |
|    policy_gradient_loss | -7.04e-06    |
|    std                  | 4.52         |
|    value_loss           | 2.86e+03     |
------------------------------------------
Eval num_timesteps=2320000, episode_reward=279.47 +/- 160.14
Episode length: 447.20 +/- 54.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 279           |
| time/                   |               |
|    total_timesteps      | 2320000       |
| train/                  |               |
|    approx_kl            | 1.8773106e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.838         |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+03      |
|    n_updates            | 11320         |
|    policy_gradient_loss | -9.23e-05     |
|    std                  | 4.52          |
|    value_loss           | 7.04e+03      |
-------------------------------------------
Eval num_timesteps=2322000, episode_reward=243.56 +/- 198.69
Episode length: 391.60 +/- 36.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 2322000      |
| train/                  |              |
|    approx_kl            | 9.452633e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 11330        |
|    policy_gradient_loss | -0.000232    |
|    std                  | 4.53         |
|    value_loss           | 3.08e+03     |
------------------------------------------
Eval num_timesteps=2324000, episode_reward=318.88 +/- 486.82
Episode length: 463.00 +/- 58.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 463           |
|    mean_reward          | 319           |
| time/                   |               |
|    total_timesteps      | 2324000       |
| train/                  |               |
|    approx_kl            | 9.7333104e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.5         |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.001         |
|    loss                 | 863           |
|    n_updates            | 11340         |
|    policy_gradient_loss | -0.000294     |
|    std                  | 4.53          |
|    value_loss           | 2.83e+03      |
-------------------------------------------
Eval num_timesteps=2326000, episode_reward=474.35 +/- 176.04
Episode length: 434.80 +/- 48.05
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 435            |
|    mean_reward          | 474            |
| time/                   |                |
|    total_timesteps      | 2326000        |
| train/                  |                |
|    approx_kl            | 0.000115809176 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -11.5          |
|    explained_variance   | 0.93           |
|    learning_rate        | 0.001          |
|    loss                 | 970            |
|    n_updates            | 11350          |
|    policy_gradient_loss | -2.23e-06      |
|    std                  | 4.53           |
|    value_loss           | 2.58e+03       |
--------------------------------------------
Eval num_timesteps=2328000, episode_reward=356.81 +/- 107.13
Episode length: 450.00 +/- 48.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 357          |
| time/                   |              |
|    total_timesteps      | 2328000      |
| train/                  |              |
|    approx_kl            | 0.0006718776 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 237          |
|    n_updates            | 11360        |
|    policy_gradient_loss | -0.000821    |
|    std                  | 4.53         |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=2330000, episode_reward=209.79 +/- 182.79
Episode length: 438.60 +/- 44.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 210          |
| time/                   |              |
|    total_timesteps      | 2330000      |
| train/                  |              |
|    approx_kl            | 0.0027854783 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 60.9         |
|    n_updates            | 11370        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 4.53         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=2332000, episode_reward=289.70 +/- 352.05
Episode length: 417.60 +/- 92.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 2332000     |
| train/                  |             |
|    approx_kl            | 0.009393893 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 892         |
|    n_updates            | 11380       |
|    policy_gradient_loss | -0.000264   |
|    std                  | 4.54        |
|    value_loss           | 2.4e+03     |
-----------------------------------------
Eval num_timesteps=2334000, episode_reward=387.66 +/- 336.95
Episode length: 435.00 +/- 59.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 2334000      |
| train/                  |              |
|    approx_kl            | 0.0007532886 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.5        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 978          |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.000116    |
|    std                  | 4.54         |
|    value_loss           | 2.68e+03     |
------------------------------------------
Eval num_timesteps=2336000, episode_reward=416.08 +/- 209.75
Episode length: 409.60 +/- 22.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 2336000     |
| train/                  |             |
|    approx_kl            | 0.010141234 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.001       |
|    loss                 | 32.3        |
|    n_updates            | 11400       |
|    policy_gradient_loss | -0.00258    |
|    std                  | 4.54        |
|    value_loss           | 107         |
-----------------------------------------
Eval num_timesteps=2338000, episode_reward=174.83 +/- 273.80
Episode length: 456.00 +/- 23.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 456           |
|    mean_reward          | 175           |
| time/                   |               |
|    total_timesteps      | 2338000       |
| train/                  |               |
|    approx_kl            | 0.00057427975 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 11410         |
|    policy_gradient_loss | -5.15e-05     |
|    std                  | 4.54          |
|    value_loss           | 2.44e+03      |
-------------------------------------------
Eval num_timesteps=2340000, episode_reward=501.77 +/- 477.30
Episode length: 436.80 +/- 44.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 2340000      |
| train/                  |              |
|    approx_kl            | 0.0003532215 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 11420        |
|    policy_gradient_loss | -0.000338    |
|    std                  | 4.55         |
|    value_loss           | 706          |
------------------------------------------
Eval num_timesteps=2342000, episode_reward=305.97 +/- 169.20
Episode length: 450.80 +/- 61.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 2342000      |
| train/                  |              |
|    approx_kl            | 0.0005020501 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 47.8         |
|    n_updates            | 11430        |
|    policy_gradient_loss | -0.000322    |
|    std                  | 4.55         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=2344000, episode_reward=292.13 +/- 182.67
Episode length: 442.60 +/- 46.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 2344000      |
| train/                  |              |
|    approx_kl            | 0.0015410082 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 516          |
|    n_updates            | 11440        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 4.56         |
|    value_loss           | 1.9e+03      |
------------------------------------------
Eval num_timesteps=2346000, episode_reward=312.30 +/- 365.73
Episode length: 458.80 +/- 49.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 2346000      |
| train/                  |              |
|    approx_kl            | 0.0015470979 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 94.7         |
|    n_updates            | 11450        |
|    policy_gradient_loss | -0.000905    |
|    std                  | 4.57         |
|    value_loss           | 583          |
------------------------------------------
Eval num_timesteps=2348000, episode_reward=220.60 +/- 217.82
Episode length: 455.20 +/- 56.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 2348000      |
| train/                  |              |
|    approx_kl            | 0.0018596424 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.001        |
|    loss                 | 2.34e+03     |
|    n_updates            | 11460        |
|    policy_gradient_loss | 0.00124      |
|    std                  | 4.58         |
|    value_loss           | 5.96e+03     |
------------------------------------------
Eval num_timesteps=2350000, episode_reward=423.82 +/- 262.31
Episode length: 466.00 +/- 69.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 2350000      |
| train/                  |              |
|    approx_kl            | 3.816816e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+03      |
|    n_updates            | 11470        |
|    policy_gradient_loss | 8.78e-05     |
|    std                  | 4.58         |
|    value_loss           | 4.27e+03     |
------------------------------------------
Eval num_timesteps=2352000, episode_reward=510.67 +/- 354.75
Episode length: 461.40 +/- 40.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 461           |
|    mean_reward          | 511           |
| time/                   |               |
|    total_timesteps      | 2352000       |
| train/                  |               |
|    approx_kl            | 1.9651081e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+03      |
|    n_updates            | 11480         |
|    policy_gradient_loss | 0.00018       |
|    std                  | 4.58          |
|    value_loss           | 4.91e+03      |
-------------------------------------------
Eval num_timesteps=2354000, episode_reward=369.71 +/- 129.52
Episode length: 436.60 +/- 45.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 437           |
|    mean_reward          | 370           |
| time/                   |               |
|    total_timesteps      | 2354000       |
| train/                  |               |
|    approx_kl            | 0.00015496262 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.001         |
|    loss                 | 809           |
|    n_updates            | 11490         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 4.59          |
|    value_loss           | 2.15e+03      |
-------------------------------------------
Eval num_timesteps=2356000, episode_reward=165.66 +/- 271.36
Episode length: 434.00 +/- 31.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 166          |
| time/                   |              |
|    total_timesteps      | 2356000      |
| train/                  |              |
|    approx_kl            | 0.0054044295 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 19.4         |
|    n_updates            | 11500        |
|    policy_gradient_loss | -0.00301     |
|    std                  | 4.58         |
|    value_loss           | 92.8         |
------------------------------------------
Eval num_timesteps=2358000, episode_reward=223.05 +/- 178.22
Episode length: 462.20 +/- 84.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 2358000      |
| train/                  |              |
|    approx_kl            | 0.0038485117 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 67           |
|    n_updates            | 11510        |
|    policy_gradient_loss | -0.000749    |
|    std                  | 4.58         |
|    value_loss           | 269          |
------------------------------------------
Eval num_timesteps=2360000, episode_reward=78.14 +/- 66.49
Episode length: 416.60 +/- 53.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 417           |
|    mean_reward          | 78.1          |
| time/                   |               |
|    total_timesteps      | 2360000       |
| train/                  |               |
|    approx_kl            | 0.00045940944 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.6         |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 87.9          |
|    n_updates            | 11520         |
|    policy_gradient_loss | 0.000366      |
|    std                  | 4.58          |
|    value_loss           | 394           |
-------------------------------------------
Eval num_timesteps=2362000, episode_reward=83.63 +/- 196.56
Episode length: 448.00 +/- 59.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 83.6         |
| time/                   |              |
|    total_timesteps      | 2362000      |
| train/                  |              |
|    approx_kl            | 0.0019859434 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 128          |
|    n_updates            | 11530        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 4.6          |
|    value_loss           | 547          |
------------------------------------------
Eval num_timesteps=2364000, episode_reward=391.64 +/- 353.56
Episode length: 400.20 +/- 33.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 392          |
| time/                   |              |
|    total_timesteps      | 2364000      |
| train/                  |              |
|    approx_kl            | 0.0065415506 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 24.3         |
|    n_updates            | 11540        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 4.63         |
|    value_loss           | 87.3         |
------------------------------------------
Eval num_timesteps=2366000, episode_reward=393.69 +/- 457.90
Episode length: 402.40 +/- 41.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 394         |
| time/                   |             |
|    total_timesteps      | 2366000     |
| train/                  |             |
|    approx_kl            | 0.005923539 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 15.4        |
|    n_updates            | 11550       |
|    policy_gradient_loss | -0.00131    |
|    std                  | 4.64        |
|    value_loss           | 73          |
-----------------------------------------
Eval num_timesteps=2368000, episode_reward=220.84 +/- 113.49
Episode length: 411.00 +/- 61.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 2368000      |
| train/                  |              |
|    approx_kl            | 0.0019245199 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 30.8         |
|    n_updates            | 11560        |
|    policy_gradient_loss | -0.000484    |
|    std                  | 4.66         |
|    value_loss           | 109          |
------------------------------------------
Eval num_timesteps=2370000, episode_reward=341.50 +/- 363.62
Episode length: 430.60 +/- 51.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 2370000      |
| train/                  |              |
|    approx_kl            | 0.0034327398 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.6        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 45.3         |
|    n_updates            | 11570        |
|    policy_gradient_loss | -0.000986    |
|    std                  | 4.68         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=2372000, episode_reward=328.00 +/- 186.49
Episode length: 437.40 +/- 32.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 328          |
| time/                   |              |
|    total_timesteps      | 2372000      |
| train/                  |              |
|    approx_kl            | 0.0025693174 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 15.6         |
|    n_updates            | 11580        |
|    policy_gradient_loss | -0.00183     |
|    std                  | 4.75         |
|    value_loss           | 82.6         |
------------------------------------------
Eval num_timesteps=2374000, episode_reward=179.73 +/- 243.55
Episode length: 420.20 +/- 33.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 2374000      |
| train/                  |              |
|    approx_kl            | 0.0036030854 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.7        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 26.7         |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.000493    |
|    std                  | 4.81         |
|    value_loss           | 94.7         |
------------------------------------------
Eval num_timesteps=2376000, episode_reward=378.37 +/- 229.51
Episode length: 416.60 +/- 36.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 378          |
| time/                   |              |
|    total_timesteps      | 2376000      |
| train/                  |              |
|    approx_kl            | 0.0013921746 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 940          |
|    n_updates            | 11600        |
|    policy_gradient_loss | 0.000443     |
|    std                  | 4.84         |
|    value_loss           | 3.38e+03     |
------------------------------------------
Eval num_timesteps=2378000, episode_reward=332.87 +/- 237.06
Episode length: 418.60 +/- 36.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | 333         |
| time/                   |             |
|    total_timesteps      | 2378000     |
| train/                  |             |
|    approx_kl            | 0.000257734 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.001       |
|    loss                 | 1.5e+03     |
|    n_updates            | 11610       |
|    policy_gradient_loss | -0.0007     |
|    std                  | 4.85        |
|    value_loss           | 3.72e+03    |
-----------------------------------------
Eval num_timesteps=2380000, episode_reward=248.85 +/- 146.08
Episode length: 388.00 +/- 28.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 249           |
| time/                   |               |
|    total_timesteps      | 2380000       |
| train/                  |               |
|    approx_kl            | 0.00060281693 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 66.7          |
|    n_updates            | 11620         |
|    policy_gradient_loss | -0.000758     |
|    std                  | 4.85          |
|    value_loss           | 328           |
-------------------------------------------
Eval num_timesteps=2382000, episode_reward=439.09 +/- 269.96
Episode length: 405.80 +/- 29.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 406          |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 2382000      |
| train/                  |              |
|    approx_kl            | 0.0008904009 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+03     |
|    n_updates            | 11630        |
|    policy_gradient_loss | 0.00139      |
|    std                  | 4.86         |
|    value_loss           | 8.59e+03     |
------------------------------------------
Eval num_timesteps=2384000, episode_reward=428.89 +/- 221.56
Episode length: 382.80 +/- 41.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 429           |
| time/                   |               |
|    total_timesteps      | 2384000       |
| train/                  |               |
|    approx_kl            | 0.00023700666 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 85.9          |
|    n_updates            | 11640         |
|    policy_gradient_loss | -0.000516     |
|    std                  | 4.86          |
|    value_loss           | 446           |
-------------------------------------------
Eval num_timesteps=2386000, episode_reward=252.55 +/- 190.61
Episode length: 363.40 +/- 30.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 363           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 2386000       |
| train/                  |               |
|    approx_kl            | 0.00036245596 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+03      |
|    n_updates            | 11650         |
|    policy_gradient_loss | -0.000173     |
|    std                  | 4.86          |
|    value_loss           | 6.53e+03      |
-------------------------------------------
Eval num_timesteps=2388000, episode_reward=457.59 +/- 209.50
Episode length: 348.00 +/- 19.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 348          |
|    mean_reward          | 458          |
| time/                   |              |
|    total_timesteps      | 2388000      |
| train/                  |              |
|    approx_kl            | 0.0012639257 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 11660        |
|    policy_gradient_loss | -0.000856    |
|    std                  | 4.86         |
|    value_loss           | 479          |
------------------------------------------
Eval num_timesteps=2390000, episode_reward=380.63 +/- 234.45
Episode length: 370.40 +/- 24.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 370      |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 2390000  |
---------------------------------
Eval num_timesteps=2392000, episode_reward=320.93 +/- 200.97
Episode length: 361.20 +/- 34.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 361          |
|    mean_reward          | 321          |
| time/                   |              |
|    total_timesteps      | 2392000      |
| train/                  |              |
|    approx_kl            | 0.0023754293 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 2.96e+03     |
|    n_updates            | 11670        |
|    policy_gradient_loss | -0.000964    |
|    std                  | 4.87         |
|    value_loss           | 8.07e+03     |
------------------------------------------
Eval num_timesteps=2394000, episode_reward=335.76 +/- 147.94
Episode length: 358.20 +/- 22.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 358           |
|    mean_reward          | 336           |
| time/                   |               |
|    total_timesteps      | 2394000       |
| train/                  |               |
|    approx_kl            | 0.00064178486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.946         |
|    learning_rate        | 0.001         |
|    loss                 | 262           |
|    n_updates            | 11680         |
|    policy_gradient_loss | -0.000369     |
|    std                  | 4.87          |
|    value_loss           | 1.09e+03      |
-------------------------------------------
Eval num_timesteps=2396000, episode_reward=248.80 +/- 209.56
Episode length: 336.00 +/- 18.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 336          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 2396000      |
| train/                  |              |
|    approx_kl            | 0.0020425364 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 62           |
|    n_updates            | 11690        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 4.89         |
|    value_loss           | 280          |
------------------------------------------
Eval num_timesteps=2398000, episode_reward=116.02 +/- 43.70
Episode length: 327.80 +/- 19.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 328           |
|    mean_reward          | 116           |
| time/                   |               |
|    total_timesteps      | 2398000       |
| train/                  |               |
|    approx_kl            | 0.00037346195 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -11.8         |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.001         |
|    loss                 | 29.2          |
|    n_updates            | 11700         |
|    policy_gradient_loss | -8.27e-05     |
|    std                  | 4.92          |
|    value_loss           | 144           |
-------------------------------------------
Eval num_timesteps=2400000, episode_reward=159.23 +/- 131.70
Episode length: 318.40 +/- 31.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 2400000      |
| train/                  |              |
|    approx_kl            | 0.0032470697 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 37           |
|    n_updates            | 11710        |
|    policy_gradient_loss | -0.000862    |
|    std                  | 4.95         |
|    value_loss           | 177          |
------------------------------------------
Eval num_timesteps=2402000, episode_reward=403.59 +/- 283.69
Episode length: 351.00 +/- 29.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 404         |
| time/                   |             |
|    total_timesteps      | 2402000     |
| train/                  |             |
|    approx_kl            | 0.002686586 |
|    clip_fraction        | 0.00273     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.001       |
|    loss                 | 2.75e+03    |
|    n_updates            | 11720       |
|    policy_gradient_loss | 0.000248    |
|    std                  | 4.97        |
|    value_loss           | 7.3e+03     |
-----------------------------------------
Eval num_timesteps=2404000, episode_reward=151.42 +/- 167.14
Episode length: 325.20 +/- 28.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 2404000     |
| train/                  |             |
|    approx_kl            | 0.003046251 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 43.9        |
|    n_updates            | 11730       |
|    policy_gradient_loss | -0.00116    |
|    std                  | 4.99        |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=2406000, episode_reward=374.21 +/- 234.14
Episode length: 374.60 +/- 23.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 374          |
| time/                   |              |
|    total_timesteps      | 2406000      |
| train/                  |              |
|    approx_kl            | 0.0076868073 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.8        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 32.6         |
|    n_updates            | 11740        |
|    policy_gradient_loss | -0.00249     |
|    std                  | 5            |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=2408000, episode_reward=347.74 +/- 268.63
Episode length: 357.80 +/- 40.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 358          |
|    mean_reward          | 348          |
| time/                   |              |
|    total_timesteps      | 2408000      |
| train/                  |              |
|    approx_kl            | 0.0045201015 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 93           |
|    n_updates            | 11750        |
|    policy_gradient_loss | -0.00096     |
|    std                  | 5.02         |
|    value_loss           | 354          |
------------------------------------------
Eval num_timesteps=2410000, episode_reward=271.81 +/- 277.67
Episode length: 395.20 +/- 49.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 2410000      |
| train/                  |              |
|    approx_kl            | 0.0029283026 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 18.6         |
|    n_updates            | 11760        |
|    policy_gradient_loss | -0.000924    |
|    std                  | 5.01         |
|    value_loss           | 89.4         |
------------------------------------------
Eval num_timesteps=2412000, episode_reward=470.26 +/- 219.31
Episode length: 409.60 +/- 40.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | 470         |
| time/                   |             |
|    total_timesteps      | 2412000     |
| train/                  |             |
|    approx_kl            | 0.003080654 |
|    clip_fraction        | 0.00493     |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 24.1        |
|    n_updates            | 11770       |
|    policy_gradient_loss | -0.000246   |
|    std                  | 5.03        |
|    value_loss           | 118         |
-----------------------------------------
Eval num_timesteps=2414000, episode_reward=541.00 +/- 325.55
Episode length: 405.40 +/- 75.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 541         |
| time/                   |             |
|    total_timesteps      | 2414000     |
| train/                  |             |
|    approx_kl            | 0.008395257 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 39.9        |
|    n_updates            | 11780       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 5.06        |
|    value_loss           | 225         |
-----------------------------------------
Eval num_timesteps=2416000, episode_reward=352.85 +/- 350.47
Episode length: 377.20 +/- 38.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 353          |
| time/                   |              |
|    total_timesteps      | 2416000      |
| train/                  |              |
|    approx_kl            | 0.0005269626 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -11.9        |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 11790        |
|    policy_gradient_loss | 4.75e-05     |
|    std                  | 5.07         |
|    value_loss           | 3.6e+03      |
------------------------------------------
Eval num_timesteps=2418000, episode_reward=147.54 +/- 306.90
Episode length: 407.80 +/- 44.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 148           |
| time/                   |               |
|    total_timesteps      | 2418000       |
| train/                  |               |
|    approx_kl            | 0.00070358254 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.001         |
|    loss                 | 33.6          |
|    n_updates            | 11800         |
|    policy_gradient_loss | -0.000742     |
|    std                  | 5.07          |
|    value_loss           | 148           |
-------------------------------------------
Eval num_timesteps=2420000, episode_reward=458.62 +/- 372.69
Episode length: 496.80 +/- 162.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 459         |
| time/                   |             |
|    total_timesteps      | 2420000     |
| train/                  |             |
|    approx_kl            | 0.005812552 |
|    clip_fraction        | 0.00889     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 30.4        |
|    n_updates            | 11810       |
|    policy_gradient_loss | -0.00221    |
|    std                  | 5.08        |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=2422000, episode_reward=340.72 +/- 273.38
Episode length: 450.20 +/- 57.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 2422000      |
| train/                  |              |
|    approx_kl            | 0.0057299575 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+03     |
|    n_updates            | 11820        |
|    policy_gradient_loss | 0.00134      |
|    std                  | 5.09         |
|    value_loss           | 9.21e+03     |
------------------------------------------
Eval num_timesteps=2424000, episode_reward=468.67 +/- 356.96
Episode length: 474.20 +/- 60.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | 469         |
| time/                   |             |
|    total_timesteps      | 2424000     |
| train/                  |             |
|    approx_kl            | 0.001263517 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.001       |
|    loss                 | 3.39e+03    |
|    n_updates            | 11830       |
|    policy_gradient_loss | -0.00198    |
|    std                  | 5.09        |
|    value_loss           | 8.54e+03    |
-----------------------------------------
Eval num_timesteps=2426000, episode_reward=131.41 +/- 49.15
Episode length: 410.00 +/- 57.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 131          |
| time/                   |              |
|    total_timesteps      | 2426000      |
| train/                  |              |
|    approx_kl            | 0.0031125415 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 159          |
|    n_updates            | 11840        |
|    policy_gradient_loss | -0.00176     |
|    std                  | 5.11         |
|    value_loss           | 850          |
------------------------------------------
Eval num_timesteps=2428000, episode_reward=138.12 +/- 92.06
Episode length: 429.60 +/- 56.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 138          |
| time/                   |              |
|    total_timesteps      | 2428000      |
| train/                  |              |
|    approx_kl            | 0.0042833365 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 130          |
|    n_updates            | 11850        |
|    policy_gradient_loss | -0.000615    |
|    std                  | 5.12         |
|    value_loss           | 482          |
------------------------------------------
Eval num_timesteps=2430000, episode_reward=214.62 +/- 173.64
Episode length: 494.40 +/- 38.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | 215          |
| time/                   |              |
|    total_timesteps      | 2430000      |
| train/                  |              |
|    approx_kl            | 0.0008174343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.001        |
|    loss                 | 2.11e+03     |
|    n_updates            | 11860        |
|    policy_gradient_loss | -0.000418    |
|    std                  | 5.12         |
|    value_loss           | 5.55e+03     |
------------------------------------------
Eval num_timesteps=2432000, episode_reward=287.04 +/- 429.28
Episode length: 433.80 +/- 56.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 2432000      |
| train/                  |              |
|    approx_kl            | 0.0009347952 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 136          |
|    n_updates            | 11870        |
|    policy_gradient_loss | -0.000873    |
|    std                  | 5.12         |
|    value_loss           | 469          |
------------------------------------------
Eval num_timesteps=2434000, episode_reward=284.73 +/- 145.42
Episode length: 476.00 +/- 51.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 285          |
| time/                   |              |
|    total_timesteps      | 2434000      |
| train/                  |              |
|    approx_kl            | 0.0018398054 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 11880        |
|    policy_gradient_loss | -0.000947    |
|    std                  | 5.11         |
|    value_loss           | 684          |
------------------------------------------
Eval num_timesteps=2436000, episode_reward=270.54 +/- 397.55
Episode length: 486.20 +/- 41.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 2436000      |
| train/                  |              |
|    approx_kl            | 0.0011298945 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+03     |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.000386    |
|    std                  | 5.11         |
|    value_loss           | 7.18e+03     |
------------------------------------------
Eval num_timesteps=2438000, episode_reward=217.66 +/- 162.22
Episode length: 498.40 +/- 12.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 218          |
| time/                   |              |
|    total_timesteps      | 2438000      |
| train/                  |              |
|    approx_kl            | 6.231878e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 130          |
|    n_updates            | 11900        |
|    policy_gradient_loss | 7.38e-05     |
|    std                  | 5.11         |
|    value_loss           | 663          |
------------------------------------------
Eval num_timesteps=2440000, episode_reward=269.09 +/- 212.49
Episode length: 497.00 +/- 14.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 2440000      |
| train/                  |              |
|    approx_kl            | 5.384165e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 940          |
|    n_updates            | 11910        |
|    policy_gradient_loss | -2.6e-05     |
|    std                  | 5.11         |
|    value_loss           | 2.66e+03     |
------------------------------------------
Eval num_timesteps=2442000, episode_reward=120.99 +/- 305.44
Episode length: 468.80 +/- 41.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | 121           |
| time/                   |               |
|    total_timesteps      | 2442000       |
| train/                  |               |
|    approx_kl            | 0.00018660532 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.001         |
|    loss                 | 790           |
|    n_updates            | 11920         |
|    policy_gradient_loss | -8.13e-05     |
|    std                  | 5.11          |
|    value_loss           | 2.78e+03      |
-------------------------------------------
Eval num_timesteps=2444000, episode_reward=212.38 +/- 296.96
Episode length: 437.20 +/- 48.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 437           |
|    mean_reward          | 212           |
| time/                   |               |
|    total_timesteps      | 2444000       |
| train/                  |               |
|    approx_kl            | 0.00024132273 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 80            |
|    n_updates            | 11930         |
|    policy_gradient_loss | -0.000339     |
|    std                  | 5.11          |
|    value_loss           | 275           |
-------------------------------------------
Eval num_timesteps=2446000, episode_reward=283.87 +/- 339.92
Episode length: 427.20 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 2446000      |
| train/                  |              |
|    approx_kl            | 0.0034904545 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 70           |
|    n_updates            | 11940        |
|    policy_gradient_loss | -0.00223     |
|    std                  | 5.1          |
|    value_loss           | 274          |
------------------------------------------
Eval num_timesteps=2448000, episode_reward=458.14 +/- 423.41
Episode length: 467.80 +/- 55.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 458          |
| time/                   |              |
|    total_timesteps      | 2448000      |
| train/                  |              |
|    approx_kl            | 0.0028482166 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 11950        |
|    policy_gradient_loss | -0.000144    |
|    std                  | 5.1          |
|    value_loss           | 3.44e+03     |
------------------------------------------
Eval num_timesteps=2450000, episode_reward=382.24 +/- 596.76
Episode length: 454.20 +/- 64.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 2450000      |
| train/                  |              |
|    approx_kl            | 0.0007615524 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 118          |
|    n_updates            | 11960        |
|    policy_gradient_loss | -0.000469    |
|    std                  | 5.09         |
|    value_loss           | 443          |
------------------------------------------
Eval num_timesteps=2452000, episode_reward=588.28 +/- 237.35
Episode length: 463.80 +/- 31.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 464          |
|    mean_reward          | 588          |
| time/                   |              |
|    total_timesteps      | 2452000      |
| train/                  |              |
|    approx_kl            | 0.0019094661 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 59.7         |
|    n_updates            | 11970        |
|    policy_gradient_loss | -0.000892    |
|    std                  | 5.1          |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=2454000, episode_reward=287.80 +/- 282.75
Episode length: 407.00 +/- 53.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 288          |
| time/                   |              |
|    total_timesteps      | 2454000      |
| train/                  |              |
|    approx_kl            | 0.0017652642 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.758        |
|    learning_rate        | 0.001        |
|    loss                 | 2.67e+03     |
|    n_updates            | 11980        |
|    policy_gradient_loss | 0.000822     |
|    std                  | 5.11         |
|    value_loss           | 8.98e+03     |
------------------------------------------
Eval num_timesteps=2456000, episode_reward=466.28 +/- 551.22
Episode length: 410.40 +/- 29.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 2456000      |
| train/                  |              |
|    approx_kl            | 0.0003540755 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 31.1         |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.000335    |
|    std                  | 5.12         |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=2458000, episode_reward=333.67 +/- 214.66
Episode length: 420.60 +/- 36.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 421          |
|    mean_reward          | 334          |
| time/                   |              |
|    total_timesteps      | 2458000      |
| train/                  |              |
|    approx_kl            | 0.0009868252 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+03     |
|    n_updates            | 12000        |
|    policy_gradient_loss | 0.000459     |
|    std                  | 5.13         |
|    value_loss           | 3.15e+03     |
------------------------------------------
Eval num_timesteps=2460000, episode_reward=318.26 +/- 137.45
Episode length: 413.00 +/- 72.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 413           |
|    mean_reward          | 318           |
| time/                   |               |
|    total_timesteps      | 2460000       |
| train/                  |               |
|    approx_kl            | 0.00021240124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+03      |
|    n_updates            | 12010         |
|    policy_gradient_loss | -0.000156     |
|    std                  | 5.13          |
|    value_loss           | 4.94e+03      |
-------------------------------------------
Eval num_timesteps=2462000, episode_reward=126.72 +/- 321.05
Episode length: 443.80 +/- 42.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 127           |
| time/                   |               |
|    total_timesteps      | 2462000       |
| train/                  |               |
|    approx_kl            | 0.00058945315 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 34.8          |
|    n_updates            | 12020         |
|    policy_gradient_loss | -0.000544     |
|    std                  | 5.14          |
|    value_loss           | 256           |
-------------------------------------------
Eval num_timesteps=2464000, episode_reward=285.51 +/- 180.09
Episode length: 419.40 +/- 39.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 419         |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 2464000     |
| train/                  |             |
|    approx_kl            | 0.001233431 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.001       |
|    loss                 | 1.34e+03    |
|    n_updates            | 12030       |
|    policy_gradient_loss | -3.63e-05   |
|    std                  | 5.15        |
|    value_loss           | 4.37e+03    |
-----------------------------------------
Eval num_timesteps=2466000, episode_reward=172.87 +/- 341.18
Episode length: 411.40 +/- 33.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 2466000      |
| train/                  |              |
|    approx_kl            | 0.0007959282 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 43.1         |
|    n_updates            | 12040        |
|    policy_gradient_loss | -0.000666    |
|    std                  | 5.18         |
|    value_loss           | 230          |
------------------------------------------
Eval num_timesteps=2468000, episode_reward=26.90 +/- 266.25
Episode length: 430.40 +/- 38.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 430         |
|    mean_reward          | 26.9        |
| time/                   |             |
|    total_timesteps      | 2468000     |
| train/                  |             |
|    approx_kl            | 0.002306275 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 70.3        |
|    n_updates            | 12050       |
|    policy_gradient_loss | -0.00021    |
|    std                  | 5.2         |
|    value_loss           | 316         |
-----------------------------------------
Eval num_timesteps=2470000, episode_reward=276.19 +/- 295.69
Episode length: 364.60 +/- 36.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 276          |
| time/                   |              |
|    total_timesteps      | 2470000      |
| train/                  |              |
|    approx_kl            | 0.0031855414 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 94.4         |
|    n_updates            | 12060        |
|    policy_gradient_loss | -0.00235     |
|    std                  | 5.21         |
|    value_loss           | 419          |
------------------------------------------
Eval num_timesteps=2472000, episode_reward=171.15 +/- 148.22
Episode length: 434.20 +/- 77.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 2472000      |
| train/                  |              |
|    approx_kl            | 0.0030744672 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+03     |
|    n_updates            | 12070        |
|    policy_gradient_loss | -0.000448    |
|    std                  | 5.22         |
|    value_loss           | 5.53e+03     |
------------------------------------------
Eval num_timesteps=2474000, episode_reward=207.23 +/- 197.27
Episode length: 385.20 +/- 40.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 385           |
|    mean_reward          | 207           |
| time/                   |               |
|    total_timesteps      | 2474000       |
| train/                  |               |
|    approx_kl            | 0.00025371744 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 948           |
|    n_updates            | 12080         |
|    policy_gradient_loss | 6.62e-05      |
|    std                  | 5.22          |
|    value_loss           | 3.61e+03      |
-------------------------------------------
Eval num_timesteps=2476000, episode_reward=478.16 +/- 148.54
Episode length: 388.80 +/- 43.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 2476000  |
---------------------------------
Eval num_timesteps=2478000, episode_reward=112.80 +/- 151.89
Episode length: 352.40 +/- 41.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 352           |
|    mean_reward          | 113           |
| time/                   |               |
|    total_timesteps      | 2478000       |
| train/                  |               |
|    approx_kl            | 0.00017423838 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.867         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 12090         |
|    policy_gradient_loss | -0.000225     |
|    std                  | 5.22          |
|    value_loss           | 4.14e+03      |
-------------------------------------------
Eval num_timesteps=2480000, episode_reward=388.70 +/- 224.52
Episode length: 414.80 +/- 56.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 389          |
| time/                   |              |
|    total_timesteps      | 2480000      |
| train/                  |              |
|    approx_kl            | 0.0018302009 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+03     |
|    n_updates            | 12100        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 5.23         |
|    value_loss           | 4.6e+03      |
------------------------------------------
Eval num_timesteps=2482000, episode_reward=132.48 +/- 255.37
Episode length: 398.80 +/- 51.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 132          |
| time/                   |              |
|    total_timesteps      | 2482000      |
| train/                  |              |
|    approx_kl            | 0.0020733075 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 198          |
|    n_updates            | 12110        |
|    policy_gradient_loss | -0.000974    |
|    std                  | 5.23         |
|    value_loss           | 785          |
------------------------------------------
Eval num_timesteps=2484000, episode_reward=295.05 +/- 245.04
Episode length: 452.00 +/- 53.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 295          |
| time/                   |              |
|    total_timesteps      | 2484000      |
| train/                  |              |
|    approx_kl            | 0.0015144406 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 79.7         |
|    n_updates            | 12120        |
|    policy_gradient_loss | -0.000107    |
|    std                  | 5.22         |
|    value_loss           | 447          |
------------------------------------------
Eval num_timesteps=2486000, episode_reward=8.22 +/- 210.40
Episode length: 414.00 +/- 64.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 8.22          |
| time/                   |               |
|    total_timesteps      | 2486000       |
| train/                  |               |
|    approx_kl            | 0.00029639518 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.805         |
|    learning_rate        | 0.001         |
|    loss                 | 3.8e+03       |
|    n_updates            | 12130         |
|    policy_gradient_loss | 5.74e-05      |
|    std                  | 5.22          |
|    value_loss           | 9.32e+03      |
-------------------------------------------
Eval num_timesteps=2488000, episode_reward=265.42 +/- 236.93
Episode length: 410.40 +/- 64.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 265          |
| time/                   |              |
|    total_timesteps      | 2488000      |
| train/                  |              |
|    approx_kl            | 0.0017268115 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 12140        |
|    policy_gradient_loss | -0.000971    |
|    std                  | 5.22         |
|    value_loss           | 526          |
------------------------------------------
Eval num_timesteps=2490000, episode_reward=115.22 +/- 104.60
Episode length: 411.60 +/- 60.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 115          |
| time/                   |              |
|    total_timesteps      | 2490000      |
| train/                  |              |
|    approx_kl            | 0.0045506703 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 93.2         |
|    n_updates            | 12150        |
|    policy_gradient_loss | -0.00146     |
|    std                  | 5.21         |
|    value_loss           | 306          |
------------------------------------------
Eval num_timesteps=2492000, episode_reward=453.67 +/- 248.47
Episode length: 422.80 +/- 39.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 454          |
| time/                   |              |
|    total_timesteps      | 2492000      |
| train/                  |              |
|    approx_kl            | 0.0050547845 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 79           |
|    n_updates            | 12160        |
|    policy_gradient_loss | 0.000794     |
|    std                  | 5.21         |
|    value_loss           | 475          |
------------------------------------------
Eval num_timesteps=2494000, episode_reward=324.32 +/- 382.90
Episode length: 479.80 +/- 51.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 480           |
|    mean_reward          | 324           |
| time/                   |               |
|    total_timesteps      | 2494000       |
| train/                  |               |
|    approx_kl            | 0.00045255732 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.801         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+03      |
|    n_updates            | 12170         |
|    policy_gradient_loss | -9e-05        |
|    std                  | 5.21          |
|    value_loss           | 4.96e+03      |
-------------------------------------------
Eval num_timesteps=2496000, episode_reward=243.40 +/- 166.94
Episode length: 429.00 +/- 66.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 243           |
| time/                   |               |
|    total_timesteps      | 2496000       |
| train/                  |               |
|    approx_kl            | 4.2542553e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12           |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+03      |
|    n_updates            | 12180         |
|    policy_gradient_loss | 7.28e-05      |
|    std                  | 5.21          |
|    value_loss           | 3.06e+03      |
-------------------------------------------
Eval num_timesteps=2498000, episode_reward=216.64 +/- 390.37
Episode length: 440.60 +/- 81.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 217          |
| time/                   |              |
|    total_timesteps      | 2498000      |
| train/                  |              |
|    approx_kl            | 0.0018587764 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 34.3         |
|    n_updates            | 12190        |
|    policy_gradient_loss | -0.000772    |
|    std                  | 5.21         |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=2500000, episode_reward=278.55 +/- 273.28
Episode length: 413.20 +/- 46.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 279          |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 0.0037105961 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 450          |
|    n_updates            | 12200        |
|    policy_gradient_loss | -0.000185    |
|    std                  | 5.19         |
|    value_loss           | 2.09e+03     |
------------------------------------------
Eval num_timesteps=2502000, episode_reward=419.48 +/- 412.52
Episode length: 459.80 +/- 56.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 2502000      |
| train/                  |              |
|    approx_kl            | 0.0030092464 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 118          |
|    n_updates            | 12210        |
|    policy_gradient_loss | -0.00171     |
|    std                  | 5.19         |
|    value_loss           | 534          |
------------------------------------------
Eval num_timesteps=2504000, episode_reward=44.51 +/- 309.92
Episode length: 411.60 +/- 55.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 412          |
|    mean_reward          | 44.5         |
| time/                   |              |
|    total_timesteps      | 2504000      |
| train/                  |              |
|    approx_kl            | 0.0042446335 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12          |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 12220        |
|    policy_gradient_loss | 0.000648     |
|    std                  | 5.2          |
|    value_loss           | 5.65e+03     |
------------------------------------------
Eval num_timesteps=2506000, episode_reward=183.33 +/- 373.31
Episode length: 440.60 +/- 95.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 2506000      |
| train/                  |              |
|    approx_kl            | 0.0009618439 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 61.8         |
|    n_updates            | 12230        |
|    policy_gradient_loss | -0.0005      |
|    std                  | 5.21         |
|    value_loss           | 313          |
------------------------------------------
Eval num_timesteps=2508000, episode_reward=515.68 +/- 441.89
Episode length: 474.20 +/- 63.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 516          |
| time/                   |              |
|    total_timesteps      | 2508000      |
| train/                  |              |
|    approx_kl            | 0.0011340883 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 2.55e+03     |
|    n_updates            | 12240        |
|    policy_gradient_loss | 0.000685     |
|    std                  | 5.23         |
|    value_loss           | 5.55e+03     |
------------------------------------------
Eval num_timesteps=2510000, episode_reward=347.67 +/- 213.47
Episode length: 437.20 +/- 42.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 437           |
|    mean_reward          | 348           |
| time/                   |               |
|    total_timesteps      | 2510000       |
| train/                  |               |
|    approx_kl            | 4.7273294e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+03      |
|    n_updates            | 12250         |
|    policy_gradient_loss | 0.000254      |
|    std                  | 5.23          |
|    value_loss           | 4.26e+03      |
-------------------------------------------
Eval num_timesteps=2512000, episode_reward=426.78 +/- 271.91
Episode length: 451.60 +/- 63.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 427          |
| time/                   |              |
|    total_timesteps      | 2512000      |
| train/                  |              |
|    approx_kl            | 0.0011270521 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 126          |
|    n_updates            | 12260        |
|    policy_gradient_loss | -0.000957    |
|    std                  | 5.24         |
|    value_loss           | 559          |
------------------------------------------
Eval num_timesteps=2514000, episode_reward=600.64 +/- 195.54
Episode length: 439.80 +/- 42.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 601          |
| time/                   |              |
|    total_timesteps      | 2514000      |
| train/                  |              |
|    approx_kl            | 0.0022525555 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+03     |
|    n_updates            | 12270        |
|    policy_gradient_loss | -0.00034     |
|    std                  | 5.25         |
|    value_loss           | 6.05e+03     |
------------------------------------------
Eval num_timesteps=2516000, episode_reward=202.67 +/- 379.49
Episode length: 417.20 +/- 44.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 203          |
| time/                   |              |
|    total_timesteps      | 2516000      |
| train/                  |              |
|    approx_kl            | 0.0010274844 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 139          |
|    n_updates            | 12280        |
|    policy_gradient_loss | -0.00122     |
|    std                  | 5.25         |
|    value_loss           | 839          |
------------------------------------------
Eval num_timesteps=2518000, episode_reward=131.98 +/- 95.84
Episode length: 439.20 +/- 37.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 132          |
| time/                   |              |
|    total_timesteps      | 2518000      |
| train/                  |              |
|    approx_kl            | 0.0011110578 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 507          |
|    n_updates            | 12290        |
|    policy_gradient_loss | -5.56e-05    |
|    std                  | 5.26         |
|    value_loss           | 2.06e+03     |
------------------------------------------
Eval num_timesteps=2520000, episode_reward=337.75 +/- 425.85
Episode length: 412.80 +/- 36.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 413           |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 2520000       |
| train/                  |               |
|    approx_kl            | 0.00012485788 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.852         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+03      |
|    n_updates            | 12300         |
|    policy_gradient_loss | -8.66e-05     |
|    std                  | 5.27          |
|    value_loss           | 4.86e+03      |
-------------------------------------------
Eval num_timesteps=2522000, episode_reward=308.60 +/- 156.72
Episode length: 461.00 +/- 46.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 2522000      |
| train/                  |              |
|    approx_kl            | 0.0036151758 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 29           |
|    n_updates            | 12310        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 5.27         |
|    value_loss           | 117          |
------------------------------------------
Eval num_timesteps=2524000, episode_reward=19.56 +/- 260.65
Episode length: 463.20 +/- 24.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 2524000      |
| train/                  |              |
|    approx_kl            | 0.0026124548 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+03     |
|    n_updates            | 12320        |
|    policy_gradient_loss | 0.000423     |
|    std                  | 5.27         |
|    value_loss           | 4.33e+03     |
------------------------------------------
Eval num_timesteps=2526000, episode_reward=296.38 +/- 214.24
Episode length: 448.60 +/- 53.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 296          |
| time/                   |              |
|    total_timesteps      | 2526000      |
| train/                  |              |
|    approx_kl            | 0.0018775731 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 147          |
|    n_updates            | 12330        |
|    policy_gradient_loss | -0.0012      |
|    std                  | 5.27         |
|    value_loss           | 465          |
------------------------------------------
Eval num_timesteps=2528000, episode_reward=178.84 +/- 422.49
Episode length: 446.80 +/- 55.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 447          |
|    mean_reward          | 179          |
| time/                   |              |
|    total_timesteps      | 2528000      |
| train/                  |              |
|    approx_kl            | 0.0015907849 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.001        |
|    loss                 | 91           |
|    n_updates            | 12340        |
|    policy_gradient_loss | 0.000543     |
|    std                  | 5.28         |
|    value_loss           | 524          |
------------------------------------------
Eval num_timesteps=2530000, episode_reward=294.72 +/- 431.38
Episode length: 438.60 +/- 47.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 295          |
| time/                   |              |
|    total_timesteps      | 2530000      |
| train/                  |              |
|    approx_kl            | 0.0012720806 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 60.4         |
|    n_updates            | 12350        |
|    policy_gradient_loss | -0.000126    |
|    std                  | 5.28         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=2532000, episode_reward=225.78 +/- 217.11
Episode length: 439.00 +/- 47.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 226           |
| time/                   |               |
|    total_timesteps      | 2532000       |
| train/                  |               |
|    approx_kl            | 0.00070690614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 936           |
|    n_updates            | 12360         |
|    policy_gradient_loss | 5.2e-05       |
|    std                  | 5.28          |
|    value_loss           | 3.49e+03      |
-------------------------------------------
Eval num_timesteps=2534000, episode_reward=567.59 +/- 421.34
Episode length: 465.20 +/- 45.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 465           |
|    mean_reward          | 568           |
| time/                   |               |
|    total_timesteps      | 2534000       |
| train/                  |               |
|    approx_kl            | 4.5226654e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.747         |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+03       |
|    n_updates            | 12370         |
|    policy_gradient_loss | 7.9e-05       |
|    std                  | 5.29          |
|    value_loss           | 7.53e+03      |
-------------------------------------------
Eval num_timesteps=2536000, episode_reward=271.32 +/- 258.77
Episode length: 487.40 +/- 46.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 2536000      |
| train/                  |              |
|    approx_kl            | 0.0006247924 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 81.2         |
|    n_updates            | 12380        |
|    policy_gradient_loss | -0.000748    |
|    std                  | 5.3          |
|    value_loss           | 381          |
------------------------------------------
Eval num_timesteps=2538000, episode_reward=353.01 +/- 231.39
Episode length: 515.40 +/- 8.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 353         |
| time/                   |             |
|    total_timesteps      | 2538000     |
| train/                  |             |
|    approx_kl            | 0.001919836 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.1       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 60.2        |
|    n_updates            | 12390       |
|    policy_gradient_loss | -0.000582   |
|    std                  | 5.32        |
|    value_loss           | 323         |
-----------------------------------------
Eval num_timesteps=2540000, episode_reward=200.86 +/- 163.88
Episode length: 485.00 +/- 48.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 2540000       |
| train/                  |               |
|    approx_kl            | 0.00034827873 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 125           |
|    n_updates            | 12400         |
|    policy_gradient_loss | -0.000161     |
|    std                  | 5.32          |
|    value_loss           | 524           |
-------------------------------------------
Eval num_timesteps=2542000, episode_reward=458.57 +/- 340.03
Episode length: 503.20 +/- 19.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 459          |
| time/                   |              |
|    total_timesteps      | 2542000      |
| train/                  |              |
|    approx_kl            | 0.0013649242 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.837        |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+03     |
|    n_updates            | 12410        |
|    policy_gradient_loss | -0.00137     |
|    std                  | 5.31         |
|    value_loss           | 4.53e+03     |
------------------------------------------
Eval num_timesteps=2544000, episode_reward=404.79 +/- 299.49
Episode length: 454.40 +/- 68.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 454          |
|    mean_reward          | 405          |
| time/                   |              |
|    total_timesteps      | 2544000      |
| train/                  |              |
|    approx_kl            | 0.0040327306 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 100          |
|    n_updates            | 12420        |
|    policy_gradient_loss | -0.00213     |
|    std                  | 5.31         |
|    value_loss           | 391          |
------------------------------------------
Eval num_timesteps=2546000, episode_reward=226.92 +/- 210.73
Episode length: 408.80 +/- 70.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 2546000      |
| train/                  |              |
|    approx_kl            | 0.0026828747 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.1        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.001        |
|    loss                 | 1.79e+03     |
|    n_updates            | 12430        |
|    policy_gradient_loss | -0.000149    |
|    std                  | 5.31         |
|    value_loss           | 4.42e+03     |
------------------------------------------
Eval num_timesteps=2548000, episode_reward=438.12 +/- 359.56
Episode length: 464.40 +/- 45.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 438           |
| time/                   |               |
|    total_timesteps      | 2548000       |
| train/                  |               |
|    approx_kl            | 0.00021353067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.1         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 39            |
|    n_updates            | 12440         |
|    policy_gradient_loss | -0.000453     |
|    std                  | 5.33          |
|    value_loss           | 222           |
-------------------------------------------
Eval num_timesteps=2550000, episode_reward=631.67 +/- 496.81
Episode length: 460.80 +/- 40.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 632         |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.003780495 |
|    clip_fraction        | 0.0084      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.1       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 73.2        |
|    n_updates            | 12450       |
|    policy_gradient_loss | -0.00132    |
|    std                  | 5.35        |
|    value_loss           | 265         |
-----------------------------------------
Eval num_timesteps=2552000, episode_reward=39.00 +/- 169.07
Episode length: 443.40 +/- 36.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 39           |
| time/                   |              |
|    total_timesteps      | 2552000      |
| train/                  |              |
|    approx_kl            | 0.0006213821 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 51.6         |
|    n_updates            | 12460        |
|    policy_gradient_loss | -0.000133    |
|    std                  | 5.37         |
|    value_loss           | 258          |
------------------------------------------
Eval num_timesteps=2554000, episode_reward=223.17 +/- 168.30
Episode length: 430.80 +/- 47.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 223          |
| time/                   |              |
|    total_timesteps      | 2554000      |
| train/                  |              |
|    approx_kl            | 0.0026261597 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+03     |
|    n_updates            | 12470        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 5.39         |
|    value_loss           | 3.82e+03     |
------------------------------------------
Eval num_timesteps=2556000, episode_reward=197.34 +/- 171.62
Episode length: 388.40 +/- 21.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 388           |
|    mean_reward          | 197           |
| time/                   |               |
|    total_timesteps      | 2556000       |
| train/                  |               |
|    approx_kl            | 0.00026093947 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.835         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 12480         |
|    policy_gradient_loss | 0.000452      |
|    std                  | 5.39          |
|    value_loss           | 3.53e+03      |
-------------------------------------------
Eval num_timesteps=2558000, episode_reward=291.27 +/- 107.26
Episode length: 427.20 +/- 41.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 291           |
| time/                   |               |
|    total_timesteps      | 2558000       |
| train/                  |               |
|    approx_kl            | 2.7308386e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 574           |
|    n_updates            | 12490         |
|    policy_gradient_loss | -3.54e-05     |
|    std                  | 5.39          |
|    value_loss           | 1.55e+03      |
-------------------------------------------
Eval num_timesteps=2560000, episode_reward=265.40 +/- 277.94
Episode length: 404.80 +/- 29.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2562000, episode_reward=577.18 +/- 306.98
Episode length: 397.80 +/- 29.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 398           |
|    mean_reward          | 577           |
| time/                   |               |
|    total_timesteps      | 2562000       |
| train/                  |               |
|    approx_kl            | 3.9831328e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.001         |
|    loss                 | 2.12e+03      |
|    n_updates            | 12500         |
|    policy_gradient_loss | -5.87e-05     |
|    std                  | 5.39          |
|    value_loss           | 5.79e+03      |
-------------------------------------------
Eval num_timesteps=2564000, episode_reward=118.62 +/- 228.75
Episode length: 430.00 +/- 37.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 2564000      |
| train/                  |              |
|    approx_kl            | 0.0007113826 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 52.9         |
|    n_updates            | 12510        |
|    policy_gradient_loss | -0.000466    |
|    std                  | 5.38         |
|    value_loss           | 405          |
------------------------------------------
Eval num_timesteps=2566000, episode_reward=309.76 +/- 403.42
Episode length: 407.40 +/- 34.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 310          |
| time/                   |              |
|    total_timesteps      | 2566000      |
| train/                  |              |
|    approx_kl            | 0.0029002938 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 12520        |
|    policy_gradient_loss | -0.00153     |
|    std                  | 5.37         |
|    value_loss           | 2.97e+03     |
------------------------------------------
Eval num_timesteps=2568000, episode_reward=303.09 +/- 294.65
Episode length: 452.20 +/- 48.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 303           |
| time/                   |               |
|    total_timesteps      | 2568000       |
| train/                  |               |
|    approx_kl            | 0.00069001794 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 39.2          |
|    n_updates            | 12530         |
|    policy_gradient_loss | -0.000325     |
|    std                  | 5.36          |
|    value_loss           | 163           |
-------------------------------------------
Eval num_timesteps=2570000, episode_reward=330.60 +/- 478.09
Episode length: 429.00 +/- 54.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 331          |
| time/                   |              |
|    total_timesteps      | 2570000      |
| train/                  |              |
|    approx_kl            | 0.0008127994 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 75.1         |
|    n_updates            | 12540        |
|    policy_gradient_loss | 0.000191     |
|    std                  | 5.37         |
|    value_loss           | 337          |
------------------------------------------
Eval num_timesteps=2572000, episode_reward=277.41 +/- 195.36
Episode length: 449.00 +/- 56.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 449          |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 2572000      |
| train/                  |              |
|    approx_kl            | 8.055975e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 893          |
|    n_updates            | 12550        |
|    policy_gradient_loss | 0.000168     |
|    std                  | 5.37         |
|    value_loss           | 2.38e+03     |
------------------------------------------
Eval num_timesteps=2574000, episode_reward=509.00 +/- 226.02
Episode length: 415.40 +/- 62.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 509          |
| time/                   |              |
|    total_timesteps      | 2574000      |
| train/                  |              |
|    approx_kl            | 2.939513e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 1.55e+03     |
|    n_updates            | 12560        |
|    policy_gradient_loss | -7.62e-05    |
|    std                  | 5.37         |
|    value_loss           | 4.43e+03     |
------------------------------------------
Eval num_timesteps=2576000, episode_reward=617.35 +/- 279.45
Episode length: 435.80 +/- 65.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 617           |
| time/                   |               |
|    total_timesteps      | 2576000       |
| train/                  |               |
|    approx_kl            | 5.2188756e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.001         |
|    loss                 | 860           |
|    n_updates            | 12570         |
|    policy_gradient_loss | -6.18e-05     |
|    std                  | 5.37          |
|    value_loss           | 2.42e+03      |
-------------------------------------------
Eval num_timesteps=2578000, episode_reward=313.25 +/- 596.87
Episode length: 431.40 +/- 33.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 313          |
| time/                   |              |
|    total_timesteps      | 2578000      |
| train/                  |              |
|    approx_kl            | 0.0002344001 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 12580        |
|    policy_gradient_loss | -0.000377    |
|    std                  | 5.37         |
|    value_loss           | 2.92e+03     |
------------------------------------------
Eval num_timesteps=2580000, episode_reward=307.93 +/- 187.53
Episode length: 420.00 +/- 29.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 2580000      |
| train/                  |              |
|    approx_kl            | 0.0005071143 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+03     |
|    n_updates            | 12590        |
|    policy_gradient_loss | -0.00069     |
|    std                  | 5.37         |
|    value_loss           | 6.24e+03     |
------------------------------------------
Eval num_timesteps=2582000, episode_reward=294.66 +/- 156.97
Episode length: 448.80 +/- 54.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 295           |
| time/                   |               |
|    total_timesteps      | 2582000       |
| train/                  |               |
|    approx_kl            | 0.00024810073 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+03       |
|    n_updates            | 12600         |
|    policy_gradient_loss | -0.000209     |
|    std                  | 5.37          |
|    value_loss           | 3.21e+03      |
-------------------------------------------
Eval num_timesteps=2584000, episode_reward=538.21 +/- 407.70
Episode length: 419.80 +/- 69.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 538           |
| time/                   |               |
|    total_timesteps      | 2584000       |
| train/                  |               |
|    approx_kl            | 0.00012166146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+03      |
|    n_updates            | 12610         |
|    policy_gradient_loss | -0.000209     |
|    std                  | 5.37          |
|    value_loss           | 3.09e+03      |
-------------------------------------------
Eval num_timesteps=2586000, episode_reward=507.79 +/- 291.76
Episode length: 473.20 +/- 17.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 473           |
|    mean_reward          | 508           |
| time/                   |               |
|    total_timesteps      | 2586000       |
| train/                  |               |
|    approx_kl            | 5.3929194e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.001         |
|    loss                 | 2.64e+03      |
|    n_updates            | 12620         |
|    policy_gradient_loss | -5.83e-05     |
|    std                  | 5.37          |
|    value_loss           | 6.94e+03      |
-------------------------------------------
Eval num_timesteps=2588000, episode_reward=407.70 +/- 206.74
Episode length: 459.80 +/- 48.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 408           |
| time/                   |               |
|    total_timesteps      | 2588000       |
| train/                  |               |
|    approx_kl            | 2.0644278e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+03      |
|    n_updates            | 12630         |
|    policy_gradient_loss | -5.36e-05     |
|    std                  | 5.37          |
|    value_loss           | 3.3e+03       |
-------------------------------------------
Eval num_timesteps=2590000, episode_reward=340.66 +/- 334.73
Episode length: 411.00 +/- 40.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 2590000      |
| train/                  |              |
|    approx_kl            | 0.0004931863 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 80.8         |
|    n_updates            | 12640        |
|    policy_gradient_loss | -0.000587    |
|    std                  | 5.38         |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=2592000, episode_reward=364.02 +/- 462.27
Episode length: 438.80 +/- 11.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 364           |
| time/                   |               |
|    total_timesteps      | 2592000       |
| train/                  |               |
|    approx_kl            | 0.00052240666 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 64.2          |
|    n_updates            | 12650         |
|    policy_gradient_loss | -8.6e-05      |
|    std                  | 5.39          |
|    value_loss           | 467           |
-------------------------------------------
Eval num_timesteps=2594000, episode_reward=455.91 +/- 350.75
Episode length: 435.40 +/- 58.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 435           |
|    mean_reward          | 456           |
| time/                   |               |
|    total_timesteps      | 2594000       |
| train/                  |               |
|    approx_kl            | 0.00033321255 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.874         |
|    learning_rate        | 0.001         |
|    loss                 | 930           |
|    n_updates            | 12660         |
|    policy_gradient_loss | -0.000132     |
|    std                  | 5.39          |
|    value_loss           | 3.07e+03      |
-------------------------------------------
Eval num_timesteps=2596000, episode_reward=376.33 +/- 416.21
Episode length: 459.40 +/- 58.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 2596000      |
| train/                  |              |
|    approx_kl            | 0.0010498562 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 50.6         |
|    n_updates            | 12670        |
|    policy_gradient_loss | -0.000568    |
|    std                  | 5.41         |
|    value_loss           | 183          |
------------------------------------------
Eval num_timesteps=2598000, episode_reward=282.16 +/- 98.60
Episode length: 418.40 +/- 79.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 282          |
| time/                   |              |
|    total_timesteps      | 2598000      |
| train/                  |              |
|    approx_kl            | 0.0012547724 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+03     |
|    n_updates            | 12680        |
|    policy_gradient_loss | 0.000516     |
|    std                  | 5.42         |
|    value_loss           | 3.68e+03     |
------------------------------------------
Eval num_timesteps=2600000, episode_reward=193.26 +/- 294.75
Episode length: 458.80 +/- 46.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 193           |
| time/                   |               |
|    total_timesteps      | 2600000       |
| train/                  |               |
|    approx_kl            | 0.00025900744 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.872         |
|    learning_rate        | 0.001         |
|    loss                 | 2.21e+03      |
|    n_updates            | 12690         |
|    policy_gradient_loss | -0.000305     |
|    std                  | 5.42          |
|    value_loss           | 4.76e+03      |
-------------------------------------------
Eval num_timesteps=2602000, episode_reward=23.22 +/- 246.80
Episode length: 448.60 +/- 52.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 23.2          |
| time/                   |               |
|    total_timesteps      | 2602000       |
| train/                  |               |
|    approx_kl            | 0.00050803146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 60.1          |
|    n_updates            | 12700         |
|    policy_gradient_loss | -0.000385     |
|    std                  | 5.42          |
|    value_loss           | 262           |
-------------------------------------------
Eval num_timesteps=2604000, episode_reward=321.03 +/- 205.28
Episode length: 479.60 +/- 26.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 321          |
| time/                   |              |
|    total_timesteps      | 2604000      |
| train/                  |              |
|    approx_kl            | 0.0038335877 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 33.6         |
|    n_updates            | 12710        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 5.43         |
|    value_loss           | 180          |
------------------------------------------
Eval num_timesteps=2606000, episode_reward=182.81 +/- 155.05
Episode length: 469.20 +/- 41.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 469          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 2606000      |
| train/                  |              |
|    approx_kl            | 0.0028605105 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 60.8         |
|    n_updates            | 12720        |
|    policy_gradient_loss | -0.00144     |
|    std                  | 5.46         |
|    value_loss           | 229          |
------------------------------------------
Eval num_timesteps=2608000, episode_reward=354.38 +/- 248.59
Episode length: 476.60 +/- 59.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 2608000      |
| train/                  |              |
|    approx_kl            | 0.0017956381 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 186          |
|    n_updates            | 12730        |
|    policy_gradient_loss | -0.000463    |
|    std                  | 5.47         |
|    value_loss           | 755          |
------------------------------------------
Eval num_timesteps=2610000, episode_reward=-31.70 +/- 142.81
Episode length: 433.20 +/- 58.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | -31.7        |
| time/                   |              |
|    total_timesteps      | 2610000      |
| train/                  |              |
|    approx_kl            | 0.0006131084 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 56.1         |
|    n_updates            | 12740        |
|    policy_gradient_loss | -2.02e-05    |
|    std                  | 5.48         |
|    value_loss           | 249          |
------------------------------------------
Eval num_timesteps=2612000, episode_reward=123.91 +/- 110.20
Episode length: 466.60 +/- 28.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | 124          |
| time/                   |              |
|    total_timesteps      | 2612000      |
| train/                  |              |
|    approx_kl            | 0.0014600474 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 12750        |
|    policy_gradient_loss | -0.0012      |
|    std                  | 5.48         |
|    value_loss           | 4.09e+03     |
------------------------------------------
Eval num_timesteps=2614000, episode_reward=-6.75 +/- 216.28
Episode length: 418.80 +/- 24.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | -6.75        |
| time/                   |              |
|    total_timesteps      | 2614000      |
| train/                  |              |
|    approx_kl            | 0.0016057284 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 52.7         |
|    n_updates            | 12760        |
|    policy_gradient_loss | -0.0013      |
|    std                  | 5.49         |
|    value_loss           | 206          |
------------------------------------------
Eval num_timesteps=2616000, episode_reward=91.96 +/- 105.20
Episode length: 430.40 +/- 58.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 92           |
| time/                   |              |
|    total_timesteps      | 2616000      |
| train/                  |              |
|    approx_kl            | 0.0038257125 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 43.8         |
|    n_updates            | 12770        |
|    policy_gradient_loss | -0.000178    |
|    std                  | 5.49         |
|    value_loss           | 168          |
------------------------------------------
Eval num_timesteps=2618000, episode_reward=431.53 +/- 159.88
Episode length: 459.60 +/- 26.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 2618000      |
| train/                  |              |
|    approx_kl            | 0.0074282195 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 33.8         |
|    n_updates            | 12780        |
|    policy_gradient_loss | -0.00281     |
|    std                  | 5.46         |
|    value_loss           | 167          |
------------------------------------------
Eval num_timesteps=2620000, episode_reward=177.59 +/- 86.49
Episode length: 413.00 +/- 67.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 2620000     |
| train/                  |             |
|    approx_kl            | 0.003253091 |
|    clip_fraction        | 0.00332     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.2       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 67.9        |
|    n_updates            | 12790       |
|    policy_gradient_loss | 4.01e-05    |
|    std                  | 5.45        |
|    value_loss           | 285         |
-----------------------------------------
Eval num_timesteps=2622000, episode_reward=155.85 +/- 93.22
Episode length: 438.80 +/- 48.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 156          |
| time/                   |              |
|    total_timesteps      | 2622000      |
| train/                  |              |
|    approx_kl            | 0.0010435953 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+03     |
|    n_updates            | 12800        |
|    policy_gradient_loss | 0.00138      |
|    std                  | 5.45         |
|    value_loss           | 6.18e+03     |
------------------------------------------
Eval num_timesteps=2624000, episode_reward=369.47 +/- 213.54
Episode length: 446.60 +/- 58.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 369           |
| time/                   |               |
|    total_timesteps      | 2624000       |
| train/                  |               |
|    approx_kl            | 0.00013841089 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+03      |
|    n_updates            | 12810         |
|    policy_gradient_loss | -0.000382     |
|    std                  | 5.45          |
|    value_loss           | 3.21e+03      |
-------------------------------------------
Eval num_timesteps=2626000, episode_reward=129.82 +/- 105.65
Episode length: 408.00 +/- 71.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 130           |
| time/                   |               |
|    total_timesteps      | 2626000       |
| train/                  |               |
|    approx_kl            | 0.00045090486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.2         |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+03      |
|    n_updates            | 12820         |
|    policy_gradient_loss | -0.000729     |
|    std                  | 5.46          |
|    value_loss           | 2.75e+03      |
-------------------------------------------
Eval num_timesteps=2628000, episode_reward=196.70 +/- 374.70
Episode length: 425.20 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 425          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 2628000      |
| train/                  |              |
|    approx_kl            | 0.0041326582 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.2        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 22.9         |
|    n_updates            | 12830        |
|    policy_gradient_loss | -0.00212     |
|    std                  | 5.5          |
|    value_loss           | 91.8         |
------------------------------------------
Eval num_timesteps=2630000, episode_reward=201.31 +/- 387.03
Episode length: 395.60 +/- 23.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 2630000      |
| train/                  |              |
|    approx_kl            | 0.0038032797 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+03     |
|    n_updates            | 12840        |
|    policy_gradient_loss | -0.00119     |
|    std                  | 5.53         |
|    value_loss           | 4.77e+03     |
------------------------------------------
Eval num_timesteps=2632000, episode_reward=109.84 +/- 189.62
Episode length: 398.00 +/- 66.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 110          |
| time/                   |              |
|    total_timesteps      | 2632000      |
| train/                  |              |
|    approx_kl            | 0.0010653797 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 38.4         |
|    n_updates            | 12850        |
|    policy_gradient_loss | -0.000464    |
|    std                  | 5.55         |
|    value_loss           | 125          |
------------------------------------------
Eval num_timesteps=2634000, episode_reward=244.59 +/- 211.41
Episode length: 406.80 +/- 32.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 245          |
| time/                   |              |
|    total_timesteps      | 2634000      |
| train/                  |              |
|    approx_kl            | 0.0027176074 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 19.6         |
|    n_updates            | 12860        |
|    policy_gradient_loss | -0.00181     |
|    std                  | 5.56         |
|    value_loss           | 77.9         |
------------------------------------------
Eval num_timesteps=2636000, episode_reward=92.75 +/- 72.97
Episode length: 401.80 +/- 48.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 92.7        |
| time/                   |             |
|    total_timesteps      | 2636000     |
| train/                  |             |
|    approx_kl            | 0.005941269 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 36.5        |
|    n_updates            | 12870       |
|    policy_gradient_loss | -0.00198    |
|    std                  | 5.57        |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=2638000, episode_reward=101.34 +/- 109.83
Episode length: 422.60 +/- 39.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 423          |
|    mean_reward          | 101          |
| time/                   |              |
|    total_timesteps      | 2638000      |
| train/                  |              |
|    approx_kl            | 0.0029150513 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 27.3         |
|    n_updates            | 12880        |
|    policy_gradient_loss | -0.000718    |
|    std                  | 5.56         |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=2640000, episode_reward=100.53 +/- 229.45
Episode length: 469.20 +/- 50.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 101         |
| time/                   |             |
|    total_timesteps      | 2640000     |
| train/                  |             |
|    approx_kl            | 0.006154692 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 27.2        |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.00195    |
|    std                  | 5.57        |
|    value_loss           | 133         |
-----------------------------------------
Eval num_timesteps=2642000, episode_reward=-6.90 +/- 41.99
Episode length: 375.60 +/- 18.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | -6.9        |
| time/                   |             |
|    total_timesteps      | 2642000     |
| train/                  |             |
|    approx_kl            | 0.006996167 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 31.1        |
|    n_updates            | 12900       |
|    policy_gradient_loss | -0.0024     |
|    std                  | 5.58        |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=2644000, episode_reward=84.36 +/- 263.04
Episode length: 442.60 +/- 35.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 443         |
|    mean_reward          | 84.4        |
| time/                   |             |
|    total_timesteps      | 2644000     |
| train/                  |             |
|    approx_kl            | 0.010606343 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 12910       |
|    policy_gradient_loss | -0.00195    |
|    std                  | 5.57        |
|    value_loss           | 581         |
-----------------------------------------
Eval num_timesteps=2646000, episode_reward=18.91 +/- 45.71
Episode length: 385.00 +/- 64.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 385      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 2646000  |
---------------------------------
Eval num_timesteps=2648000, episode_reward=162.08 +/- 162.94
Episode length: 377.40 +/- 50.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 377           |
|    mean_reward          | 162           |
| time/                   |               |
|    total_timesteps      | 2648000       |
| train/                  |               |
|    approx_kl            | 0.00043260618 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.3         |
|    explained_variance   | 0.819         |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+03      |
|    n_updates            | 12920         |
|    policy_gradient_loss | 0.000197      |
|    std                  | 5.57          |
|    value_loss           | 4.63e+03      |
-------------------------------------------
Eval num_timesteps=2650000, episode_reward=360.43 +/- 274.20
Episode length: 394.80 +/- 44.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 395           |
|    mean_reward          | 360           |
| time/                   |               |
|    total_timesteps      | 2650000       |
| train/                  |               |
|    approx_kl            | 0.00042311443 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.3         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 47            |
|    n_updates            | 12930         |
|    policy_gradient_loss | -0.000403     |
|    std                  | 5.58          |
|    value_loss           | 242           |
-------------------------------------------
Eval num_timesteps=2652000, episode_reward=91.88 +/- 97.18
Episode length: 410.20 +/- 60.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 91.9          |
| time/                   |               |
|    total_timesteps      | 2652000       |
| train/                  |               |
|    approx_kl            | 0.00057057524 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.3         |
|    explained_variance   | 0.854         |
|    learning_rate        | 0.001         |
|    loss                 | 1.42e+03      |
|    n_updates            | 12940         |
|    policy_gradient_loss | 0.000352      |
|    std                  | 5.58          |
|    value_loss           | 3.74e+03      |
-------------------------------------------
Eval num_timesteps=2654000, episode_reward=130.61 +/- 112.81
Episode length: 404.00 +/- 37.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 2654000     |
| train/                  |             |
|    approx_kl            | 0.005310266 |
|    clip_fraction        | 0.00884     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 17.5        |
|    n_updates            | 12950       |
|    policy_gradient_loss | -0.0015     |
|    std                  | 5.6         |
|    value_loss           | 88.3        |
-----------------------------------------
Eval num_timesteps=2656000, episode_reward=237.93 +/- 230.71
Episode length: 389.00 +/- 39.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 389         |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 2656000     |
| train/                  |             |
|    approx_kl            | 0.002041767 |
|    clip_fraction        | 0.000635    |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.3       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+03    |
|    n_updates            | 12960       |
|    policy_gradient_loss | 0.00129     |
|    std                  | 5.62        |
|    value_loss           | 2.79e+03    |
-----------------------------------------
Eval num_timesteps=2658000, episode_reward=165.90 +/- 306.52
Episode length: 410.80 +/- 57.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 166          |
| time/                   |              |
|    total_timesteps      | 2658000      |
| train/                  |              |
|    approx_kl            | 0.0035427753 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.3        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 19.7         |
|    n_updates            | 12970        |
|    policy_gradient_loss | -0.00158     |
|    std                  | 5.64         |
|    value_loss           | 72.9         |
------------------------------------------
Eval num_timesteps=2660000, episode_reward=106.47 +/- 124.23
Episode length: 376.40 +/- 70.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 2660000      |
| train/                  |              |
|    approx_kl            | 0.0042457595 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.4        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 51.6         |
|    n_updates            | 12980        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 5.67         |
|    value_loss           | 175          |
------------------------------------------
Eval num_timesteps=2662000, episode_reward=215.34 +/- 159.17
Episode length: 431.80 +/- 57.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 2662000     |
| train/                  |             |
|    approx_kl            | 0.002713479 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 34.4        |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.000845   |
|    std                  | 5.69        |
|    value_loss           | 280         |
-----------------------------------------
Eval num_timesteps=2664000, episode_reward=204.42 +/- 77.13
Episode length: 374.40 +/- 22.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 2664000     |
| train/                  |             |
|    approx_kl            | 7.74893e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 61.7        |
|    n_updates            | 13000       |
|    policy_gradient_loss | 0.000575    |
|    std                  | 5.7         |
|    value_loss           | 396         |
-----------------------------------------
Eval num_timesteps=2666000, episode_reward=82.03 +/- 265.89
Episode length: 409.80 +/- 66.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | 82          |
| time/                   |             |
|    total_timesteps      | 2666000     |
| train/                  |             |
|    approx_kl            | 0.003957765 |
|    clip_fraction        | 0.00396     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.001       |
|    loss                 | 16.1        |
|    n_updates            | 13010       |
|    policy_gradient_loss | -0.00175    |
|    std                  | 5.72        |
|    value_loss           | 48.2        |
-----------------------------------------
Eval num_timesteps=2668000, episode_reward=105.72 +/- 71.11
Episode length: 428.40 +/- 50.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 2668000      |
| train/                  |              |
|    approx_kl            | 0.0029630237 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.4        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 50.7         |
|    n_updates            | 13020        |
|    policy_gradient_loss | -0.000845    |
|    std                  | 5.73         |
|    value_loss           | 397          |
------------------------------------------
Eval num_timesteps=2670000, episode_reward=172.85 +/- 251.32
Episode length: 439.20 +/- 64.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 439         |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 2670000     |
| train/                  |             |
|    approx_kl            | 0.003600542 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 20.2        |
|    n_updates            | 13030       |
|    policy_gradient_loss | -0.0018     |
|    std                  | 5.73        |
|    value_loss           | 79.3        |
-----------------------------------------
Eval num_timesteps=2672000, episode_reward=363.40 +/- 424.22
Episode length: 484.20 +/- 57.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 484         |
|    mean_reward          | 363         |
| time/                   |             |
|    total_timesteps      | 2672000     |
| train/                  |             |
|    approx_kl            | 0.003784576 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 60.9        |
|    n_updates            | 13040       |
|    policy_gradient_loss | -0.000967   |
|    std                  | 5.72        |
|    value_loss           | 208         |
-----------------------------------------
Eval num_timesteps=2674000, episode_reward=95.53 +/- 154.20
Episode length: 496.80 +/- 15.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 95.5        |
| time/                   |             |
|    total_timesteps      | 2674000     |
| train/                  |             |
|    approx_kl            | 0.005606179 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 93.9        |
|    n_updates            | 13050       |
|    policy_gradient_loss | -0.000516   |
|    std                  | 5.71        |
|    value_loss           | 476         |
-----------------------------------------
Eval num_timesteps=2676000, episode_reward=416.68 +/- 285.47
Episode length: 524.60 +/- 19.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 2676000      |
| train/                  |              |
|    approx_kl            | 0.0010818294 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.4        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 44.2         |
|    n_updates            | 13060        |
|    policy_gradient_loss | -0.00065     |
|    std                  | 5.72         |
|    value_loss           | 399          |
------------------------------------------
Eval num_timesteps=2678000, episode_reward=101.36 +/- 177.20
Episode length: 495.00 +/- 53.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 101          |
| time/                   |              |
|    total_timesteps      | 2678000      |
| train/                  |              |
|    approx_kl            | 0.0020534813 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.4        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 50.3         |
|    n_updates            | 13070        |
|    policy_gradient_loss | -0.000555    |
|    std                  | 5.73         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=2680000, episode_reward=263.77 +/- 159.42
Episode length: 439.20 +/- 29.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 439         |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 2680000     |
| train/                  |             |
|    approx_kl            | 0.003565717 |
|    clip_fraction        | 0.00327     |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 89.7        |
|    n_updates            | 13080       |
|    policy_gradient_loss | -0.00163    |
|    std                  | 5.74        |
|    value_loss           | 306         |
-----------------------------------------
Eval num_timesteps=2682000, episode_reward=220.72 +/- 391.64
Episode length: 444.00 +/- 74.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 2682000      |
| train/                  |              |
|    approx_kl            | 0.0016393346 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 69.2         |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.000192    |
|    std                  | 5.76         |
|    value_loss           | 276          |
------------------------------------------
Eval num_timesteps=2684000, episode_reward=299.15 +/- 292.69
Episode length: 517.80 +/- 11.92
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 518            |
|    mean_reward          | 299            |
| time/                   |                |
|    total_timesteps      | 2684000        |
| train/                  |                |
|    approx_kl            | 0.000121383346 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -12.5          |
|    explained_variance   | 0.862          |
|    learning_rate        | 0.001          |
|    loss                 | 963            |
|    n_updates            | 13100          |
|    policy_gradient_loss | 0.00048        |
|    std                  | 5.77           |
|    value_loss           | 3.19e+03       |
--------------------------------------------
Eval num_timesteps=2686000, episode_reward=240.76 +/- 335.21
Episode length: 476.20 +/- 49.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 241          |
| time/                   |              |
|    total_timesteps      | 2686000      |
| train/                  |              |
|    approx_kl            | 5.938785e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+03     |
|    n_updates            | 13110        |
|    policy_gradient_loss | -0.000217    |
|    std                  | 5.77         |
|    value_loss           | 3.19e+03     |
------------------------------------------
Eval num_timesteps=2688000, episode_reward=292.44 +/- 165.04
Episode length: 492.00 +/- 31.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 292          |
| time/                   |              |
|    total_timesteps      | 2688000      |
| train/                  |              |
|    approx_kl            | 7.779832e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+03     |
|    n_updates            | 13120        |
|    policy_gradient_loss | -5.21e-05    |
|    std                  | 5.77         |
|    value_loss           | 3.17e+03     |
------------------------------------------
Eval num_timesteps=2690000, episode_reward=441.99 +/- 341.95
Episode length: 477.40 +/- 44.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 442          |
| time/                   |              |
|    total_timesteps      | 2690000      |
| train/                  |              |
|    approx_kl            | 0.0010503418 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 74.5         |
|    n_updates            | 13130        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 5.78         |
|    value_loss           | 312          |
------------------------------------------
Eval num_timesteps=2692000, episode_reward=71.52 +/- 222.81
Episode length: 457.40 +/- 62.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | 71.5         |
| time/                   |              |
|    total_timesteps      | 2692000      |
| train/                  |              |
|    approx_kl            | 0.0015522904 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.001        |
|    loss                 | 90.6         |
|    n_updates            | 13140        |
|    policy_gradient_loss | -0.000372    |
|    std                  | 5.78         |
|    value_loss           | 575          |
------------------------------------------
Eval num_timesteps=2694000, episode_reward=244.63 +/- 176.84
Episode length: 398.60 +/- 65.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 245          |
| time/                   |              |
|    total_timesteps      | 2694000      |
| train/                  |              |
|    approx_kl            | 0.0018767046 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 42.8         |
|    n_updates            | 13150        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 5.81         |
|    value_loss           | 188          |
------------------------------------------
Eval num_timesteps=2696000, episode_reward=226.73 +/- 265.31
Episode length: 396.40 +/- 58.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 227          |
| time/                   |              |
|    total_timesteps      | 2696000      |
| train/                  |              |
|    approx_kl            | 0.0020881114 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 313          |
|    n_updates            | 13160        |
|    policy_gradient_loss | -0.000348    |
|    std                  | 5.84         |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=2698000, episode_reward=299.37 +/- 260.08
Episode length: 394.60 +/- 33.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 2698000     |
| train/                  |             |
|    approx_kl            | 0.002179631 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 19.9        |
|    n_updates            | 13170       |
|    policy_gradient_loss | -0.00107    |
|    std                  | 5.85        |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=2700000, episode_reward=172.49 +/- 125.10
Episode length: 378.00 +/- 16.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 2700000      |
| train/                  |              |
|    approx_kl            | 0.0034169266 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.001        |
|    loss                 | 2.96e+03     |
|    n_updates            | 13180        |
|    policy_gradient_loss | -0.00195     |
|    std                  | 5.84         |
|    value_loss           | 7.72e+03     |
------------------------------------------
Eval num_timesteps=2702000, episode_reward=14.70 +/- 365.24
Episode length: 408.00 +/- 32.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 14.7         |
| time/                   |              |
|    total_timesteps      | 2702000      |
| train/                  |              |
|    approx_kl            | 0.0013833472 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00049     |
|    std                  | 5.85         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=2704000, episode_reward=310.02 +/- 267.55
Episode length: 367.00 +/- 58.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 367          |
|    mean_reward          | 310          |
| time/                   |              |
|    total_timesteps      | 2704000      |
| train/                  |              |
|    approx_kl            | 0.0020026169 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 946          |
|    n_updates            | 13200        |
|    policy_gradient_loss | -0.000282    |
|    std                  | 5.85         |
|    value_loss           | 2.29e+03     |
------------------------------------------
Eval num_timesteps=2706000, episode_reward=401.82 +/- 512.96
Episode length: 449.80 +/- 81.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | 402         |
| time/                   |             |
|    total_timesteps      | 2706000     |
| train/                  |             |
|    approx_kl            | 0.001100986 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 13210       |
|    policy_gradient_loss | -0.0012     |
|    std                  | 5.86        |
|    value_loss           | 565         |
-----------------------------------------
Eval num_timesteps=2708000, episode_reward=31.72 +/- 197.66
Episode length: 387.20 +/- 24.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 31.7         |
| time/                   |              |
|    total_timesteps      | 2708000      |
| train/                  |              |
|    approx_kl            | 0.0014427877 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.5        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+03     |
|    n_updates            | 13220        |
|    policy_gradient_loss | 0.000748     |
|    std                  | 5.86         |
|    value_loss           | 2.4e+03      |
------------------------------------------
Eval num_timesteps=2710000, episode_reward=433.37 +/- 345.87
Episode length: 435.80 +/- 70.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 433           |
| time/                   |               |
|    total_timesteps      | 2710000       |
| train/                  |               |
|    approx_kl            | 0.00077131286 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.5         |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.001         |
|    loss                 | 27.7          |
|    n_updates            | 13230         |
|    policy_gradient_loss | -0.000589     |
|    std                  | 5.87          |
|    value_loss           | 122           |
-------------------------------------------
Eval num_timesteps=2712000, episode_reward=330.22 +/- 198.15
Episode length: 431.60 +/- 53.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 2712000     |
| train/                  |             |
|    approx_kl            | 0.000812176 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.001       |
|    loss                 | 1.38e+03    |
|    n_updates            | 13240       |
|    policy_gradient_loss | 0.000383    |
|    std                  | 5.89        |
|    value_loss           | 4.05e+03    |
-----------------------------------------
Eval num_timesteps=2714000, episode_reward=434.31 +/- 227.33
Episode length: 508.40 +/- 41.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 434           |
| time/                   |               |
|    total_timesteps      | 2714000       |
| train/                  |               |
|    approx_kl            | 0.00013269242 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 55.9          |
|    n_updates            | 13250         |
|    policy_gradient_loss | -3.11e-05     |
|    std                  | 5.89          |
|    value_loss           | 442           |
-------------------------------------------
Eval num_timesteps=2716000, episode_reward=116.10 +/- 354.16
Episode length: 397.80 +/- 19.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 116          |
| time/                   |              |
|    total_timesteps      | 2716000      |
| train/                  |              |
|    approx_kl            | 0.0003421305 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+03     |
|    n_updates            | 13260        |
|    policy_gradient_loss | -8.36e-06    |
|    std                  | 5.9          |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=2718000, episode_reward=245.12 +/- 257.23
Episode length: 395.40 +/- 29.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 395           |
|    mean_reward          | 245           |
| time/                   |               |
|    total_timesteps      | 2718000       |
| train/                  |               |
|    approx_kl            | 0.00012278886 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+03      |
|    n_updates            | 13270         |
|    policy_gradient_loss | -0.000328     |
|    std                  | 5.9           |
|    value_loss           | 5.1e+03       |
-------------------------------------------
Eval num_timesteps=2720000, episode_reward=455.06 +/- 387.61
Episode length: 431.40 +/- 37.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 455           |
| time/                   |               |
|    total_timesteps      | 2720000       |
| train/                  |               |
|    approx_kl            | 0.00086671533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 40.9          |
|    n_updates            | 13280         |
|    policy_gradient_loss | -0.000737     |
|    std                  | 5.91          |
|    value_loss           | 222           |
-------------------------------------------
Eval num_timesteps=2722000, episode_reward=394.04 +/- 289.63
Episode length: 400.00 +/- 23.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | 394          |
| time/                   |              |
|    total_timesteps      | 2722000      |
| train/                  |              |
|    approx_kl            | 0.0013377479 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+03     |
|    n_updates            | 13290        |
|    policy_gradient_loss | 0.000225     |
|    std                  | 5.93         |
|    value_loss           | 3.38e+03     |
------------------------------------------
Eval num_timesteps=2724000, episode_reward=417.59 +/- 272.36
Episode length: 411.20 +/- 69.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 411          |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 2724000      |
| train/                  |              |
|    approx_kl            | 0.0001945479 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.001        |
|    loss                 | 5.42e+03     |
|    n_updates            | 13300        |
|    policy_gradient_loss | -0.000105    |
|    std                  | 5.94         |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=2726000, episode_reward=133.88 +/- 226.10
Episode length: 421.80 +/- 50.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 134           |
| time/                   |               |
|    total_timesteps      | 2726000       |
| train/                  |               |
|    approx_kl            | 1.5830388e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.857         |
|    learning_rate        | 0.001         |
|    loss                 | 2.25e+03      |
|    n_updates            | 13310         |
|    policy_gradient_loss | 3.63e-05      |
|    std                  | 5.94          |
|    value_loss           | 6.2e+03       |
-------------------------------------------
Eval num_timesteps=2728000, episode_reward=358.08 +/- 426.65
Episode length: 408.20 +/- 45.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 358           |
| time/                   |               |
|    total_timesteps      | 2728000       |
| train/                  |               |
|    approx_kl            | 0.00020633067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 91            |
|    n_updates            | 13320         |
|    policy_gradient_loss | -0.000264     |
|    std                  | 5.93          |
|    value_loss           | 598           |
-------------------------------------------
Eval num_timesteps=2730000, episode_reward=489.10 +/- 370.45
Episode length: 439.40 +/- 47.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 439           |
|    mean_reward          | 489           |
| time/                   |               |
|    total_timesteps      | 2730000       |
| train/                  |               |
|    approx_kl            | 0.00020073779 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 13330         |
|    policy_gradient_loss | 0.000613      |
|    std                  | 5.92          |
|    value_loss           | 5.43e+03      |
-------------------------------------------
Eval num_timesteps=2732000, episode_reward=194.85 +/- 128.36
Episode length: 405.00 +/- 27.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 2732000  |
---------------------------------
Eval num_timesteps=2734000, episode_reward=461.62 +/- 144.50
Episode length: 459.60 +/- 35.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 462           |
| time/                   |               |
|    total_timesteps      | 2734000       |
| train/                  |               |
|    approx_kl            | 0.00011840256 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+03      |
|    n_updates            | 13340         |
|    policy_gradient_loss | -8.87e-05     |
|    std                  | 5.92          |
|    value_loss           | 3.4e+03       |
-------------------------------------------
Eval num_timesteps=2736000, episode_reward=131.33 +/- 192.40
Episode length: 415.40 +/- 80.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 415           |
|    mean_reward          | 131           |
| time/                   |               |
|    total_timesteps      | 2736000       |
| train/                  |               |
|    approx_kl            | 4.5820692e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+03      |
|    n_updates            | 13350         |
|    policy_gradient_loss | -8.77e-05     |
|    std                  | 5.92          |
|    value_loss           | 5.87e+03      |
-------------------------------------------
Eval num_timesteps=2738000, episode_reward=465.35 +/- 304.43
Episode length: 408.00 +/- 39.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 2738000      |
| train/                  |              |
|    approx_kl            | 6.873347e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.001        |
|    loss                 | 777          |
|    n_updates            | 13360        |
|    policy_gradient_loss | -0.000196    |
|    std                  | 5.92         |
|    value_loss           | 3.17e+03     |
------------------------------------------
Eval num_timesteps=2740000, episode_reward=604.78 +/- 322.54
Episode length: 455.40 +/- 67.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 605           |
| time/                   |               |
|    total_timesteps      | 2740000       |
| train/                  |               |
|    approx_kl            | 0.00056338613 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 50.9          |
|    n_updates            | 13370         |
|    policy_gradient_loss | -0.000631     |
|    std                  | 5.93          |
|    value_loss           | 303           |
-------------------------------------------
Eval num_timesteps=2742000, episode_reward=135.70 +/- 225.52
Episode length: 385.20 +/- 15.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 385          |
|    mean_reward          | 136          |
| time/                   |              |
|    total_timesteps      | 2742000      |
| train/                  |              |
|    approx_kl            | 0.0008793407 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.811        |
|    learning_rate        | 0.001        |
|    loss                 | 2.36e+03     |
|    n_updates            | 13380        |
|    policy_gradient_loss | 0.000105     |
|    std                  | 5.94         |
|    value_loss           | 6.41e+03     |
------------------------------------------
Eval num_timesteps=2744000, episode_reward=279.88 +/- 214.31
Episode length: 405.20 +/- 58.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 280           |
| time/                   |               |
|    total_timesteps      | 2744000       |
| train/                  |               |
|    approx_kl            | 0.00014076463 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 64            |
|    n_updates            | 13390         |
|    policy_gradient_loss | -0.000212     |
|    std                  | 5.95          |
|    value_loss           | 447           |
-------------------------------------------
Eval num_timesteps=2746000, episode_reward=399.65 +/- 206.25
Episode length: 374.20 +/- 21.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 374           |
|    mean_reward          | 400           |
| time/                   |               |
|    total_timesteps      | 2746000       |
| train/                  |               |
|    approx_kl            | 0.00029711955 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+03      |
|    n_updates            | 13400         |
|    policy_gradient_loss | -0.000371     |
|    std                  | 5.96          |
|    value_loss           | 3.78e+03      |
-------------------------------------------
Eval num_timesteps=2748000, episode_reward=353.81 +/- 326.74
Episode length: 385.40 +/- 35.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 385           |
|    mean_reward          | 354           |
| time/                   |               |
|    total_timesteps      | 2748000       |
| train/                  |               |
|    approx_kl            | 0.00017987026 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 13410         |
|    policy_gradient_loss | -0.000221     |
|    std                  | 5.97          |
|    value_loss           | 3.89e+03      |
-------------------------------------------
Eval num_timesteps=2750000, episode_reward=189.64 +/- 260.62
Episode length: 403.00 +/- 31.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 403         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 9.13431e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.001       |
|    loss                 | 2.86e+03    |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.000139   |
|    std                  | 5.98        |
|    value_loss           | 6.22e+03    |
-----------------------------------------
Eval num_timesteps=2752000, episode_reward=7.35 +/- 94.12
Episode length: 380.00 +/- 38.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | 7.35          |
| time/                   |               |
|    total_timesteps      | 2752000       |
| train/                  |               |
|    approx_kl            | 7.0279348e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+03      |
|    n_updates            | 13430         |
|    policy_gradient_loss | -3.44e-05     |
|    std                  | 5.98          |
|    value_loss           | 4.31e+03      |
-------------------------------------------
Eval num_timesteps=2754000, episode_reward=438.94 +/- 219.48
Episode length: 382.80 +/- 20.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 383           |
|    mean_reward          | 439           |
| time/                   |               |
|    total_timesteps      | 2754000       |
| train/                  |               |
|    approx_kl            | 0.00020857056 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+03      |
|    n_updates            | 13440         |
|    policy_gradient_loss | -0.00044      |
|    std                  | 5.99          |
|    value_loss           | 2.67e+03      |
-------------------------------------------
Eval num_timesteps=2756000, episode_reward=298.50 +/- 281.14
Episode length: 420.80 +/- 48.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 421           |
|    mean_reward          | 298           |
| time/                   |               |
|    total_timesteps      | 2756000       |
| train/                  |               |
|    approx_kl            | 0.00060087035 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.6         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 85.1          |
|    n_updates            | 13450         |
|    policy_gradient_loss | -0.000789     |
|    std                  | 6             |
|    value_loss           | 437           |
-------------------------------------------
Eval num_timesteps=2758000, episode_reward=203.54 +/- 108.59
Episode length: 400.40 +/- 33.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 2758000     |
| train/                  |             |
|    approx_kl            | 0.000642522 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 41.4        |
|    n_updates            | 13460       |
|    policy_gradient_loss | -8.91e-06   |
|    std                  | 6.01        |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=2760000, episode_reward=450.47 +/- 155.75
Episode length: 437.20 +/- 44.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 437          |
|    mean_reward          | 450          |
| time/                   |              |
|    total_timesteps      | 2760000      |
| train/                  |              |
|    approx_kl            | 0.0033185529 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.6        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 922          |
|    n_updates            | 13470        |
|    policy_gradient_loss | 0.00101      |
|    std                  | 6.01         |
|    value_loss           | 2.99e+03     |
------------------------------------------
Eval num_timesteps=2762000, episode_reward=117.16 +/- 176.34
Episode length: 414.00 +/- 54.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 117           |
| time/                   |               |
|    total_timesteps      | 2762000       |
| train/                  |               |
|    approx_kl            | 0.00096601754 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 357           |
|    n_updates            | 13480         |
|    policy_gradient_loss | -0.000479     |
|    std                  | 6.03          |
|    value_loss           | 884           |
-------------------------------------------
Eval num_timesteps=2764000, episode_reward=167.22 +/- 292.88
Episode length: 395.20 +/- 41.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 167          |
| time/                   |              |
|    total_timesteps      | 2764000      |
| train/                  |              |
|    approx_kl            | 0.0010809017 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+03     |
|    n_updates            | 13490        |
|    policy_gradient_loss | -0.000422    |
|    std                  | 6.04         |
|    value_loss           | 6.32e+03     |
------------------------------------------
Eval num_timesteps=2766000, episode_reward=213.31 +/- 168.25
Episode length: 429.80 +/- 54.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 213           |
| time/                   |               |
|    total_timesteps      | 2766000       |
| train/                  |               |
|    approx_kl            | 0.00023313728 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.798         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+03       |
|    n_updates            | 13500         |
|    policy_gradient_loss | -0.000195     |
|    std                  | 6.05          |
|    value_loss           | 4e+03         |
-------------------------------------------
Eval num_timesteps=2768000, episode_reward=108.85 +/- 135.08
Episode length: 420.40 +/- 55.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 420           |
|    mean_reward          | 109           |
| time/                   |               |
|    total_timesteps      | 2768000       |
| train/                  |               |
|    approx_kl            | 0.00093885756 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+03      |
|    n_updates            | 13510         |
|    policy_gradient_loss | -0.00066      |
|    std                  | 6.05          |
|    value_loss           | 3.44e+03      |
-------------------------------------------
Eval num_timesteps=2770000, episode_reward=267.18 +/- 168.09
Episode length: 417.80 +/- 66.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 2770000      |
| train/                  |              |
|    approx_kl            | 0.0004375511 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 2.75e+03     |
|    n_updates            | 13520        |
|    policy_gradient_loss | 0.000387     |
|    std                  | 6.05         |
|    value_loss           | 6.59e+03     |
------------------------------------------
Eval num_timesteps=2772000, episode_reward=338.22 +/- 462.05
Episode length: 408.60 +/- 27.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 409        |
|    mean_reward          | 338        |
| time/                   |            |
|    total_timesteps      | 2772000    |
| train/                  |            |
|    approx_kl            | 7.0218e-05 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.001      |
|    loss                 | 1.44e+03   |
|    n_updates            | 13530      |
|    policy_gradient_loss | -0.000149  |
|    std                  | 6.06       |
|    value_loss           | 3.65e+03   |
----------------------------------------
Eval num_timesteps=2774000, episode_reward=86.92 +/- 143.92
Episode length: 392.40 +/- 47.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 392           |
|    mean_reward          | 86.9          |
| time/                   |               |
|    total_timesteps      | 2774000       |
| train/                  |               |
|    approx_kl            | 1.9480533e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.881         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+03       |
|    n_updates            | 13540         |
|    policy_gradient_loss | 3.9e-05       |
|    std                  | 6.06          |
|    value_loss           | 3.74e+03      |
-------------------------------------------
Eval num_timesteps=2776000, episode_reward=118.48 +/- 81.95
Episode length: 417.80 +/- 30.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 118           |
| time/                   |               |
|    total_timesteps      | 2776000       |
| train/                  |               |
|    approx_kl            | 2.3927278e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.358         |
|    learning_rate        | 0.001         |
|    loss                 | 3.48e+03      |
|    n_updates            | 13550         |
|    policy_gradient_loss | -0.000143     |
|    std                  | 6.06          |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=2778000, episode_reward=432.87 +/- 379.34
Episode length: 470.60 +/- 47.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 433         |
| time/                   |             |
|    total_timesteps      | 2778000     |
| train/                  |             |
|    approx_kl            | 0.000634889 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.7       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.001       |
|    loss                 | 245         |
|    n_updates            | 13560       |
|    policy_gradient_loss | -0.000872   |
|    std                  | 6.07        |
|    value_loss           | 848         |
-----------------------------------------
Eval num_timesteps=2780000, episode_reward=331.69 +/- 264.64
Episode length: 402.40 +/- 59.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 2780000      |
| train/                  |              |
|    approx_kl            | 0.0012267963 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+03     |
|    n_updates            | 13570        |
|    policy_gradient_loss | 2.87e-05     |
|    std                  | 6.09         |
|    value_loss           | 4.25e+03     |
------------------------------------------
Eval num_timesteps=2782000, episode_reward=536.43 +/- 333.44
Episode length: 429.80 +/- 45.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 536           |
| time/                   |               |
|    total_timesteps      | 2782000       |
| train/                  |               |
|    approx_kl            | 0.00014208138 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.7         |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 901           |
|    n_updates            | 13580         |
|    policy_gradient_loss | 0.000236      |
|    std                  | 6.09          |
|    value_loss           | 2.16e+03      |
-------------------------------------------
Eval num_timesteps=2784000, episode_reward=130.23 +/- 152.90
Episode length: 413.40 +/- 55.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 130         |
| time/                   |             |
|    total_timesteps      | 2784000     |
| train/                  |             |
|    approx_kl            | 2.68929e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.7       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.001       |
|    loss                 | 1.48e+03    |
|    n_updates            | 13590       |
|    policy_gradient_loss | -7.62e-05   |
|    std                  | 6.09        |
|    value_loss           | 3.97e+03    |
-----------------------------------------
Eval num_timesteps=2786000, episode_reward=251.16 +/- 171.99
Episode length: 465.00 +/- 37.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 251          |
| time/                   |              |
|    total_timesteps      | 2786000      |
| train/                  |              |
|    approx_kl            | 0.0003822317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.001        |
|    loss                 | 202          |
|    n_updates            | 13600        |
|    policy_gradient_loss | -0.000493    |
|    std                  | 6.09         |
|    value_loss           | 779          |
------------------------------------------
Eval num_timesteps=2788000, episode_reward=85.97 +/- 259.40
Episode length: 426.60 +/- 42.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | 86          |
| time/                   |             |
|    total_timesteps      | 2788000     |
| train/                  |             |
|    approx_kl            | 0.005604299 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.001       |
|    loss                 | 44.6        |
|    n_updates            | 13610       |
|    policy_gradient_loss | -0.00176    |
|    std                  | 6.09        |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=2790000, episode_reward=195.71 +/- 185.60
Episode length: 480.60 +/- 45.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 2790000      |
| train/                  |              |
|    approx_kl            | 0.0024578199 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 29.6         |
|    n_updates            | 13620        |
|    policy_gradient_loss | -0.000271    |
|    std                  | 6.08         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=2792000, episode_reward=206.27 +/- 141.69
Episode length: 500.00 +/- 24.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 206          |
| time/                   |              |
|    total_timesteps      | 2792000      |
| train/                  |              |
|    approx_kl            | 0.0020639428 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 329          |
|    n_updates            | 13630        |
|    policy_gradient_loss | -0.00169     |
|    std                  | 6.07         |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=2794000, episode_reward=190.26 +/- 428.84
Episode length: 488.20 +/- 18.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 2794000      |
| train/                  |              |
|    approx_kl            | 0.0014108226 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 246          |
|    n_updates            | 13640        |
|    policy_gradient_loss | 2.05e-05     |
|    std                  | 6.08         |
|    value_loss           | 843          |
------------------------------------------
Eval num_timesteps=2796000, episode_reward=164.93 +/- 414.46
Episode length: 487.00 +/- 54.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 165          |
| time/                   |              |
|    total_timesteps      | 2796000      |
| train/                  |              |
|    approx_kl            | 0.0016819506 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 167          |
|    n_updates            | 13650        |
|    policy_gradient_loss | -0.00076     |
|    std                  | 6.09         |
|    value_loss           | 507          |
------------------------------------------
Eval num_timesteps=2798000, episode_reward=18.83 +/- 49.25
Episode length: 455.20 +/- 45.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 18.8         |
| time/                   |              |
|    total_timesteps      | 2798000      |
| train/                  |              |
|    approx_kl            | 0.0008795204 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 105          |
|    n_updates            | 13660        |
|    policy_gradient_loss | 0.000298     |
|    std                  | 6.11         |
|    value_loss           | 452          |
------------------------------------------
Eval num_timesteps=2800000, episode_reward=205.46 +/- 142.53
Episode length: 544.00 +/- 45.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 544          |
|    mean_reward          | 205          |
| time/                   |              |
|    total_timesteps      | 2800000      |
| train/                  |              |
|    approx_kl            | 0.0007575545 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.001        |
|    loss                 | 2.35e+03     |
|    n_updates            | 13670        |
|    policy_gradient_loss | -0.000814    |
|    std                  | 6.12         |
|    value_loss           | 6.39e+03     |
------------------------------------------
Eval num_timesteps=2802000, episode_reward=34.67 +/- 124.26
Episode length: 461.00 +/- 66.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 34.7         |
| time/                   |              |
|    total_timesteps      | 2802000      |
| train/                  |              |
|    approx_kl            | 0.0013581878 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.7        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 79           |
|    n_updates            | 13680        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 6.15         |
|    value_loss           | 269          |
------------------------------------------
Eval num_timesteps=2804000, episode_reward=249.22 +/- 141.64
Episode length: 546.20 +/- 13.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 2804000      |
| train/                  |              |
|    approx_kl            | 0.0017703683 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 13690        |
|    policy_gradient_loss | 0.00043      |
|    std                  | 6.19         |
|    value_loss           | 421          |
------------------------------------------
Eval num_timesteps=2806000, episode_reward=257.85 +/- 231.72
Episode length: 514.60 +/- 80.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 515           |
|    mean_reward          | 258           |
| time/                   |               |
|    total_timesteps      | 2806000       |
| train/                  |               |
|    approx_kl            | 0.00025236903 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.8         |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.001         |
|    loss                 | 250           |
|    n_updates            | 13700         |
|    policy_gradient_loss | -0.000297     |
|    std                  | 6.22          |
|    value_loss           | 718           |
-------------------------------------------
Eval num_timesteps=2808000, episode_reward=31.64 +/- 81.80
Episode length: 492.40 +/- 43.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 31.6         |
| time/                   |              |
|    total_timesteps      | 2808000      |
| train/                  |              |
|    approx_kl            | 0.0008883422 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 91.6         |
|    n_updates            | 13710        |
|    policy_gradient_loss | -0.000721    |
|    std                  | 6.23         |
|    value_loss           | 367          |
------------------------------------------
Eval num_timesteps=2810000, episode_reward=78.96 +/- 79.50
Episode length: 503.00 +/- 72.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 79           |
| time/                   |              |
|    total_timesteps      | 2810000      |
| train/                  |              |
|    approx_kl            | 0.0019729896 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 66.4         |
|    n_updates            | 13720        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 6.25         |
|    value_loss           | 263          |
------------------------------------------
Eval num_timesteps=2812000, episode_reward=380.86 +/- 378.05
Episode length: 521.40 +/- 30.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 521          |
|    mean_reward          | 381          |
| time/                   |              |
|    total_timesteps      | 2812000      |
| train/                  |              |
|    approx_kl            | 0.0016889994 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.001        |
|    loss                 | 125          |
|    n_updates            | 13730        |
|    policy_gradient_loss | -0.000354    |
|    std                  | 6.29         |
|    value_loss           | 531          |
------------------------------------------
Eval num_timesteps=2814000, episode_reward=-21.93 +/- 211.91
Episode length: 491.60 +/- 30.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | -21.9        |
| time/                   |              |
|    total_timesteps      | 2814000      |
| train/                  |              |
|    approx_kl            | 0.0047065048 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 72.8         |
|    n_updates            | 13740        |
|    policy_gradient_loss | -0.00264     |
|    std                  | 6.31         |
|    value_loss           | 312          |
------------------------------------------
Eval num_timesteps=2816000, episode_reward=324.73 +/- 304.65
Episode length: 489.60 +/- 75.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 325      |
| time/              |          |
|    total_timesteps | 2816000  |
---------------------------------
Eval num_timesteps=2818000, episode_reward=170.82 +/- 304.33
Episode length: 439.60 +/- 52.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 2818000     |
| train/                  |             |
|    approx_kl            | 0.005657083 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.8       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 75.4        |
|    n_updates            | 13750       |
|    policy_gradient_loss | -0.0017     |
|    std                  | 6.3         |
|    value_loss           | 291         |
-----------------------------------------
Eval num_timesteps=2820000, episode_reward=502.44 +/- 374.95
Episode length: 547.40 +/- 77.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 547          |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 0.0012037862 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.8        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 55.3         |
|    n_updates            | 13760        |
|    policy_gradient_loss | -0.000283    |
|    std                  | 6.32         |
|    value_loss           | 227          |
------------------------------------------
Eval num_timesteps=2822000, episode_reward=226.42 +/- 255.51
Episode length: 453.40 +/- 53.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 226          |
| time/                   |              |
|    total_timesteps      | 2822000      |
| train/                  |              |
|    approx_kl            | 0.0019868515 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 50.9         |
|    n_updates            | 13770        |
|    policy_gradient_loss | -0.00103     |
|    std                  | 6.37         |
|    value_loss           | 197          |
------------------------------------------
Eval num_timesteps=2824000, episode_reward=522.05 +/- 300.98
Episode length: 489.20 +/- 36.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 522          |
| time/                   |              |
|    total_timesteps      | 2824000      |
| train/                  |              |
|    approx_kl            | 0.0025778497 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+03     |
|    n_updates            | 13780        |
|    policy_gradient_loss | -0.000294    |
|    std                  | 6.39         |
|    value_loss           | 3.34e+03     |
------------------------------------------
Eval num_timesteps=2826000, episode_reward=116.68 +/- 252.66
Episode length: 448.40 +/- 53.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 117          |
| time/                   |              |
|    total_timesteps      | 2826000      |
| train/                  |              |
|    approx_kl            | 0.0006200194 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 887          |
|    n_updates            | 13790        |
|    policy_gradient_loss | -0.000525    |
|    std                  | 6.4          |
|    value_loss           | 2.97e+03     |
------------------------------------------
Eval num_timesteps=2828000, episode_reward=466.62 +/- 425.51
Episode length: 452.00 +/- 60.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 467           |
| time/                   |               |
|    total_timesteps      | 2828000       |
| train/                  |               |
|    approx_kl            | 0.00029539075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.9         |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+03      |
|    n_updates            | 13800         |
|    policy_gradient_loss | -0.000143     |
|    std                  | 6.4           |
|    value_loss           | 3.16e+03      |
-------------------------------------------
Eval num_timesteps=2830000, episode_reward=302.97 +/- 282.46
Episode length: 429.40 +/- 62.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 303          |
| time/                   |              |
|    total_timesteps      | 2830000      |
| train/                  |              |
|    approx_kl            | 4.242055e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+03     |
|    n_updates            | 13810        |
|    policy_gradient_loss | -2.59e-05    |
|    std                  | 6.4          |
|    value_loss           | 3.26e+03     |
------------------------------------------
Eval num_timesteps=2832000, episode_reward=757.91 +/- 438.07
Episode length: 503.60 +/- 46.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 758          |
| time/                   |              |
|    total_timesteps      | 2832000      |
| train/                  |              |
|    approx_kl            | 6.094569e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 13820        |
|    policy_gradient_loss | -0.000187    |
|    std                  | 6.4          |
|    value_loss           | 470          |
------------------------------------------
Eval num_timesteps=2834000, episode_reward=418.76 +/- 226.15
Episode length: 457.40 +/- 45.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 419           |
| time/                   |               |
|    total_timesteps      | 2834000       |
| train/                  |               |
|    approx_kl            | 0.00016717607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -12.9         |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 13830         |
|    policy_gradient_loss | -0.00014      |
|    std                  | 6.41          |
|    value_loss           | 3.95e+03      |
-------------------------------------------
Eval num_timesteps=2836000, episode_reward=443.95 +/- 406.78
Episode length: 486.00 +/- 169.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 444          |
| time/                   |              |
|    total_timesteps      | 2836000      |
| train/                  |              |
|    approx_kl            | 0.0020720498 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 45.2         |
|    n_updates            | 13840        |
|    policy_gradient_loss | -0.00137     |
|    std                  | 6.43         |
|    value_loss           | 248          |
------------------------------------------
Eval num_timesteps=2838000, episode_reward=125.96 +/- 132.02
Episode length: 392.60 +/- 52.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 2838000      |
| train/                  |              |
|    approx_kl            | 0.0024818429 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -12.9        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 83.7         |
|    n_updates            | 13850        |
|    policy_gradient_loss | -0.000791    |
|    std                  | 6.47         |
|    value_loss           | 394          |
------------------------------------------
Eval num_timesteps=2840000, episode_reward=259.80 +/- 277.56
Episode length: 465.60 +/- 27.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 260          |
| time/                   |              |
|    total_timesteps      | 2840000      |
| train/                  |              |
|    approx_kl            | 0.0017172721 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 43.5         |
|    n_updates            | 13860        |
|    policy_gradient_loss | 0.000342     |
|    std                  | 6.49         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=2842000, episode_reward=121.44 +/- 237.82
Episode length: 374.80 +/- 39.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 121          |
| time/                   |              |
|    total_timesteps      | 2842000      |
| train/                  |              |
|    approx_kl            | 0.0013803712 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 13870        |
|    policy_gradient_loss | 0.000197     |
|    std                  | 6.51         |
|    value_loss           | 698          |
------------------------------------------
Eval num_timesteps=2844000, episode_reward=253.30 +/- 287.70
Episode length: 404.20 +/- 13.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 2844000       |
| train/                  |               |
|    approx_kl            | 0.00040652353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 44.1          |
|    n_updates            | 13880         |
|    policy_gradient_loss | -0.000108     |
|    std                  | 6.51          |
|    value_loss           | 185           |
-------------------------------------------
Eval num_timesteps=2846000, episode_reward=247.58 +/- 151.34
Episode length: 402.20 +/- 35.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 402          |
|    mean_reward          | 248          |
| time/                   |              |
|    total_timesteps      | 2846000      |
| train/                  |              |
|    approx_kl            | 0.0002679876 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 63.9         |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.000244    |
|    std                  | 6.51         |
|    value_loss           | 353          |
------------------------------------------
Eval num_timesteps=2848000, episode_reward=293.28 +/- 342.97
Episode length: 405.20 +/- 33.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 293           |
| time/                   |               |
|    total_timesteps      | 2848000       |
| train/                  |               |
|    approx_kl            | 0.00074755645 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 47.4          |
|    n_updates            | 13900         |
|    policy_gradient_loss | -0.000363     |
|    std                  | 6.52          |
|    value_loss           | 188           |
-------------------------------------------
Eval num_timesteps=2850000, episode_reward=6.23 +/- 65.51
Episode length: 366.80 +/- 19.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 367          |
|    mean_reward          | 6.23         |
| time/                   |              |
|    total_timesteps      | 2850000      |
| train/                  |              |
|    approx_kl            | 0.0015068369 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 43.4         |
|    n_updates            | 13910        |
|    policy_gradient_loss | -0.00134     |
|    std                  | 6.53         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=2852000, episode_reward=127.31 +/- 153.79
Episode length: 381.60 +/- 59.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 382           |
|    mean_reward          | 127           |
| time/                   |               |
|    total_timesteps      | 2852000       |
| train/                  |               |
|    approx_kl            | 0.00026417835 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.001         |
|    loss                 | 31.6          |
|    n_updates            | 13920         |
|    policy_gradient_loss | 4.15e-06      |
|    std                  | 6.54          |
|    value_loss           | 95.1          |
-------------------------------------------
Eval num_timesteps=2854000, episode_reward=89.86 +/- 248.87
Episode length: 392.20 +/- 92.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 89.9         |
| time/                   |              |
|    total_timesteps      | 2854000      |
| train/                  |              |
|    approx_kl            | 0.0051232055 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 23.5         |
|    n_updates            | 13930        |
|    policy_gradient_loss | -0.00245     |
|    std                  | 6.6          |
|    value_loss           | 86           |
------------------------------------------
Eval num_timesteps=2856000, episode_reward=12.44 +/- 127.55
Episode length: 374.40 +/- 70.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 2856000     |
| train/                  |             |
|    approx_kl            | 0.002943663 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.001       |
|    loss                 | 32.2        |
|    n_updates            | 13940       |
|    policy_gradient_loss | -0.00173    |
|    std                  | 6.63        |
|    value_loss           | 95.6        |
-----------------------------------------
Eval num_timesteps=2858000, episode_reward=-5.19 +/- 79.25
Episode length: 334.60 +/- 23.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | -5.19        |
| time/                   |              |
|    total_timesteps      | 2858000      |
| train/                  |              |
|    approx_kl            | 0.0032391378 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 13950        |
|    policy_gradient_loss | -0.00058     |
|    std                  | 6.63         |
|    value_loss           | 386          |
------------------------------------------
Eval num_timesteps=2860000, episode_reward=-7.90 +/- 95.65
Episode length: 373.60 +/- 83.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 374          |
|    mean_reward          | -7.9         |
| time/                   |              |
|    total_timesteps      | 2860000      |
| train/                  |              |
|    approx_kl            | 0.0025900863 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 21.5         |
|    n_updates            | 13960        |
|    policy_gradient_loss | -0.000752    |
|    std                  | 6.63         |
|    value_loss           | 78.2         |
------------------------------------------
Eval num_timesteps=2862000, episode_reward=154.51 +/- 298.02
Episode length: 400.60 +/- 54.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | 155         |
| time/                   |             |
|    total_timesteps      | 2862000     |
| train/                  |             |
|    approx_kl            | 0.008191974 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 20.6        |
|    n_updates            | 13970       |
|    policy_gradient_loss | -0.00229    |
|    std                  | 6.65        |
|    value_loss           | 79.3        |
-----------------------------------------
Eval num_timesteps=2864000, episode_reward=58.81 +/- 116.82
Episode length: 387.20 +/- 68.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 58.8         |
| time/                   |              |
|    total_timesteps      | 2864000      |
| train/                  |              |
|    approx_kl            | 0.0006582093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.001        |
|    loss                 | 3.69e+03     |
|    n_updates            | 13980        |
|    policy_gradient_loss | 0.000433     |
|    std                  | 6.66         |
|    value_loss           | 9.77e+03     |
------------------------------------------
Eval num_timesteps=2866000, episode_reward=345.40 +/- 300.99
Episode length: 477.00 +/- 77.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 477           |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 2866000       |
| train/                  |               |
|    approx_kl            | 5.5499724e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.608         |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+03      |
|    n_updates            | 13990         |
|    policy_gradient_loss | -2.52e-05     |
|    std                  | 6.67          |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=2868000, episode_reward=503.03 +/- 507.63
Episode length: 545.60 +/- 160.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 546          |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 2868000      |
| train/                  |              |
|    approx_kl            | 0.0032103336 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 47           |
|    n_updates            | 14000        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 6.69         |
|    value_loss           | 212          |
------------------------------------------
Eval num_timesteps=2870000, episode_reward=647.48 +/- 652.24
Episode length: 524.60 +/- 77.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 647         |
| time/                   |             |
|    total_timesteps      | 2870000     |
| train/                  |             |
|    approx_kl            | 0.003560584 |
|    clip_fraction        | 0.00591     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.001       |
|    loss                 | 161         |
|    n_updates            | 14010       |
|    policy_gradient_loss | 0.000171    |
|    std                  | 6.72        |
|    value_loss           | 781         |
-----------------------------------------
Eval num_timesteps=2872000, episode_reward=227.17 +/- 143.05
Episode length: 449.20 +/- 56.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 227           |
| time/                   |               |
|    total_timesteps      | 2872000       |
| train/                  |               |
|    approx_kl            | 0.00054631144 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.1         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 30.9          |
|    n_updates            | 14020         |
|    policy_gradient_loss | -0.000698     |
|    std                  | 6.74          |
|    value_loss           | 125           |
-------------------------------------------
Eval num_timesteps=2874000, episode_reward=406.69 +/- 325.31
Episode length: 452.40 +/- 96.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 2874000      |
| train/                  |              |
|    approx_kl            | 0.0007791312 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.821        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+03      |
|    n_updates            | 14030        |
|    policy_gradient_loss | -0.000207    |
|    std                  | 6.74         |
|    value_loss           | 3.35e+03     |
------------------------------------------
Eval num_timesteps=2876000, episode_reward=525.61 +/- 394.35
Episode length: 512.00 +/- 88.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 512          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 2876000      |
| train/                  |              |
|    approx_kl            | 0.0004178231 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 14040        |
|    policy_gradient_loss | -0.000424    |
|    std                  | 6.74         |
|    value_loss           | 1.06e+03     |
------------------------------------------
Eval num_timesteps=2878000, episode_reward=24.60 +/- 258.99
Episode length: 406.60 +/- 45.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 24.6         |
| time/                   |              |
|    total_timesteps      | 2878000      |
| train/                  |              |
|    approx_kl            | 0.0008666706 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 59.8         |
|    n_updates            | 14050        |
|    policy_gradient_loss | -0.00051     |
|    std                  | 6.74         |
|    value_loss           | 264          |
------------------------------------------
Eval num_timesteps=2880000, episode_reward=184.43 +/- 248.10
Episode length: 485.60 +/- 70.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 486           |
|    mean_reward          | 184           |
| time/                   |               |
|    total_timesteps      | 2880000       |
| train/                  |               |
|    approx_kl            | 0.00089096464 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 75.3          |
|    n_updates            | 14060         |
|    policy_gradient_loss | -0.000492     |
|    std                  | 6.76          |
|    value_loss           | 314           |
-------------------------------------------
Eval num_timesteps=2882000, episode_reward=228.27 +/- 485.02
Episode length: 478.40 +/- 102.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 228          |
| time/                   |              |
|    total_timesteps      | 2882000      |
| train/                  |              |
|    approx_kl            | 0.0026133065 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 831          |
|    n_updates            | 14070        |
|    policy_gradient_loss | -0.000145    |
|    std                  | 6.77         |
|    value_loss           | 2.33e+03     |
------------------------------------------
Eval num_timesteps=2884000, episode_reward=391.79 +/- 252.54
Episode length: 419.20 +/- 51.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 419          |
|    mean_reward          | 392          |
| time/                   |              |
|    total_timesteps      | 2884000      |
| train/                  |              |
|    approx_kl            | 0.0001018451 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 49.4         |
|    n_updates            | 14080        |
|    policy_gradient_loss | -0.000114    |
|    std                  | 6.77         |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=2886000, episode_reward=327.78 +/- 438.92
Episode length: 418.40 +/- 57.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 418        |
|    mean_reward          | 328        |
| time/                   |            |
|    total_timesteps      | 2886000    |
| train/                  |            |
|    approx_kl            | 0.00263878 |
|    clip_fraction        | 0.000879   |
|    clip_range           | 0.2        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.001      |
|    loss                 | 43.3       |
|    n_updates            | 14090      |
|    policy_gradient_loss | -0.00118   |
|    std                  | 6.76       |
|    value_loss           | 192        |
----------------------------------------
Eval num_timesteps=2888000, episode_reward=246.21 +/- 435.94
Episode length: 473.20 +/- 66.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 246          |
| time/                   |              |
|    total_timesteps      | 2888000      |
| train/                  |              |
|    approx_kl            | 0.0013878617 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 1.31e+03     |
|    n_updates            | 14100        |
|    policy_gradient_loss | -0.000472    |
|    std                  | 6.77         |
|    value_loss           | 3.77e+03     |
------------------------------------------
Eval num_timesteps=2890000, episode_reward=346.17 +/- 246.91
Episode length: 446.40 +/- 44.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 346           |
| time/                   |               |
|    total_timesteps      | 2890000       |
| train/                  |               |
|    approx_kl            | 0.00019203287 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.1         |
|    explained_variance   | 0.851         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+03      |
|    n_updates            | 14110         |
|    policy_gradient_loss | 0.000366      |
|    std                  | 6.78          |
|    value_loss           | 3.78e+03      |
-------------------------------------------
Eval num_timesteps=2892000, episode_reward=81.17 +/- 76.08
Episode length: 421.60 +/- 69.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 81.2          |
| time/                   |               |
|    total_timesteps      | 2892000       |
| train/                  |               |
|    approx_kl            | 0.00015547607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.1         |
|    explained_variance   | 0.81          |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+03      |
|    n_updates            | 14120         |
|    policy_gradient_loss | 0.000136      |
|    std                  | 6.78          |
|    value_loss           | 3.89e+03      |
-------------------------------------------
Eval num_timesteps=2894000, episode_reward=164.82 +/- 123.58
Episode length: 409.60 +/- 80.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 165          |
| time/                   |              |
|    total_timesteps      | 2894000      |
| train/                  |              |
|    approx_kl            | 0.0007156476 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 60.3         |
|    n_updates            | 14130        |
|    policy_gradient_loss | -0.000931    |
|    std                  | 6.79         |
|    value_loss           | 266          |
------------------------------------------
Eval num_timesteps=2896000, episode_reward=106.98 +/- 162.93
Episode length: 376.80 +/- 47.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 107          |
| time/                   |              |
|    total_timesteps      | 2896000      |
| train/                  |              |
|    approx_kl            | 0.0015787601 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.779        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 14140        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 6.8          |
|    value_loss           | 4.78e+03     |
------------------------------------------
Eval num_timesteps=2898000, episode_reward=325.39 +/- 298.73
Episode length: 383.20 +/- 66.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 2898000      |
| train/                  |              |
|    approx_kl            | 0.0014051127 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 427          |
|    n_updates            | 14150        |
|    policy_gradient_loss | -0.00129     |
|    std                  | 6.8          |
|    value_loss           | 1.08e+03     |
------------------------------------------
Eval num_timesteps=2900000, episode_reward=455.10 +/- 278.18
Episode length: 394.60 +/- 31.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 395          |
|    mean_reward          | 455          |
| time/                   |              |
|    total_timesteps      | 2900000      |
| train/                  |              |
|    approx_kl            | 0.0015767377 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 38.5         |
|    n_updates            | 14160        |
|    policy_gradient_loss | -0.000207    |
|    std                  | 6.79         |
|    value_loss           | 164          |
------------------------------------------
Eval num_timesteps=2902000, episode_reward=336.43 +/- 269.37
Episode length: 414.80 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 2902000  |
---------------------------------
Eval num_timesteps=2904000, episode_reward=348.33 +/- 205.21
Episode length: 389.60 +/- 31.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 348          |
| time/                   |              |
|    total_timesteps      | 2904000      |
| train/                  |              |
|    approx_kl            | 0.0015088418 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 50.3         |
|    n_updates            | 14170        |
|    policy_gradient_loss | -0.000336    |
|    std                  | 6.79         |
|    value_loss           | 283          |
------------------------------------------
Eval num_timesteps=2906000, episode_reward=541.01 +/- 297.81
Episode length: 377.60 +/- 51.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 378        |
|    mean_reward          | 541        |
| time/                   |            |
|    total_timesteps      | 2906000    |
| train/                  |            |
|    approx_kl            | 0.00349803 |
|    clip_fraction        | 0.00327    |
|    clip_range           | 0.2        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.001      |
|    loss                 | 61.2       |
|    n_updates            | 14180      |
|    policy_gradient_loss | -0.000616  |
|    std                  | 6.8        |
|    value_loss           | 272        |
----------------------------------------
Eval num_timesteps=2908000, episode_reward=261.34 +/- 149.37
Episode length: 413.00 +/- 102.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 2908000      |
| train/                  |              |
|    approx_kl            | 0.0015055486 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.001        |
|    loss                 | 2.66e+03     |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 6.8          |
|    value_loss           | 7.09e+03     |
------------------------------------------
Eval num_timesteps=2910000, episode_reward=144.71 +/- 94.93
Episode length: 385.80 +/- 48.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 145           |
| time/                   |               |
|    total_timesteps      | 2910000       |
| train/                  |               |
|    approx_kl            | 0.00065349287 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13           |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 129           |
|    n_updates            | 14200         |
|    policy_gradient_loss | -0.000517     |
|    std                  | 6.81          |
|    value_loss           | 619           |
-------------------------------------------
Eval num_timesteps=2912000, episode_reward=144.81 +/- 112.22
Episode length: 324.40 +/- 14.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 324        |
|    mean_reward          | 145        |
| time/                   |            |
|    total_timesteps      | 2912000    |
| train/                  |            |
|    approx_kl            | 0.00059456 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 100        |
|    n_updates            | 14210      |
|    policy_gradient_loss | -0.000869  |
|    std                  | 6.81       |
|    value_loss           | 536        |
----------------------------------------
Eval num_timesteps=2914000, episode_reward=519.12 +/- 435.34
Episode length: 453.40 +/- 131.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 519          |
| time/                   |              |
|    total_timesteps      | 2914000      |
| train/                  |              |
|    approx_kl            | 0.0031609102 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 36.1         |
|    n_updates            | 14220        |
|    policy_gradient_loss | -0.00103     |
|    std                  | 6.78         |
|    value_loss           | 209          |
------------------------------------------
Eval num_timesteps=2916000, episode_reward=619.63 +/- 224.68
Episode length: 430.00 +/- 68.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 620          |
| time/                   |              |
|    total_timesteps      | 2916000      |
| train/                  |              |
|    approx_kl            | 0.0028359052 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 84.8         |
|    n_updates            | 14230        |
|    policy_gradient_loss | -0.000455    |
|    std                  | 6.75         |
|    value_loss           | 406          |
------------------------------------------
Eval num_timesteps=2918000, episode_reward=354.87 +/- 500.71
Episode length: 395.60 +/- 85.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 355         |
| time/                   |             |
|    total_timesteps      | 2918000     |
| train/                  |             |
|    approx_kl            | 0.003353455 |
|    clip_fraction        | 0.00278     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 26.3        |
|    n_updates            | 14240       |
|    policy_gradient_loss | -0.00148    |
|    std                  | 6.74        |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=2920000, episode_reward=304.44 +/- 233.74
Episode length: 374.40 +/- 20.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 374         |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 2920000     |
| train/                  |             |
|    approx_kl            | 0.007254131 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.001       |
|    loss                 | 1.55e+03    |
|    n_updates            | 14250       |
|    policy_gradient_loss | 0.00166     |
|    std                  | 6.74        |
|    value_loss           | 4.2e+03     |
-----------------------------------------
Eval num_timesteps=2922000, episode_reward=191.09 +/- 162.55
Episode length: 443.80 +/- 30.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 2922000      |
| train/                  |              |
|    approx_kl            | 0.0015697344 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13          |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 59.1         |
|    n_updates            | 14260        |
|    policy_gradient_loss | -0.00072     |
|    std                  | 6.75         |
|    value_loss           | 266          |
------------------------------------------
Eval num_timesteps=2924000, episode_reward=428.92 +/- 293.26
Episode length: 385.60 +/- 40.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 429          |
| time/                   |              |
|    total_timesteps      | 2924000      |
| train/                  |              |
|    approx_kl            | 0.0009621375 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+03     |
|    n_updates            | 14270        |
|    policy_gradient_loss | -0.000536    |
|    std                  | 6.77         |
|    value_loss           | 3.7e+03      |
------------------------------------------
Eval num_timesteps=2926000, episode_reward=379.58 +/- 193.90
Episode length: 460.40 +/- 69.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 380           |
| time/                   |               |
|    total_timesteps      | 2926000       |
| train/                  |               |
|    approx_kl            | 0.00022338325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.1         |
|    explained_variance   | 0.857         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 14280         |
|    policy_gradient_loss | -0.000237     |
|    std                  | 6.78          |
|    value_loss           | 2.95e+03      |
-------------------------------------------
Eval num_timesteps=2928000, episode_reward=401.68 +/- 282.75
Episode length: 452.80 +/- 92.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 402           |
| time/                   |               |
|    total_timesteps      | 2928000       |
| train/                  |               |
|    approx_kl            | 0.00026394453 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.1         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 110           |
|    n_updates            | 14290         |
|    policy_gradient_loss | -0.00022      |
|    std                  | 6.8           |
|    value_loss           | 561           |
-------------------------------------------
Eval num_timesteps=2930000, episode_reward=120.01 +/- 108.44
Episode length: 440.80 +/- 76.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 2930000      |
| train/                  |              |
|    approx_kl            | 0.0014561866 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 60.4         |
|    n_updates            | 14300        |
|    policy_gradient_loss | -0.00119     |
|    std                  | 6.83         |
|    value_loss           | 215          |
------------------------------------------
Eval num_timesteps=2932000, episode_reward=124.71 +/- 188.38
Episode length: 431.40 +/- 48.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 125          |
| time/                   |              |
|    total_timesteps      | 2932000      |
| train/                  |              |
|    approx_kl            | 0.0020236913 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+03     |
|    n_updates            | 14310        |
|    policy_gradient_loss | 0.001        |
|    std                  | 6.86         |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=2934000, episode_reward=198.70 +/- 242.81
Episode length: 429.20 +/- 79.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 199          |
| time/                   |              |
|    total_timesteps      | 2934000      |
| train/                  |              |
|    approx_kl            | 0.0008930068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 92.8         |
|    n_updates            | 14320        |
|    policy_gradient_loss | -0.000609    |
|    std                  | 6.88         |
|    value_loss           | 428          |
------------------------------------------
Eval num_timesteps=2936000, episode_reward=570.88 +/- 342.94
Episode length: 432.00 +/- 86.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 571         |
| time/                   |             |
|    total_timesteps      | 2936000     |
| train/                  |             |
|    approx_kl            | 0.008364337 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 59.8        |
|    n_updates            | 14330       |
|    policy_gradient_loss | -0.00288    |
|    std                  | 6.9         |
|    value_loss           | 289         |
-----------------------------------------
Eval num_timesteps=2938000, episode_reward=108.74 +/- 218.77
Episode length: 385.80 +/- 17.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 109          |
| time/                   |              |
|    total_timesteps      | 2938000      |
| train/                  |              |
|    approx_kl            | 0.0013899342 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.1        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 50.4         |
|    n_updates            | 14340        |
|    policy_gradient_loss | -0.000944    |
|    std                  | 6.94         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=2940000, episode_reward=33.58 +/- 101.51
Episode length: 473.00 +/- 62.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | 33.6        |
| time/                   |             |
|    total_timesteps      | 2940000     |
| train/                  |             |
|    approx_kl            | 0.001921921 |
|    clip_fraction        | 0.0021      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 151         |
|    n_updates            | 14350       |
|    policy_gradient_loss | -0.000936   |
|    std                  | 6.98        |
|    value_loss           | 654         |
-----------------------------------------
Eval num_timesteps=2942000, episode_reward=229.49 +/- 166.10
Episode length: 481.40 +/- 66.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 229           |
| time/                   |               |
|    total_timesteps      | 2942000       |
| train/                  |               |
|    approx_kl            | 0.00041785958 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.2         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 150           |
|    n_updates            | 14360         |
|    policy_gradient_loss | 9.82e-05      |
|    std                  | 6.99          |
|    value_loss           | 592           |
-------------------------------------------
Eval num_timesteps=2944000, episode_reward=119.42 +/- 113.37
Episode length: 494.00 +/- 83.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | 119          |
| time/                   |              |
|    total_timesteps      | 2944000      |
| train/                  |              |
|    approx_kl            | 0.0003020091 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.2        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 191          |
|    n_updates            | 14370        |
|    policy_gradient_loss | -0.000357    |
|    std                  | 6.98         |
|    value_loss           | 828          |
------------------------------------------
Eval num_timesteps=2946000, episode_reward=350.61 +/- 320.70
Episode length: 498.00 +/- 39.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 2946000      |
| train/                  |              |
|    approx_kl            | 0.0006458475 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.2        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 65.2         |
|    n_updates            | 14380        |
|    policy_gradient_loss | -0.000384    |
|    std                  | 6.99         |
|    value_loss           | 224          |
------------------------------------------
Eval num_timesteps=2948000, episode_reward=106.37 +/- 170.12
Episode length: 475.20 +/- 56.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 106           |
| time/                   |               |
|    total_timesteps      | 2948000       |
| train/                  |               |
|    approx_kl            | 0.00024758253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.2         |
|    explained_variance   | 0.78          |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+03      |
|    n_updates            | 14390         |
|    policy_gradient_loss | -6.64e-05     |
|    std                  | 7.01          |
|    value_loss           | 6.07e+03      |
-------------------------------------------
Eval num_timesteps=2950000, episode_reward=44.07 +/- 109.21
Episode length: 426.00 +/- 47.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 44.1          |
| time/                   |               |
|    total_timesteps      | 2950000       |
| train/                  |               |
|    approx_kl            | 0.00046021567 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.2         |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 99.2          |
|    n_updates            | 14400         |
|    policy_gradient_loss | -0.000582     |
|    std                  | 7.03          |
|    value_loss           | 357           |
-------------------------------------------
Eval num_timesteps=2952000, episode_reward=-116.08 +/- 407.24
Episode length: 531.80 +/- 105.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 532          |
|    mean_reward          | -116         |
| time/                   |              |
|    total_timesteps      | 2952000      |
| train/                  |              |
|    approx_kl            | 0.0013221237 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.2        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 93.1         |
|    n_updates            | 14410        |
|    policy_gradient_loss | -0.000914    |
|    std                  | 7.07         |
|    value_loss           | 357          |
------------------------------------------
Eval num_timesteps=2954000, episode_reward=30.79 +/- 80.98
Episode length: 455.60 +/- 50.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 30.8        |
| time/                   |             |
|    total_timesteps      | 2954000     |
| train/                  |             |
|    approx_kl            | 0.005771448 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 87.2        |
|    n_updates            | 14420       |
|    policy_gradient_loss | -0.00115    |
|    std                  | 7.09        |
|    value_loss           | 304         |
-----------------------------------------
Eval num_timesteps=2956000, episode_reward=386.79 +/- 800.72
Episode length: 504.80 +/- 135.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 505           |
|    mean_reward          | 387           |
| time/                   |               |
|    total_timesteps      | 2956000       |
| train/                  |               |
|    approx_kl            | 0.00067061454 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.2         |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 82.8          |
|    n_updates            | 14430         |
|    policy_gradient_loss | -0.000244     |
|    std                  | 7.1           |
|    value_loss           | 390           |
-------------------------------------------
Eval num_timesteps=2958000, episode_reward=68.22 +/- 183.45
Episode length: 463.00 +/- 58.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 463           |
|    mean_reward          | 68.2          |
| time/                   |               |
|    total_timesteps      | 2958000       |
| train/                  |               |
|    approx_kl            | 0.00070886075 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.2         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 80.2          |
|    n_updates            | 14440         |
|    policy_gradient_loss | -0.000427     |
|    std                  | 7.12          |
|    value_loss           | 268           |
-------------------------------------------
Eval num_timesteps=2960000, episode_reward=39.03 +/- 152.56
Episode length: 482.60 +/- 67.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 483          |
|    mean_reward          | 39           |
| time/                   |              |
|    total_timesteps      | 2960000      |
| train/                  |              |
|    approx_kl            | 0.0018309592 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 88.3         |
|    n_updates            | 14450        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 7.15         |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=2962000, episode_reward=576.26 +/- 929.11
Episode length: 531.40 +/- 109.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 531          |
|    mean_reward          | 576          |
| time/                   |              |
|    total_timesteps      | 2962000      |
| train/                  |              |
|    approx_kl            | 0.0068545477 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 45           |
|    n_updates            | 14460        |
|    policy_gradient_loss | -0.00148     |
|    std                  | 7.18         |
|    value_loss           | 169          |
------------------------------------------
Eval num_timesteps=2964000, episode_reward=111.36 +/- 113.54
Episode length: 465.60 +/- 66.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 111          |
| time/                   |              |
|    total_timesteps      | 2964000      |
| train/                  |              |
|    approx_kl            | 0.0012985417 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 14470        |
|    policy_gradient_loss | -0.00089     |
|    std                  | 7.2          |
|    value_loss           | 378          |
------------------------------------------
Eval num_timesteps=2966000, episode_reward=355.47 +/- 566.72
Episode length: 472.40 +/- 49.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 2966000      |
| train/                  |              |
|    approx_kl            | 0.0022511722 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 192          |
|    n_updates            | 14480        |
|    policy_gradient_loss | -0.00105     |
|    std                  | 7.22         |
|    value_loss           | 708          |
------------------------------------------
Eval num_timesteps=2968000, episode_reward=358.71 +/- 202.75
Episode length: 418.00 +/- 68.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 2968000       |
| train/                  |               |
|    approx_kl            | 0.00048990257 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.3         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 120           |
|    n_updates            | 14490         |
|    policy_gradient_loss | 0.000311      |
|    std                  | 7.24          |
|    value_loss           | 485           |
-------------------------------------------
Eval num_timesteps=2970000, episode_reward=346.45 +/- 553.32
Episode length: 424.60 +/- 73.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 346           |
| time/                   |               |
|    total_timesteps      | 2970000       |
| train/                  |               |
|    approx_kl            | 0.00025133873 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.3         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 73.3          |
|    n_updates            | 14500         |
|    policy_gradient_loss | -0.000485     |
|    std                  | 7.26          |
|    value_loss           | 355           |
-------------------------------------------
Eval num_timesteps=2972000, episode_reward=517.97 +/- 260.88
Episode length: 447.20 +/- 65.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 447           |
|    mean_reward          | 518           |
| time/                   |               |
|    total_timesteps      | 2972000       |
| train/                  |               |
|    approx_kl            | 0.00041842193 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.3         |
|    explained_variance   | 0.783         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 14510         |
|    policy_gradient_loss | 0.000239      |
|    std                  | 7.29          |
|    value_loss           | 4.3e+03       |
-------------------------------------------
Eval num_timesteps=2974000, episode_reward=275.98 +/- 224.80
Episode length: 391.60 +/- 32.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | 276          |
| time/                   |              |
|    total_timesteps      | 2974000      |
| train/                  |              |
|    approx_kl            | 8.097102e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 94.6         |
|    n_updates            | 14520        |
|    policy_gradient_loss | -0.00022     |
|    std                  | 7.31         |
|    value_loss           | 359          |
------------------------------------------
Eval num_timesteps=2976000, episode_reward=1287.23 +/- 1750.57
Episode length: 475.40 +/- 99.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 2976000       |
| train/                  |               |
|    approx_kl            | 0.00033977462 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.3         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 88.6          |
|    n_updates            | 14530         |
|    policy_gradient_loss | -0.000426     |
|    std                  | 7.33          |
|    value_loss           | 383           |
-------------------------------------------
Eval num_timesteps=2978000, episode_reward=496.76 +/- 381.03
Episode length: 408.80 +/- 85.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 497          |
| time/                   |              |
|    total_timesteps      | 2978000      |
| train/                  |              |
|    approx_kl            | 0.0009957561 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 54.4         |
|    n_updates            | 14540        |
|    policy_gradient_loss | -0.000521    |
|    std                  | 7.35         |
|    value_loss           | 210          |
------------------------------------------
Eval num_timesteps=2980000, episode_reward=284.01 +/- 214.97
Episode length: 445.80 +/- 68.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 2980000      |
| train/                  |              |
|    approx_kl            | 0.0010356065 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 2.93e+03     |
|    n_updates            | 14550        |
|    policy_gradient_loss | -3.63e-06    |
|    std                  | 7.36         |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=2982000, episode_reward=115.42 +/- 88.58
Episode length: 372.60 +/- 48.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 373         |
|    mean_reward          | 115         |
| time/                   |             |
|    total_timesteps      | 2982000     |
| train/                  |             |
|    approx_kl            | 8.92877e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 90.8        |
|    n_updates            | 14560       |
|    policy_gradient_loss | -0.000231   |
|    std                  | 7.36        |
|    value_loss           | 293         |
-----------------------------------------
Eval num_timesteps=2984000, episode_reward=992.96 +/- 1227.06
Episode length: 431.00 +/- 106.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 993           |
| time/                   |               |
|    total_timesteps      | 2984000       |
| train/                  |               |
|    approx_kl            | 0.00047477204 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 164           |
|    n_updates            | 14570         |
|    policy_gradient_loss | -0.000667     |
|    std                  | 7.34          |
|    value_loss           | 692           |
-------------------------------------------
Eval num_timesteps=2986000, episode_reward=568.03 +/- 159.19
Episode length: 396.20 +/- 32.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | 568          |
| time/                   |              |
|    total_timesteps      | 2986000      |
| train/                  |              |
|    approx_kl            | 0.0005557968 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.832        |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+03     |
|    n_updates            | 14580        |
|    policy_gradient_loss | -0.000506    |
|    std                  | 7.33         |
|    value_loss           | 4.46e+03     |
------------------------------------------
Eval num_timesteps=2988000, episode_reward=219.75 +/- 129.90
Episode length: 399.20 +/- 60.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=227.59 +/- 188.56
Episode length: 418.20 +/- 45.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 228           |
| time/                   |               |
|    total_timesteps      | 2990000       |
| train/                  |               |
|    approx_kl            | 0.00019463466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.796         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+03      |
|    n_updates            | 14590         |
|    policy_gradient_loss | -0.00031      |
|    std                  | 7.33          |
|    value_loss           | 7.2e+03       |
-------------------------------------------
Eval num_timesteps=2992000, episode_reward=220.24 +/- 379.31
Episode length: 443.20 +/- 79.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 220          |
| time/                   |              |
|    total_timesteps      | 2992000      |
| train/                  |              |
|    approx_kl            | 4.435712e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.001        |
|    loss                 | 2.06e+03     |
|    n_updates            | 14600        |
|    policy_gradient_loss | 1.27e-05     |
|    std                  | 7.34         |
|    value_loss           | 5.7e+03      |
------------------------------------------
Eval num_timesteps=2994000, episode_reward=335.88 +/- 221.37
Episode length: 409.60 +/- 60.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 336          |
| time/                   |              |
|    total_timesteps      | 2994000      |
| train/                  |              |
|    approx_kl            | 0.0005676357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 74.5         |
|    n_updates            | 14610        |
|    policy_gradient_loss | -0.000649    |
|    std                  | 7.33         |
|    value_loss           | 247          |
------------------------------------------
Eval num_timesteps=2996000, episode_reward=452.28 +/- 417.51
Episode length: 444.80 +/- 57.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 445           |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 2996000       |
| train/                  |               |
|    approx_kl            | 0.00050570106 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.001         |
|    loss                 | 1.96e+03      |
|    n_updates            | 14620         |
|    policy_gradient_loss | -0.000391     |
|    std                  | 7.32          |
|    value_loss           | 6.07e+03      |
-------------------------------------------
Eval num_timesteps=2998000, episode_reward=434.97 +/- 327.21
Episode length: 483.20 +/- 61.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 2998000       |
| train/                  |               |
|    approx_kl            | 0.00020661697 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.3         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 79.1          |
|    n_updates            | 14630         |
|    policy_gradient_loss | -0.000221     |
|    std                  | 7.33          |
|    value_loss           | 366           |
-------------------------------------------
Eval num_timesteps=3000000, episode_reward=179.18 +/- 168.97
Episode length: 374.60 +/- 78.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 375           |
|    mean_reward          | 179           |
| time/                   |               |
|    total_timesteps      | 3000000       |
| train/                  |               |
|    approx_kl            | 0.00018429567 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+03      |
|    n_updates            | 14640         |
|    policy_gradient_loss | 8.36e-05      |
|    std                  | 7.34          |
|    value_loss           | 3.32e+03      |
-------------------------------------------
Eval num_timesteps=3002000, episode_reward=98.46 +/- 201.86
Episode length: 486.20 +/- 50.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 98.5         |
| time/                   |              |
|    total_timesteps      | 3002000      |
| train/                  |              |
|    approx_kl            | 8.507958e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 128          |
|    n_updates            | 14650        |
|    policy_gradient_loss | -0.000168    |
|    std                  | 7.35         |
|    value_loss           | 499          |
------------------------------------------
Eval num_timesteps=3004000, episode_reward=298.92 +/- 192.66
Episode length: 407.80 +/- 44.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 299           |
| time/                   |               |
|    total_timesteps      | 3004000       |
| train/                  |               |
|    approx_kl            | 0.00020034978 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.616         |
|    learning_rate        | 0.001         |
|    loss                 | 2.69e+03      |
|    n_updates            | 14660         |
|    policy_gradient_loss | 7.45e-06      |
|    std                  | 7.36          |
|    value_loss           | 7.69e+03      |
-------------------------------------------
Eval num_timesteps=3006000, episode_reward=305.12 +/- 264.46
Episode length: 424.60 +/- 80.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 305           |
| time/                   |               |
|    total_timesteps      | 3006000       |
| train/                  |               |
|    approx_kl            | 0.00040383355 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.001         |
|    loss                 | 49.7          |
|    n_updates            | 14670         |
|    policy_gradient_loss | -0.000447     |
|    std                  | 7.35          |
|    value_loss           | 195           |
-------------------------------------------
Eval num_timesteps=3008000, episode_reward=222.23 +/- 245.47
Episode length: 458.60 +/- 72.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 3008000     |
| train/                  |             |
|    approx_kl            | 0.001558559 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 91.7        |
|    n_updates            | 14680       |
|    policy_gradient_loss | -0.00118    |
|    std                  | 7.34        |
|    value_loss           | 535         |
-----------------------------------------
Eval num_timesteps=3010000, episode_reward=179.80 +/- 142.88
Episode length: 445.80 +/- 49.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 446          |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 3010000      |
| train/                  |              |
|    approx_kl            | 0.0019879853 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 43.3         |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.00131     |
|    std                  | 7.31         |
|    value_loss           | 178          |
------------------------------------------
Eval num_timesteps=3012000, episode_reward=327.16 +/- 274.47
Episode length: 460.80 +/- 109.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 327          |
| time/                   |              |
|    total_timesteps      | 3012000      |
| train/                  |              |
|    approx_kl            | 0.0049593165 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 90.9         |
|    n_updates            | 14700        |
|    policy_gradient_loss | -0.00149     |
|    std                  | 7.29         |
|    value_loss           | 354          |
------------------------------------------
Eval num_timesteps=3014000, episode_reward=174.91 +/- 140.89
Episode length: 396.00 +/- 40.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 3014000     |
| train/                  |             |
|    approx_kl            | 0.003527958 |
|    clip_fraction        | 0.00259     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 54.7        |
|    n_updates            | 14710       |
|    policy_gradient_loss | -0.00128    |
|    std                  | 7.28        |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=3016000, episode_reward=260.68 +/- 175.78
Episode length: 402.00 +/- 31.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 3016000     |
| train/                  |             |
|    approx_kl            | 0.005219628 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 99.8        |
|    n_updates            | 14720       |
|    policy_gradient_loss | -0.000542   |
|    std                  | 7.26        |
|    value_loss           | 315         |
-----------------------------------------
Eval num_timesteps=3018000, episode_reward=274.92 +/- 263.42
Episode length: 394.80 +/- 32.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 3018000     |
| train/                  |             |
|    approx_kl            | 0.001242321 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 36.9        |
|    n_updates            | 14730       |
|    policy_gradient_loss | -0.000612   |
|    std                  | 7.27        |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=3020000, episode_reward=286.19 +/- 424.26
Episode length: 365.60 +/- 60.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 366           |
|    mean_reward          | 286           |
| time/                   |               |
|    total_timesteps      | 3020000       |
| train/                  |               |
|    approx_kl            | 0.00091882644 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.001         |
|    loss                 | 914           |
|    n_updates            | 14740         |
|    policy_gradient_loss | 8.39e-05      |
|    std                  | 7.29          |
|    value_loss           | 2.45e+03      |
-------------------------------------------
Eval num_timesteps=3022000, episode_reward=130.96 +/- 115.23
Episode length: 372.00 +/- 45.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | 131           |
| time/                   |               |
|    total_timesteps      | 3022000       |
| train/                  |               |
|    approx_kl            | 0.00024335374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.001         |
|    loss                 | 29.7          |
|    n_updates            | 14750         |
|    policy_gradient_loss | -0.000375     |
|    std                  | 7.32          |
|    value_loss           | 102           |
-------------------------------------------
Eval num_timesteps=3024000, episode_reward=143.18 +/- 293.28
Episode length: 375.00 +/- 51.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 143          |
| time/                   |              |
|    total_timesteps      | 3024000      |
| train/                  |              |
|    approx_kl            | 0.0026306943 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 43.3         |
|    n_updates            | 14760        |
|    policy_gradient_loss | -0.00212     |
|    std                  | 7.36         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=3026000, episode_reward=151.78 +/- 331.48
Episode length: 444.80 +/- 83.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 152          |
| time/                   |              |
|    total_timesteps      | 3026000      |
| train/                  |              |
|    approx_kl            | 0.0035582245 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 36           |
|    n_updates            | 14770        |
|    policy_gradient_loss | -0.000761    |
|    std                  | 7.4          |
|    value_loss           | 145          |
------------------------------------------
Eval num_timesteps=3028000, episode_reward=364.23 +/- 336.98
Episode length: 453.00 +/- 72.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 364          |
| time/                   |              |
|    total_timesteps      | 3028000      |
| train/                  |              |
|    approx_kl            | 0.0013699487 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 45.2         |
|    n_updates            | 14780        |
|    policy_gradient_loss | -0.000202    |
|    std                  | 7.42         |
|    value_loss           | 237          |
------------------------------------------
Eval num_timesteps=3030000, episode_reward=249.35 +/- 84.88
Episode length: 373.20 +/- 41.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 3030000      |
| train/                  |              |
|    approx_kl            | 0.0003068806 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 70.2         |
|    n_updates            | 14790        |
|    policy_gradient_loss | 3.86e-05     |
|    std                  | 7.45         |
|    value_loss           | 240          |
------------------------------------------
Eval num_timesteps=3032000, episode_reward=483.50 +/- 401.99
Episode length: 450.40 +/- 25.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 484          |
| time/                   |              |
|    total_timesteps      | 3032000      |
| train/                  |              |
|    approx_kl            | 0.0007329888 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 236          |
|    n_updates            | 14800        |
|    policy_gradient_loss | 2.06e-05     |
|    std                  | 7.47         |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=3034000, episode_reward=329.38 +/- 284.83
Episode length: 443.00 +/- 124.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 3034000      |
| train/                  |              |
|    approx_kl            | 0.0021325215 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 40.7         |
|    n_updates            | 14810        |
|    policy_gradient_loss | -0.00153     |
|    std                  | 7.47         |
|    value_loss           | 206          |
------------------------------------------
Eval num_timesteps=3036000, episode_reward=256.32 +/- 303.04
Episode length: 416.80 +/- 96.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 256          |
| time/                   |              |
|    total_timesteps      | 3036000      |
| train/                  |              |
|    approx_kl            | 0.0019041267 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 75.5         |
|    n_updates            | 14820        |
|    policy_gradient_loss | 0.000772     |
|    std                  | 7.48         |
|    value_loss           | 303          |
------------------------------------------
Eval num_timesteps=3038000, episode_reward=241.50 +/- 164.34
Episode length: 363.80 +/- 48.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 364           |
|    mean_reward          | 242           |
| time/                   |               |
|    total_timesteps      | 3038000       |
| train/                  |               |
|    approx_kl            | 0.00021634359 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.801         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+03      |
|    n_updates            | 14830         |
|    policy_gradient_loss | -0.000111     |
|    std                  | 7.5           |
|    value_loss           | 4.22e+03      |
-------------------------------------------
Eval num_timesteps=3040000, episode_reward=238.63 +/- 297.53
Episode length: 468.60 +/- 94.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 239         |
| time/                   |             |
|    total_timesteps      | 3040000     |
| train/                  |             |
|    approx_kl            | 0.000678012 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.5       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.001       |
|    loss                 | 1.66e+03    |
|    n_updates            | 14840       |
|    policy_gradient_loss | -0.00113    |
|    std                  | 7.51        |
|    value_loss           | 4.37e+03    |
-----------------------------------------
Eval num_timesteps=3042000, episode_reward=159.10 +/- 73.54
Episode length: 537.60 +/- 80.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 538          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 3042000      |
| train/                  |              |
|    approx_kl            | 0.0018020971 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 79.2         |
|    n_updates            | 14850        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 7.52         |
|    value_loss           | 280          |
------------------------------------------
Eval num_timesteps=3044000, episode_reward=198.14 +/- 168.47
Episode length: 435.00 +/- 80.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 3044000      |
| train/                  |              |
|    approx_kl            | 0.0014756238 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 53.9         |
|    n_updates            | 14860        |
|    policy_gradient_loss | 2.15e-05     |
|    std                  | 7.53         |
|    value_loss           | 230          |
------------------------------------------
Eval num_timesteps=3046000, episode_reward=175.93 +/- 88.72
Episode length: 407.20 +/- 29.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 176           |
| time/                   |               |
|    total_timesteps      | 3046000       |
| train/                  |               |
|    approx_kl            | 0.00029459246 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.5         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 1.64e+03      |
|    n_updates            | 14870         |
|    policy_gradient_loss | -0.000185     |
|    std                  | 7.53          |
|    value_loss           | 4.38e+03      |
-------------------------------------------
Eval num_timesteps=3048000, episode_reward=188.06 +/- 284.71
Episode length: 479.80 +/- 91.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 480           |
|    mean_reward          | 188           |
| time/                   |               |
|    total_timesteps      | 3048000       |
| train/                  |               |
|    approx_kl            | 0.00045625117 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.5         |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.001         |
|    loss                 | 50.1          |
|    n_updates            | 14880         |
|    policy_gradient_loss | -0.000542     |
|    std                  | 7.53          |
|    value_loss           | 168           |
-------------------------------------------
Eval num_timesteps=3050000, episode_reward=555.43 +/- 347.71
Episode length: 414.60 +/- 59.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 555          |
| time/                   |              |
|    total_timesteps      | 3050000      |
| train/                  |              |
|    approx_kl            | 0.0008594283 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.4        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 43.8         |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.000457    |
|    std                  | 7.52         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=3052000, episode_reward=140.67 +/- 60.28
Episode length: 429.00 +/- 79.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 141           |
| time/                   |               |
|    total_timesteps      | 3052000       |
| train/                  |               |
|    approx_kl            | 0.00045678575 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.4         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 121           |
|    n_updates            | 14900         |
|    policy_gradient_loss | 7.61e-05      |
|    std                  | 7.52          |
|    value_loss           | 736           |
-------------------------------------------
Eval num_timesteps=3054000, episode_reward=331.24 +/- 158.96
Episode length: 487.20 +/- 69.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 3054000       |
| train/                  |               |
|    approx_kl            | 0.00024782968 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.5         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 80.5          |
|    n_updates            | 14910         |
|    policy_gradient_loss | -0.000292     |
|    std                  | 7.53          |
|    value_loss           | 344           |
-------------------------------------------
Eval num_timesteps=3056000, episode_reward=308.12 +/- 427.14
Episode length: 489.20 +/- 50.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 308           |
| time/                   |               |
|    total_timesteps      | 3056000       |
| train/                  |               |
|    approx_kl            | 0.00020715594 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.5         |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+03       |
|    n_updates            | 14920         |
|    policy_gradient_loss | -0.00034      |
|    std                  | 7.54          |
|    value_loss           | 4.95e+03      |
-------------------------------------------
Eval num_timesteps=3058000, episode_reward=246.82 +/- 351.58
Episode length: 459.40 +/- 42.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 247          |
| time/                   |              |
|    total_timesteps      | 3058000      |
| train/                  |              |
|    approx_kl            | 5.538654e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.001        |
|    loss                 | 2.53e+03     |
|    n_updates            | 14930        |
|    policy_gradient_loss | 6.15e-05     |
|    std                  | 7.55         |
|    value_loss           | 6.21e+03     |
------------------------------------------
Eval num_timesteps=3060000, episode_reward=161.45 +/- 161.13
Episode length: 420.20 +/- 45.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 3060000      |
| train/                  |              |
|    approx_kl            | 0.0013483628 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 14940        |
|    policy_gradient_loss | -0.00082     |
|    std                  | 7.56         |
|    value_loss           | 122          |
------------------------------------------
Eval num_timesteps=3062000, episode_reward=554.24 +/- 490.16
Episode length: 478.40 +/- 63.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 554          |
| time/                   |              |
|    total_timesteps      | 3062000      |
| train/                  |              |
|    approx_kl            | 0.0022390515 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 5.29e+03     |
|    n_updates            | 14950        |
|    policy_gradient_loss | 0.00262      |
|    std                  | 7.58         |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=3064000, episode_reward=285.92 +/- 113.46
Episode length: 394.40 +/- 62.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 286          |
| time/                   |              |
|    total_timesteps      | 3064000      |
| train/                  |              |
|    approx_kl            | 0.0003834908 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 60.3         |
|    n_updates            | 14960        |
|    policy_gradient_loss | -0.000466    |
|    std                  | 7.6          |
|    value_loss           | 246          |
------------------------------------------
Eval num_timesteps=3066000, episode_reward=349.58 +/- 421.12
Episode length: 416.60 +/- 115.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 3066000      |
| train/                  |              |
|    approx_kl            | 0.0013316278 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 22.6         |
|    n_updates            | 14970        |
|    policy_gradient_loss | -0.000985    |
|    std                  | 7.63         |
|    value_loss           | 90           |
------------------------------------------
Eval num_timesteps=3068000, episode_reward=239.10 +/- 219.07
Episode length: 423.60 +/- 49.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 3068000      |
| train/                  |              |
|    approx_kl            | 0.0011549992 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+03     |
|    n_updates            | 14980        |
|    policy_gradient_loss | 0.00128      |
|    std                  | 7.65         |
|    value_loss           | 7.23e+03     |
------------------------------------------
Eval num_timesteps=3070000, episode_reward=332.57 +/- 194.29
Episode length: 416.20 +/- 40.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 333           |
| time/                   |               |
|    total_timesteps      | 3070000       |
| train/                  |               |
|    approx_kl            | 0.00013432303 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.5         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 70.9          |
|    n_updates            | 14990         |
|    policy_gradient_loss | -0.000262     |
|    std                  | 7.66          |
|    value_loss           | 379           |
-------------------------------------------
Eval num_timesteps=3072000, episode_reward=342.93 +/- 359.59
Episode length: 411.80 +/- 71.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3074000, episode_reward=290.49 +/- 166.18
Episode length: 390.20 +/- 67.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 390          |
|    mean_reward          | 290          |
| time/                   |              |
|    total_timesteps      | 3074000      |
| train/                  |              |
|    approx_kl            | 0.0023251795 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 41.2         |
|    n_updates            | 15000        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 7.69         |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=3076000, episode_reward=269.70 +/- 340.54
Episode length: 369.20 +/- 60.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 369          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 3076000      |
| train/                  |              |
|    approx_kl            | 0.0023798933 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.5        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 979          |
|    n_updates            | 15010        |
|    policy_gradient_loss | -3.1e-05     |
|    std                  | 7.72         |
|    value_loss           | 3.66e+03     |
------------------------------------------
Eval num_timesteps=3078000, episode_reward=223.09 +/- 316.88
Episode length: 369.80 +/- 56.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 370         |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 3078000     |
| train/                  |             |
|    approx_kl            | 0.000390415 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 97.5        |
|    n_updates            | 15020       |
|    policy_gradient_loss | -0.000199   |
|    std                  | 7.73        |
|    value_loss           | 350         |
-----------------------------------------
Eval num_timesteps=3080000, episode_reward=178.14 +/- 192.94
Episode length: 375.60 +/- 45.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 3080000      |
| train/                  |              |
|    approx_kl            | 0.0015200413 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 28.6         |
|    n_updates            | 15030        |
|    policy_gradient_loss | -0.000873    |
|    std                  | 7.76         |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=3082000, episode_reward=53.69 +/- 46.22
Episode length: 317.00 +/- 30.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 317          |
|    mean_reward          | 53.7         |
| time/                   |              |
|    total_timesteps      | 3082000      |
| train/                  |              |
|    approx_kl            | 0.0018254047 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 48           |
|    n_updates            | 15040        |
|    policy_gradient_loss | 0.000107     |
|    std                  | 7.78         |
|    value_loss           | 189          |
------------------------------------------
Eval num_timesteps=3084000, episode_reward=140.53 +/- 151.44
Episode length: 337.60 +/- 47.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 338          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 3084000      |
| train/                  |              |
|    approx_kl            | 0.0026863795 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.001        |
|    loss                 | 27.4         |
|    n_updates            | 15050        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 7.78         |
|    value_loss           | 102          |
------------------------------------------
Eval num_timesteps=3086000, episode_reward=169.69 +/- 228.17
Episode length: 366.00 +/- 48.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 170          |
| time/                   |              |
|    total_timesteps      | 3086000      |
| train/                  |              |
|    approx_kl            | 0.0027573062 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+03     |
|    n_updates            | 15060        |
|    policy_gradient_loss | -0.000872    |
|    std                  | 7.76         |
|    value_loss           | 4.05e+03     |
------------------------------------------
Eval num_timesteps=3088000, episode_reward=132.34 +/- 106.60
Episode length: 342.20 +/- 40.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | 132         |
| time/                   |             |
|    total_timesteps      | 3088000     |
| train/                  |             |
|    approx_kl            | 0.001202065 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 43.7        |
|    n_updates            | 15070       |
|    policy_gradient_loss | -0.000631   |
|    std                  | 7.76        |
|    value_loss           | 221         |
-----------------------------------------
Eval num_timesteps=3090000, episode_reward=14.97 +/- 30.93
Episode length: 311.40 +/- 24.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 15            |
| time/                   |               |
|    total_timesteps      | 3090000       |
| train/                  |               |
|    approx_kl            | 0.00095730997 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.6         |
|    explained_variance   | 0.872         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+03      |
|    n_updates            | 15080         |
|    policy_gradient_loss | 0.000304      |
|    std                  | 7.78          |
|    value_loss           | 2.89e+03      |
-------------------------------------------
Eval num_timesteps=3092000, episode_reward=9.00 +/- 68.22
Episode length: 334.00 +/- 15.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 334          |
|    mean_reward          | 9            |
| time/                   |              |
|    total_timesteps      | 3092000      |
| train/                  |              |
|    approx_kl            | 0.0004625943 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 53.7         |
|    n_updates            | 15090        |
|    policy_gradient_loss | -0.000734    |
|    std                  | 7.82         |
|    value_loss           | 161          |
------------------------------------------
Eval num_timesteps=3094000, episode_reward=142.48 +/- 274.83
Episode length: 343.80 +/- 52.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 344           |
|    mean_reward          | 142           |
| time/                   |               |
|    total_timesteps      | 3094000       |
| train/                  |               |
|    approx_kl            | 0.00063198595 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.6         |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.001         |
|    loss                 | 33            |
|    n_updates            | 15100         |
|    policy_gradient_loss | -0.000139     |
|    std                  | 7.83          |
|    value_loss           | 139           |
-------------------------------------------
Eval num_timesteps=3096000, episode_reward=66.31 +/- 41.22
Episode length: 326.00 +/- 12.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | 66.3         |
| time/                   |              |
|    total_timesteps      | 3096000      |
| train/                  |              |
|    approx_kl            | 0.0024535377 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.001        |
|    loss                 | 20.9         |
|    n_updates            | 15110        |
|    policy_gradient_loss | -0.00134     |
|    std                  | 7.86         |
|    value_loss           | 93.5         |
------------------------------------------
Eval num_timesteps=3098000, episode_reward=-3.90 +/- 54.35
Episode length: 287.60 +/- 28.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | -3.9        |
| time/                   |             |
|    total_timesteps      | 3098000     |
| train/                  |             |
|    approx_kl            | 0.005692446 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.6       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 25.7        |
|    n_updates            | 15120       |
|    policy_gradient_loss | -0.00105    |
|    std                  | 7.86        |
|    value_loss           | 98.5        |
-----------------------------------------
Eval num_timesteps=3100000, episode_reward=13.91 +/- 40.27
Episode length: 314.60 +/- 31.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 315           |
|    mean_reward          | 13.9          |
| time/                   |               |
|    total_timesteps      | 3100000       |
| train/                  |               |
|    approx_kl            | 0.00023491404 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.6         |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.001         |
|    loss                 | 47            |
|    n_updates            | 15130         |
|    policy_gradient_loss | -0.000242     |
|    std                  | 7.87          |
|    value_loss           | 126           |
-------------------------------------------
Eval num_timesteps=3102000, episode_reward=59.39 +/- 99.13
Episode length: 320.80 +/- 46.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | 59.4         |
| time/                   |              |
|    total_timesteps      | 3102000      |
| train/                  |              |
|    approx_kl            | 0.0008308978 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.6        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 18.8         |
|    n_updates            | 15140        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 7.93         |
|    value_loss           | 70.1         |
------------------------------------------
Eval num_timesteps=3104000, episode_reward=41.89 +/- 95.43
Episode length: 337.00 +/- 23.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 337          |
|    mean_reward          | 41.9         |
| time/                   |              |
|    total_timesteps      | 3104000      |
| train/                  |              |
|    approx_kl            | 0.0017387852 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.7        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 18.6         |
|    n_updates            | 15150        |
|    policy_gradient_loss | -0.000817    |
|    std                  | 8.01         |
|    value_loss           | 67.9         |
------------------------------------------
Eval num_timesteps=3106000, episode_reward=51.04 +/- 30.75
Episode length: 317.20 +/- 35.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 317          |
|    mean_reward          | 51           |
| time/                   |              |
|    total_timesteps      | 3106000      |
| train/                  |              |
|    approx_kl            | 0.0009983736 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.7        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 23.2         |
|    n_updates            | 15160        |
|    policy_gradient_loss | -0.000667    |
|    std                  | 8.06         |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=3108000, episode_reward=47.15 +/- 125.60
Episode length: 325.00 +/- 49.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | 47.2         |
| time/                   |              |
|    total_timesteps      | 3108000      |
| train/                  |              |
|    approx_kl            | 0.0055518756 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.8        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.001        |
|    loss                 | 15.9         |
|    n_updates            | 15170        |
|    policy_gradient_loss | -0.00221     |
|    std                  | 8.12         |
|    value_loss           | 64.3         |
------------------------------------------
Eval num_timesteps=3110000, episode_reward=226.40 +/- 133.21
Episode length: 386.00 +/- 20.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 386         |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 3110000     |
| train/                  |             |
|    approx_kl            | 0.004657142 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.8       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.001       |
|    loss                 | 26.7        |
|    n_updates            | 15180       |
|    policy_gradient_loss | -0.00118    |
|    std                  | 8.17        |
|    value_loss           | 84.4        |
-----------------------------------------
Eval num_timesteps=3112000, episode_reward=713.96 +/- 864.54
Episode length: 402.80 +/- 52.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 403         |
|    mean_reward          | 714         |
| time/                   |             |
|    total_timesteps      | 3112000     |
| train/                  |             |
|    approx_kl            | 0.005449441 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.8       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.001       |
|    loss                 | 10.9        |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.00166    |
|    std                  | 8.21        |
|    value_loss           | 55.7        |
-----------------------------------------
Eval num_timesteps=3114000, episode_reward=311.00 +/- 417.60
Episode length: 464.20 +/- 66.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 3114000     |
| train/                  |             |
|    approx_kl            | 0.004068177 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.8       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 92.9        |
|    n_updates            | 15200       |
|    policy_gradient_loss | -0.00124    |
|    std                  | 8.23        |
|    value_loss           | 385         |
-----------------------------------------
Eval num_timesteps=3116000, episode_reward=270.52 +/- 156.32
Episode length: 445.40 +/- 64.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 445         |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 3116000     |
| train/                  |             |
|    approx_kl            | 0.004097636 |
|    clip_fraction        | 0.00527     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 32          |
|    n_updates            | 15210       |
|    policy_gradient_loss | -0.00122    |
|    std                  | 8.25        |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=3118000, episode_reward=266.79 +/- 279.29
Episode length: 468.40 +/- 77.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 3118000      |
| train/                  |              |
|    approx_kl            | 0.0012718467 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.8        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 2.1e+03      |
|    n_updates            | 15220        |
|    policy_gradient_loss | -0.000745    |
|    std                  | 8.27         |
|    value_loss           | 5.68e+03     |
------------------------------------------
Eval num_timesteps=3120000, episode_reward=206.60 +/- 276.01
Episode length: 480.80 +/- 43.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 207           |
| time/                   |               |
|    total_timesteps      | 3120000       |
| train/                  |               |
|    approx_kl            | 0.00034405285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.8         |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.001         |
|    loss                 | 321           |
|    n_updates            | 15230         |
|    policy_gradient_loss | -0.000268     |
|    std                  | 8.28          |
|    value_loss           | 1.48e+03      |
-------------------------------------------
Eval num_timesteps=3122000, episode_reward=234.40 +/- 136.85
Episode length: 453.20 +/- 73.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 234           |
| time/                   |               |
|    total_timesteps      | 3122000       |
| train/                  |               |
|    approx_kl            | 0.00049362786 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 518           |
|    n_updates            | 15240         |
|    policy_gradient_loss | -0.000428     |
|    std                  | 8.3           |
|    value_loss           | 1.51e+03      |
-------------------------------------------
Eval num_timesteps=3124000, episode_reward=202.41 +/- 202.86
Episode length: 485.60 +/- 55.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 202          |
| time/                   |              |
|    total_timesteps      | 3124000      |
| train/                  |              |
|    approx_kl            | 0.0002917076 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 93.9         |
|    n_updates            | 15250        |
|    policy_gradient_loss | 4.3e-05      |
|    std                  | 8.31         |
|    value_loss           | 587          |
------------------------------------------
Eval num_timesteps=3126000, episode_reward=347.45 +/- 377.61
Episode length: 437.60 +/- 77.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 347           |
| time/                   |               |
|    total_timesteps      | 3126000       |
| train/                  |               |
|    approx_kl            | 0.00017249444 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.001         |
|    loss                 | 1.52e+03      |
|    n_updates            | 15260         |
|    policy_gradient_loss | -0.000257     |
|    std                  | 8.32          |
|    value_loss           | 4.46e+03      |
-------------------------------------------
Eval num_timesteps=3128000, episode_reward=309.96 +/- 430.56
Episode length: 443.60 +/- 69.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 444          |
|    mean_reward          | 310          |
| time/                   |              |
|    total_timesteps      | 3128000      |
| train/                  |              |
|    approx_kl            | 0.0008279424 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 32.7         |
|    n_updates            | 15270        |
|    policy_gradient_loss | -0.00071     |
|    std                  | 8.34         |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=3130000, episode_reward=298.20 +/- 199.27
Episode length: 475.80 +/- 83.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 298           |
| time/                   |               |
|    total_timesteps      | 3130000       |
| train/                  |               |
|    approx_kl            | 0.00069551915 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 90.8          |
|    n_updates            | 15280         |
|    policy_gradient_loss | 3.58e-05      |
|    std                  | 8.34          |
|    value_loss           | 356           |
-------------------------------------------
Eval num_timesteps=3132000, episode_reward=271.57 +/- 436.98
Episode length: 430.80 +/- 19.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 3132000      |
| train/                  |              |
|    approx_kl            | 0.0014181684 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 72.1         |
|    n_updates            | 15290        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 8.32         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=3134000, episode_reward=17.07 +/- 102.23
Episode length: 413.80 +/- 60.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 3134000      |
| train/                  |              |
|    approx_kl            | 0.0027222973 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 40.7         |
|    n_updates            | 15300        |
|    policy_gradient_loss | -0.000366    |
|    std                  | 8.3          |
|    value_loss           | 291          |
------------------------------------------
Eval num_timesteps=3136000, episode_reward=133.12 +/- 252.34
Episode length: 419.60 +/- 86.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 3136000     |
| train/                  |             |
|    approx_kl            | 0.001884744 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.9       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 79.8        |
|    n_updates            | 15310       |
|    policy_gradient_loss | -0.000712   |
|    std                  | 8.33        |
|    value_loss           | 274         |
-----------------------------------------
Eval num_timesteps=3138000, episode_reward=521.95 +/- 402.27
Episode length: 470.40 +/- 94.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 470          |
|    mean_reward          | 522          |
| time/                   |              |
|    total_timesteps      | 3138000      |
| train/                  |              |
|    approx_kl            | 0.0011366305 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+03     |
|    n_updates            | 15320        |
|    policy_gradient_loss | 0.000547     |
|    std                  | 8.36         |
|    value_loss           | 3e+03        |
------------------------------------------
Eval num_timesteps=3140000, episode_reward=236.47 +/- 438.89
Episode length: 399.80 +/- 52.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 236           |
| time/                   |               |
|    total_timesteps      | 3140000       |
| train/                  |               |
|    approx_kl            | 0.00089212705 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.001         |
|    loss                 | 31.4          |
|    n_updates            | 15330         |
|    policy_gradient_loss | -0.00108      |
|    std                  | 8.39          |
|    value_loss           | 120           |
-------------------------------------------
Eval num_timesteps=3142000, episode_reward=170.89 +/- 111.76
Episode length: 462.00 +/- 83.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 3142000      |
| train/                  |              |
|    approx_kl            | 0.0025630675 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 32.4         |
|    n_updates            | 15340        |
|    policy_gradient_loss | -0.000359    |
|    std                  | 8.4          |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=3144000, episode_reward=255.24 +/- 270.65
Episode length: 427.40 +/- 84.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 427           |
|    mean_reward          | 255           |
| time/                   |               |
|    total_timesteps      | 3144000       |
| train/                  |               |
|    approx_kl            | 0.00040104453 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.84          |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+03      |
|    n_updates            | 15350         |
|    policy_gradient_loss | 0.000193      |
|    std                  | 8.42          |
|    value_loss           | 4.29e+03      |
-------------------------------------------
Eval num_timesteps=3146000, episode_reward=107.58 +/- 98.55
Episode length: 436.20 +/- 64.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 108           |
| time/                   |               |
|    total_timesteps      | 3146000       |
| train/                  |               |
|    approx_kl            | 0.00041486428 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 127           |
|    n_updates            | 15360         |
|    policy_gradient_loss | -0.000522     |
|    std                  | 8.43          |
|    value_loss           | 558           |
-------------------------------------------
Eval num_timesteps=3148000, episode_reward=232.32 +/- 155.79
Episode length: 410.00 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 232          |
| time/                   |              |
|    total_timesteps      | 3148000      |
| train/                  |              |
|    approx_kl            | 0.0007129959 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 105          |
|    n_updates            | 15370        |
|    policy_gradient_loss | -0.000332    |
|    std                  | 8.46         |
|    value_loss           | 458          |
------------------------------------------
Eval num_timesteps=3150000, episode_reward=208.76 +/- 169.37
Episode length: 518.60 +/- 57.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 519          |
|    mean_reward          | 209          |
| time/                   |              |
|    total_timesteps      | 3150000      |
| train/                  |              |
|    approx_kl            | 0.0057835598 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 32.1         |
|    n_updates            | 15380        |
|    policy_gradient_loss | -0.00286     |
|    std                  | 8.5          |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=3152000, episode_reward=182.54 +/- 196.13
Episode length: 438.40 +/- 61.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 438         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 3152000     |
| train/                  |             |
|    approx_kl            | 0.005693569 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14         |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 37.7        |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.00134    |
|    std                  | 8.54        |
|    value_loss           | 229         |
-----------------------------------------
Eval num_timesteps=3154000, episode_reward=415.35 +/- 397.85
Episode length: 431.40 +/- 34.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 415           |
| time/                   |               |
|    total_timesteps      | 3154000       |
| train/                  |               |
|    approx_kl            | 0.00039378362 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+03      |
|    n_updates            | 15400         |
|    policy_gradient_loss | -0.000244     |
|    std                  | 8.57          |
|    value_loss           | 3.3e+03       |
-------------------------------------------
Eval num_timesteps=3156000, episode_reward=574.33 +/- 661.40
Episode length: 440.20 +/- 86.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 574          |
| time/                   |              |
|    total_timesteps      | 3156000      |
| train/                  |              |
|    approx_kl            | 0.0002697724 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 84.8         |
|    n_updates            | 15410        |
|    policy_gradient_loss | -0.000336    |
|    std                  | 8.58         |
|    value_loss           | 475          |
------------------------------------------
Eval num_timesteps=3158000, episode_reward=228.09 +/- 339.72
Episode length: 468.20 +/- 40.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 3158000  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=102.25 +/- 191.74
Episode length: 497.60 +/- 70.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | 102           |
| time/                   |               |
|    total_timesteps      | 3160000       |
| train/                  |               |
|    approx_kl            | 0.00022952972 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 115           |
|    n_updates            | 15420         |
|    policy_gradient_loss | -0.000307     |
|    std                  | 8.59          |
|    value_loss           | 360           |
-------------------------------------------
Eval num_timesteps=3162000, episode_reward=7.97 +/- 220.59
Episode length: 456.20 +/- 82.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 456           |
|    mean_reward          | 7.97          |
| time/                   |               |
|    total_timesteps      | 3162000       |
| train/                  |               |
|    approx_kl            | 0.00037167312 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.574         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+03      |
|    n_updates            | 15430         |
|    policy_gradient_loss | 9.19e-05      |
|    std                  | 8.58          |
|    value_loss           | 7.74e+03      |
-------------------------------------------
Eval num_timesteps=3164000, episode_reward=382.81 +/- 359.84
Episode length: 466.60 +/- 67.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 383           |
| time/                   |               |
|    total_timesteps      | 3164000       |
| train/                  |               |
|    approx_kl            | 0.00010634461 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.779         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+03      |
|    n_updates            | 15440         |
|    policy_gradient_loss | -0.000149     |
|    std                  | 8.58          |
|    value_loss           | 6.64e+03      |
-------------------------------------------
Eval num_timesteps=3166000, episode_reward=129.69 +/- 100.09
Episode length: 382.20 +/- 75.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 382           |
|    mean_reward          | 130           |
| time/                   |               |
|    total_timesteps      | 3166000       |
| train/                  |               |
|    approx_kl            | 0.00028626315 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 60.9          |
|    n_updates            | 15450         |
|    policy_gradient_loss | -0.000564     |
|    std                  | 8.56          |
|    value_loss           | 314           |
-------------------------------------------
Eval num_timesteps=3168000, episode_reward=114.07 +/- 54.92
Episode length: 509.00 +/- 107.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 509          |
|    mean_reward          | 114          |
| time/                   |              |
|    total_timesteps      | 3168000      |
| train/                  |              |
|    approx_kl            | 0.0007892365 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+03     |
|    n_updates            | 15460        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 8.53         |
|    value_loss           | 3.78e+03     |
------------------------------------------
Eval num_timesteps=3170000, episode_reward=305.03 +/- 444.45
Episode length: 430.20 +/- 60.43
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 430            |
|    mean_reward          | 305            |
| time/                   |                |
|    total_timesteps      | 3170000        |
| train/                  |                |
|    approx_kl            | 0.000105501415 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -13.9          |
|    explained_variance   | 0.862          |
|    learning_rate        | 0.001          |
|    loss                 | 355            |
|    n_updates            | 15470          |
|    policy_gradient_loss | -5.63e-05      |
|    std                  | 8.53           |
|    value_loss           | 984            |
--------------------------------------------
Eval num_timesteps=3172000, episode_reward=366.11 +/- 160.40
Episode length: 458.20 +/- 60.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 366          |
| time/                   |              |
|    total_timesteps      | 3172000      |
| train/                  |              |
|    approx_kl            | 6.772211e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 81.7         |
|    n_updates            | 15480        |
|    policy_gradient_loss | -0.000213    |
|    std                  | 8.52         |
|    value_loss           | 365          |
------------------------------------------
Eval num_timesteps=3174000, episode_reward=347.68 +/- 387.60
Episode length: 468.80 +/- 80.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | 348           |
| time/                   |               |
|    total_timesteps      | 3174000       |
| train/                  |               |
|    approx_kl            | 0.00019547853 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.817         |
|    learning_rate        | 0.001         |
|    loss                 | 3.07e+03      |
|    n_updates            | 15490         |
|    policy_gradient_loss | -0.000272     |
|    std                  | 8.51          |
|    value_loss           | 7.57e+03      |
-------------------------------------------
Eval num_timesteps=3176000, episode_reward=262.78 +/- 266.53
Episode length: 460.00 +/- 63.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 263           |
| time/                   |               |
|    total_timesteps      | 3176000       |
| train/                  |               |
|    approx_kl            | 0.00015144996 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.001         |
|    loss                 | 104           |
|    n_updates            | 15500         |
|    policy_gradient_loss | -0.000198     |
|    std                  | 8.51          |
|    value_loss           | 510           |
-------------------------------------------
Eval num_timesteps=3178000, episode_reward=145.50 +/- 194.73
Episode length: 457.00 +/- 79.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 457          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 3178000      |
| train/                  |              |
|    approx_kl            | 0.0008039726 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 63.2         |
|    n_updates            | 15510        |
|    policy_gradient_loss | -0.000388    |
|    std                  | 8.52         |
|    value_loss           | 310          |
------------------------------------------
Eval num_timesteps=3180000, episode_reward=149.80 +/- 184.25
Episode length: 492.00 +/- 56.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 492          |
|    mean_reward          | 150          |
| time/                   |              |
|    total_timesteps      | 3180000      |
| train/                  |              |
|    approx_kl            | 0.0017163695 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 70.9         |
|    n_updates            | 15520        |
|    policy_gradient_loss | -0.0014      |
|    std                  | 8.52         |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=3182000, episode_reward=289.70 +/- 316.66
Episode length: 484.80 +/- 48.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 485          |
|    mean_reward          | 290          |
| time/                   |              |
|    total_timesteps      | 3182000      |
| train/                  |              |
|    approx_kl            | 0.0012195131 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.9        |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.001        |
|    loss                 | 368          |
|    n_updates            | 15530        |
|    policy_gradient_loss | 0.00134      |
|    std                  | 8.54         |
|    value_loss           | 1.29e+03     |
------------------------------------------
Eval num_timesteps=3184000, episode_reward=300.23 +/- 231.39
Episode length: 451.00 +/- 65.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 451         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 3184000     |
| train/                  |             |
|    approx_kl            | 0.003224849 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.9       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 20.9        |
|    n_updates            | 15540       |
|    policy_gradient_loss | -0.00112    |
|    std                  | 8.55        |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=3186000, episode_reward=45.86 +/- 180.26
Episode length: 440.60 +/- 23.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 441        |
|    mean_reward          | 45.9       |
| time/                   |            |
|    total_timesteps      | 3186000    |
| train/                  |            |
|    approx_kl            | 0.00410047 |
|    clip_fraction        | 0.00664    |
|    clip_range           | 0.2        |
|    entropy_loss         | -13.9      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.001      |
|    loss                 | 126        |
|    n_updates            | 15550      |
|    policy_gradient_loss | 0.000591   |
|    std                  | 8.55       |
|    value_loss           | 523        |
----------------------------------------
Eval num_timesteps=3188000, episode_reward=-62.35 +/- 180.73
Episode length: 498.40 +/- 31.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 498           |
|    mean_reward          | -62.4         |
| time/                   |               |
|    total_timesteps      | 3188000       |
| train/                  |               |
|    approx_kl            | 0.00025218146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 43            |
|    n_updates            | 15560         |
|    policy_gradient_loss | -0.000747     |
|    std                  | 8.52          |
|    value_loss           | 177           |
-------------------------------------------
Eval num_timesteps=3190000, episode_reward=114.26 +/- 128.77
Episode length: 468.00 +/- 47.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 468           |
|    mean_reward          | 114           |
| time/                   |               |
|    total_timesteps      | 3190000       |
| train/                  |               |
|    approx_kl            | 0.00035646878 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 15570         |
|    policy_gradient_loss | 0.000128      |
|    std                  | 8.48          |
|    value_loss           | 4.44e+03      |
-------------------------------------------
Eval num_timesteps=3192000, episode_reward=256.89 +/- 150.16
Episode length: 441.20 +/- 85.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | 257           |
| time/                   |               |
|    total_timesteps      | 3192000       |
| train/                  |               |
|    approx_kl            | 0.00026526238 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 56            |
|    n_updates            | 15580         |
|    policy_gradient_loss | -0.000277     |
|    std                  | 8.47          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3194000, episode_reward=221.94 +/- 131.06
Episode length: 455.60 +/- 76.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 456           |
|    mean_reward          | 222           |
| time/                   |               |
|    total_timesteps      | 3194000       |
| train/                  |               |
|    approx_kl            | 0.00041687157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 76.2          |
|    n_updates            | 15590         |
|    policy_gradient_loss | -0.000317     |
|    std                  | 8.49          |
|    value_loss           | 322           |
-------------------------------------------
Eval num_timesteps=3196000, episode_reward=70.17 +/- 152.73
Episode length: 457.80 +/- 67.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 458           |
|    mean_reward          | 70.2          |
| time/                   |               |
|    total_timesteps      | 3196000       |
| train/                  |               |
|    approx_kl            | 0.00043709102 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.725         |
|    learning_rate        | 0.001         |
|    loss                 | 3.28e+03      |
|    n_updates            | 15600         |
|    policy_gradient_loss | -0.000623     |
|    std                  | 8.52          |
|    value_loss           | 9.24e+03      |
-------------------------------------------
Eval num_timesteps=3198000, episode_reward=232.08 +/- 348.12
Episode length: 406.40 +/- 85.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 232           |
| time/                   |               |
|    total_timesteps      | 3198000       |
| train/                  |               |
|    approx_kl            | 0.00082032033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -13.9         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 46.3          |
|    n_updates            | 15610         |
|    policy_gradient_loss | -0.000917     |
|    std                  | 8.53          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3200000, episode_reward=24.53 +/- 282.08
Episode length: 471.20 +/- 37.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 24.5         |
| time/                   |              |
|    total_timesteps      | 3200000      |
| train/                  |              |
|    approx_kl            | 0.0015793442 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.001        |
|    loss                 | 4.7e+03      |
|    n_updates            | 15620        |
|    policy_gradient_loss | 0.000633     |
|    std                  | 8.54         |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=3202000, episode_reward=332.92 +/- 271.43
Episode length: 463.00 +/- 74.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 463          |
|    mean_reward          | 333          |
| time/                   |              |
|    total_timesteps      | 3202000      |
| train/                  |              |
|    approx_kl            | 0.0004046471 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 32.3         |
|    n_updates            | 15630        |
|    policy_gradient_loss | -0.000539    |
|    std                  | 8.53         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=3204000, episode_reward=-12.86 +/- 167.95
Episode length: 476.00 +/- 41.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | -12.9        |
| time/                   |              |
|    total_timesteps      | 3204000      |
| train/                  |              |
|    approx_kl            | 0.0013331465 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 162          |
|    n_updates            | 15640        |
|    policy_gradient_loss | -0.000548    |
|    std                  | 8.53         |
|    value_loss           | 534          |
------------------------------------------
Eval num_timesteps=3206000, episode_reward=228.33 +/- 188.45
Episode length: 417.80 +/- 42.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 228           |
| time/                   |               |
|    total_timesteps      | 3206000       |
| train/                  |               |
|    approx_kl            | 0.00058103574 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.001         |
|    loss                 | 837           |
|    n_updates            | 15650         |
|    policy_gradient_loss | -0.00052      |
|    std                  | 8.55          |
|    value_loss           | 2.84e+03      |
-------------------------------------------
Eval num_timesteps=3208000, episode_reward=257.67 +/- 284.84
Episode length: 429.20 +/- 97.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 258           |
| time/                   |               |
|    total_timesteps      | 3208000       |
| train/                  |               |
|    approx_kl            | 0.00020076643 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+03      |
|    n_updates            | 15660         |
|    policy_gradient_loss | 8.58e-06      |
|    std                  | 8.55          |
|    value_loss           | 2.9e+03       |
-------------------------------------------
Eval num_timesteps=3210000, episode_reward=118.88 +/- 94.50
Episode length: 464.80 +/- 66.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 465           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 3210000       |
| train/                  |               |
|    approx_kl            | 1.5305588e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.001         |
|    loss                 | 571           |
|    n_updates            | 15670         |
|    policy_gradient_loss | -2.51e-05     |
|    std                  | 8.56          |
|    value_loss           | 2.29e+03      |
-------------------------------------------
Eval num_timesteps=3212000, episode_reward=248.68 +/- 124.54
Episode length: 452.20 +/- 63.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 249           |
| time/                   |               |
|    total_timesteps      | 3212000       |
| train/                  |               |
|    approx_kl            | 0.00031113086 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.001         |
|    loss                 | 53.9          |
|    n_updates            | 15680         |
|    policy_gradient_loss | -0.000422     |
|    std                  | 8.57          |
|    value_loss           | 278           |
-------------------------------------------
Eval num_timesteps=3214000, episode_reward=318.35 +/- 292.66
Episode length: 473.00 +/- 64.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 318          |
| time/                   |              |
|    total_timesteps      | 3214000      |
| train/                  |              |
|    approx_kl            | 0.0018887559 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 31.2         |
|    n_updates            | 15690        |
|    policy_gradient_loss | -0.00119     |
|    std                  | 8.58         |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=3216000, episode_reward=219.43 +/- 279.16
Episode length: 417.20 +/- 96.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 3216000      |
| train/                  |              |
|    approx_kl            | 0.0016346187 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.001        |
|    loss                 | 4.06e+03     |
|    n_updates            | 15700        |
|    policy_gradient_loss | 0.00072      |
|    std                  | 8.59         |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=3218000, episode_reward=236.97 +/- 339.36
Episode length: 423.60 +/- 76.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 424           |
|    mean_reward          | 237           |
| time/                   |               |
|    total_timesteps      | 3218000       |
| train/                  |               |
|    approx_kl            | 0.00016515527 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+03      |
|    n_updates            | 15710         |
|    policy_gradient_loss | 1.33e-05      |
|    std                  | 8.59          |
|    value_loss           | 2.81e+03      |
-------------------------------------------
Eval num_timesteps=3220000, episode_reward=438.86 +/- 480.89
Episode length: 459.60 +/- 73.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | 439         |
| time/                   |             |
|    total_timesteps      | 3220000     |
| train/                  |             |
|    approx_kl            | 0.002153403 |
|    clip_fraction        | 0.000293    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 37.2        |
|    n_updates            | 15720       |
|    policy_gradient_loss | -0.00158    |
|    std                  | 8.59        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=3222000, episode_reward=184.44 +/- 376.01
Episode length: 457.60 +/- 73.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 458          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 3222000      |
| train/                  |              |
|    approx_kl            | 0.0040324526 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 143          |
|    n_updates            | 15730        |
|    policy_gradient_loss | -0.00225     |
|    std                  | 8.59         |
|    value_loss           | 538          |
------------------------------------------
Eval num_timesteps=3224000, episode_reward=214.12 +/- 210.77
Episode length: 474.20 +/- 54.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 214          |
| time/                   |              |
|    total_timesteps      | 3224000      |
| train/                  |              |
|    approx_kl            | 0.0013350453 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 174          |
|    n_updates            | 15740        |
|    policy_gradient_loss | 2.83e-05     |
|    std                  | 8.6          |
|    value_loss           | 571          |
------------------------------------------
Eval num_timesteps=3226000, episode_reward=77.42 +/- 157.11
Episode length: 495.40 +/- 77.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 495           |
|    mean_reward          | 77.4          |
| time/                   |               |
|    total_timesteps      | 3226000       |
| train/                  |               |
|    approx_kl            | 0.00031811165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.899         |
|    learning_rate        | 0.001         |
|    loss                 | 321           |
|    n_updates            | 15750         |
|    policy_gradient_loss | -0.000266     |
|    std                  | 8.6           |
|    value_loss           | 1.11e+03      |
-------------------------------------------
Eval num_timesteps=3228000, episode_reward=32.95 +/- 170.45
Episode length: 471.00 +/- 88.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 33           |
| time/                   |              |
|    total_timesteps      | 3228000      |
| train/                  |              |
|    approx_kl            | 0.0004774236 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 21.2         |
|    n_updates            | 15760        |
|    policy_gradient_loss | -0.000473    |
|    std                  | 8.63         |
|    value_loss           | 101          |
------------------------------------------
Eval num_timesteps=3230000, episode_reward=106.34 +/- 185.06
Episode length: 449.80 +/- 76.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 450          |
|    mean_reward          | 106          |
| time/                   |              |
|    total_timesteps      | 3230000      |
| train/                  |              |
|    approx_kl            | 0.0005878437 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 780          |
|    n_updates            | 15770        |
|    policy_gradient_loss | 0.00102      |
|    std                  | 8.65         |
|    value_loss           | 2.26e+03     |
------------------------------------------
Eval num_timesteps=3232000, episode_reward=140.12 +/- 209.34
Episode length: 496.00 +/- 54.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 496           |
|    mean_reward          | 140           |
| time/                   |               |
|    total_timesteps      | 3232000       |
| train/                  |               |
|    approx_kl            | 0.00010246056 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+03       |
|    n_updates            | 15780         |
|    policy_gradient_loss | -0.000118     |
|    std                  | 8.67          |
|    value_loss           | 3.65e+03      |
-------------------------------------------
Eval num_timesteps=3234000, episode_reward=360.28 +/- 183.90
Episode length: 488.80 +/- 80.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 3234000      |
| train/                  |              |
|    approx_kl            | 9.329396e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 40.8         |
|    n_updates            | 15790        |
|    policy_gradient_loss | -0.00015     |
|    std                  | 8.69         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=3236000, episode_reward=341.31 +/- 308.80
Episode length: 507.00 +/- 115.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 507           |
|    mean_reward          | 341           |
| time/                   |               |
|    total_timesteps      | 3236000       |
| train/                  |               |
|    approx_kl            | 0.00014828236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14           |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.001         |
|    loss                 | 784           |
|    n_updates            | 15800         |
|    policy_gradient_loss | 0.000197      |
|    std                  | 8.71          |
|    value_loss           | 2.77e+03      |
-------------------------------------------
Eval num_timesteps=3238000, episode_reward=381.54 +/- 344.95
Episode length: 498.20 +/- 43.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 3238000      |
| train/                  |              |
|    approx_kl            | 9.958708e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 15810        |
|    policy_gradient_loss | -0.000259    |
|    std                  | 8.71         |
|    value_loss           | 3.27e+03     |
------------------------------------------
Eval num_timesteps=3240000, episode_reward=69.05 +/- 242.63
Episode length: 418.40 +/- 88.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 69.1         |
| time/                   |              |
|    total_timesteps      | 3240000      |
| train/                  |              |
|    approx_kl            | 0.0003730173 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14          |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 103          |
|    n_updates            | 15820        |
|    policy_gradient_loss | -0.000473    |
|    std                  | 8.72         |
|    value_loss           | 460          |
------------------------------------------
Eval num_timesteps=3242000, episode_reward=147.71 +/- 233.61
Episode length: 433.20 +/- 51.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 148          |
| time/                   |              |
|    total_timesteps      | 3242000      |
| train/                  |              |
|    approx_kl            | 0.0005150314 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.1        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 299          |
|    n_updates            | 15830        |
|    policy_gradient_loss | -0.000291    |
|    std                  | 8.74         |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=3244000, episode_reward=258.61 +/- 369.56
Episode length: 483.40 +/- 55.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 3244000  |
---------------------------------
Eval num_timesteps=3246000, episode_reward=41.00 +/- 103.37
Episode length: 455.40 +/- 35.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 41            |
| time/                   |               |
|    total_timesteps      | 3246000       |
| train/                  |               |
|    approx_kl            | 0.00044618553 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.1         |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 37.1          |
|    n_updates            | 15840         |
|    policy_gradient_loss | -0.00034      |
|    std                  | 8.76          |
|    value_loss           | 224           |
-------------------------------------------
Eval num_timesteps=3248000, episode_reward=276.95 +/- 363.38
Episode length: 464.40 +/- 58.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 277           |
| time/                   |               |
|    total_timesteps      | 3248000       |
| train/                  |               |
|    approx_kl            | 0.00039856575 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.1         |
|    explained_variance   | 0.739         |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+03      |
|    n_updates            | 15850         |
|    policy_gradient_loss | -0.0003       |
|    std                  | 8.78          |
|    value_loss           | 3.27e+03      |
-------------------------------------------
Eval num_timesteps=3250000, episode_reward=-60.37 +/- 187.86
Episode length: 458.60 +/- 60.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | -60.4         |
| time/                   |               |
|    total_timesteps      | 3250000       |
| train/                  |               |
|    approx_kl            | 5.0491974e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.1         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 118           |
|    n_updates            | 15860         |
|    policy_gradient_loss | -8.56e-05     |
|    std                  | 8.8           |
|    value_loss           | 661           |
-------------------------------------------
Eval num_timesteps=3252000, episode_reward=171.66 +/- 301.09
Episode length: 525.60 +/- 50.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 526          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 3252000      |
| train/                  |              |
|    approx_kl            | 0.0023090742 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.1        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 15870        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 8.82         |
|    value_loss           | 399          |
------------------------------------------
Eval num_timesteps=3254000, episode_reward=184.78 +/- 198.33
Episode length: 441.60 +/- 65.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 185           |
| time/                   |               |
|    total_timesteps      | 3254000       |
| train/                  |               |
|    approx_kl            | 0.00019835291 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.1         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 59.1          |
|    n_updates            | 15880         |
|    policy_gradient_loss | 0.000354      |
|    std                  | 8.83          |
|    value_loss           | 180           |
-------------------------------------------
Eval num_timesteps=3256000, episode_reward=198.95 +/- 262.74
Episode length: 451.80 +/- 70.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 452          |
|    mean_reward          | 199          |
| time/                   |              |
|    total_timesteps      | 3256000      |
| train/                  |              |
|    approx_kl            | 0.0020414772 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.1        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 224          |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.000543    |
|    std                  | 8.84         |
|    value_loss           | 969          |
------------------------------------------
Eval num_timesteps=3258000, episode_reward=253.53 +/- 275.60
Episode length: 452.20 +/- 55.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 254           |
| time/                   |               |
|    total_timesteps      | 3258000       |
| train/                  |               |
|    approx_kl            | 0.00052807614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.1         |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+03      |
|    n_updates            | 15900         |
|    policy_gradient_loss | -0.000374     |
|    std                  | 8.85          |
|    value_loss           | 4.8e+03       |
-------------------------------------------
Eval num_timesteps=3260000, episode_reward=145.78 +/- 196.68
Episode length: 467.60 +/- 89.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 468          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 3260000      |
| train/                  |              |
|    approx_kl            | 9.838818e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.1        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+03     |
|    n_updates            | 15910        |
|    policy_gradient_loss | -4.46e-05    |
|    std                  | 8.87         |
|    value_loss           | 3.24e+03     |
------------------------------------------
Eval num_timesteps=3262000, episode_reward=537.23 +/- 425.07
Episode length: 472.40 +/- 57.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 537          |
| time/                   |              |
|    total_timesteps      | 3262000      |
| train/                  |              |
|    approx_kl            | 6.411952e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.1        |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.001        |
|    loss                 | 236          |
|    n_updates            | 15920        |
|    policy_gradient_loss | -0.000284    |
|    std                  | 8.89         |
|    value_loss           | 923          |
------------------------------------------
Eval num_timesteps=3264000, episode_reward=172.20 +/- 241.05
Episode length: 460.60 +/- 69.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 461           |
|    mean_reward          | 172           |
| time/                   |               |
|    total_timesteps      | 3264000       |
| train/                  |               |
|    approx_kl            | 6.5124215e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.001         |
|    loss                 | 130           |
|    n_updates            | 15930         |
|    policy_gradient_loss | -0.000314     |
|    std                  | 8.92          |
|    value_loss           | 692           |
-------------------------------------------
Eval num_timesteps=3266000, episode_reward=303.99 +/- 413.16
Episode length: 486.80 +/- 43.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 3266000      |
| train/                  |              |
|    approx_kl            | 0.0002633477 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 52.8         |
|    n_updates            | 15940        |
|    policy_gradient_loss | -0.000692    |
|    std                  | 8.96         |
|    value_loss           | 219          |
------------------------------------------
Eval num_timesteps=3268000, episode_reward=-51.55 +/- 109.84
Episode length: 466.00 +/- 81.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | -51.6        |
| time/                   |              |
|    total_timesteps      | 3268000      |
| train/                  |              |
|    approx_kl            | 0.0014933024 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 65.7         |
|    n_updates            | 15950        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 9            |
|    value_loss           | 285          |
------------------------------------------
Eval num_timesteps=3270000, episode_reward=284.54 +/- 323.24
Episode length: 398.60 +/- 58.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 285          |
| time/                   |              |
|    total_timesteps      | 3270000      |
| train/                  |              |
|    approx_kl            | 0.0010490965 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.833        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+03     |
|    n_updates            | 15960        |
|    policy_gradient_loss | -4.79e-05    |
|    std                  | 9.04         |
|    value_loss           | 3.67e+03     |
------------------------------------------
Eval num_timesteps=3272000, episode_reward=-27.14 +/- 101.29
Episode length: 367.20 +/- 44.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 367          |
|    mean_reward          | -27.1        |
| time/                   |              |
|    total_timesteps      | 3272000      |
| train/                  |              |
|    approx_kl            | 0.0013135055 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 40.9         |
|    n_updates            | 15970        |
|    policy_gradient_loss | -0.00149     |
|    std                  | 9.04         |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=3274000, episode_reward=175.97 +/- 231.55
Episode length: 479.20 +/- 66.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 3274000     |
| train/                  |             |
|    approx_kl            | 0.003046609 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 1.67e+03    |
|    n_updates            | 15980       |
|    policy_gradient_loss | 0.00155     |
|    std                  | 9.04        |
|    value_loss           | 4.81e+03    |
-----------------------------------------
Eval num_timesteps=3276000, episode_reward=279.67 +/- 331.26
Episode length: 394.20 +/- 28.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 280           |
| time/                   |               |
|    total_timesteps      | 3276000       |
| train/                  |               |
|    approx_kl            | 0.00015879702 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.001         |
|    loss                 | 184           |
|    n_updates            | 15990         |
|    policy_gradient_loss | -2.06e-06     |
|    std                  | 9.04          |
|    value_loss           | 766           |
-------------------------------------------
Eval num_timesteps=3278000, episode_reward=667.97 +/- 538.19
Episode length: 489.40 +/- 67.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 668           |
| time/                   |               |
|    total_timesteps      | 3278000       |
| train/                  |               |
|    approx_kl            | 2.6876369e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 728           |
|    n_updates            | 16000         |
|    policy_gradient_loss | -9.38e-06     |
|    std                  | 9.04          |
|    value_loss           | 2.14e+03      |
-------------------------------------------
Eval num_timesteps=3280000, episode_reward=351.03 +/- 275.86
Episode length: 454.80 +/- 82.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 455           |
|    mean_reward          | 351           |
| time/                   |               |
|    total_timesteps      | 3280000       |
| train/                  |               |
|    approx_kl            | 0.00016817002 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 89.9          |
|    n_updates            | 16010         |
|    policy_gradient_loss | -0.000425     |
|    std                  | 9.07          |
|    value_loss           | 312           |
-------------------------------------------
Eval num_timesteps=3282000, episode_reward=262.57 +/- 110.00
Episode length: 415.80 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 416           |
|    mean_reward          | 263           |
| time/                   |               |
|    total_timesteps      | 3282000       |
| train/                  |               |
|    approx_kl            | 0.00023035755 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 92.7          |
|    n_updates            | 16020         |
|    policy_gradient_loss | -0.000178     |
|    std                  | 9.12          |
|    value_loss           | 332           |
-------------------------------------------
Eval num_timesteps=3284000, episode_reward=187.25 +/- 263.42
Episode length: 418.80 +/- 26.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 187           |
| time/                   |               |
|    total_timesteps      | 3284000       |
| train/                  |               |
|    approx_kl            | 0.00045703308 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 16030         |
|    policy_gradient_loss | 0.0003        |
|    std                  | 9.15          |
|    value_loss           | 3.23e+03      |
-------------------------------------------
Eval num_timesteps=3286000, episode_reward=139.15 +/- 119.49
Episode length: 340.00 +/- 44.10
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 340            |
|    mean_reward          | 139            |
| time/                   |                |
|    total_timesteps      | 3286000        |
| train/                  |                |
|    approx_kl            | 0.000102240185 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -14.3          |
|    explained_variance   | 0.935          |
|    learning_rate        | 0.001          |
|    loss                 | 122            |
|    n_updates            | 16040          |
|    policy_gradient_loss | -0.000183      |
|    std                  | 9.17           |
|    value_loss           | 647            |
--------------------------------------------
Eval num_timesteps=3288000, episode_reward=125.79 +/- 179.14
Episode length: 376.40 +/- 88.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 376           |
|    mean_reward          | 126           |
| time/                   |               |
|    total_timesteps      | 3288000       |
| train/                  |               |
|    approx_kl            | 3.9866223e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.001         |
|    loss                 | 924           |
|    n_updates            | 16050         |
|    policy_gradient_loss | -1.62e-05     |
|    std                  | 9.17          |
|    value_loss           | 2.72e+03      |
-------------------------------------------
Eval num_timesteps=3290000, episode_reward=319.19 +/- 212.79
Episode length: 387.00 +/- 46.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 387          |
|    mean_reward          | 319          |
| time/                   |              |
|    total_timesteps      | 3290000      |
| train/                  |              |
|    approx_kl            | 2.190363e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+03     |
|    n_updates            | 16060        |
|    policy_gradient_loss | -7.18e-05    |
|    std                  | 9.17         |
|    value_loss           | 3.38e+03     |
------------------------------------------
Eval num_timesteps=3292000, episode_reward=569.01 +/- 326.22
Episode length: 434.20 +/- 45.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 569           |
| time/                   |               |
|    total_timesteps      | 3292000       |
| train/                  |               |
|    approx_kl            | 0.00015285102 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 101           |
|    n_updates            | 16070         |
|    policy_gradient_loss | -0.00038      |
|    std                  | 9.18          |
|    value_loss           | 342           |
-------------------------------------------
Eval num_timesteps=3294000, episode_reward=133.49 +/- 163.99
Episode length: 375.80 +/- 52.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 376           |
|    mean_reward          | 133           |
| time/                   |               |
|    total_timesteps      | 3294000       |
| train/                  |               |
|    approx_kl            | 0.00020136597 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 86.4          |
|    n_updates            | 16080         |
|    policy_gradient_loss | -0.000327     |
|    std                  | 9.17          |
|    value_loss           | 272           |
-------------------------------------------
Eval num_timesteps=3296000, episode_reward=135.48 +/- 149.84
Episode length: 434.80 +/- 40.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 3296000      |
| train/                  |              |
|    approx_kl            | 0.0007949319 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 107          |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.000623    |
|    std                  | 9.17         |
|    value_loss           | 454          |
------------------------------------------
Eval num_timesteps=3298000, episode_reward=196.11 +/- 218.98
Episode length: 349.20 +/- 75.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 349          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 3298000      |
| train/                  |              |
|    approx_kl            | 0.0002918581 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 283          |
|    n_updates            | 16100        |
|    policy_gradient_loss | 0.000613     |
|    std                  | 9.18         |
|    value_loss           | 1.03e+03     |
------------------------------------------
Eval num_timesteps=3300000, episode_reward=659.73 +/- 455.28
Episode length: 428.80 +/- 98.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 660           |
| time/                   |               |
|    total_timesteps      | 3300000       |
| train/                  |               |
|    approx_kl            | 1.4327292e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.001         |
|    loss                 | 978           |
|    n_updates            | 16110         |
|    policy_gradient_loss | 5.73e-05      |
|    std                  | 9.19          |
|    value_loss           | 2.67e+03      |
-------------------------------------------
Eval num_timesteps=3302000, episode_reward=458.70 +/- 524.03
Episode length: 456.80 +/- 68.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 459           |
| time/                   |               |
|    total_timesteps      | 3302000       |
| train/                  |               |
|    approx_kl            | 0.00022940757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 111           |
|    n_updates            | 16120         |
|    policy_gradient_loss | -0.000502     |
|    std                  | 9.2           |
|    value_loss           | 410           |
-------------------------------------------
Eval num_timesteps=3304000, episode_reward=182.75 +/- 182.56
Episode length: 461.00 +/- 62.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 3304000     |
| train/                  |             |
|    approx_kl            | 0.000290185 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 89.8        |
|    n_updates            | 16130       |
|    policy_gradient_loss | -5.7e-05    |
|    std                  | 9.21        |
|    value_loss           | 403         |
-----------------------------------------
Eval num_timesteps=3306000, episode_reward=581.23 +/- 248.72
Episode length: 418.40 +/- 67.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 581           |
| time/                   |               |
|    total_timesteps      | 3306000       |
| train/                  |               |
|    approx_kl            | 0.00033891722 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 66.2          |
|    n_updates            | 16140         |
|    policy_gradient_loss | -0.000509     |
|    std                  | 9.22          |
|    value_loss           | 227           |
-------------------------------------------
Eval num_timesteps=3308000, episode_reward=264.83 +/- 248.13
Episode length: 365.60 +/- 48.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 366            |
|    mean_reward          | 265            |
| time/                   |                |
|    total_timesteps      | 3308000        |
| train/                  |                |
|    approx_kl            | 0.000107780215 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -14.3          |
|    explained_variance   | 0.975          |
|    learning_rate        | 0.001          |
|    loss                 | 48.1           |
|    n_updates            | 16150          |
|    policy_gradient_loss | 0.00011        |
|    std                  | 9.21           |
|    value_loss           | 182            |
--------------------------------------------
Eval num_timesteps=3310000, episode_reward=337.14 +/- 278.14
Episode length: 430.00 +/- 61.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 337          |
| time/                   |              |
|    total_timesteps      | 3310000      |
| train/                  |              |
|    approx_kl            | 0.0005287974 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 87           |
|    n_updates            | 16160        |
|    policy_gradient_loss | -0.000541    |
|    std                  | 9.19         |
|    value_loss           | 360          |
------------------------------------------
Eval num_timesteps=3312000, episode_reward=201.02 +/- 237.96
Episode length: 411.40 +/- 82.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 3312000       |
| train/                  |               |
|    approx_kl            | 0.00038763834 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 87            |
|    n_updates            | 16170         |
|    policy_gradient_loss | 9.03e-05      |
|    std                  | 9.17          |
|    value_loss           | 467           |
-------------------------------------------
Eval num_timesteps=3314000, episode_reward=265.89 +/- 264.35
Episode length: 499.00 +/- 38.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 499          |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 3314000      |
| train/                  |              |
|    approx_kl            | 0.0008596381 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 54.1         |
|    n_updates            | 16180        |
|    policy_gradient_loss | -0.000846    |
|    std                  | 9.15         |
|    value_loss           | 225          |
------------------------------------------
Eval num_timesteps=3316000, episode_reward=402.21 +/- 359.01
Episode length: 423.60 +/- 93.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 402          |
| time/                   |              |
|    total_timesteps      | 3316000      |
| train/                  |              |
|    approx_kl            | 0.0028816764 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 55.1         |
|    n_updates            | 16190        |
|    policy_gradient_loss | -0.00152     |
|    std                  | 9.13         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=3318000, episode_reward=444.81 +/- 543.97
Episode length: 414.40 +/- 89.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 445         |
| time/                   |             |
|    total_timesteps      | 3318000     |
| train/                  |             |
|    approx_kl            | 0.004852142 |
|    clip_fraction        | 0.00767     |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 37.2        |
|    n_updates            | 16200       |
|    policy_gradient_loss | -0.00174    |
|    std                  | 9.11        |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=3320000, episode_reward=324.34 +/- 360.05
Episode length: 470.20 +/- 72.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 470           |
|    mean_reward          | 324           |
| time/                   |               |
|    total_timesteps      | 3320000       |
| train/                  |               |
|    approx_kl            | 0.00074123667 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 48.8          |
|    n_updates            | 16210         |
|    policy_gradient_loss | -0.000354     |
|    std                  | 9.1           |
|    value_loss           | 156           |
-------------------------------------------
Eval num_timesteps=3322000, episode_reward=268.41 +/- 628.63
Episode length: 494.40 +/- 55.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 3322000      |
| train/                  |              |
|    approx_kl            | 0.0006682694 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 33.3         |
|    n_updates            | 16220        |
|    policy_gradient_loss | -0.00034     |
|    std                  | 9.11         |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=3324000, episode_reward=172.18 +/- 69.19
Episode length: 439.60 +/- 73.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 172          |
| time/                   |              |
|    total_timesteps      | 3324000      |
| train/                  |              |
|    approx_kl            | 0.0012305467 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 82.8         |
|    n_updates            | 16230        |
|    policy_gradient_loss | -0.00151     |
|    std                  | 9.12         |
|    value_loss           | 318          |
------------------------------------------
Eval num_timesteps=3326000, episode_reward=384.18 +/- 357.71
Episode length: 510.40 +/- 59.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 510           |
|    mean_reward          | 384           |
| time/                   |               |
|    total_timesteps      | 3326000       |
| train/                  |               |
|    approx_kl            | 0.00039285037 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 66            |
|    n_updates            | 16240         |
|    policy_gradient_loss | 0.0003        |
|    std                  | 9.12          |
|    value_loss           | 225           |
-------------------------------------------
Eval num_timesteps=3328000, episode_reward=444.84 +/- 247.89
Episode length: 501.40 +/- 30.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 3328000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=130.42 +/- 112.72
Episode length: 410.20 +/- 87.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 130          |
| time/                   |              |
|    total_timesteps      | 3330000      |
| train/                  |              |
|    approx_kl            | 0.0020918918 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 70.2         |
|    n_updates            | 16250        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 9.11         |
|    value_loss           | 243          |
------------------------------------------
Eval num_timesteps=3332000, episode_reward=119.64 +/- 55.97
Episode length: 475.20 +/- 60.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 120           |
| time/                   |               |
|    total_timesteps      | 3332000       |
| train/                  |               |
|    approx_kl            | 0.00077926763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 96.4          |
|    n_updates            | 16260         |
|    policy_gradient_loss | -0.000359     |
|    std                  | 9.1           |
|    value_loss           | 408           |
-------------------------------------------
Eval num_timesteps=3334000, episode_reward=541.51 +/- 496.08
Episode length: 470.60 +/- 40.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 3334000      |
| train/                  |              |
|    approx_kl            | 0.0010899733 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.001        |
|    loss                 | 33           |
|    n_updates            | 16270        |
|    policy_gradient_loss | -0.000624    |
|    std                  | 9.11         |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=3336000, episode_reward=734.97 +/- 676.16
Episode length: 465.80 +/- 91.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | 735           |
| time/                   |               |
|    total_timesteps      | 3336000       |
| train/                  |               |
|    approx_kl            | 0.00095104426 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 68.2          |
|    n_updates            | 16280         |
|    policy_gradient_loss | -0.00028      |
|    std                  | 9.12          |
|    value_loss           | 259           |
-------------------------------------------
Eval num_timesteps=3338000, episode_reward=281.48 +/- 223.23
Episode length: 394.00 +/- 44.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 281           |
| time/                   |               |
|    total_timesteps      | 3338000       |
| train/                  |               |
|    approx_kl            | 0.00020139798 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.001         |
|    loss                 | 40.2          |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.000213     |
|    std                  | 9.14          |
|    value_loss           | 145           |
-------------------------------------------
Eval num_timesteps=3340000, episode_reward=176.95 +/- 190.28
Episode length: 459.40 +/- 138.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 3340000      |
| train/                  |              |
|    approx_kl            | 0.0003776984 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 159          |
|    n_updates            | 16300        |
|    policy_gradient_loss | -0.000492    |
|    std                  | 9.15         |
|    value_loss           | 795          |
------------------------------------------
Eval num_timesteps=3342000, episode_reward=110.19 +/- 94.36
Episode length: 479.00 +/- 55.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | 110          |
| time/                   |              |
|    total_timesteps      | 3342000      |
| train/                  |              |
|    approx_kl            | 0.0013159262 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 16310        |
|    policy_gradient_loss | -0.00095     |
|    std                  | 9.16         |
|    value_loss           | 3.64e+03     |
------------------------------------------
Eval num_timesteps=3344000, episode_reward=55.21 +/- 159.37
Episode length: 472.40 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 472          |
|    mean_reward          | 55.2         |
| time/                   |              |
|    total_timesteps      | 3344000      |
| train/                  |              |
|    approx_kl            | 0.0012037605 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.001        |
|    loss                 | 206          |
|    n_updates            | 16320        |
|    policy_gradient_loss | -0.000713    |
|    std                  | 9.17         |
|    value_loss           | 711          |
------------------------------------------
Eval num_timesteps=3346000, episode_reward=290.61 +/- 386.13
Episode length: 462.40 +/- 51.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 3346000      |
| train/                  |              |
|    approx_kl            | 0.0010620672 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 120          |
|    n_updates            | 16330        |
|    policy_gradient_loss | -9.7e-05     |
|    std                  | 9.17         |
|    value_loss           | 439          |
------------------------------------------
Eval num_timesteps=3348000, episode_reward=193.42 +/- 437.27
Episode length: 505.00 +/- 78.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 3348000      |
| train/                  |              |
|    approx_kl            | 8.715654e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 43.6         |
|    n_updates            | 16340        |
|    policy_gradient_loss | -0.000211    |
|    std                  | 9.17         |
|    value_loss           | 193          |
------------------------------------------
Eval num_timesteps=3350000, episode_reward=40.90 +/- 141.13
Episode length: 452.20 +/- 87.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 452           |
|    mean_reward          | 40.9          |
| time/                   |               |
|    total_timesteps      | 3350000       |
| train/                  |               |
|    approx_kl            | 0.00017740141 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.843         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+03      |
|    n_updates            | 16350         |
|    policy_gradient_loss | -0.000127     |
|    std                  | 9.18          |
|    value_loss           | 3.81e+03      |
-------------------------------------------
Eval num_timesteps=3352000, episode_reward=119.44 +/- 261.11
Episode length: 428.60 +/- 31.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 3352000       |
| train/                  |               |
|    approx_kl            | 5.3072697e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.754         |
|    learning_rate        | 0.001         |
|    loss                 | 3.72e+03      |
|    n_updates            | 16360         |
|    policy_gradient_loss | -0.000156     |
|    std                  | 9.18          |
|    value_loss           | 9.44e+03      |
-------------------------------------------
Eval num_timesteps=3354000, episode_reward=448.62 +/- 468.18
Episode length: 397.20 +/- 95.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 397           |
|    mean_reward          | 449           |
| time/                   |               |
|    total_timesteps      | 3354000       |
| train/                  |               |
|    approx_kl            | 0.00011383672 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+03      |
|    n_updates            | 16370         |
|    policy_gradient_loss | -0.000478     |
|    std                  | 9.18          |
|    value_loss           | 4e+03         |
-------------------------------------------
Eval num_timesteps=3356000, episode_reward=328.46 +/- 304.23
Episode length: 438.40 +/- 22.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 438           |
|    mean_reward          | 328           |
| time/                   |               |
|    total_timesteps      | 3356000       |
| train/                  |               |
|    approx_kl            | 0.00035957093 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.2         |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.001         |
|    loss                 | 408           |
|    n_updates            | 16380         |
|    policy_gradient_loss | -0.000507     |
|    std                  | 9.19          |
|    value_loss           | 1.34e+03      |
-------------------------------------------
Eval num_timesteps=3358000, episode_reward=366.62 +/- 312.58
Episode length: 428.60 +/- 37.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 3358000      |
| train/                  |              |
|    approx_kl            | 0.0012249718 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 57.7         |
|    n_updates            | 16390        |
|    policy_gradient_loss | -0.0014      |
|    std                  | 9.23         |
|    value_loss           | 345          |
------------------------------------------
Eval num_timesteps=3360000, episode_reward=579.93 +/- 465.69
Episode length: 442.60 +/- 81.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 443          |
|    mean_reward          | 580          |
| time/                   |              |
|    total_timesteps      | 3360000      |
| train/                  |              |
|    approx_kl            | 0.0011134722 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 69.1         |
|    n_updates            | 16400        |
|    policy_gradient_loss | -0.000112    |
|    std                  | 9.28         |
|    value_loss           | 259          |
------------------------------------------
Eval num_timesteps=3362000, episode_reward=459.55 +/- 477.99
Episode length: 459.00 +/- 82.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 459           |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 3362000       |
| train/                  |               |
|    approx_kl            | 0.00018119815 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.86          |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+03      |
|    n_updates            | 16410         |
|    policy_gradient_loss | -0.000322     |
|    std                  | 9.31          |
|    value_loss           | 3.98e+03      |
-------------------------------------------
Eval num_timesteps=3364000, episode_reward=180.71 +/- 123.29
Episode length: 409.20 +/- 84.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 3364000      |
| train/                  |              |
|    approx_kl            | 7.859894e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+03     |
|    n_updates            | 16420        |
|    policy_gradient_loss | -1.89e-05    |
|    std                  | 9.32         |
|    value_loss           | 5.47e+03     |
------------------------------------------
Eval num_timesteps=3366000, episode_reward=170.89 +/- 165.01
Episode length: 405.20 +/- 85.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 171           |
| time/                   |               |
|    total_timesteps      | 3366000       |
| train/                  |               |
|    approx_kl            | 4.7800684e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.857         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+03      |
|    n_updates            | 16430         |
|    policy_gradient_loss | -0.000224     |
|    std                  | 9.34          |
|    value_loss           | 3.25e+03      |
-------------------------------------------
Eval num_timesteps=3368000, episode_reward=429.97 +/- 238.68
Episode length: 455.20 +/- 63.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 430          |
| time/                   |              |
|    total_timesteps      | 3368000      |
| train/                  |              |
|    approx_kl            | 0.0005400116 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 56           |
|    n_updates            | 16440        |
|    policy_gradient_loss | -0.000977    |
|    std                  | 9.37         |
|    value_loss           | 339          |
------------------------------------------
Eval num_timesteps=3370000, episode_reward=457.29 +/- 496.85
Episode length: 440.40 +/- 90.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | 457           |
| time/                   |               |
|    total_timesteps      | 3370000       |
| train/                  |               |
|    approx_kl            | 0.00033565355 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 64.1          |
|    n_updates            | 16450         |
|    policy_gradient_loss | 0.00032       |
|    std                  | 9.39          |
|    value_loss           | 322           |
-------------------------------------------
Eval num_timesteps=3372000, episode_reward=348.37 +/- 348.61
Episode length: 439.80 +/- 108.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 440           |
|    mean_reward          | 348           |
| time/                   |               |
|    total_timesteps      | 3372000       |
| train/                  |               |
|    approx_kl            | 0.00043282463 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.3         |
|    explained_variance   | 0.984         |
|    learning_rate        | 0.001         |
|    loss                 | 43.2          |
|    n_updates            | 16460         |
|    policy_gradient_loss | -0.00033      |
|    std                  | 9.41          |
|    value_loss           | 200           |
-------------------------------------------
Eval num_timesteps=3374000, episode_reward=362.16 +/- 315.66
Episode length: 416.40 +/- 104.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 362          |
| time/                   |              |
|    total_timesteps      | 3374000      |
| train/                  |              |
|    approx_kl            | 0.0003982112 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.3        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 1.76e+03     |
|    n_updates            | 16470        |
|    policy_gradient_loss | -0.000335    |
|    std                  | 9.42         |
|    value_loss           | 4.37e+03     |
------------------------------------------
Eval num_timesteps=3376000, episode_reward=270.22 +/- 339.17
Episode length: 425.60 +/- 76.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 3376000     |
| train/                  |             |
|    approx_kl            | 7.96509e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 906         |
|    n_updates            | 16480       |
|    policy_gradient_loss | -0.000167   |
|    std                  | 9.42        |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=3378000, episode_reward=346.15 +/- 355.90
Episode length: 476.80 +/- 84.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 346         |
| time/                   |             |
|    total_timesteps      | 3378000     |
| train/                  |             |
|    approx_kl            | 0.002575644 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 26.5        |
|    n_updates            | 16490       |
|    policy_gradient_loss | -0.00106    |
|    std                  | 9.44        |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=3380000, episode_reward=389.38 +/- 286.90
Episode length: 433.60 +/- 71.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 389          |
| time/                   |              |
|    total_timesteps      | 3380000      |
| train/                  |              |
|    approx_kl            | 0.0030153915 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+03     |
|    n_updates            | 16500        |
|    policy_gradient_loss | 0.000614     |
|    std                  | 9.46         |
|    value_loss           | 2.86e+03     |
------------------------------------------
Eval num_timesteps=3382000, episode_reward=149.92 +/- 90.91
Episode length: 355.80 +/- 21.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 356          |
|    mean_reward          | 150          |
| time/                   |              |
|    total_timesteps      | 3382000      |
| train/                  |              |
|    approx_kl            | 0.0024862937 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.001        |
|    loss                 | 25.7         |
|    n_updates            | 16510        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 9.43         |
|    value_loss           | 119          |
------------------------------------------
Eval num_timesteps=3384000, episode_reward=258.25 +/- 143.12
Episode length: 386.40 +/- 32.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 258          |
| time/                   |              |
|    total_timesteps      | 3384000      |
| train/                  |              |
|    approx_kl            | 0.0011373836 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+03     |
|    n_updates            | 16520        |
|    policy_gradient_loss | 0.000782     |
|    std                  | 9.41         |
|    value_loss           | 3.13e+03     |
------------------------------------------
Eval num_timesteps=3386000, episode_reward=321.66 +/- 256.87
Episode length: 449.20 +/- 100.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 322           |
| time/                   |               |
|    total_timesteps      | 3386000       |
| train/                  |               |
|    approx_kl            | 3.8074242e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.4         |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 16530         |
|    policy_gradient_loss | 0.00025       |
|    std                  | 9.4           |
|    value_loss           | 2.94e+03      |
-------------------------------------------
Eval num_timesteps=3388000, episode_reward=236.87 +/- 183.28
Episode length: 453.20 +/- 76.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 237          |
| time/                   |              |
|    total_timesteps      | 3388000      |
| train/                  |              |
|    approx_kl            | 0.0009905206 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 25           |
|    n_updates            | 16540        |
|    policy_gradient_loss | -0.000947    |
|    std                  | 9.38         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=3390000, episode_reward=552.35 +/- 435.74
Episode length: 501.00 +/- 77.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 501          |
|    mean_reward          | 552          |
| time/                   |              |
|    total_timesteps      | 3390000      |
| train/                  |              |
|    approx_kl            | 0.0029493032 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 59.7         |
|    n_updates            | 16550        |
|    policy_gradient_loss | -0.0019      |
|    std                  | 9.38         |
|    value_loss           | 281          |
------------------------------------------
Eval num_timesteps=3392000, episode_reward=128.11 +/- 164.32
Episode length: 412.60 +/- 30.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 128          |
| time/                   |              |
|    total_timesteps      | 3392000      |
| train/                  |              |
|    approx_kl            | 0.0014112771 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.001        |
|    loss                 | 1.61e+03     |
|    n_updates            | 16560        |
|    policy_gradient_loss | 0.00134      |
|    std                  | 9.4          |
|    value_loss           | 3.88e+03     |
------------------------------------------
Eval num_timesteps=3394000, episode_reward=285.78 +/- 333.89
Episode length: 469.00 +/- 38.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 469           |
|    mean_reward          | 286           |
| time/                   |               |
|    total_timesteps      | 3394000       |
| train/                  |               |
|    approx_kl            | 0.00074298156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.4         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 49.8          |
|    n_updates            | 16570         |
|    policy_gradient_loss | -0.00112      |
|    std                  | 9.43          |
|    value_loss           | 227           |
-------------------------------------------
Eval num_timesteps=3396000, episode_reward=119.90 +/- 161.81
Episode length: 502.60 +/- 46.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 503          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 3396000      |
| train/                  |              |
|    approx_kl            | 0.0022744448 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 46.1         |
|    n_updates            | 16580        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 9.5          |
|    value_loss           | 204          |
------------------------------------------
Eval num_timesteps=3398000, episode_reward=106.87 +/- 178.39
Episode length: 448.40 +/- 69.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 107          |
| time/                   |              |
|    total_timesteps      | 3398000      |
| train/                  |              |
|    approx_kl            | 0.0019437023 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 16590        |
|    policy_gradient_loss | 4.7e-05      |
|    std                  | 9.57         |
|    value_loss           | 604          |
------------------------------------------
Eval num_timesteps=3400000, episode_reward=349.64 +/- 377.49
Episode length: 565.40 +/- 66.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 565           |
|    mean_reward          | 350           |
| time/                   |               |
|    total_timesteps      | 3400000       |
| train/                  |               |
|    approx_kl            | 0.00034521698 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.4         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 53.9          |
|    n_updates            | 16600         |
|    policy_gradient_loss | -9.53e-05     |
|    std                  | 9.61          |
|    value_loss           | 314           |
-------------------------------------------
Eval num_timesteps=3402000, episode_reward=200.21 +/- 100.35
Episode length: 461.20 +/- 80.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | 200          |
| time/                   |              |
|    total_timesteps      | 3402000      |
| train/                  |              |
|    approx_kl            | 0.0014526999 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.4        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 57.9         |
|    n_updates            | 16610        |
|    policy_gradient_loss | -0.000884    |
|    std                  | 9.63         |
|    value_loss           | 304          |
------------------------------------------
Eval num_timesteps=3404000, episode_reward=-53.79 +/- 388.08
Episode length: 498.20 +/- 62.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | -53.8        |
| time/                   |              |
|    total_timesteps      | 3404000      |
| train/                  |              |
|    approx_kl            | 0.0013028075 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 2.08e+03     |
|    n_updates            | 16620        |
|    policy_gradient_loss | -0.000717    |
|    std                  | 9.63         |
|    value_loss           | 5.16e+03     |
------------------------------------------
Eval num_timesteps=3406000, episode_reward=251.30 +/- 244.97
Episode length: 505.60 +/- 67.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 506           |
|    mean_reward          | 251           |
| time/                   |               |
|    total_timesteps      | 3406000       |
| train/                  |               |
|    approx_kl            | 0.00027378212 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+03      |
|    n_updates            | 16630         |
|    policy_gradient_loss | -0.000141     |
|    std                  | 9.63          |
|    value_loss           | 2.99e+03      |
-------------------------------------------
Eval num_timesteps=3408000, episode_reward=112.64 +/- 258.20
Episode length: 446.20 +/- 45.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 113           |
| time/                   |               |
|    total_timesteps      | 3408000       |
| train/                  |               |
|    approx_kl            | 0.00026680098 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 144           |
|    n_updates            | 16640         |
|    policy_gradient_loss | -0.000644     |
|    std                  | 9.64          |
|    value_loss           | 804           |
-------------------------------------------
Eval num_timesteps=3410000, episode_reward=4.68 +/- 47.29
Episode length: 488.60 +/- 75.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 489          |
|    mean_reward          | 4.68         |
| time/                   |              |
|    total_timesteps      | 3410000      |
| train/                  |              |
|    approx_kl            | 0.0021988086 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 42           |
|    n_updates            | 16650        |
|    policy_gradient_loss | -0.000957    |
|    std                  | 9.65         |
|    value_loss           | 200          |
------------------------------------------
Eval num_timesteps=3412000, episode_reward=202.05 +/- 209.38
Episode length: 433.40 +/- 77.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 433          |
|    mean_reward          | 202          |
| time/                   |              |
|    total_timesteps      | 3412000      |
| train/                  |              |
|    approx_kl            | 0.0018752054 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+03     |
|    n_updates            | 16660        |
|    policy_gradient_loss | 7.86e-05     |
|    std                  | 9.64         |
|    value_loss           | 8.3e+03      |
------------------------------------------
Eval num_timesteps=3414000, episode_reward=333.63 +/- 212.99
Episode length: 443.20 +/- 27.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 443      |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 3414000  |
---------------------------------
Eval num_timesteps=3416000, episode_reward=46.53 +/- 103.23
Episode length: 350.00 +/- 34.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 350           |
|    mean_reward          | 46.5          |
| time/                   |               |
|    total_timesteps      | 3416000       |
| train/                  |               |
|    approx_kl            | 0.00046276397 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 65.6          |
|    n_updates            | 16670         |
|    policy_gradient_loss | -0.000445     |
|    std                  | 9.67          |
|    value_loss           | 263           |
-------------------------------------------
Eval num_timesteps=3418000, episode_reward=96.81 +/- 227.10
Episode length: 480.00 +/- 78.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 480           |
|    mean_reward          | 96.8          |
| time/                   |               |
|    total_timesteps      | 3418000       |
| train/                  |               |
|    approx_kl            | 0.00034938744 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.001         |
|    loss                 | 885           |
|    n_updates            | 16680         |
|    policy_gradient_loss | -2.17e-05     |
|    std                  | 9.7           |
|    value_loss           | 2.89e+03      |
-------------------------------------------
Eval num_timesteps=3420000, episode_reward=63.81 +/- 101.61
Episode length: 515.40 +/- 89.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 515           |
|    mean_reward          | 63.8          |
| time/                   |               |
|    total_timesteps      | 3420000       |
| train/                  |               |
|    approx_kl            | 0.00015182895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 144           |
|    n_updates            | 16690         |
|    policy_gradient_loss | -0.000622     |
|    std                  | 9.72          |
|    value_loss           | 559           |
-------------------------------------------
Eval num_timesteps=3422000, episode_reward=82.79 +/- 149.35
Episode length: 417.80 +/- 69.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 82.8          |
| time/                   |               |
|    total_timesteps      | 3422000       |
| train/                  |               |
|    approx_kl            | 0.00048195772 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 54.3          |
|    n_updates            | 16700         |
|    policy_gradient_loss | -0.000398     |
|    std                  | 9.74          |
|    value_loss           | 205           |
-------------------------------------------
Eval num_timesteps=3424000, episode_reward=152.18 +/- 66.48
Episode length: 453.00 +/- 68.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 152          |
| time/                   |              |
|    total_timesteps      | 3424000      |
| train/                  |              |
|    approx_kl            | 0.0010154191 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 43.1         |
|    n_updates            | 16710        |
|    policy_gradient_loss | -0.000809    |
|    std                  | 9.74         |
|    value_loss           | 163          |
------------------------------------------
Eval num_timesteps=3426000, episode_reward=551.09 +/- 239.98
Episode length: 513.00 +/- 25.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 513        |
|    mean_reward          | 551        |
| time/                   |            |
|    total_timesteps      | 3426000    |
| train/                  |            |
|    approx_kl            | 0.00274612 |
|    clip_fraction        | 0.00107    |
|    clip_range           | 0.2        |
|    entropy_loss         | -14.5      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.001      |
|    loss                 | 232        |
|    n_updates            | 16720      |
|    policy_gradient_loss | -0.000387  |
|    std                  | 9.76       |
|    value_loss           | 866        |
----------------------------------------
Eval num_timesteps=3428000, episode_reward=137.15 +/- 315.89
Episode length: 442.00 +/- 27.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 442          |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 3428000      |
| train/                  |              |
|    approx_kl            | 0.0007212806 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 5.76e+03     |
|    n_updates            | 16730        |
|    policy_gradient_loss | -0.00066     |
|    std                  | 9.78         |
|    value_loss           | 1.4e+04      |
------------------------------------------
Eval num_timesteps=3430000, episode_reward=33.47 +/- 325.53
Episode length: 524.60 +/- 58.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | 33.5          |
| time/                   |               |
|    total_timesteps      | 3430000       |
| train/                  |               |
|    approx_kl            | 0.00013998794 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 48.2          |
|    n_updates            | 16740         |
|    policy_gradient_loss | -0.000185     |
|    std                  | 9.78          |
|    value_loss           | 212           |
-------------------------------------------
Eval num_timesteps=3432000, episode_reward=263.96 +/- 128.49
Episode length: 467.80 +/- 53.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 468           |
|    mean_reward          | 264           |
| time/                   |               |
|    total_timesteps      | 3432000       |
| train/                  |               |
|    approx_kl            | 0.00052469375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.859         |
|    learning_rate        | 0.001         |
|    loss                 | 294           |
|    n_updates            | 16750         |
|    policy_gradient_loss | -0.000753     |
|    std                  | 9.78          |
|    value_loss           | 1.52e+03      |
-------------------------------------------
Eval num_timesteps=3434000, episode_reward=746.50 +/- 731.39
Episode length: 544.60 +/- 113.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 545          |
|    mean_reward          | 746          |
| time/                   |              |
|    total_timesteps      | 3434000      |
| train/                  |              |
|    approx_kl            | 0.0009030682 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 16760        |
|    policy_gradient_loss | -0.000422    |
|    std                  | 9.79         |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=3436000, episode_reward=203.51 +/- 240.78
Episode length: 547.40 +/- 25.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 547           |
|    mean_reward          | 204           |
| time/                   |               |
|    total_timesteps      | 3436000       |
| train/                  |               |
|    approx_kl            | 0.00032759437 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.81          |
|    learning_rate        | 0.001         |
|    loss                 | 3.38e+03      |
|    n_updates            | 16770         |
|    policy_gradient_loss | -0.000372     |
|    std                  | 9.79          |
|    value_loss           | 8.47e+03      |
-------------------------------------------
Eval num_timesteps=3438000, episode_reward=391.41 +/- 208.73
Episode length: 499.00 +/- 83.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 499           |
|    mean_reward          | 391           |
| time/                   |               |
|    total_timesteps      | 3438000       |
| train/                  |               |
|    approx_kl            | 3.3674674e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.001         |
|    loss                 | 194           |
|    n_updates            | 16780         |
|    policy_gradient_loss | -5.55e-05     |
|    std                  | 9.8           |
|    value_loss           | 772           |
-------------------------------------------
Eval num_timesteps=3440000, episode_reward=393.06 +/- 417.10
Episode length: 501.00 +/- 117.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 501          |
|    mean_reward          | 393          |
| time/                   |              |
|    total_timesteps      | 3440000      |
| train/                  |              |
|    approx_kl            | 5.761406e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.001        |
|    loss                 | 215          |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.000162    |
|    std                  | 9.83         |
|    value_loss           | 834          |
------------------------------------------
Eval num_timesteps=3442000, episode_reward=459.10 +/- 290.05
Episode length: 466.80 +/- 77.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 467           |
|    mean_reward          | 459           |
| time/                   |               |
|    total_timesteps      | 3442000       |
| train/                  |               |
|    approx_kl            | 7.6951925e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.5         |
|    explained_variance   | 0.785         |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+03      |
|    n_updates            | 16800         |
|    policy_gradient_loss | 1.87e-05      |
|    std                  | 9.85          |
|    value_loss           | 5.49e+03      |
-------------------------------------------
Eval num_timesteps=3444000, episode_reward=223.86 +/- 272.78
Episode length: 477.80 +/- 63.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 224          |
| time/                   |              |
|    total_timesteps      | 3444000      |
| train/                  |              |
|    approx_kl            | 0.0004968577 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 16810        |
|    policy_gradient_loss | -0.00051     |
|    std                  | 9.86         |
|    value_loss           | 516          |
------------------------------------------
Eval num_timesteps=3446000, episode_reward=26.01 +/- 173.74
Episode length: 393.80 +/- 55.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 394          |
|    mean_reward          | 26           |
| time/                   |              |
|    total_timesteps      | 3446000      |
| train/                  |              |
|    approx_kl            | 0.0019297537 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.5        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 101          |
|    n_updates            | 16820        |
|    policy_gradient_loss | -0.000835    |
|    std                  | 9.88         |
|    value_loss           | 390          |
------------------------------------------
Eval num_timesteps=3448000, episode_reward=341.20 +/- 390.36
Episode length: 490.40 +/- 89.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 490          |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 3448000      |
| train/                  |              |
|    approx_kl            | 0.0021626768 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 47.6         |
|    n_updates            | 16830        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 9.9          |
|    value_loss           | 192          |
------------------------------------------
Eval num_timesteps=3450000, episode_reward=632.94 +/- 390.77
Episode length: 431.40 +/- 61.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 431         |
|    mean_reward          | 633         |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.010156248 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.6       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+03    |
|    n_updates            | 16840       |
|    policy_gradient_loss | -0.000629   |
|    std                  | 9.93        |
|    value_loss           | 3.4e+03     |
-----------------------------------------
Eval num_timesteps=3452000, episode_reward=267.23 +/- 169.88
Episode length: 462.00 +/- 59.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 3452000      |
| train/                  |              |
|    approx_kl            | 0.0004534382 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.001        |
|    loss                 | 122          |
|    n_updates            | 16850        |
|    policy_gradient_loss | -8.48e-05    |
|    std                  | 9.94         |
|    value_loss           | 526          |
------------------------------------------
Eval num_timesteps=3454000, episode_reward=372.74 +/- 402.20
Episode length: 425.60 +/- 38.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 373           |
| time/                   |               |
|    total_timesteps      | 3454000       |
| train/                  |               |
|    approx_kl            | 0.00014521962 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.6         |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 88.1          |
|    n_updates            | 16860         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 9.95          |
|    value_loss           | 524           |
-------------------------------------------
Eval num_timesteps=3456000, episode_reward=319.10 +/- 252.60
Episode length: 462.40 +/- 94.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 319           |
| time/                   |               |
|    total_timesteps      | 3456000       |
| train/                  |               |
|    approx_kl            | 0.00019575984 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.6         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 105           |
|    n_updates            | 16870         |
|    policy_gradient_loss | -0.000447     |
|    std                  | 9.96          |
|    value_loss           | 330           |
-------------------------------------------
Eval num_timesteps=3458000, episode_reward=217.97 +/- 307.11
Episode length: 480.60 +/- 74.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 481          |
|    mean_reward          | 218          |
| time/                   |              |
|    total_timesteps      | 3458000      |
| train/                  |              |
|    approx_kl            | 0.0015339636 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 16880        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 9.98         |
|    value_loss           | 424          |
------------------------------------------
Eval num_timesteps=3460000, episode_reward=269.51 +/- 379.74
Episode length: 520.00 +/- 95.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 520          |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 3460000      |
| train/                  |              |
|    approx_kl            | 0.0025070878 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.001        |
|    loss                 | 907          |
|    n_updates            | 16890        |
|    policy_gradient_loss | -0.000194    |
|    std                  | 10           |
|    value_loss           | 2.49e+03     |
------------------------------------------
Eval num_timesteps=3462000, episode_reward=119.50 +/- 250.38
Episode length: 471.00 +/- 32.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 471           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 3462000       |
| train/                  |               |
|    approx_kl            | 0.00035021603 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.6         |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+03      |
|    n_updates            | 16900         |
|    policy_gradient_loss | 0.000152      |
|    std                  | 10            |
|    value_loss           | 4.49e+03      |
-------------------------------------------
Eval num_timesteps=3464000, episode_reward=745.42 +/- 320.56
Episode length: 533.60 +/- 66.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 534          |
|    mean_reward          | 745          |
| time/                   |              |
|    total_timesteps      | 3464000      |
| train/                  |              |
|    approx_kl            | 8.977519e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 142          |
|    n_updates            | 16910        |
|    policy_gradient_loss | -0.000198    |
|    std                  | 10           |
|    value_loss           | 610          |
------------------------------------------
Eval num_timesteps=3466000, episode_reward=140.08 +/- 108.47
Episode length: 500.80 +/- 53.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 501          |
|    mean_reward          | 140          |
| time/                   |              |
|    total_timesteps      | 3466000      |
| train/                  |              |
|    approx_kl            | 0.0003162937 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.001        |
|    loss                 | 112          |
|    n_updates            | 16920        |
|    policy_gradient_loss | -0.000429    |
|    std                  | 10           |
|    value_loss           | 756          |
------------------------------------------
Eval num_timesteps=3468000, episode_reward=1048.90 +/- 853.59
Episode length: 535.60 +/- 87.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 536          |
|    mean_reward          | 1.05e+03     |
| time/                   |              |
|    total_timesteps      | 3468000      |
| train/                  |              |
|    approx_kl            | 0.0004015441 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 175          |
|    n_updates            | 16930        |
|    policy_gradient_loss | -0.000505    |
|    std                  | 10           |
|    value_loss           | 674          |
------------------------------------------
Eval num_timesteps=3470000, episode_reward=249.00 +/- 314.22
Episode length: 431.60 +/- 91.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 249          |
| time/                   |              |
|    total_timesteps      | 3470000      |
| train/                  |              |
|    approx_kl            | 0.0004269004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 130          |
|    n_updates            | 16940        |
|    policy_gradient_loss | -0.000283    |
|    std                  | 10           |
|    value_loss           | 472          |
------------------------------------------
Eval num_timesteps=3472000, episode_reward=24.03 +/- 265.20
Episode length: 478.00 +/- 53.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 24          |
| time/                   |             |
|    total_timesteps      | 3472000     |
| train/                  |             |
|    approx_kl            | 0.004841822 |
|    clip_fraction        | 0.00684     |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.6       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 66.9        |
|    n_updates            | 16950       |
|    policy_gradient_loss | -0.0025     |
|    std                  | 10          |
|    value_loss           | 306         |
-----------------------------------------
Eval num_timesteps=3474000, episode_reward=131.26 +/- 192.28
Episode length: 381.40 +/- 87.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 381          |
|    mean_reward          | 131          |
| time/                   |              |
|    total_timesteps      | 3474000      |
| train/                  |              |
|    approx_kl            | 0.0021148701 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+03     |
|    n_updates            | 16960        |
|    policy_gradient_loss | 0.00207      |
|    std                  | 10           |
|    value_loss           | 5.24e+03     |
------------------------------------------
Eval num_timesteps=3476000, episode_reward=98.46 +/- 343.74
Episode length: 412.80 +/- 71.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 98.5        |
| time/                   |             |
|    total_timesteps      | 3476000     |
| train/                  |             |
|    approx_kl            | 0.001975087 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 36.9        |
|    n_updates            | 16970       |
|    policy_gradient_loss | -0.00178    |
|    std                  | 10.1        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=3478000, episode_reward=28.85 +/- 177.72
Episode length: 384.40 +/- 29.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 384          |
|    mean_reward          | 28.8         |
| time/                   |              |
|    total_timesteps      | 3478000      |
| train/                  |              |
|    approx_kl            | 0.0007997467 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.611        |
|    learning_rate        | 0.001        |
|    loss                 | 2.03e+03     |
|    n_updates            | 16980        |
|    policy_gradient_loss | 0.000332     |
|    std                  | 10.1         |
|    value_loss           | 5.1e+03      |
------------------------------------------
Eval num_timesteps=3480000, episode_reward=216.01 +/- 256.21
Episode length: 551.80 +/- 138.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 552         |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 3480000     |
| train/                  |             |
|    approx_kl            | 0.000640174 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.6       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 47.9        |
|    n_updates            | 16990       |
|    policy_gradient_loss | -0.000432   |
|    std                  | 10.2        |
|    value_loss           | 210         |
-----------------------------------------
Eval num_timesteps=3482000, episode_reward=535.20 +/- 343.65
Episode length: 413.40 +/- 70.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 413          |
|    mean_reward          | 535          |
| time/                   |              |
|    total_timesteps      | 3482000      |
| train/                  |              |
|    approx_kl            | 0.0028798236 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 34.3         |
|    n_updates            | 17000        |
|    policy_gradient_loss | -0.0011      |
|    std                  | 10.1         |
|    value_loss           | 160          |
------------------------------------------
Eval num_timesteps=3484000, episode_reward=160.94 +/- 214.77
Episode length: 331.40 +/- 40.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 331           |
|    mean_reward          | 161           |
| time/                   |               |
|    total_timesteps      | 3484000       |
| train/                  |               |
|    approx_kl            | 0.00093655544 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.6         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 49.9          |
|    n_updates            | 17010         |
|    policy_gradient_loss | -0.000371     |
|    std                  | 10.1          |
|    value_loss           | 213           |
-------------------------------------------
Eval num_timesteps=3486000, episode_reward=264.08 +/- 185.32
Episode length: 436.40 +/- 114.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | 264          |
| time/                   |              |
|    total_timesteps      | 3486000      |
| train/                  |              |
|    approx_kl            | 0.0010677685 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 67.9         |
|    n_updates            | 17020        |
|    policy_gradient_loss | -0.000165    |
|    std                  | 10.1         |
|    value_loss           | 264          |
------------------------------------------
Eval num_timesteps=3488000, episode_reward=300.93 +/- 228.29
Episode length: 401.20 +/- 118.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 401          |
|    mean_reward          | 301          |
| time/                   |              |
|    total_timesteps      | 3488000      |
| train/                  |              |
|    approx_kl            | 0.0003467414 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.783        |
|    learning_rate        | 0.001        |
|    loss                 | 1.61e+03     |
|    n_updates            | 17030        |
|    policy_gradient_loss | -0.000192    |
|    std                  | 10.1         |
|    value_loss           | 4.06e+03     |
------------------------------------------
Eval num_timesteps=3490000, episode_reward=111.59 +/- 193.83
Episode length: 402.20 +/- 103.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 112         |
| time/                   |             |
|    total_timesteps      | 3490000     |
| train/                  |             |
|    approx_kl            | 0.000508794 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.6       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 40.6        |
|    n_updates            | 17040       |
|    policy_gradient_loss | -0.000663   |
|    std                  | 10.2        |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=3492000, episode_reward=181.05 +/- 120.18
Episode length: 397.60 +/- 36.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 398          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 3492000      |
| train/                  |              |
|    approx_kl            | 0.0010633835 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.6        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 62.4         |
|    n_updates            | 17050        |
|    policy_gradient_loss | -0.000779    |
|    std                  | 10.2         |
|    value_loss           | 269          |
------------------------------------------
Eval num_timesteps=3494000, episode_reward=235.86 +/- 219.70
Episode length: 365.80 +/- 38.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 366         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 3494000     |
| train/                  |             |
|    approx_kl            | 0.001582316 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 67.5        |
|    n_updates            | 17060       |
|    policy_gradient_loss | -0.000796   |
|    std                  | 10.3        |
|    value_loss           | 324         |
-----------------------------------------
Eval num_timesteps=3496000, episode_reward=207.07 +/- 207.29
Episode length: 361.40 +/- 44.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 361          |
|    mean_reward          | 207          |
| time/                   |              |
|    total_timesteps      | 3496000      |
| train/                  |              |
|    approx_kl            | 0.0032624784 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 48.5         |
|    n_updates            | 17070        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 10.3         |
|    value_loss           | 176          |
------------------------------------------
Eval num_timesteps=3498000, episode_reward=192.44 +/- 244.31
Episode length: 340.40 +/- 50.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 340          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 3498000      |
| train/                  |              |
|    approx_kl            | 0.0018379119 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 33.9         |
|    n_updates            | 17080        |
|    policy_gradient_loss | -0.000771    |
|    std                  | 10.3         |
|    value_loss           | 138          |
------------------------------------------
Eval num_timesteps=3500000, episode_reward=445.34 +/- 363.70
Episode length: 473.60 +/- 80.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3502000, episode_reward=203.07 +/- 118.61
Episode length: 396.60 +/- 43.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 3502000     |
| train/                  |             |
|    approx_kl            | 0.004089324 |
|    clip_fraction        | 0.00518     |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 37.6        |
|    n_updates            | 17090       |
|    policy_gradient_loss | -0.00162    |
|    std                  | 10.4        |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=3504000, episode_reward=72.99 +/- 77.07
Episode length: 348.20 +/- 43.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 348          |
|    mean_reward          | 73           |
| time/                   |              |
|    total_timesteps      | 3504000      |
| train/                  |              |
|    approx_kl            | 0.0032844478 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 29.7         |
|    n_updates            | 17100        |
|    policy_gradient_loss | -0.000541    |
|    std                  | 10.4         |
|    value_loss           | 119          |
------------------------------------------
Eval num_timesteps=3506000, episode_reward=400.58 +/- 511.23
Episode length: 499.20 +/- 161.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 499          |
|    mean_reward          | 401          |
| time/                   |              |
|    total_timesteps      | 3506000      |
| train/                  |              |
|    approx_kl            | 0.0029407325 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 97.5         |
|    n_updates            | 17110        |
|    policy_gradient_loss | -0.000956    |
|    std                  | 10.4         |
|    value_loss           | 377          |
------------------------------------------
Eval num_timesteps=3508000, episode_reward=6.02 +/- 95.30
Episode length: 326.40 +/- 59.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | 6.02         |
| time/                   |              |
|    total_timesteps      | 3508000      |
| train/                  |              |
|    approx_kl            | 0.0038858447 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 50.3         |
|    n_updates            | 17120        |
|    policy_gradient_loss | -0.00156     |
|    std                  | 10.5         |
|    value_loss           | 245          |
------------------------------------------
Eval num_timesteps=3510000, episode_reward=50.34 +/- 63.41
Episode length: 327.00 +/- 25.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 327           |
|    mean_reward          | 50.3          |
| time/                   |               |
|    total_timesteps      | 3510000       |
| train/                  |               |
|    approx_kl            | 0.00066633883 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.001         |
|    loss                 | 96            |
|    n_updates            | 17130         |
|    policy_gradient_loss | -7.22e-05     |
|    std                  | 10.5          |
|    value_loss           | 363           |
-------------------------------------------
Eval num_timesteps=3512000, episode_reward=29.01 +/- 52.87
Episode length: 376.20 +/- 65.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 376           |
|    mean_reward          | 29            |
| time/                   |               |
|    total_timesteps      | 3512000       |
| train/                  |               |
|    approx_kl            | 0.00015934277 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 55            |
|    n_updates            | 17140         |
|    policy_gradient_loss | 0.000134      |
|    std                  | 10.5          |
|    value_loss           | 257           |
-------------------------------------------
Eval num_timesteps=3514000, episode_reward=99.03 +/- 88.24
Episode length: 379.60 +/- 18.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 380           |
|    mean_reward          | 99            |
| time/                   |               |
|    total_timesteps      | 3514000       |
| train/                  |               |
|    approx_kl            | 0.00029134518 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.001         |
|    loss                 | 38            |
|    n_updates            | 17150         |
|    policy_gradient_loss | -0.00039      |
|    std                  | 10.5          |
|    value_loss           | 115           |
-------------------------------------------
Eval num_timesteps=3516000, episode_reward=35.40 +/- 19.17
Episode length: 331.20 +/- 40.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 331           |
|    mean_reward          | 35.4          |
| time/                   |               |
|    total_timesteps      | 3516000       |
| train/                  |               |
|    approx_kl            | 0.00012661851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+03      |
|    n_updates            | 17160         |
|    policy_gradient_loss | 0.000202      |
|    std                  | 10.5          |
|    value_loss           | 4.62e+03      |
-------------------------------------------
Eval num_timesteps=3518000, episode_reward=35.79 +/- 134.23
Episode length: 408.80 +/- 76.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 35.8        |
| time/                   |             |
|    total_timesteps      | 3518000     |
| train/                  |             |
|    approx_kl            | 0.000216354 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | 29.8        |
|    n_updates            | 17170       |
|    policy_gradient_loss | -0.000449   |
|    std                  | 10.5        |
|    value_loss           | 110         |
-----------------------------------------
Eval num_timesteps=3520000, episode_reward=143.05 +/- 186.43
Episode length: 417.80 +/- 76.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 143          |
| time/                   |              |
|    total_timesteps      | 3520000      |
| train/                  |              |
|    approx_kl            | 0.0023279404 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 35.8         |
|    n_updates            | 17180        |
|    policy_gradient_loss | -0.00144     |
|    std                  | 10.5         |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=3522000, episode_reward=-16.93 +/- 90.99
Episode length: 333.20 +/- 43.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 333          |
|    mean_reward          | -16.9        |
| time/                   |              |
|    total_timesteps      | 3522000      |
| train/                  |              |
|    approx_kl            | 0.0023463676 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 31.4         |
|    n_updates            | 17190        |
|    policy_gradient_loss | -0.000989    |
|    std                  | 10.6         |
|    value_loss           | 110          |
------------------------------------------
Eval num_timesteps=3524000, episode_reward=-42.90 +/- 79.12
Episode length: 346.40 +/- 45.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 346           |
|    mean_reward          | -42.9         |
| time/                   |               |
|    total_timesteps      | 3524000       |
| train/                  |               |
|    approx_kl            | 0.00069996866 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 57.1          |
|    n_updates            | 17200         |
|    policy_gradient_loss | -0.000786     |
|    std                  | 10.6          |
|    value_loss           | 196           |
-------------------------------------------
Eval num_timesteps=3526000, episode_reward=42.76 +/- 104.22
Episode length: 412.20 +/- 30.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 412           |
|    mean_reward          | 42.8          |
| time/                   |               |
|    total_timesteps      | 3526000       |
| train/                  |               |
|    approx_kl            | 0.00021689344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.001         |
|    loss                 | 17.4          |
|    n_updates            | 17210         |
|    policy_gradient_loss | -8.47e-05     |
|    std                  | 10.6          |
|    value_loss           | 73.8          |
-------------------------------------------
Eval num_timesteps=3528000, episode_reward=10.69 +/- 57.12
Episode length: 393.20 +/- 75.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | 10.7        |
| time/                   |             |
|    total_timesteps      | 3528000     |
| train/                  |             |
|    approx_kl            | 0.001781467 |
|    clip_fraction        | 0.000342    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 33.6        |
|    n_updates            | 17220       |
|    policy_gradient_loss | -0.0013     |
|    std                  | 10.6        |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=3530000, episode_reward=291.19 +/- 690.27
Episode length: 402.60 +/- 93.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 403          |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 3530000      |
| train/                  |              |
|    approx_kl            | 0.0011270151 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.001        |
|    loss                 | 26           |
|    n_updates            | 17230        |
|    policy_gradient_loss | -0.000467    |
|    std                  | 10.6         |
|    value_loss           | 99.1         |
------------------------------------------
Eval num_timesteps=3532000, episode_reward=-50.11 +/- 55.04
Episode length: 368.80 +/- 18.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 369           |
|    mean_reward          | -50.1         |
| time/                   |               |
|    total_timesteps      | 3532000       |
| train/                  |               |
|    approx_kl            | 0.00069970347 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 54            |
|    n_updates            | 17240         |
|    policy_gradient_loss | 7.27e-05      |
|    std                  | 10.5          |
|    value_loss           | 202           |
-------------------------------------------
Eval num_timesteps=3534000, episode_reward=23.55 +/- 89.01
Episode length: 385.60 +/- 32.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 386          |
|    mean_reward          | 23.6         |
| time/                   |              |
|    total_timesteps      | 3534000      |
| train/                  |              |
|    approx_kl            | 0.0007506581 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 72.5         |
|    n_updates            | 17250        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 10.5         |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=3536000, episode_reward=-13.56 +/- 69.46
Episode length: 473.00 +/- 88.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | -13.6        |
| time/                   |              |
|    total_timesteps      | 3536000      |
| train/                  |              |
|    approx_kl            | 0.0020344912 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 44.1         |
|    n_updates            | 17260        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 10.5         |
|    value_loss           | 150          |
------------------------------------------
Eval num_timesteps=3538000, episode_reward=142.57 +/- 270.62
Episode length: 420.60 +/- 70.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 421         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 3538000     |
| train/                  |             |
|    approx_kl            | 0.002342191 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.001       |
|    loss                 | 26.8        |
|    n_updates            | 17270       |
|    policy_gradient_loss | -0.00054    |
|    std                  | 10.5        |
|    value_loss           | 99.4        |
-----------------------------------------
Eval num_timesteps=3540000, episode_reward=-50.24 +/- 81.30
Episode length: 416.80 +/- 52.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 417          |
|    mean_reward          | -50.2        |
| time/                   |              |
|    total_timesteps      | 3540000      |
| train/                  |              |
|    approx_kl            | 0.0003306319 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 17280        |
|    policy_gradient_loss | -3.12e-05    |
|    std                  | 10.5         |
|    value_loss           | 4.8e+03      |
------------------------------------------
Eval num_timesteps=3542000, episode_reward=488.77 +/- 453.45
Episode length: 499.20 +/- 71.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 499           |
|    mean_reward          | 489           |
| time/                   |               |
|    total_timesteps      | 3542000       |
| train/                  |               |
|    approx_kl            | 0.00014466769 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 17290         |
|    policy_gradient_loss | -0.000194     |
|    std                  | 10.5          |
|    value_loss           | 3.56e+03      |
-------------------------------------------
Eval num_timesteps=3544000, episode_reward=302.88 +/- 402.81
Episode length: 476.00 +/- 99.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 303          |
| time/                   |              |
|    total_timesteps      | 3544000      |
| train/                  |              |
|    approx_kl            | 9.574168e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 56.9         |
|    n_updates            | 17300        |
|    policy_gradient_loss | -0.000102    |
|    std                  | 10.5         |
|    value_loss           | 262          |
------------------------------------------
Eval num_timesteps=3546000, episode_reward=304.41 +/- 396.22
Episode length: 516.40 +/- 84.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 3546000      |
| train/                  |              |
|    approx_kl            | 0.0003811657 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 17310        |
|    policy_gradient_loss | -0.000404    |
|    std                  | 10.5         |
|    value_loss           | 387          |
------------------------------------------
Eval num_timesteps=3548000, episode_reward=122.14 +/- 58.74
Episode length: 391.00 +/- 39.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 391          |
|    mean_reward          | 122          |
| time/                   |              |
|    total_timesteps      | 3548000      |
| train/                  |              |
|    approx_kl            | 0.0004944641 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 63.7         |
|    n_updates            | 17320        |
|    policy_gradient_loss | -0.000402    |
|    std                  | 10.5         |
|    value_loss           | 293          |
------------------------------------------
Eval num_timesteps=3550000, episode_reward=309.18 +/- 222.15
Episode length: 451.40 +/- 72.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 451         |
|    mean_reward          | 309         |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.002371374 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.001       |
|    loss                 | 39.9        |
|    n_updates            | 17330       |
|    policy_gradient_loss | -0.00156    |
|    std                  | 10.5        |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=3552000, episode_reward=301.10 +/- 290.86
Episode length: 508.80 +/- 86.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 509           |
|    mean_reward          | 301           |
| time/                   |               |
|    total_timesteps      | 3552000       |
| train/                  |               |
|    approx_kl            | 0.00097077375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 60.8          |
|    n_updates            | 17340         |
|    policy_gradient_loss | 0.000477      |
|    std                  | 10.5          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3554000, episode_reward=183.64 +/- 252.37
Episode length: 456.20 +/- 79.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 3554000      |
| train/                  |              |
|    approx_kl            | 0.0044415053 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 40.9         |
|    n_updates            | 17350        |
|    policy_gradient_loss | -0.00184     |
|    std                  | 10.5         |
|    value_loss           | 232          |
------------------------------------------
Eval num_timesteps=3556000, episode_reward=127.38 +/- 451.25
Episode length: 459.00 +/- 76.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 3556000      |
| train/                  |              |
|    approx_kl            | 0.0027421617 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+03     |
|    n_updates            | 17360        |
|    policy_gradient_loss | -0.0027      |
|    std                  | 10.5         |
|    value_loss           | 3.41e+03     |
------------------------------------------
Eval num_timesteps=3558000, episode_reward=561.61 +/- 369.79
Episode length: 424.20 +/- 57.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 562          |
| time/                   |              |
|    total_timesteps      | 3558000      |
| train/                  |              |
|    approx_kl            | 0.0009230736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.7        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+03     |
|    n_updates            | 17370        |
|    policy_gradient_loss | -0.000546    |
|    std                  | 10.5         |
|    value_loss           | 3.97e+03     |
------------------------------------------
Eval num_timesteps=3560000, episode_reward=276.06 +/- 278.54
Episode length: 464.60 +/- 114.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 465           |
|    mean_reward          | 276           |
| time/                   |               |
|    total_timesteps      | 3560000       |
| train/                  |               |
|    approx_kl            | 0.00035246028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.001         |
|    loss                 | 44.7          |
|    n_updates            | 17380         |
|    policy_gradient_loss | -0.000236     |
|    std                  | 10.5          |
|    value_loss           | 172           |
-------------------------------------------
Eval num_timesteps=3562000, episode_reward=307.53 +/- 145.90
Episode length: 400.00 +/- 12.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 400           |
|    mean_reward          | 308           |
| time/                   |               |
|    total_timesteps      | 3562000       |
| train/                  |               |
|    approx_kl            | 0.00030444187 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 97.5          |
|    n_updates            | 17390         |
|    policy_gradient_loss | -0.000175     |
|    std                  | 10.5          |
|    value_loss           | 518           |
-------------------------------------------
Eval num_timesteps=3564000, episode_reward=83.60 +/- 162.84
Episode length: 488.60 +/- 39.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 83.6          |
| time/                   |               |
|    total_timesteps      | 3564000       |
| train/                  |               |
|    approx_kl            | 0.00025205986 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 72.3          |
|    n_updates            | 17400         |
|    policy_gradient_loss | -0.000478     |
|    std                  | 10.6          |
|    value_loss           | 255           |
-------------------------------------------
Eval num_timesteps=3566000, episode_reward=191.11 +/- 256.38
Episode length: 414.20 +/- 47.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 414           |
|    mean_reward          | 191           |
| time/                   |               |
|    total_timesteps      | 3566000       |
| train/                  |               |
|    approx_kl            | 0.00057230145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.7         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 98.8          |
|    n_updates            | 17410         |
|    policy_gradient_loss | -0.000605     |
|    std                  | 10.6          |
|    value_loss           | 419           |
-------------------------------------------
Eval num_timesteps=3568000, episode_reward=416.63 +/- 413.41
Episode length: 494.00 +/- 83.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 3568000      |
| train/                  |              |
|    approx_kl            | 0.0004803846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 47.3         |
|    n_updates            | 17420        |
|    policy_gradient_loss | -0.000336    |
|    std                  | 10.7         |
|    value_loss           | 233          |
------------------------------------------
Eval num_timesteps=3570000, episode_reward=452.14 +/- 491.64
Episode length: 481.00 +/- 71.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 3570000       |
| train/                  |               |
|    approx_kl            | 0.00011026504 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.946         |
|    learning_rate        | 0.001         |
|    loss                 | 205           |
|    n_updates            | 17430         |
|    policy_gradient_loss | -0.000148     |
|    std                  | 10.7          |
|    value_loss           | 709           |
-------------------------------------------
Eval num_timesteps=3572000, episode_reward=188.87 +/- 250.81
Episode length: 482.80 +/- 55.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 189           |
| time/                   |               |
|    total_timesteps      | 3572000       |
| train/                  |               |
|    approx_kl            | 0.00017296476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 83            |
|    n_updates            | 17440         |
|    policy_gradient_loss | -0.000561     |
|    std                  | 10.7          |
|    value_loss           | 323           |
-------------------------------------------
Eval num_timesteps=3574000, episode_reward=23.65 +/- 148.36
Episode length: 434.80 +/- 64.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 435         |
|    mean_reward          | 23.6        |
| time/                   |             |
|    total_timesteps      | 3574000     |
| train/                  |             |
|    approx_kl            | 0.000984502 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.8       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 230         |
|    n_updates            | 17450       |
|    policy_gradient_loss | -0.000789   |
|    std                  | 10.8        |
|    value_loss           | 684         |
-----------------------------------------
Eval num_timesteps=3576000, episode_reward=272.10 +/- 333.77
Episode length: 476.40 +/- 55.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 3576000      |
| train/                  |              |
|    approx_kl            | 0.0013112196 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 47.5         |
|    n_updates            | 17460        |
|    policy_gradient_loss | -0.000122    |
|    std                  | 10.8         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=3578000, episode_reward=322.57 +/- 394.95
Episode length: 398.60 +/- 86.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | 323          |
| time/                   |              |
|    total_timesteps      | 3578000      |
| train/                  |              |
|    approx_kl            | 0.0010283223 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 49.5         |
|    n_updates            | 17470        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 10.8         |
|    value_loss           | 159          |
------------------------------------------
Eval num_timesteps=3580000, episode_reward=118.30 +/- 119.52
Episode length: 452.80 +/- 90.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 118          |
| time/                   |              |
|    total_timesteps      | 3580000      |
| train/                  |              |
|    approx_kl            | 0.0006454729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.8        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 59.5         |
|    n_updates            | 17480        |
|    policy_gradient_loss | -0.000203    |
|    std                  | 10.8         |
|    value_loss           | 286          |
------------------------------------------
Eval num_timesteps=3582000, episode_reward=162.66 +/- 146.51
Episode length: 402.20 +/- 73.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 402           |
|    mean_reward          | 163           |
| time/                   |               |
|    total_timesteps      | 3582000       |
| train/                  |               |
|    approx_kl            | 0.00018258576 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 53.1          |
|    n_updates            | 17490         |
|    policy_gradient_loss | -0.000151     |
|    std                  | 10.8          |
|    value_loss           | 238           |
-------------------------------------------
Eval num_timesteps=3584000, episode_reward=28.63 +/- 54.05
Episode length: 350.40 +/- 49.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 3584000  |
---------------------------------
Eval num_timesteps=3586000, episode_reward=160.01 +/- 82.77
Episode length: 369.20 +/- 18.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 369           |
|    mean_reward          | 160           |
| time/                   |               |
|    total_timesteps      | 3586000       |
| train/                  |               |
|    approx_kl            | 0.00013038845 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 61            |
|    n_updates            | 17500         |
|    policy_gradient_loss | -0.000263     |
|    std                  | 10.9          |
|    value_loss           | 256           |
-------------------------------------------
Eval num_timesteps=3588000, episode_reward=128.73 +/- 86.89
Episode length: 369.40 +/- 30.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 369           |
|    mean_reward          | 129           |
| time/                   |               |
|    total_timesteps      | 3588000       |
| train/                  |               |
|    approx_kl            | 0.00025119612 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.8         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 52.1          |
|    n_updates            | 17510         |
|    policy_gradient_loss | -0.000307     |
|    std                  | 10.9          |
|    value_loss           | 266           |
-------------------------------------------
Eval num_timesteps=3590000, episode_reward=184.50 +/- 195.85
Episode length: 430.80 +/- 30.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 184           |
| time/                   |               |
|    total_timesteps      | 3590000       |
| train/                  |               |
|    approx_kl            | 0.00076089153 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.001         |
|    loss                 | 20.3          |
|    n_updates            | 17520         |
|    policy_gradient_loss | -0.00059      |
|    std                  | 10.9          |
|    value_loss           | 75.5          |
-------------------------------------------
Eval num_timesteps=3592000, episode_reward=-9.41 +/- 387.50
Episode length: 462.20 +/- 111.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | -9.41        |
| time/                   |              |
|    total_timesteps      | 3592000      |
| train/                  |              |
|    approx_kl            | 0.0029903867 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 29.9         |
|    n_updates            | 17530        |
|    policy_gradient_loss | -0.00204     |
|    std                  | 11           |
|    value_loss           | 142          |
------------------------------------------
Eval num_timesteps=3594000, episode_reward=184.89 +/- 359.18
Episode length: 448.20 +/- 85.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 448          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 3594000      |
| train/                  |              |
|    approx_kl            | 0.0023648385 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.001        |
|    loss                 | 3.35e+03     |
|    n_updates            | 17540        |
|    policy_gradient_loss | -0.000228    |
|    std                  | 11           |
|    value_loss           | 8.95e+03     |
------------------------------------------
Eval num_timesteps=3596000, episode_reward=320.09 +/- 531.62
Episode length: 445.60 +/- 109.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 446           |
|    mean_reward          | 320           |
| time/                   |               |
|    total_timesteps      | 3596000       |
| train/                  |               |
|    approx_kl            | 0.00024014685 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 68.6          |
|    n_updates            | 17550         |
|    policy_gradient_loss | -3.68e-05     |
|    std                  | 11            |
|    value_loss           | 284           |
-------------------------------------------
Eval num_timesteps=3598000, episode_reward=417.06 +/- 391.19
Episode length: 434.40 +/- 57.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 3598000      |
| train/                  |              |
|    approx_kl            | 8.987813e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 76.8         |
|    n_updates            | 17560        |
|    policy_gradient_loss | -0.000221    |
|    std                  | 11           |
|    value_loss           | 336          |
------------------------------------------
Eval num_timesteps=3600000, episode_reward=185.65 +/- 203.15
Episode length: 539.60 +/- 69.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 540           |
|    mean_reward          | 186           |
| time/                   |               |
|    total_timesteps      | 3600000       |
| train/                  |               |
|    approx_kl            | 9.5322495e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.863         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+03      |
|    n_updates            | 17570         |
|    policy_gradient_loss | -3.96e-05     |
|    std                  | 11.1          |
|    value_loss           | 3.54e+03      |
-------------------------------------------
Eval num_timesteps=3602000, episode_reward=293.94 +/- 365.86
Episode length: 498.00 +/- 52.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 498          |
|    mean_reward          | 294          |
| time/                   |              |
|    total_timesteps      | 3602000      |
| train/                  |              |
|    approx_kl            | 7.163544e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 50.9         |
|    n_updates            | 17580        |
|    policy_gradient_loss | -0.000201    |
|    std                  | 11           |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=3604000, episode_reward=192.94 +/- 394.46
Episode length: 454.80 +/- 84.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 3604000      |
| train/                  |              |
|    approx_kl            | 9.620763e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 82           |
|    n_updates            | 17590        |
|    policy_gradient_loss | -0.000151    |
|    std                  | 11.1         |
|    value_loss           | 338          |
------------------------------------------
Eval num_timesteps=3606000, episode_reward=95.31 +/- 205.78
Episode length: 410.80 +/- 89.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 411           |
|    mean_reward          | 95.3          |
| time/                   |               |
|    total_timesteps      | 3606000       |
| train/                  |               |
|    approx_kl            | 0.00043045433 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.867         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 17600         |
|    policy_gradient_loss | -0.000695     |
|    std                  | 11.1          |
|    value_loss           | 3.83e+03      |
-------------------------------------------
Eval num_timesteps=3608000, episode_reward=217.68 +/- 129.31
Episode length: 533.60 +/- 35.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 534           |
|    mean_reward          | 218           |
| time/                   |               |
|    total_timesteps      | 3608000       |
| train/                  |               |
|    approx_kl            | 0.00055194454 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 93.5          |
|    n_updates            | 17610         |
|    policy_gradient_loss | -0.000595     |
|    std                  | 11.1          |
|    value_loss           | 493           |
-------------------------------------------
Eval num_timesteps=3610000, episode_reward=198.59 +/- 213.57
Episode length: 470.00 +/- 71.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 470        |
|    mean_reward          | 199        |
| time/                   |            |
|    total_timesteps      | 3610000    |
| train/                  |            |
|    approx_kl            | 0.00031136 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -14.9      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.001      |
|    loss                 | 68.5       |
|    n_updates            | 17620      |
|    policy_gradient_loss | -0.000329  |
|    std                  | 11.1       |
|    value_loss           | 276        |
----------------------------------------
Eval num_timesteps=3612000, episode_reward=175.59 +/- 238.46
Episode length: 428.80 +/- 86.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 429          |
|    mean_reward          | 176          |
| time/                   |              |
|    total_timesteps      | 3612000      |
| train/                  |              |
|    approx_kl            | 0.0005130888 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 53.6         |
|    n_updates            | 17630        |
|    policy_gradient_loss | -0.000384    |
|    std                  | 11.2         |
|    value_loss           | 225          |
------------------------------------------
Eval num_timesteps=3614000, episode_reward=251.04 +/- 261.95
Episode length: 475.00 +/- 86.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 475           |
|    mean_reward          | 251           |
| time/                   |               |
|    total_timesteps      | 3614000       |
| train/                  |               |
|    approx_kl            | 0.00061681494 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.848         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 17640         |
|    policy_gradient_loss | -2.69e-05     |
|    std                  | 11.2          |
|    value_loss           | 4.19e+03      |
-------------------------------------------
Eval num_timesteps=3616000, episode_reward=330.81 +/- 301.33
Episode length: 392.60 +/- 59.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 393          |
|    mean_reward          | 331          |
| time/                   |              |
|    total_timesteps      | 3616000      |
| train/                  |              |
|    approx_kl            | 0.0001847703 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 67.3         |
|    n_updates            | 17650        |
|    policy_gradient_loss | -0.000331    |
|    std                  | 11.2         |
|    value_loss           | 269          |
------------------------------------------
Eval num_timesteps=3618000, episode_reward=98.60 +/- 74.56
Episode length: 347.40 +/- 65.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 98.6         |
| time/                   |              |
|    total_timesteps      | 3618000      |
| train/                  |              |
|    approx_kl            | 0.0010543743 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 96.5         |
|    n_updates            | 17660        |
|    policy_gradient_loss | -0.000764    |
|    std                  | 11.1         |
|    value_loss           | 375          |
------------------------------------------
Eval num_timesteps=3620000, episode_reward=-14.21 +/- 83.66
Episode length: 438.60 +/- 65.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | -14.2        |
| time/                   |              |
|    total_timesteps      | 3620000      |
| train/                  |              |
|    approx_kl            | 0.0005185787 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 44.3         |
|    n_updates            | 17670        |
|    policy_gradient_loss | -0.000275    |
|    std                  | 11.1         |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=3622000, episode_reward=200.93 +/- 235.20
Episode length: 422.40 +/- 69.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 201          |
| time/                   |              |
|    total_timesteps      | 3622000      |
| train/                  |              |
|    approx_kl            | 0.0018692074 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 57.5         |
|    n_updates            | 17680        |
|    policy_gradient_loss | -0.000407    |
|    std                  | 11.1         |
|    value_loss           | 242          |
------------------------------------------
Eval num_timesteps=3624000, episode_reward=235.90 +/- 213.14
Episode length: 416.20 +/- 87.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 416          |
|    mean_reward          | 236          |
| time/                   |              |
|    total_timesteps      | 3624000      |
| train/                  |              |
|    approx_kl            | 0.0011216147 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+03     |
|    n_updates            | 17690        |
|    policy_gradient_loss | -4.01e-05    |
|    std                  | 11.1         |
|    value_loss           | 4.18e+03     |
------------------------------------------
Eval num_timesteps=3626000, episode_reward=158.44 +/- 153.42
Episode length: 442.80 +/- 72.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 158           |
| time/                   |               |
|    total_timesteps      | 3626000       |
| train/                  |               |
|    approx_kl            | 0.00023856875 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.856         |
|    learning_rate        | 0.001         |
|    loss                 | 787           |
|    n_updates            | 17700         |
|    policy_gradient_loss | -0.000326     |
|    std                  | 11.1          |
|    value_loss           | 2.2e+03       |
-------------------------------------------
Eval num_timesteps=3628000, episode_reward=-10.00 +/- 89.90
Episode length: 490.00 +/- 80.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 490           |
|    mean_reward          | -10           |
| time/                   |               |
|    total_timesteps      | 3628000       |
| train/                  |               |
|    approx_kl            | 0.00014219809 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 95.5          |
|    n_updates            | 17710         |
|    policy_gradient_loss | -0.000211     |
|    std                  | 11.1          |
|    value_loss           | 383           |
-------------------------------------------
Eval num_timesteps=3630000, episode_reward=195.18 +/- 177.71
Episode length: 442.80 +/- 63.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 195           |
| time/                   |               |
|    total_timesteps      | 3630000       |
| train/                  |               |
|    approx_kl            | 0.00024056315 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 95.3          |
|    n_updates            | 17720         |
|    policy_gradient_loss | -0.000194     |
|    std                  | 11.1          |
|    value_loss           | 654           |
-------------------------------------------
Eval num_timesteps=3632000, episode_reward=271.95 +/- 136.88
Episode length: 475.60 +/- 81.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 3632000      |
| train/                  |              |
|    approx_kl            | 9.142421e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 140          |
|    n_updates            | 17730        |
|    policy_gradient_loss | -0.000113    |
|    std                  | 11.1         |
|    value_loss           | 793          |
------------------------------------------
Eval num_timesteps=3634000, episode_reward=124.64 +/- 191.00
Episode length: 453.20 +/- 35.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 125           |
| time/                   |               |
|    total_timesteps      | 3634000       |
| train/                  |               |
|    approx_kl            | 0.00029519922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 54.8          |
|    n_updates            | 17740         |
|    policy_gradient_loss | -0.000527     |
|    std                  | 11.1          |
|    value_loss           | 327           |
-------------------------------------------
Eval num_timesteps=3636000, episode_reward=361.06 +/- 301.72
Episode length: 478.20 +/- 65.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 478           |
|    mean_reward          | 361           |
| time/                   |               |
|    total_timesteps      | 3636000       |
| train/                  |               |
|    approx_kl            | 0.00025852871 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 108           |
|    n_updates            | 17750         |
|    policy_gradient_loss | 9.35e-05      |
|    std                  | 11.1          |
|    value_loss           | 464           |
-------------------------------------------
Eval num_timesteps=3638000, episode_reward=374.31 +/- 437.63
Episode length: 373.80 +/- 60.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 374           |
|    mean_reward          | 374           |
| time/                   |               |
|    total_timesteps      | 3638000       |
| train/                  |               |
|    approx_kl            | 0.00014425188 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 114           |
|    n_updates            | 17760         |
|    policy_gradient_loss | -0.000234     |
|    std                  | 11.1          |
|    value_loss           | 413           |
-------------------------------------------
Eval num_timesteps=3640000, episode_reward=567.38 +/- 222.31
Episode length: 453.20 +/- 27.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 567           |
| time/                   |               |
|    total_timesteps      | 3640000       |
| train/                  |               |
|    approx_kl            | 0.00013920304 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+03      |
|    n_updates            | 17770         |
|    policy_gradient_loss | -0.000226     |
|    std                  | 11.1          |
|    value_loss           | 4.23e+03      |
-------------------------------------------
Eval num_timesteps=3642000, episode_reward=851.66 +/- 571.19
Episode length: 538.60 +/- 136.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 539          |
|    mean_reward          | 852          |
| time/                   |              |
|    total_timesteps      | 3642000      |
| train/                  |              |
|    approx_kl            | 5.008513e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.815        |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+03     |
|    n_updates            | 17780        |
|    policy_gradient_loss | -0.000218    |
|    std                  | 11.1         |
|    value_loss           | 3.78e+03     |
------------------------------------------
Eval num_timesteps=3644000, episode_reward=422.61 +/- 343.97
Episode length: 449.40 +/- 60.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 449           |
|    mean_reward          | 423           |
| time/                   |               |
|    total_timesteps      | 3644000       |
| train/                  |               |
|    approx_kl            | 0.00024940562 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 83.8          |
|    n_updates            | 17790         |
|    policy_gradient_loss | -0.000528     |
|    std                  | 11.1          |
|    value_loss           | 383           |
-------------------------------------------
Eval num_timesteps=3646000, episode_reward=170.78 +/- 170.71
Episode length: 350.20 +/- 66.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 3646000      |
| train/                  |              |
|    approx_kl            | 0.0004807587 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+03      |
|    n_updates            | 17800        |
|    policy_gradient_loss | -0.000507    |
|    std                  | 11.1         |
|    value_loss           | 3.74e+03     |
------------------------------------------
Eval num_timesteps=3648000, episode_reward=112.95 +/- 187.90
Episode length: 418.80 +/- 86.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 113           |
| time/                   |               |
|    total_timesteps      | 3648000       |
| train/                  |               |
|    approx_kl            | 0.00030150104 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.79          |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+03      |
|    n_updates            | 17810         |
|    policy_gradient_loss | -0.000665     |
|    std                  | 11.1          |
|    value_loss           | 5.4e+03       |
-------------------------------------------
Eval num_timesteps=3650000, episode_reward=161.07 +/- 102.79
Episode length: 377.40 +/- 56.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0010295387 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.9        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.001        |
|    loss                 | 50.9         |
|    n_updates            | 17820        |
|    policy_gradient_loss | -0.000812    |
|    std                  | 11.1         |
|    value_loss           | 274          |
------------------------------------------
Eval num_timesteps=3652000, episode_reward=123.51 +/- 166.63
Episode length: 442.40 +/- 95.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 124           |
| time/                   |               |
|    total_timesteps      | 3652000       |
| train/                  |               |
|    approx_kl            | 0.00048697033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -14.9         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 54.6          |
|    n_updates            | 17830         |
|    policy_gradient_loss | 0.000265      |
|    std                  | 11.2          |
|    value_loss           | 317           |
-------------------------------------------
Eval num_timesteps=3654000, episode_reward=43.02 +/- 77.29
Episode length: 342.00 +/- 24.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 342           |
|    mean_reward          | 43            |
| time/                   |               |
|    total_timesteps      | 3654000       |
| train/                  |               |
|    approx_kl            | 0.00066779024 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 86.4          |
|    n_updates            | 17840         |
|    policy_gradient_loss | -0.000825     |
|    std                  | 11.3          |
|    value_loss           | 337           |
-------------------------------------------
Eval num_timesteps=3656000, episode_reward=14.52 +/- 91.22
Episode length: 375.40 +/- 89.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 375           |
|    mean_reward          | 14.5          |
| time/                   |               |
|    total_timesteps      | 3656000       |
| train/                  |               |
|    approx_kl            | 0.00080439576 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.001         |
|    loss                 | 52.6          |
|    n_updates            | 17850         |
|    policy_gradient_loss | -0.000115     |
|    std                  | 11.3          |
|    value_loss           | 256           |
-------------------------------------------
Eval num_timesteps=3658000, episode_reward=305.65 +/- 298.73
Episode length: 366.00 +/- 85.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 366          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 3658000      |
| train/                  |              |
|    approx_kl            | 0.0004356208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 57.9         |
|    n_updates            | 17860        |
|    policy_gradient_loss | -0.000308    |
|    std                  | 11.4         |
|    value_loss           | 235          |
------------------------------------------
Eval num_timesteps=3660000, episode_reward=376.45 +/- 355.12
Episode length: 496.80 +/- 188.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 3660000      |
| train/                  |              |
|    approx_kl            | 0.0013448754 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 48.4         |
|    n_updates            | 17870        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 11.4         |
|    value_loss           | 191          |
------------------------------------------
Eval num_timesteps=3662000, episode_reward=191.70 +/- 242.94
Episode length: 410.20 +/- 89.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 3662000      |
| train/                  |              |
|    approx_kl            | 0.0014934964 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+03     |
|    n_updates            | 17880        |
|    policy_gradient_loss | -0.00105     |
|    std                  | 11.4         |
|    value_loss           | 2.91e+03     |
------------------------------------------
Eval num_timesteps=3664000, episode_reward=263.63 +/- 124.99
Episode length: 389.20 +/- 48.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 389           |
|    mean_reward          | 264           |
| time/                   |               |
|    total_timesteps      | 3664000       |
| train/                  |               |
|    approx_kl            | 0.00030496437 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.001         |
|    loss                 | 82.5          |
|    n_updates            | 17890         |
|    policy_gradient_loss | -6.36e-05     |
|    std                  | 11.4          |
|    value_loss           | 340           |
-------------------------------------------
Eval num_timesteps=3666000, episode_reward=177.16 +/- 118.80
Episode length: 378.20 +/- 85.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 378           |
|    mean_reward          | 177           |
| time/                   |               |
|    total_timesteps      | 3666000       |
| train/                  |               |
|    approx_kl            | 0.00026349758 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 54.6          |
|    n_updates            | 17900         |
|    policy_gradient_loss | -0.000438     |
|    std                  | 11.5          |
|    value_loss           | 214           |
-------------------------------------------
Eval num_timesteps=3668000, episode_reward=129.79 +/- 141.32
Episode length: 371.80 +/- 48.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 372           |
|    mean_reward          | 130           |
| time/                   |               |
|    total_timesteps      | 3668000       |
| train/                  |               |
|    approx_kl            | 0.00021998229 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.836         |
|    learning_rate        | 0.001         |
|    loss                 | 3.02e+03      |
|    n_updates            | 17910         |
|    policy_gradient_loss | 0.000316      |
|    std                  | 11.5          |
|    value_loss           | 7.77e+03      |
-------------------------------------------
Eval num_timesteps=3670000, episode_reward=707.36 +/- 504.47
Episode length: 466.60 +/- 50.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 3670000  |
---------------------------------
Eval num_timesteps=3672000, episode_reward=373.88 +/- 509.61
Episode length: 422.40 +/- 89.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 374           |
| time/                   |               |
|    total_timesteps      | 3672000       |
| train/                  |               |
|    approx_kl            | 0.00014639855 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.001         |
|    loss                 | 29.2          |
|    n_updates            | 17920         |
|    policy_gradient_loss | -0.00033      |
|    std                  | 11.5          |
|    value_loss           | 117           |
-------------------------------------------
Eval num_timesteps=3674000, episode_reward=140.14 +/- 134.02
Episode length: 374.60 +/- 51.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 375          |
|    mean_reward          | 140          |
| time/                   |              |
|    total_timesteps      | 3674000      |
| train/                  |              |
|    approx_kl            | 0.0007434744 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 77.3         |
|    n_updates            | 17930        |
|    policy_gradient_loss | -0.000845    |
|    std                  | 11.5         |
|    value_loss           | 462          |
------------------------------------------
Eval num_timesteps=3676000, episode_reward=333.29 +/- 353.48
Episode length: 406.00 +/- 160.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 406           |
|    mean_reward          | 333           |
| time/                   |               |
|    total_timesteps      | 3676000       |
| train/                  |               |
|    approx_kl            | 0.00080203917 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+03      |
|    n_updates            | 17940         |
|    policy_gradient_loss | -0.000814     |
|    std                  | 11.4          |
|    value_loss           | 2.7e+03       |
-------------------------------------------
Eval num_timesteps=3678000, episode_reward=196.63 +/- 240.92
Episode length: 434.80 +/- 47.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 435           |
|    mean_reward          | 197           |
| time/                   |               |
|    total_timesteps      | 3678000       |
| train/                  |               |
|    approx_kl            | 0.00016845673 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 17950         |
|    policy_gradient_loss | -9.71e-05     |
|    std                  | 11.4          |
|    value_loss           | 3.38e+03      |
-------------------------------------------
Eval num_timesteps=3680000, episode_reward=381.73 +/- 443.05
Episode length: 455.60 +/- 127.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 3680000      |
| train/                  |              |
|    approx_kl            | 0.0003132399 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 376          |
|    n_updates            | 17960        |
|    policy_gradient_loss | -0.000607    |
|    std                  | 11.4         |
|    value_loss           | 902          |
------------------------------------------
Eval num_timesteps=3682000, episode_reward=16.84 +/- 126.23
Episode length: 453.00 +/- 100.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 453          |
|    mean_reward          | 16.8         |
| time/                   |              |
|    total_timesteps      | 3682000      |
| train/                  |              |
|    approx_kl            | 0.0006615283 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 83.1         |
|    n_updates            | 17970        |
|    policy_gradient_loss | -0.000441    |
|    std                  | 11.3         |
|    value_loss           | 385          |
------------------------------------------
Eval num_timesteps=3684000, episode_reward=424.97 +/- 398.62
Episode length: 453.00 +/- 87.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 3684000     |
| train/                  |             |
|    approx_kl            | 0.001353866 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 59.8        |
|    n_updates            | 17980       |
|    policy_gradient_loss | -0.00106    |
|    std                  | 11.3        |
|    value_loss           | 278         |
-----------------------------------------
Eval num_timesteps=3686000, episode_reward=168.78 +/- 220.44
Episode length: 378.20 +/- 87.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 3686000      |
| train/                  |              |
|    approx_kl            | 0.0015175473 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 17990        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 11.3         |
|    value_loss           | 5.06e+03     |
------------------------------------------
Eval num_timesteps=3688000, episode_reward=188.84 +/- 140.92
Episode length: 435.20 +/- 90.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 435          |
|    mean_reward          | 189          |
| time/                   |              |
|    total_timesteps      | 3688000      |
| train/                  |              |
|    approx_kl            | 0.0005167016 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 65.4         |
|    n_updates            | 18000        |
|    policy_gradient_loss | -0.000267    |
|    std                  | 11.3         |
|    value_loss           | 254          |
------------------------------------------
Eval num_timesteps=3690000, episode_reward=719.41 +/- 587.45
Episode length: 496.00 +/- 85.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 496           |
|    mean_reward          | 719           |
| time/                   |               |
|    total_timesteps      | 3690000       |
| train/                  |               |
|    approx_kl            | 0.00032087698 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 108           |
|    n_updates            | 18010         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 11.3          |
|    value_loss           | 454           |
-------------------------------------------
Eval num_timesteps=3692000, episode_reward=122.14 +/- 184.65
Episode length: 428.60 +/- 69.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 122           |
| time/                   |               |
|    total_timesteps      | 3692000       |
| train/                  |               |
|    approx_kl            | 0.00030283374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.834         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+03      |
|    n_updates            | 18020         |
|    policy_gradient_loss | -0.000639     |
|    std                  | 11.3          |
|    value_loss           | 3.99e+03      |
-------------------------------------------
Eval num_timesteps=3694000, episode_reward=130.45 +/- 326.20
Episode length: 425.00 +/- 106.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 425           |
|    mean_reward          | 130           |
| time/                   |               |
|    total_timesteps      | 3694000       |
| train/                  |               |
|    approx_kl            | 0.00043833346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.001         |
|    loss                 | 62.4          |
|    n_updates            | 18030         |
|    policy_gradient_loss | -0.000782     |
|    std                  | 11.2          |
|    value_loss           | 343           |
-------------------------------------------
Eval num_timesteps=3696000, episode_reward=320.37 +/- 270.45
Episode length: 432.40 +/- 43.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 320          |
| time/                   |              |
|    total_timesteps      | 3696000      |
| train/                  |              |
|    approx_kl            | 0.0016784463 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 36.9         |
|    n_updates            | 18040        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 11.2         |
|    value_loss           | 183          |
------------------------------------------
Eval num_timesteps=3698000, episode_reward=463.92 +/- 431.90
Episode length: 475.60 +/- 58.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 476          |
|    mean_reward          | 464          |
| time/                   |              |
|    total_timesteps      | 3698000      |
| train/                  |              |
|    approx_kl            | 0.0018558905 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 50.7         |
|    n_updates            | 18050        |
|    policy_gradient_loss | 0.000809     |
|    std                  | 11.2         |
|    value_loss           | 221          |
------------------------------------------
Eval num_timesteps=3700000, episode_reward=320.14 +/- 235.90
Episode length: 404.80 +/- 83.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 320           |
| time/                   |               |
|    total_timesteps      | 3700000       |
| train/                  |               |
|    approx_kl            | 0.00052518357 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 71.2          |
|    n_updates            | 18060         |
|    policy_gradient_loss | -0.000493     |
|    std                  | 11.2          |
|    value_loss           | 390           |
-------------------------------------------
Eval num_timesteps=3702000, episode_reward=395.91 +/- 329.58
Episode length: 405.00 +/- 62.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 405           |
|    mean_reward          | 396           |
| time/                   |               |
|    total_timesteps      | 3702000       |
| train/                  |               |
|    approx_kl            | 0.00052709674 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.001         |
|    loss                 | 170           |
|    n_updates            | 18070         |
|    policy_gradient_loss | -0.000579     |
|    std                  | 11.2          |
|    value_loss           | 508           |
-------------------------------------------
Eval num_timesteps=3704000, episode_reward=259.44 +/- 203.87
Episode length: 428.20 +/- 109.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 428           |
|    mean_reward          | 259           |
| time/                   |               |
|    total_timesteps      | 3704000       |
| train/                  |               |
|    approx_kl            | 0.00043450578 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 42.8          |
|    n_updates            | 18080         |
|    policy_gradient_loss | -0.000421     |
|    std                  | 11.3          |
|    value_loss           | 187           |
-------------------------------------------
Eval num_timesteps=3706000, episode_reward=374.94 +/- 404.29
Episode length: 475.20 +/- 66.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 3706000      |
| train/                  |              |
|    approx_kl            | 0.0010854905 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 57.2         |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00078     |
|    std                  | 11.3         |
|    value_loss           | 273          |
------------------------------------------
Eval num_timesteps=3708000, episode_reward=13.37 +/- 249.42
Episode length: 525.80 +/- 70.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 526           |
|    mean_reward          | 13.4          |
| time/                   |               |
|    total_timesteps      | 3708000       |
| train/                  |               |
|    approx_kl            | 0.00084222853 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.845         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+03      |
|    n_updates            | 18100         |
|    policy_gradient_loss | 0.000233      |
|    std                  | 11.3          |
|    value_loss           | 4.16e+03      |
-------------------------------------------
Eval num_timesteps=3710000, episode_reward=-54.55 +/- 254.52
Episode length: 454.40 +/- 75.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | -54.6         |
| time/                   |               |
|    total_timesteps      | 3710000       |
| train/                  |               |
|    approx_kl            | 0.00014949049 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 41.7          |
|    n_updates            | 18110         |
|    policy_gradient_loss | -0.000262     |
|    std                  | 11.4          |
|    value_loss           | 254           |
-------------------------------------------
Eval num_timesteps=3712000, episode_reward=118.59 +/- 181.58
Episode length: 397.00 +/- 120.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 397           |
|    mean_reward          | 119           |
| time/                   |               |
|    total_timesteps      | 3712000       |
| train/                  |               |
|    approx_kl            | 0.00034686705 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.001         |
|    loss                 | 348           |
|    n_updates            | 18120         |
|    policy_gradient_loss | -0.000353     |
|    std                  | 11.4          |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=3714000, episode_reward=298.31 +/- 221.22
Episode length: 428.60 +/- 93.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 429           |
|    mean_reward          | 298           |
| time/                   |               |
|    total_timesteps      | 3714000       |
| train/                  |               |
|    approx_kl            | 2.9838848e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 166           |
|    n_updates            | 18130         |
|    policy_gradient_loss | 0.000256      |
|    std                  | 11.4          |
|    value_loss           | 491           |
-------------------------------------------
Eval num_timesteps=3716000, episode_reward=48.78 +/- 317.05
Episode length: 491.80 +/- 57.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 492           |
|    mean_reward          | 48.8          |
| time/                   |               |
|    total_timesteps      | 3716000       |
| train/                  |               |
|    approx_kl            | 0.00022120829 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.001         |
|    loss                 | 100           |
|    n_updates            | 18140         |
|    policy_gradient_loss | -0.000269     |
|    std                  | 11.4          |
|    value_loss           | 520           |
-------------------------------------------
Eval num_timesteps=3718000, episode_reward=157.98 +/- 124.14
Episode length: 376.80 +/- 80.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 377           |
|    mean_reward          | 158           |
| time/                   |               |
|    total_timesteps      | 3718000       |
| train/                  |               |
|    approx_kl            | 3.0728697e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.946         |
|    learning_rate        | 0.001         |
|    loss                 | 104           |
|    n_updates            | 18150         |
|    policy_gradient_loss | 0.000134      |
|    std                  | 11.4          |
|    value_loss           | 409           |
-------------------------------------------
Eval num_timesteps=3720000, episode_reward=282.65 +/- 240.71
Episode length: 409.60 +/- 99.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 283           |
| time/                   |               |
|    total_timesteps      | 3720000       |
| train/                  |               |
|    approx_kl            | 0.00030826146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 73.8          |
|    n_updates            | 18160         |
|    policy_gradient_loss | -0.000382     |
|    std                  | 11.4          |
|    value_loss           | 315           |
-------------------------------------------
Eval num_timesteps=3722000, episode_reward=375.91 +/- 243.57
Episode length: 423.60 +/- 71.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 424          |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 3722000      |
| train/                  |              |
|    approx_kl            | 0.0006350995 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15          |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 124          |
|    n_updates            | 18170        |
|    policy_gradient_loss | -0.000772    |
|    std                  | 11.4         |
|    value_loss           | 481          |
------------------------------------------
Eval num_timesteps=3724000, episode_reward=72.82 +/- 262.67
Episode length: 462.00 +/- 100.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 72.8          |
| time/                   |               |
|    total_timesteps      | 3724000       |
| train/                  |               |
|    approx_kl            | 0.00058730855 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 69.9          |
|    n_updates            | 18180         |
|    policy_gradient_loss | -0.000748     |
|    std                  | 11.4          |
|    value_loss           | 253           |
-------------------------------------------
Eval num_timesteps=3726000, episode_reward=561.65 +/- 657.19
Episode length: 475.80 +/- 139.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 562           |
| time/                   |               |
|    total_timesteps      | 3726000       |
| train/                  |               |
|    approx_kl            | 0.00090496463 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 86.5          |
|    n_updates            | 18190         |
|    policy_gradient_loss | -0.000627     |
|    std                  | 11.4          |
|    value_loss           | 293           |
-------------------------------------------
Eval num_timesteps=3728000, episode_reward=90.76 +/- 322.44
Episode length: 422.40 +/- 102.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 422           |
|    mean_reward          | 90.8          |
| time/                   |               |
|    total_timesteps      | 3728000       |
| train/                  |               |
|    approx_kl            | 0.00021037325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 95.1          |
|    n_updates            | 18200         |
|    policy_gradient_loss | 0.00011       |
|    std                  | 11.4          |
|    value_loss           | 406           |
-------------------------------------------
Eval num_timesteps=3730000, episode_reward=235.66 +/- 161.74
Episode length: 418.20 +/- 56.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 418           |
|    mean_reward          | 236           |
| time/                   |               |
|    total_timesteps      | 3730000       |
| train/                  |               |
|    approx_kl            | 8.1461156e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 173           |
|    n_updates            | 18210         |
|    policy_gradient_loss | 6.12e-06      |
|    std                  | 11.4          |
|    value_loss           | 734           |
-------------------------------------------
Eval num_timesteps=3732000, episode_reward=134.21 +/- 228.65
Episode length: 384.40 +/- 54.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | 134           |
| time/                   |               |
|    total_timesteps      | 3732000       |
| train/                  |               |
|    approx_kl            | 0.00013491331 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15           |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 87.3          |
|    n_updates            | 18220         |
|    policy_gradient_loss | -0.000376     |
|    std                  | 11.4          |
|    value_loss           | 406           |
-------------------------------------------
Eval num_timesteps=3734000, episode_reward=-28.67 +/- 194.85
Episode length: 387.20 +/- 69.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | -28.7       |
| time/                   |             |
|    total_timesteps      | 3734000     |
| train/                  |             |
|    approx_kl            | 0.000390166 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -15.1       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.001       |
|    loss                 | 1.67e+03    |
|    n_updates            | 18230       |
|    policy_gradient_loss | -0.000385   |
|    std                  | 11.4        |
|    value_loss           | 4.16e+03    |
-----------------------------------------
Eval num_timesteps=3736000, episode_reward=212.27 +/- 257.78
Episode length: 388.00 +/- 96.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 388          |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 3736000      |
| train/                  |              |
|    approx_kl            | 7.446838e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.1        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+03     |
|    n_updates            | 18240        |
|    policy_gradient_loss | -8.72e-06    |
|    std                  | 11.4         |
|    value_loss           | 4.1e+03      |
------------------------------------------
Eval num_timesteps=3738000, episode_reward=106.74 +/- 182.98
Episode length: 400.80 +/- 85.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 401           |
|    mean_reward          | 107           |
| time/                   |               |
|    total_timesteps      | 3738000       |
| train/                  |               |
|    approx_kl            | 2.7364382e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+03      |
|    n_updates            | 18250         |
|    policy_gradient_loss | -0.000117     |
|    std                  | 11.5          |
|    value_loss           | 3.73e+03      |
-------------------------------------------
Eval num_timesteps=3740000, episode_reward=224.49 +/- 88.20
Episode length: 386.40 +/- 26.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 386           |
|    mean_reward          | 224           |
| time/                   |               |
|    total_timesteps      | 3740000       |
| train/                  |               |
|    approx_kl            | 6.2792096e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 174           |
|    n_updates            | 18260         |
|    policy_gradient_loss | -3.57e-06     |
|    std                  | 11.5          |
|    value_loss           | 655           |
-------------------------------------------
Eval num_timesteps=3742000, episode_reward=26.15 +/- 76.58
Episode length: 414.60 +/- 71.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 415          |
|    mean_reward          | 26.2         |
| time/                   |              |
|    total_timesteps      | 3742000      |
| train/                  |              |
|    approx_kl            | 6.531237e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.1        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 91.8         |
|    n_updates            | 18270        |
|    policy_gradient_loss | -0.000231    |
|    std                  | 11.5         |
|    value_loss           | 378          |
------------------------------------------
Eval num_timesteps=3744000, episode_reward=202.97 +/- 248.70
Episode length: 423.00 +/- 34.69
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 423            |
|    mean_reward          | 203            |
| time/                   |                |
|    total_timesteps      | 3744000        |
| train/                  |                |
|    approx_kl            | 0.000100042234 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -15.1          |
|    explained_variance   | 0.979          |
|    learning_rate        | 0.001          |
|    loss                 | 55.8           |
|    n_updates            | 18280          |
|    policy_gradient_loss | -0.000226      |
|    std                  | 11.5           |
|    value_loss           | 229            |
--------------------------------------------
Eval num_timesteps=3746000, episode_reward=224.39 +/- 211.73
Episode length: 435.80 +/- 64.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 224           |
| time/                   |               |
|    total_timesteps      | 3746000       |
| train/                  |               |
|    approx_kl            | 0.00013695273 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.001         |
|    loss                 | 66.3          |
|    n_updates            | 18290         |
|    policy_gradient_loss | -0.000125     |
|    std                  | 11.5          |
|    value_loss           | 242           |
-------------------------------------------
Eval num_timesteps=3748000, episode_reward=302.87 +/- 469.56
Episode length: 425.80 +/- 116.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 426           |
|    mean_reward          | 303           |
| time/                   |               |
|    total_timesteps      | 3748000       |
| train/                  |               |
|    approx_kl            | 0.00037247004 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 83.3          |
|    n_updates            | 18300         |
|    policy_gradient_loss | -0.00044      |
|    std                  | 11.5          |
|    value_loss           | 346           |
-------------------------------------------
Eval num_timesteps=3750000, episode_reward=184.37 +/- 413.73
Episode length: 373.20 +/- 105.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 373          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 3750000      |
| train/                  |              |
|    approx_kl            | 0.0013264755 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.1        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.001        |
|    loss                 | 39.6         |
|    n_updates            | 18310        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 11.6         |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=3752000, episode_reward=348.62 +/- 151.43
Episode length: 427.60 +/- 81.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 349          |
| time/                   |              |
|    total_timesteps      | 3752000      |
| train/                  |              |
|    approx_kl            | 0.0028737271 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.1        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 52.5         |
|    n_updates            | 18320        |
|    policy_gradient_loss | -0.000294    |
|    std                  | 11.6         |
|    value_loss           | 180          |
------------------------------------------
Eval num_timesteps=3754000, episode_reward=97.86 +/- 170.91
Episode length: 408.00 +/- 35.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 408           |
|    mean_reward          | 97.9          |
| time/                   |               |
|    total_timesteps      | 3754000       |
| train/                  |               |
|    approx_kl            | 0.00045718532 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.741         |
|    learning_rate        | 0.001         |
|    loss                 | 4.98e+03      |
|    n_updates            | 18330         |
|    policy_gradient_loss | 5.57e-05      |
|    std                  | 11.6          |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=3756000, episode_reward=207.77 +/- 143.04
Episode length: 447.20 +/- 70.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 3756000  |
---------------------------------
Eval num_timesteps=3758000, episode_reward=420.36 +/- 374.51
Episode length: 541.60 +/- 90.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 542           |
|    mean_reward          | 420           |
| time/                   |               |
|    total_timesteps      | 3758000       |
| train/                  |               |
|    approx_kl            | 0.00054902106 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.1         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 51.8          |
|    n_updates            | 18340         |
|    policy_gradient_loss | -0.001        |
|    std                  | 11.7          |
|    value_loss           | 206           |
-------------------------------------------
Eval num_timesteps=3760000, episode_reward=63.82 +/- 177.60
Episode length: 547.60 +/- 54.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 548           |
|    mean_reward          | 63.8          |
| time/                   |               |
|    total_timesteps      | 3760000       |
| train/                  |               |
|    approx_kl            | 0.00067997386 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.2         |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 116           |
|    n_updates            | 18350         |
|    policy_gradient_loss | -0.000766     |
|    std                  | 11.8          |
|    value_loss           | 435           |
-------------------------------------------
Eval num_timesteps=3762000, episode_reward=314.60 +/- 184.34
Episode length: 440.60 +/- 85.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 441          |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 3762000      |
| train/                  |              |
|    approx_kl            | 0.0010290497 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.2        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 75.9         |
|    n_updates            | 18360        |
|    policy_gradient_loss | -0.000261    |
|    std                  | 11.9         |
|    value_loss           | 371          |
------------------------------------------
Eval num_timesteps=3764000, episode_reward=297.30 +/- 230.38
Episode length: 495.60 +/- 84.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 496           |
|    mean_reward          | 297           |
| time/                   |               |
|    total_timesteps      | 3764000       |
| train/                  |               |
|    approx_kl            | 0.00060581946 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.2         |
|    explained_variance   | 0.832         |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+03      |
|    n_updates            | 18370         |
|    policy_gradient_loss | -0.000544     |
|    std                  | 12            |
|    value_loss           | 4.32e+03      |
-------------------------------------------
Eval num_timesteps=3766000, episode_reward=63.27 +/- 127.30
Episode length: 457.20 +/- 96.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 457           |
|    mean_reward          | 63.3          |
| time/                   |               |
|    total_timesteps      | 3766000       |
| train/                  |               |
|    approx_kl            | 4.3996493e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.2         |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.001         |
|    loss                 | 181           |
|    n_updates            | 18380         |
|    policy_gradient_loss | 4.39e-05      |
|    std                  | 12            |
|    value_loss           | 602           |
-------------------------------------------
Eval num_timesteps=3768000, episode_reward=427.68 +/- 320.50
Episode length: 474.40 +/- 123.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | 428          |
| time/                   |              |
|    total_timesteps      | 3768000      |
| train/                  |              |
|    approx_kl            | 0.0004957418 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.2        |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.001        |
|    loss                 | 78.8         |
|    n_updates            | 18390        |
|    policy_gradient_loss | -0.000792    |
|    std                  | 12           |
|    value_loss           | 625          |
------------------------------------------
Eval num_timesteps=3770000, episode_reward=-4.40 +/- 164.38
Episode length: 499.00 +/- 82.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 499          |
|    mean_reward          | -4.4         |
| time/                   |              |
|    total_timesteps      | 3770000      |
| train/                  |              |
|    approx_kl            | 0.0010734537 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.2        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.001        |
|    loss                 | 229          |
|    n_updates            | 18400        |
|    policy_gradient_loss | -0.000515    |
|    std                  | 12           |
|    value_loss           | 948          |
------------------------------------------
Eval num_timesteps=3772000, episode_reward=362.34 +/- 398.61
Episode length: 495.40 +/- 78.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 495           |
|    mean_reward          | 362           |
| time/                   |               |
|    total_timesteps      | 3772000       |
| train/                  |               |
|    approx_kl            | 0.00024029124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.001         |
|    loss                 | 164           |
|    n_updates            | 18410         |
|    policy_gradient_loss | 7.35e-05      |
|    std                  | 12.1          |
|    value_loss           | 515           |
-------------------------------------------
Eval num_timesteps=3774000, episode_reward=-6.84 +/- 68.75
Episode length: 409.40 +/- 48.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | -6.84        |
| time/                   |              |
|    total_timesteps      | 3774000      |
| train/                  |              |
|    approx_kl            | 0.0005236004 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 34.3         |
|    n_updates            | 18420        |
|    policy_gradient_loss | -0.000815    |
|    std                  | 12.1         |
|    value_loss           | 138          |
------------------------------------------
Eval num_timesteps=3776000, episode_reward=180.32 +/- 164.67
Episode length: 493.80 +/- 86.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 494           |
|    mean_reward          | 180           |
| time/                   |               |
|    total_timesteps      | 3776000       |
| train/                  |               |
|    approx_kl            | 0.00095018424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.801         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+03       |
|    n_updates            | 18430         |
|    policy_gradient_loss | -0.000225     |
|    std                  | 12.1          |
|    value_loss           | 3.96e+03      |
-------------------------------------------
Eval num_timesteps=3778000, episode_reward=237.25 +/- 485.34
Episode length: 447.20 +/- 53.75
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 447            |
|    mean_reward          | 237            |
| time/                   |                |
|    total_timesteps      | 3778000        |
| train/                  |                |
|    approx_kl            | 0.000110052235 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -15.3          |
|    explained_variance   | 0.952          |
|    learning_rate        | 0.001          |
|    loss                 | 175            |
|    n_updates            | 18440          |
|    policy_gradient_loss | 0.000245       |
|    std                  | 12.2           |
|    value_loss           | 679            |
--------------------------------------------
Eval num_timesteps=3780000, episode_reward=149.37 +/- 200.35
Episode length: 462.20 +/- 159.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 462          |
|    mean_reward          | 149          |
| time/                   |              |
|    total_timesteps      | 3780000      |
| train/                  |              |
|    approx_kl            | 7.090566e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+03     |
|    n_updates            | 18450        |
|    policy_gradient_loss | -6.7e-05     |
|    std                  | 12.2         |
|    value_loss           | 3.28e+03     |
------------------------------------------
Eval num_timesteps=3782000, episode_reward=253.48 +/- 362.53
Episode length: 492.80 +/- 74.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 493           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 3782000       |
| train/                  |               |
|    approx_kl            | 1.7333747e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.783         |
|    learning_rate        | 0.001         |
|    loss                 | 3.65e+03      |
|    n_updates            | 18460         |
|    policy_gradient_loss | -0.000127     |
|    std                  | 12.2          |
|    value_loss           | 8.85e+03      |
-------------------------------------------
Eval num_timesteps=3784000, episode_reward=107.07 +/- 202.12
Episode length: 440.80 +/- 87.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | 107           |
| time/                   |               |
|    total_timesteps      | 3784000       |
| train/                  |               |
|    approx_kl            | 3.2965385e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 147           |
|    n_updates            | 18470         |
|    policy_gradient_loss | -0.000116     |
|    std                  | 12.2          |
|    value_loss           | 526           |
-------------------------------------------
Eval num_timesteps=3786000, episode_reward=339.24 +/- 454.37
Episode length: 471.00 +/- 90.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 471          |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 3786000      |
| train/                  |              |
|    approx_kl            | 6.856577e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 226          |
|    n_updates            | 18480        |
|    policy_gradient_loss | -3.92e-05    |
|    std                  | 12.2         |
|    value_loss           | 791          |
------------------------------------------
Eval num_timesteps=3788000, episode_reward=60.99 +/- 201.93
Episode length: 465.20 +/- 52.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 465          |
|    mean_reward          | 61           |
| time/                   |              |
|    total_timesteps      | 3788000      |
| train/                  |              |
|    approx_kl            | 0.0005427209 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 64.1         |
|    n_updates            | 18490        |
|    policy_gradient_loss | -0.000759    |
|    std                  | 12.2         |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=3790000, episode_reward=165.08 +/- 422.35
Episode length: 439.20 +/- 44.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 165          |
| time/                   |              |
|    total_timesteps      | 3790000      |
| train/                  |              |
|    approx_kl            | 0.0013877682 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 46.3         |
|    n_updates            | 18500        |
|    policy_gradient_loss | -0.00031     |
|    std                  | 12.2         |
|    value_loss           | 234          |
------------------------------------------
Eval num_timesteps=3792000, episode_reward=-44.40 +/- 308.52
Episode length: 466.00 +/- 80.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 466           |
|    mean_reward          | -44.4         |
| time/                   |               |
|    total_timesteps      | 3792000       |
| train/                  |               |
|    approx_kl            | 0.00059731095 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 84.9          |
|    n_updates            | 18510         |
|    policy_gradient_loss | -2.45e-05     |
|    std                  | 12.2          |
|    value_loss           | 481           |
-------------------------------------------
Eval num_timesteps=3794000, episode_reward=201.38 +/- 203.70
Episode length: 485.40 +/- 80.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 485           |
|    mean_reward          | 201           |
| time/                   |               |
|    total_timesteps      | 3794000       |
| train/                  |               |
|    approx_kl            | 0.00019285793 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 45.9          |
|    n_updates            | 18520         |
|    policy_gradient_loss | -0.000284     |
|    std                  | 12.1          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3796000, episode_reward=255.53 +/- 333.29
Episode length: 419.20 +/- 106.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 256           |
| time/                   |               |
|    total_timesteps      | 3796000       |
| train/                  |               |
|    approx_kl            | 0.00011489756 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 71.7          |
|    n_updates            | 18530         |
|    policy_gradient_loss | -0.000142     |
|    std                  | 12.2          |
|    value_loss           | 291           |
-------------------------------------------
Eval num_timesteps=3798000, episode_reward=86.19 +/- 119.57
Episode length: 510.40 +/- 101.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 510          |
|    mean_reward          | 86.2         |
| time/                   |              |
|    total_timesteps      | 3798000      |
| train/                  |              |
|    approx_kl            | 0.0002607778 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.001        |
|    loss                 | 45.5         |
|    n_updates            | 18540        |
|    policy_gradient_loss | -0.000305    |
|    std                  | 12.2         |
|    value_loss           | 241          |
------------------------------------------
Eval num_timesteps=3800000, episode_reward=237.03 +/- 389.41
Episode length: 419.00 +/- 53.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | 237           |
| time/                   |               |
|    total_timesteps      | 3800000       |
| train/                  |               |
|    approx_kl            | 0.00065724854 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+03      |
|    n_updates            | 18550         |
|    policy_gradient_loss | 0.000274      |
|    std                  | 12.1          |
|    value_loss           | 4.07e+03      |
-------------------------------------------
Eval num_timesteps=3802000, episode_reward=192.69 +/- 292.18
Episode length: 409.60 +/- 33.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 410          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 3802000      |
| train/                  |              |
|    approx_kl            | 0.0009848557 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 63.3         |
|    n_updates            | 18560        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 12.1         |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=3804000, episode_reward=175.82 +/- 242.79
Episode length: 438.40 +/- 91.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 438         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 3804000     |
| train/                  |             |
|    approx_kl            | 0.001545586 |
|    clip_fraction        | 0.0021      |
|    clip_range           | 0.2         |
|    entropy_loss         | -15.3       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 95.4        |
|    n_updates            | 18570       |
|    policy_gradient_loss | -0.000279   |
|    std                  | 12.1        |
|    value_loss           | 473         |
-----------------------------------------
Eval num_timesteps=3806000, episode_reward=25.89 +/- 156.07
Episode length: 429.60 +/- 83.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 430          |
|    mean_reward          | 25.9         |
| time/                   |              |
|    total_timesteps      | 3806000      |
| train/                  |              |
|    approx_kl            | 0.0021409397 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 43.6         |
|    n_updates            | 18580        |
|    policy_gradient_loss | -0.00291     |
|    std                  | 12.1         |
|    value_loss           | 199          |
------------------------------------------
Eval num_timesteps=3808000, episode_reward=633.09 +/- 425.28
Episode length: 479.20 +/- 77.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | 633          |
| time/                   |              |
|    total_timesteps      | 3808000      |
| train/                  |              |
|    approx_kl            | 0.0042973543 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 1.79e+03     |
|    n_updates            | 18590        |
|    policy_gradient_loss | -0.0022      |
|    std                  | 12.1         |
|    value_loss           | 4.37e+03     |
------------------------------------------
Eval num_timesteps=3810000, episode_reward=303.02 +/- 238.41
Episode length: 432.20 +/- 34.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 432          |
|    mean_reward          | 303          |
| time/                   |              |
|    total_timesteps      | 3810000      |
| train/                  |              |
|    approx_kl            | 0.0005795497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+03     |
|    n_updates            | 18600        |
|    policy_gradient_loss | 0.000511     |
|    std                  | 12.1         |
|    value_loss           | 5.23e+03     |
------------------------------------------
Eval num_timesteps=3812000, episode_reward=211.76 +/- 279.02
Episode length: 358.60 +/- 63.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 359           |
|    mean_reward          | 212           |
| time/                   |               |
|    total_timesteps      | 3812000       |
| train/                  |               |
|    approx_kl            | 5.4252712e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+03      |
|    n_updates            | 18610         |
|    policy_gradient_loss | -6.93e-05     |
|    std                  | 12.1          |
|    value_loss           | 3.97e+03      |
-------------------------------------------
Eval num_timesteps=3814000, episode_reward=526.34 +/- 565.98
Episode length: 475.40 +/- 128.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 3814000      |
| train/                  |              |
|    approx_kl            | 2.108168e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+03     |
|    n_updates            | 18620        |
|    policy_gradient_loss | -4.82e-05    |
|    std                  | 12.1         |
|    value_loss           | 3.67e+03     |
------------------------------------------
Eval num_timesteps=3816000, episode_reward=203.81 +/- 137.73
Episode length: 436.00 +/- 97.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 436           |
|    mean_reward          | 204           |
| time/                   |               |
|    total_timesteps      | 3816000       |
| train/                  |               |
|    approx_kl            | 3.4739845e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 99.9          |
|    n_updates            | 18630         |
|    policy_gradient_loss | -0.000151     |
|    std                  | 12.1          |
|    value_loss           | 433           |
-------------------------------------------
Eval num_timesteps=3818000, episode_reward=136.41 +/- 208.14
Episode length: 408.40 +/- 44.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 408          |
|    mean_reward          | 136          |
| time/                   |              |
|    total_timesteps      | 3818000      |
| train/                  |              |
|    approx_kl            | 6.482669e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 196          |
|    n_updates            | 18640        |
|    policy_gradient_loss | -7.16e-05    |
|    std                  | 12.1         |
|    value_loss           | 821          |
------------------------------------------
Eval num_timesteps=3820000, episode_reward=123.48 +/- 208.37
Episode length: 475.60 +/- 88.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 476           |
|    mean_reward          | 123           |
| time/                   |               |
|    total_timesteps      | 3820000       |
| train/                  |               |
|    approx_kl            | 6.8060384e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 36            |
|    n_updates            | 18650         |
|    policy_gradient_loss | -0.000272     |
|    std                  | 12.1          |
|    value_loss           | 169           |
-------------------------------------------
Eval num_timesteps=3822000, episode_reward=173.70 +/- 161.39
Episode length: 441.80 +/- 69.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 174           |
| time/                   |               |
|    total_timesteps      | 3822000       |
| train/                  |               |
|    approx_kl            | 0.00017646671 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 18660         |
|    policy_gradient_loss | -0.000219     |
|    std                  | 12.1          |
|    value_loss           | 4.28e+03      |
-------------------------------------------
Eval num_timesteps=3824000, episode_reward=646.76 +/- 566.43
Episode length: 460.40 +/- 52.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 647          |
| time/                   |              |
|    total_timesteps      | 3824000      |
| train/                  |              |
|    approx_kl            | 0.0003371051 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.3        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 91.4         |
|    n_updates            | 18670        |
|    policy_gradient_loss | -0.000486    |
|    std                  | 12.2         |
|    value_loss           | 435          |
------------------------------------------
Eval num_timesteps=3826000, episode_reward=97.71 +/- 125.19
Episode length: 454.40 +/- 91.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 97.7          |
| time/                   |               |
|    total_timesteps      | 3826000       |
| train/                  |               |
|    approx_kl            | 0.00020661004 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 79.2          |
|    n_updates            | 18680         |
|    policy_gradient_loss | 4.56e-05      |
|    std                  | 12.2          |
|    value_loss           | 288           |
-------------------------------------------
Eval num_timesteps=3828000, episode_reward=99.23 +/- 132.51
Episode length: 406.60 +/- 78.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 407           |
|    mean_reward          | 99.2          |
| time/                   |               |
|    total_timesteps      | 3828000       |
| train/                  |               |
|    approx_kl            | 0.00051483256 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.3         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 37.6          |
|    n_updates            | 18690         |
|    policy_gradient_loss | -0.000307     |
|    std                  | 12.3          |
|    value_loss           | 143           |
-------------------------------------------
Eval num_timesteps=3830000, episode_reward=362.57 +/- 426.92
Episode length: 461.60 +/- 152.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 363           |
| time/                   |               |
|    total_timesteps      | 3830000       |
| train/                  |               |
|    approx_kl            | 0.00089015276 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+03      |
|    n_updates            | 18700         |
|    policy_gradient_loss | 0.000384      |
|    std                  | 12.3          |
|    value_loss           | 4.88e+03      |
-------------------------------------------
Eval num_timesteps=3832000, episode_reward=238.00 +/- 134.19
Episode length: 459.60 +/- 73.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 460           |
|    mean_reward          | 238           |
| time/                   |               |
|    total_timesteps      | 3832000       |
| train/                  |               |
|    approx_kl            | 0.00014865107 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 55.2          |
|    n_updates            | 18710         |
|    policy_gradient_loss | -0.000149     |
|    std                  | 12.3          |
|    value_loss           | 191           |
-------------------------------------------
Eval num_timesteps=3834000, episode_reward=647.76 +/- 544.97
Episode length: 505.40 +/- 90.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 505           |
|    mean_reward          | 648           |
| time/                   |               |
|    total_timesteps      | 3834000       |
| train/                  |               |
|    approx_kl            | 0.00023483412 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.001         |
|    loss                 | 51.9          |
|    n_updates            | 18720         |
|    policy_gradient_loss | -0.000442     |
|    std                  | 12.4          |
|    value_loss           | 179           |
-------------------------------------------
Eval num_timesteps=3836000, episode_reward=215.47 +/- 400.81
Episode length: 418.40 +/- 36.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 215          |
| time/                   |              |
|    total_timesteps      | 3836000      |
| train/                  |              |
|    approx_kl            | 0.0012678171 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.4        |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 57.4         |
|    n_updates            | 18730        |
|    policy_gradient_loss | -0.000632    |
|    std                  | 12.4         |
|    value_loss           | 361          |
------------------------------------------
Eval num_timesteps=3838000, episode_reward=180.54 +/- 337.61
Episode length: 444.00 +/- 63.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 444           |
|    mean_reward          | 181           |
| time/                   |               |
|    total_timesteps      | 3838000       |
| train/                  |               |
|    approx_kl            | 0.00081134064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 80            |
|    n_updates            | 18740         |
|    policy_gradient_loss | -0.0002       |
|    std                  | 12.4          |
|    value_loss           | 390           |
-------------------------------------------
Eval num_timesteps=3840000, episode_reward=50.92 +/- 104.14
Episode length: 352.80 +/- 42.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 353      |
|    mean_reward     | 50.9     |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
Eval num_timesteps=3842000, episode_reward=424.03 +/- 626.11
Episode length: 483.20 +/- 133.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 483           |
|    mean_reward          | 424           |
| time/                   |               |
|    total_timesteps      | 3842000       |
| train/                  |               |
|    approx_kl            | 0.00094306277 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 72.7          |
|    n_updates            | 18750         |
|    policy_gradient_loss | -0.000725     |
|    std                  | 12.4          |
|    value_loss           | 332           |
-------------------------------------------
Eval num_timesteps=3844000, episode_reward=406.28 +/- 468.31
Episode length: 456.00 +/- 84.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 3844000      |
| train/                  |              |
|    approx_kl            | 0.0009609573 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.4        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 18760        |
|    policy_gradient_loss | 0.000347     |
|    std                  | 12.5         |
|    value_loss           | 4.18e+03     |
------------------------------------------
Eval num_timesteps=3846000, episode_reward=140.01 +/- 236.18
Episode length: 443.80 +/- 53.02
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 444            |
|    mean_reward          | 140            |
| time/                   |                |
|    total_timesteps      | 3846000        |
| train/                  |                |
|    approx_kl            | 0.000116686075 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -15.4          |
|    explained_variance   | 0.965          |
|    learning_rate        | 0.001          |
|    loss                 | 88             |
|    n_updates            | 18770          |
|    policy_gradient_loss | -0.000212      |
|    std                  | 12.5           |
|    value_loss           | 377            |
--------------------------------------------
Eval num_timesteps=3848000, episode_reward=1047.47 +/- 1375.37
Episode length: 480.60 +/- 123.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 481           |
|    mean_reward          | 1.05e+03      |
| time/                   |               |
|    total_timesteps      | 3848000       |
| train/                  |               |
|    approx_kl            | 0.00013029101 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.4         |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.001         |
|    loss                 | 37.6          |
|    n_updates            | 18780         |
|    policy_gradient_loss | -0.000186     |
|    std                  | 12.6          |
|    value_loss           | 156           |
-------------------------------------------
Eval num_timesteps=3850000, episode_reward=342.61 +/- 405.03
Episode length: 450.20 +/- 60.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 450           |
|    mean_reward          | 343           |
| time/                   |               |
|    total_timesteps      | 3850000       |
| train/                  |               |
|    approx_kl            | 0.00021279344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 52.2          |
|    n_updates            | 18790         |
|    policy_gradient_loss | -0.000657     |
|    std                  | 12.6          |
|    value_loss           | 232           |
-------------------------------------------
Eval num_timesteps=3852000, episode_reward=133.76 +/- 208.21
Episode length: 344.20 +/- 80.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 344           |
|    mean_reward          | 134           |
| time/                   |               |
|    total_timesteps      | 3852000       |
| train/                  |               |
|    approx_kl            | 8.8798115e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+03      |
|    n_updates            | 18800         |
|    policy_gradient_loss | 1.66e-05      |
|    std                  | 12.6          |
|    value_loss           | 5.05e+03      |
-------------------------------------------
Eval num_timesteps=3854000, episode_reward=56.97 +/- 82.11
Episode length: 445.40 +/- 46.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | 57           |
| time/                   |              |
|    total_timesteps      | 3854000      |
| train/                  |              |
|    approx_kl            | 4.039923e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.5        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 110          |
|    n_updates            | 18810        |
|    policy_gradient_loss | -0.000263    |
|    std                  | 12.6         |
|    value_loss           | 411          |
------------------------------------------
Eval num_timesteps=3856000, episode_reward=193.89 +/- 148.56
Episode length: 431.40 +/- 85.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 431           |
|    mean_reward          | 194           |
| time/                   |               |
|    total_timesteps      | 3856000       |
| train/                  |               |
|    approx_kl            | 8.4633415e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.001         |
|    loss                 | 32.9          |
|    n_updates            | 18820         |
|    policy_gradient_loss | -0.000191     |
|    std                  | 12.7          |
|    value_loss           | 156           |
-------------------------------------------
Eval num_timesteps=3858000, episode_reward=252.78 +/- 182.29
Episode length: 443.20 +/- 36.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 443           |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 3858000       |
| train/                  |               |
|    approx_kl            | 0.00044666562 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 41.6          |
|    n_updates            | 18830         |
|    policy_gradient_loss | -0.000645     |
|    std                  | 12.7          |
|    value_loss           | 218           |
-------------------------------------------
Eval num_timesteps=3860000, episode_reward=9.89 +/- 55.60
Episode length: 528.80 +/- 71.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 529          |
|    mean_reward          | 9.89         |
| time/                   |              |
|    total_timesteps      | 3860000      |
| train/                  |              |
|    approx_kl            | 0.0013000045 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.5        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 92.5         |
|    n_updates            | 18840        |
|    policy_gradient_loss | -0.000193    |
|    std                  | 12.7         |
|    value_loss           | 349          |
------------------------------------------
Eval num_timesteps=3862000, episode_reward=74.60 +/- 66.38
Episode length: 370.00 +/- 32.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 370           |
|    mean_reward          | 74.6          |
| time/                   |               |
|    total_timesteps      | 3862000       |
| train/                  |               |
|    approx_kl            | 0.00019154919 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+03      |
|    n_updates            | 18850         |
|    policy_gradient_loss | 0.000143      |
|    std                  | 12.7          |
|    value_loss           | 4.46e+03      |
-------------------------------------------
Eval num_timesteps=3864000, episode_reward=173.21 +/- 248.92
Episode length: 470.80 +/- 80.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 471           |
|    mean_reward          | 173           |
| time/                   |               |
|    total_timesteps      | 3864000       |
| train/                  |               |
|    approx_kl            | 5.8363017e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 119           |
|    n_updates            | 18860         |
|    policy_gradient_loss | -0.000109     |
|    std                  | 12.7          |
|    value_loss           | 452           |
-------------------------------------------
Eval num_timesteps=3866000, episode_reward=158.43 +/- 305.56
Episode length: 430.40 +/- 62.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 158           |
| time/                   |               |
|    total_timesteps      | 3866000       |
| train/                  |               |
|    approx_kl            | 0.00034852474 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.001         |
|    loss                 | 43.8          |
|    n_updates            | 18870         |
|    policy_gradient_loss | -0.000576     |
|    std                  | 12.7          |
|    value_loss           | 128           |
-------------------------------------------
Eval num_timesteps=3868000, episode_reward=102.74 +/- 217.59
Episode length: 462.40 +/- 96.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 462           |
|    mean_reward          | 103           |
| time/                   |               |
|    total_timesteps      | 3868000       |
| train/                  |               |
|    approx_kl            | 0.00023673242 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.001         |
|    loss                 | 155           |
|    n_updates            | 18880         |
|    policy_gradient_loss | 3.05e-05      |
|    std                  | 12.7          |
|    value_loss           | 714           |
-------------------------------------------
Eval num_timesteps=3870000, episode_reward=290.04 +/- 401.70
Episode length: 453.00 +/- 92.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 290           |
| time/                   |               |
|    total_timesteps      | 3870000       |
| train/                  |               |
|    approx_kl            | 0.00028141445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 70.8          |
|    n_updates            | 18890         |
|    policy_gradient_loss | -0.000681     |
|    std                  | 12.7          |
|    value_loss           | 259           |
-------------------------------------------
Eval num_timesteps=3872000, episode_reward=30.35 +/- 74.89
Episode length: 433.80 +/- 73.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 434           |
|    mean_reward          | 30.3          |
| time/                   |               |
|    total_timesteps      | 3872000       |
| train/                  |               |
|    approx_kl            | 0.00025528178 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 38            |
|    n_updates            | 18900         |
|    policy_gradient_loss | -0.000224     |
|    std                  | 12.8          |
|    value_loss           | 304           |
-------------------------------------------
Eval num_timesteps=3874000, episode_reward=95.01 +/- 159.88
Episode length: 430.40 +/- 119.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 430           |
|    mean_reward          | 95            |
| time/                   |               |
|    total_timesteps      | 3874000       |
| train/                  |               |
|    approx_kl            | 0.00019281104 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 115           |
|    n_updates            | 18910         |
|    policy_gradient_loss | 0.000146      |
|    std                  | 12.9          |
|    value_loss           | 366           |
-------------------------------------------
Eval num_timesteps=3876000, episode_reward=48.29 +/- 115.39
Episode length: 373.20 +/- 107.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 373           |
|    mean_reward          | 48.3          |
| time/                   |               |
|    total_timesteps      | 3876000       |
| train/                  |               |
|    approx_kl            | 0.00018166265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.5         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.001         |
|    loss                 | 48.3          |
|    n_updates            | 18920         |
|    policy_gradient_loss | -0.000239     |
|    std                  | 13            |
|    value_loss           | 159           |
-------------------------------------------
Eval num_timesteps=3878000, episode_reward=117.13 +/- 249.17
Episode length: 382.20 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 382           |
|    mean_reward          | 117           |
| time/                   |               |
|    total_timesteps      | 3878000       |
| train/                  |               |
|    approx_kl            | 0.00033077315 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.6         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 47.3          |
|    n_updates            | 18930         |
|    policy_gradient_loss | -0.000338     |
|    std                  | 13            |
|    value_loss           | 206           |
-------------------------------------------
Eval num_timesteps=3880000, episode_reward=43.26 +/- 154.15
Episode length: 454.80 +/- 91.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 43.3         |
| time/                   |              |
|    total_timesteps      | 3880000      |
| train/                  |              |
|    approx_kl            | 0.0004575695 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.6        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 91.5         |
|    n_updates            | 18940        |
|    policy_gradient_loss | -0.000361    |
|    std                  | 13           |
|    value_loss           | 309          |
------------------------------------------
Eval num_timesteps=3882000, episode_reward=162.28 +/- 90.72
Episode length: 453.00 +/- 73.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 453           |
|    mean_reward          | 162           |
| time/                   |               |
|    total_timesteps      | 3882000       |
| train/                  |               |
|    approx_kl            | 0.00013549265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.6         |
|    explained_variance   | 0.793         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+03       |
|    n_updates            | 18950         |
|    policy_gradient_loss | 0.000164      |
|    std                  | 13            |
|    value_loss           | 5.38e+03      |
-------------------------------------------
Eval num_timesteps=3884000, episode_reward=308.37 +/- 363.56
Episode length: 454.00 +/- 64.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 454           |
|    mean_reward          | 308           |
| time/                   |               |
|    total_timesteps      | 3884000       |
| train/                  |               |
|    approx_kl            | 1.0209886e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.6         |
|    explained_variance   | 0.793         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+03      |
|    n_updates            | 18960         |
|    policy_gradient_loss | 6.1e-06       |
|    std                  | 13            |
|    value_loss           | 5.08e+03      |
-------------------------------------------
Eval num_timesteps=3886000, episode_reward=108.30 +/- 131.68
Episode length: 403.80 +/- 47.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 404           |
|    mean_reward          | 108           |
| time/                   |               |
|    total_timesteps      | 3886000       |
| train/                  |               |
|    approx_kl            | 2.0158535e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.6         |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 95.5          |
|    n_updates            | 18970         |
|    policy_gradient_loss | -0.000147     |
|    std                  | 13.1          |
|    value_loss           | 321           |
-------------------------------------------
Eval num_timesteps=3888000, episode_reward=-29.21 +/- 85.64
Episode length: 438.40 +/- 78.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 438          |
|    mean_reward          | -29.2        |
| time/                   |              |
|    total_timesteps      | 3888000      |
| train/                  |              |
|    approx_kl            | 0.0005736259 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.6        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 50.6         |
|    n_updates            | 18980        |
|    policy_gradient_loss | -0.000776    |
|    std                  | 13.1         |
|    value_loss           | 172          |
------------------------------------------
Eval num_timesteps=3890000, episode_reward=-19.35 +/- 121.67
Episode length: 456.40 +/- 155.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 456          |
|    mean_reward          | -19.4        |
| time/                   |              |
|    total_timesteps      | 3890000      |
| train/                  |              |
|    approx_kl            | 0.0006439641 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.6        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 30.9         |
|    n_updates            | 18990        |
|    policy_gradient_loss | -0.000365    |
|    std                  | 13.2         |
|    value_loss           | 129          |
------------------------------------------
Eval num_timesteps=3892000, episode_reward=167.20 +/- 280.59
Episode length: 406.60 +/- 50.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 407          |
|    mean_reward          | 167          |
| time/                   |              |
|    total_timesteps      | 3892000      |
| train/                  |              |
|    approx_kl            | 0.0009111265 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.7        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 19000        |
|    policy_gradient_loss | -0.000927    |
|    std                  | 13.3         |
|    value_loss           | 575          |
------------------------------------------
Eval num_timesteps=3894000, episode_reward=71.16 +/- 267.45
Episode length: 480.40 +/- 85.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 480          |
|    mean_reward          | 71.2         |
| time/                   |              |
|    total_timesteps      | 3894000      |
| train/                  |              |
|    approx_kl            | 0.0001800046 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.7        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.001        |
|    loss                 | 34.2         |
|    n_updates            | 19010        |
|    policy_gradient_loss | 0.000357     |
|    std                  | 13.4         |
|    value_loss           | 134          |
------------------------------------------
Eval num_timesteps=3896000, episode_reward=132.85 +/- 175.69
Episode length: 487.40 +/- 98.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 487          |
|    mean_reward          | 133          |
| time/                   |              |
|    total_timesteps      | 3896000      |
| train/                  |              |
|    approx_kl            | 0.0003748516 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.7        |
|    explained_variance   | 0.754        |
|    learning_rate        | 0.001        |
|    loss                 | 2.14e+03     |
|    n_updates            | 19020        |
|    policy_gradient_loss | 0.000343     |
|    std                  | 13.4         |
|    value_loss           | 5.26e+03     |
------------------------------------------
Eval num_timesteps=3898000, episode_reward=164.92 +/- 269.98
Episode length: 441.80 +/- 134.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 442           |
|    mean_reward          | 165           |
| time/                   |               |
|    total_timesteps      | 3898000       |
| train/                  |               |
|    approx_kl            | 0.00016147309 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.7         |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 61.3          |
|    n_updates            | 19030         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 13.4          |
|    value_loss           | 350           |
-------------------------------------------
Eval num_timesteps=3900000, episode_reward=98.58 +/- 124.27
Episode length: 410.40 +/- 30.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 410           |
|    mean_reward          | 98.6          |
| time/                   |               |
|    total_timesteps      | 3900000       |
| train/                  |               |
|    approx_kl            | 0.00025818337 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.7         |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 75.2          |
|    n_updates            | 19040         |
|    policy_gradient_loss | -0.000378     |
|    std                  | 13.5          |
|    value_loss           | 372           |
-------------------------------------------
Eval num_timesteps=3902000, episode_reward=503.41 +/- 564.24
Episode length: 476.60 +/- 67.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 477          |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 3902000      |
| train/                  |              |
|    approx_kl            | 0.0007366169 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.7        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 50.5         |
|    n_updates            | 19050        |
|    policy_gradient_loss | 0.000197     |
|    std                  | 13.5         |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=3904000, episode_reward=187.13 +/- 321.61
Episode length: 518.20 +/- 60.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 518           |
|    mean_reward          | 187           |
| time/                   |               |
|    total_timesteps      | 3904000       |
| train/                  |               |
|    approx_kl            | 0.00010331912 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.7         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 60.2          |
|    n_updates            | 19060         |
|    policy_gradient_loss | -0.000157     |
|    std                  | 13.5          |
|    value_loss           | 270           |
-------------------------------------------
Eval num_timesteps=3906000, episode_reward=71.36 +/- 155.81
Episode length: 489.20 +/- 118.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 489           |
|    mean_reward          | 71.4          |
| time/                   |               |
|    total_timesteps      | 3906000       |
| train/                  |               |
|    approx_kl            | 0.00063310174 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.7         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 46.7          |
|    n_updates            | 19070         |
|    policy_gradient_loss | -0.000865     |
|    std                  | 13.5          |
|    value_loss           | 190           |
-------------------------------------------
Eval num_timesteps=3908000, episode_reward=114.76 +/- 253.04
Episode length: 428.20 +/- 68.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 428          |
|    mean_reward          | 115          |
| time/                   |              |
|    total_timesteps      | 3908000      |
| train/                  |              |
|    approx_kl            | 0.0015799127 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.7        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 34.2         |
|    n_updates            | 19080        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 13.6         |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=3910000, episode_reward=141.43 +/- 146.15
Episode length: 418.20 +/- 92.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 418          |
|    mean_reward          | 141          |
| time/                   |              |
|    total_timesteps      | 3910000      |
| train/                  |              |
|    approx_kl            | 0.0006938116 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 19090        |
|    policy_gradient_loss | -0.00127     |
|    std                  | 13.7         |
|    value_loss           | 116          |
------------------------------------------
Eval num_timesteps=3912000, episode_reward=501.29 +/- 621.23
Episode length: 472.00 +/- 144.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 472           |
|    mean_reward          | 501           |
| time/                   |               |
|    total_timesteps      | 3912000       |
| train/                  |               |
|    approx_kl            | 0.00074822025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+03      |
|    n_updates            | 19100         |
|    policy_gradient_loss | 0.000375      |
|    std                  | 13.8          |
|    value_loss           | 4.51e+03      |
-------------------------------------------
Eval num_timesteps=3914000, episode_reward=600.88 +/- 730.47
Episode length: 473.20 +/- 101.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 473          |
|    mean_reward          | 601          |
| time/                   |              |
|    total_timesteps      | 3914000      |
| train/                  |              |
|    approx_kl            | 0.0009370932 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.001        |
|    loss                 | 32.2         |
|    n_updates            | 19110        |
|    policy_gradient_loss | -0.000711    |
|    std                  | 13.8         |
|    value_loss           | 144          |
------------------------------------------
Eval num_timesteps=3916000, episode_reward=-21.69 +/- 65.84
Episode length: 423.60 +/- 74.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 424         |
|    mean_reward          | -21.7       |
| time/                   |             |
|    total_timesteps      | 3916000     |
| train/                  |             |
|    approx_kl            | 0.001140374 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -15.8       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 60          |
|    n_updates            | 19120       |
|    policy_gradient_loss | 0.000196    |
|    std                  | 13.8        |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=3918000, episode_reward=7.35 +/- 168.76
Episode length: 393.80 +/- 79.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 394           |
|    mean_reward          | 7.35          |
| time/                   |               |
|    total_timesteps      | 3918000       |
| train/                  |               |
|    approx_kl            | 0.00030486763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.001         |
|    loss                 | 45.7          |
|    n_updates            | 19130         |
|    policy_gradient_loss | -0.000411     |
|    std                  | 13.8          |
|    value_loss           | 190           |
-------------------------------------------
Eval num_timesteps=3920000, episode_reward=144.92 +/- 199.97
Episode length: 439.00 +/- 78.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 439          |
|    mean_reward          | 145          |
| time/                   |              |
|    total_timesteps      | 3920000      |
| train/                  |              |
|    approx_kl            | 0.0010659674 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 50.2         |
|    n_updates            | 19140        |
|    policy_gradient_loss | -0.00109     |
|    std                  | 13.9         |
|    value_loss           | 368          |
------------------------------------------
Eval num_timesteps=3922000, episode_reward=-23.76 +/- 84.98
Episode length: 396.40 +/- 107.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 396           |
|    mean_reward          | -23.8         |
| time/                   |               |
|    total_timesteps      | 3922000       |
| train/                  |               |
|    approx_kl            | 0.00078728516 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 119           |
|    n_updates            | 19150         |
|    policy_gradient_loss | -0.00028      |
|    std                  | 13.9          |
|    value_loss           | 588           |
-------------------------------------------
Eval num_timesteps=3924000, episode_reward=115.77 +/- 286.27
Episode length: 365.20 +/- 91.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 365           |
|    mean_reward          | 116           |
| time/                   |               |
|    total_timesteps      | 3924000       |
| train/                  |               |
|    approx_kl            | 0.00021903706 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 77.3          |
|    n_updates            | 19160         |
|    policy_gradient_loss | -0.000278     |
|    std                  | 13.9          |
|    value_loss           | 313           |
-------------------------------------------
Eval num_timesteps=3926000, episode_reward=91.12 +/- 206.66
Episode length: 464.00 +/- 68.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 91.1     |
| time/              |          |
|    total_timesteps | 3926000  |
---------------------------------
Eval num_timesteps=3928000, episode_reward=-62.51 +/- 40.60
Episode length: 419.00 +/- 91.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 419           |
|    mean_reward          | -62.5         |
| time/                   |               |
|    total_timesteps      | 3928000       |
| train/                  |               |
|    approx_kl            | 0.00038980358 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.001         |
|    loss                 | 38.7          |
|    n_updates            | 19170         |
|    policy_gradient_loss | -0.000407     |
|    std                  | 13.9          |
|    value_loss           | 126           |
-------------------------------------------
Eval num_timesteps=3930000, episode_reward=4.56 +/- 137.14
Episode length: 466.20 +/- 91.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 4.56         |
| time/                   |              |
|    total_timesteps      | 3930000      |
| train/                  |              |
|    approx_kl            | 0.0005113063 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.001        |
|    loss                 | 24.7         |
|    n_updates            | 19180        |
|    policy_gradient_loss | -0.000879    |
|    std                  | 14           |
|    value_loss           | 110          |
------------------------------------------
Eval num_timesteps=3932000, episode_reward=-61.94 +/- 48.78
Episode length: 444.60 +/- 106.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 445          |
|    mean_reward          | -61.9        |
| time/                   |              |
|    total_timesteps      | 3932000      |
| train/                  |              |
|    approx_kl            | 0.0026660766 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 34.1         |
|    n_updates            | 19190        |
|    policy_gradient_loss | -0.000724    |
|    std                  | 14.1         |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=3934000, episode_reward=173.99 +/- 189.69
Episode length: 486.60 +/- 87.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 487           |
|    mean_reward          | 174           |
| time/                   |               |
|    total_timesteps      | 3934000       |
| train/                  |               |
|    approx_kl            | 0.00090721686 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 70.6          |
|    n_updates            | 19200         |
|    policy_gradient_loss | -0.00112      |
|    std                  | 14.1          |
|    value_loss           | 232           |
-------------------------------------------
Eval num_timesteps=3936000, episode_reward=96.59 +/- 140.99
Episode length: 508.00 +/- 93.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 508           |
|    mean_reward          | 96.6          |
| time/                   |               |
|    total_timesteps      | 3936000       |
| train/                  |               |
|    approx_kl            | 0.00023738269 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.9         |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 42.9          |
|    n_updates            | 19210         |
|    policy_gradient_loss | -7.04e-05     |
|    std                  | 14.1          |
|    value_loss           | 176           |
-------------------------------------------
Eval num_timesteps=3938000, episode_reward=415.29 +/- 521.90
Episode length: 440.60 +/- 40.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 441           |
|    mean_reward          | 415           |
| time/                   |               |
|    total_timesteps      | 3938000       |
| train/                  |               |
|    approx_kl            | 0.00016023847 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.9         |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 62.1          |
|    n_updates            | 19220         |
|    policy_gradient_loss | -0.000562     |
|    std                  | 14.1          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3940000, episode_reward=15.73 +/- 77.11
Episode length: 463.80 +/- 56.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 464           |
|    mean_reward          | 15.7          |
| time/                   |               |
|    total_timesteps      | 3940000       |
| train/                  |               |
|    approx_kl            | 0.00037917966 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.001         |
|    loss                 | 47.3          |
|    n_updates            | 19230         |
|    policy_gradient_loss | -0.00058      |
|    std                  | 14.1          |
|    value_loss           | 164           |
-------------------------------------------
Eval num_timesteps=3942000, episode_reward=-16.26 +/- 79.06
Episode length: 450.80 +/- 71.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 451          |
|    mean_reward          | -16.3        |
| time/                   |              |
|    total_timesteps      | 3942000      |
| train/                  |              |
|    approx_kl            | 7.357949e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -15.8        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 67.7         |
|    n_updates            | 19240        |
|    policy_gradient_loss | 8.34e-05     |
|    std                  | 14.1         |
|    value_loss           | 349          |
------------------------------------------
Eval num_timesteps=3944000, episode_reward=-54.59 +/- 110.25
Episode length: 384.20 +/- 48.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 384           |
|    mean_reward          | -54.6         |
| time/                   |               |
|    total_timesteps      | 3944000       |
| train/                  |               |
|    approx_kl            | 4.8079964e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.695         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+03      |
|    n_updates            | 19250         |
|    policy_gradient_loss | -7.34e-05     |
|    std                  | 14.1          |
|    value_loss           | 5.5e+03       |
-------------------------------------------
Eval num_timesteps=3946000, episode_reward=409.94 +/- 393.66
Episode length: 516.40 +/- 100.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 516           |
|    mean_reward          | 410           |
| time/                   |               |
|    total_timesteps      | 3946000       |
| train/                  |               |
|    approx_kl            | 6.0333376e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.9         |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.001         |
|    loss                 | 71.3          |
|    n_updates            | 19260         |
|    policy_gradient_loss | -0.00019      |
|    std                  | 14.1          |
|    value_loss           | 287           |
-------------------------------------------
Eval num_timesteps=3948000, episode_reward=397.74 +/- 500.27
Episode length: 510.40 +/- 56.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 510           |
|    mean_reward          | 398           |
| time/                   |               |
|    total_timesteps      | 3948000       |
| train/                  |               |
|    approx_kl            | 0.00038201603 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.9         |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 70            |
|    n_updates            | 19270         |
|    policy_gradient_loss | -0.000523     |
|    std                  | 14.1          |
|    value_loss           | 362           |
-------------------------------------------
Eval num_timesteps=3950000, episode_reward=181.88 +/- 292.82
Episode length: 447.60 +/- 69.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 448           |
|    mean_reward          | 182           |
| time/                   |               |
|    total_timesteps      | 3950000       |
| train/                  |               |
|    approx_kl            | 0.00013672232 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.001         |
|    loss                 | 52.3          |
|    n_updates            | 19280         |
|    policy_gradient_loss | 7.58e-05      |
|    std                  | 14.1          |
|    value_loss           | 182           |
-------------------------------------------
Eval num_timesteps=3952000, episode_reward=291.12 +/- 159.17
Episode length: 472.20 +/- 70.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 472           |
|    mean_reward          | 291           |
| time/                   |               |
|    total_timesteps      | 3952000       |
| train/                  |               |
|    approx_kl            | 0.00021399846 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -15.8         |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+03       |
|    n_updates            | 19290         |
|    policy_gradient_loss | -0.000235     |
|    std                  | 14.1          |
|    value_loss           | 4.5e+03       |
-------------------------------------------
Traceback (most recent call last):
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] The pipe has been ended
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 678, in <module>
    sim.run_full()
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 416, in run_full
    model.learn(total_timesteps=int(args.max_steps),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 200, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 219, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 460, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\evaluation.py", line 94, in evaluate_policy
    new_observations, rewards, dones, infos = env.step(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 206, in step
    return self.step_wait()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 321, in _recv_bytes
    raise EOFError
EOFError