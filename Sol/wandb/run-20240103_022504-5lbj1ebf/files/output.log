AVIARY DIM [-1 -1  0  1  1  1]
Attempting to open: C:\Files\Egyetem\Szakdolgozat\RL\Sol/resources
[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:
[INFO] m 0.027000, L 0.039700,
[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,
[INFO] kf 0.000000, km 0.000000,
[INFO] t2w 2.250000, max_speed_kmh 30.000000,
[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,
[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,
[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000
Using cuda device
Logging to ./logs/ppo_tensorboard/PPO_110
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=1992, episode_reward=-300.00 +/- 0.00
Episode length: 218.00 +/- 55.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1992     |
---------------------------------
New best mean reward!
Eval num_timesteps=3984, episode_reward=-300.00 +/- 0.00
Episode length: 190.20 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 3984     |
---------------------------------
Eval num_timesteps=5976, episode_reward=-300.00 +/- 0.00
Episode length: 275.00 +/- 110.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 5976     |
---------------------------------
Eval num_timesteps=7968, episode_reward=-300.00 +/- 0.00
Episode length: 170.60 +/- 30.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 7968     |
---------------------------------
Eval num_timesteps=9960, episode_reward=-300.00 +/- 0.00
Episode length: 189.60 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 9960     |
---------------------------------
Eval num_timesteps=11952, episode_reward=-300.00 +/- 0.00
Episode length: 241.20 +/- 45.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 11952    |
---------------------------------
Eval num_timesteps=13944, episode_reward=-300.00 +/- 0.00
Episode length: 219.20 +/- 29.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 13944    |
---------------------------------
Eval num_timesteps=15936, episode_reward=-300.00 +/- 0.00
Episode length: 247.60 +/- 66.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 15936    |
---------------------------------
Eval num_timesteps=17928, episode_reward=-300.00 +/- 0.00
Episode length: 244.20 +/- 48.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 17928    |
---------------------------------
Eval num_timesteps=19920, episode_reward=-300.00 +/- 0.00
Episode length: 229.40 +/- 39.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 19920    |
---------------------------------
Eval num_timesteps=21912, episode_reward=-300.00 +/- 0.00
Episode length: 209.60 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 21912    |
---------------------------------
Eval num_timesteps=23904, episode_reward=-300.00 +/- 0.00
Episode length: 200.60 +/- 55.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 23904    |
---------------------------------
Eval num_timesteps=25896, episode_reward=-300.00 +/- 0.00
Episode length: 265.80 +/- 73.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 25896    |
---------------------------------
Eval num_timesteps=27888, episode_reward=-300.00 +/- 0.00
Episode length: 227.40 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 27888    |
---------------------------------
Eval num_timesteps=29880, episode_reward=-300.00 +/- 0.00
Episode length: 198.60 +/- 35.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 29880    |
---------------------------------
Eval num_timesteps=31872, episode_reward=-300.00 +/- 0.00
Episode length: 232.00 +/- 54.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 31872    |
---------------------------------
Eval num_timesteps=33864, episode_reward=-300.00 +/- 0.00
Episode length: 194.80 +/- 40.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 33864    |
---------------------------------
Eval num_timesteps=35856, episode_reward=-300.00 +/- 0.00
Episode length: 256.00 +/- 108.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 35856    |
---------------------------------
Eval num_timesteps=37848, episode_reward=-300.00 +/- 0.00
Episode length: 215.20 +/- 44.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 37848    |
---------------------------------
Eval num_timesteps=39840, episode_reward=-300.00 +/- 0.00
Episode length: 248.00 +/- 65.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 39840    |
---------------------------------
Eval num_timesteps=41832, episode_reward=-300.00 +/- 0.00
Episode length: 228.80 +/- 45.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 41832    |
---------------------------------
Eval num_timesteps=43824, episode_reward=-300.00 +/- 0.00
Episode length: 218.40 +/- 91.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 43824    |
---------------------------------
Eval num_timesteps=45816, episode_reward=-300.00 +/- 0.00
Episode length: 194.20 +/- 34.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 45816    |
---------------------------------
Eval num_timesteps=47808, episode_reward=-300.00 +/- 0.00
Episode length: 210.40 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 47808    |
---------------------------------
Eval num_timesteps=49800, episode_reward=-300.00 +/- 0.00
Episode length: 249.00 +/- 55.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 49800       |
| train/                  |             |
|    approx_kl            | 0.004248304 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.74       |
|    explained_variance   | -0.0445     |
|    learning_rate        | 0.001       |
|    loss                 | 0.669       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0017     |
|    std                  | 1.02        |
|    value_loss           | 1.61        |
-----------------------------------------
Eval num_timesteps=51792, episode_reward=-300.00 +/- 0.00
Episode length: 216.40 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 51792    |
---------------------------------
Eval num_timesteps=53784, episode_reward=-300.00 +/- 0.00
Episode length: 233.00 +/- 58.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 53784    |
---------------------------------
Eval num_timesteps=55776, episode_reward=-300.00 +/- 0.00
Episode length: 222.20 +/- 41.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 55776    |
---------------------------------
Eval num_timesteps=57768, episode_reward=-300.00 +/- 0.00
Episode length: 253.00 +/- 71.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 57768    |
---------------------------------
Eval num_timesteps=59760, episode_reward=-300.00 +/- 0.00
Episode length: 213.40 +/- 37.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 59760    |
---------------------------------
Eval num_timesteps=61752, episode_reward=-300.00 +/- 0.00
Episode length: 249.80 +/- 37.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 61752    |
---------------------------------
Eval num_timesteps=63744, episode_reward=-300.00 +/- 0.00
Episode length: 220.80 +/- 51.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 63744    |
---------------------------------
Eval num_timesteps=65736, episode_reward=-300.00 +/- 0.00
Episode length: 235.20 +/- 64.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 65736    |
---------------------------------
Eval num_timesteps=67728, episode_reward=-300.00 +/- 0.00
Episode length: 207.20 +/- 54.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 67728    |
---------------------------------
Eval num_timesteps=69720, episode_reward=-300.00 +/- 0.00
Episode length: 226.80 +/- 39.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 69720    |
---------------------------------
Eval num_timesteps=71712, episode_reward=-300.00 +/- 0.00
Episode length: 207.00 +/- 41.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 71712    |
---------------------------------
Eval num_timesteps=73704, episode_reward=-300.00 +/- 0.00
Episode length: 182.80 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 73704    |
---------------------------------
Eval num_timesteps=75696, episode_reward=-300.00 +/- 0.00
Episode length: 219.00 +/- 43.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 75696    |
---------------------------------
Eval num_timesteps=77688, episode_reward=-300.00 +/- 0.00
Episode length: 226.40 +/- 56.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 77688    |
---------------------------------
Eval num_timesteps=79680, episode_reward=-300.00 +/- 0.00
Episode length: 188.60 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 79680    |
---------------------------------
Eval num_timesteps=81672, episode_reward=-300.00 +/- 0.00
Episode length: 250.40 +/- 65.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 81672    |
---------------------------------
Eval num_timesteps=83664, episode_reward=-300.00 +/- 0.00
Episode length: 211.80 +/- 51.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 83664    |
---------------------------------
Eval num_timesteps=85656, episode_reward=-300.00 +/- 0.00
Episode length: 221.00 +/- 41.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 85656    |
---------------------------------
Eval num_timesteps=87648, episode_reward=-300.00 +/- 0.00
Episode length: 295.40 +/- 51.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 87648    |
---------------------------------
Eval num_timesteps=89640, episode_reward=-300.00 +/- 0.00
Episode length: 225.60 +/- 36.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 89640    |
---------------------------------
Eval num_timesteps=91632, episode_reward=-300.00 +/- 0.00
Episode length: 205.40 +/- 71.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 91632    |
---------------------------------
Eval num_timesteps=93624, episode_reward=-300.00 +/- 0.00
Episode length: 204.60 +/- 45.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 93624    |
---------------------------------
Eval num_timesteps=95616, episode_reward=-300.00 +/- 0.00
Episode length: 222.40 +/- 42.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 95616    |
---------------------------------
Eval num_timesteps=97608, episode_reward=-300.00 +/- 0.00
Episode length: 221.60 +/- 44.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 97608    |
---------------------------------
Eval num_timesteps=99600, episode_reward=-300.00 +/- 0.00
Episode length: 237.80 +/- 30.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 99600       |
| train/                  |             |
|    approx_kl            | 0.005183922 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 0.183       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00255    |
|    std                  | 1.04        |
|    value_loss           | 0.567       |
-----------------------------------------
Eval num_timesteps=101592, episode_reward=-300.00 +/- 0.00
Episode length: 236.20 +/- 90.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 101592   |
---------------------------------
Eval num_timesteps=103584, episode_reward=-300.00 +/- 0.00
Episode length: 268.20 +/- 112.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 103584   |
---------------------------------
Eval num_timesteps=105576, episode_reward=-300.00 +/- 0.00
Episode length: 260.20 +/- 63.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 105576   |
---------------------------------
Eval num_timesteps=107568, episode_reward=-300.00 +/- 0.00
Episode length: 214.60 +/- 36.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 107568   |
---------------------------------
Eval num_timesteps=109560, episode_reward=-300.00 +/- 0.00
Episode length: 274.60 +/- 67.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 109560   |
---------------------------------
Eval num_timesteps=111552, episode_reward=-300.00 +/- 0.00
Episode length: 233.40 +/- 62.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 111552   |
---------------------------------
Eval num_timesteps=113544, episode_reward=-300.00 +/- 0.00
Episode length: 232.20 +/- 27.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 113544   |
---------------------------------
Eval num_timesteps=115536, episode_reward=-300.00 +/- 0.00
Episode length: 280.20 +/- 91.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 115536   |
---------------------------------
Eval num_timesteps=117528, episode_reward=-300.00 +/- 0.00
Episode length: 229.00 +/- 42.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 117528   |
---------------------------------
Eval num_timesteps=119520, episode_reward=-300.00 +/- 0.00
Episode length: 291.00 +/- 100.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 119520   |
---------------------------------
Eval num_timesteps=121512, episode_reward=-300.00 +/- 0.00
Episode length: 222.60 +/- 31.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 121512   |
---------------------------------
Eval num_timesteps=123504, episode_reward=-300.00 +/- 0.00
Episode length: 275.80 +/- 66.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 123504   |
---------------------------------
Eval num_timesteps=125496, episode_reward=-300.00 +/- 0.00
Episode length: 222.00 +/- 71.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 125496   |
---------------------------------
Eval num_timesteps=127488, episode_reward=-300.00 +/- 0.00
Episode length: 224.00 +/- 51.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 127488   |
---------------------------------
Eval num_timesteps=129480, episode_reward=-300.00 +/- 0.00
Episode length: 232.80 +/- 48.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 129480   |
---------------------------------
Eval num_timesteps=131472, episode_reward=-300.00 +/- 0.00
Episode length: 255.80 +/- 63.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 131472   |
---------------------------------
Eval num_timesteps=133464, episode_reward=-300.00 +/- 0.00
Episode length: 215.80 +/- 41.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 133464   |
---------------------------------
Eval num_timesteps=135456, episode_reward=-300.00 +/- 0.00
Episode length: 244.00 +/- 26.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 135456   |
---------------------------------
Eval num_timesteps=137448, episode_reward=-300.00 +/- 0.00
Episode length: 246.60 +/- 44.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 137448   |
---------------------------------
Eval num_timesteps=139440, episode_reward=-300.00 +/- 0.00
Episode length: 243.80 +/- 100.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 139440   |
---------------------------------
Eval num_timesteps=141432, episode_reward=-300.00 +/- 0.00
Episode length: 221.60 +/- 43.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 141432   |
---------------------------------
Eval num_timesteps=143424, episode_reward=-300.00 +/- 0.00
Episode length: 204.20 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 143424   |
---------------------------------
Eval num_timesteps=145416, episode_reward=-300.00 +/- 0.00
Episode length: 249.00 +/- 61.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 145416   |
---------------------------------
Eval num_timesteps=147408, episode_reward=-300.00 +/- 0.00
Episode length: 217.60 +/- 32.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 147408   |
---------------------------------
Eval num_timesteps=149400, episode_reward=-300.00 +/- 0.00
Episode length: 246.60 +/- 56.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 149400       |
| train/                  |              |
|    approx_kl            | 0.0049744956 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0883       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00253     |
|    std                  | 1.05         |
|    value_loss           | 0.325        |
------------------------------------------
Eval num_timesteps=151392, episode_reward=-300.00 +/- 0.00
Episode length: 254.40 +/- 54.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 151392   |
---------------------------------
Eval num_timesteps=153384, episode_reward=-300.00 +/- 0.00
Episode length: 213.60 +/- 38.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 153384   |
---------------------------------
Eval num_timesteps=155376, episode_reward=-300.00 +/- 0.00
Episode length: 276.80 +/- 66.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 155376   |
---------------------------------
Eval num_timesteps=157368, episode_reward=-300.00 +/- 0.00
Episode length: 219.60 +/- 49.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 157368   |
---------------------------------
Eval num_timesteps=159360, episode_reward=-300.00 +/- 0.00
Episode length: 255.40 +/- 65.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 159360   |
---------------------------------
Eval num_timesteps=161352, episode_reward=-300.00 +/- 0.00
Episode length: 249.80 +/- 71.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 161352   |
---------------------------------
Eval num_timesteps=163344, episode_reward=-300.00 +/- 0.00
Episode length: 226.60 +/- 39.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 163344   |
---------------------------------
Eval num_timesteps=165336, episode_reward=-300.00 +/- 0.00
Episode length: 229.80 +/- 36.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 165336   |
---------------------------------
Eval num_timesteps=167328, episode_reward=-300.00 +/- 0.00
Episode length: 220.00 +/- 55.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 167328   |
---------------------------------
Eval num_timesteps=169320, episode_reward=-300.00 +/- 0.00
Episode length: 250.20 +/- 45.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 169320   |
---------------------------------
Eval num_timesteps=171312, episode_reward=-300.00 +/- 0.00
Episode length: 243.20 +/- 56.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 171312   |
---------------------------------
Eval num_timesteps=173304, episode_reward=-300.00 +/- 0.00
Episode length: 297.00 +/- 60.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 173304   |
---------------------------------
Eval num_timesteps=175296, episode_reward=-300.00 +/- 0.00
Episode length: 232.40 +/- 59.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 175296   |
---------------------------------
Eval num_timesteps=177288, episode_reward=-300.00 +/- 0.00
Episode length: 221.60 +/- 50.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 177288   |
---------------------------------
Eval num_timesteps=179280, episode_reward=-300.00 +/- 0.00
Episode length: 198.80 +/- 25.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 179280   |
---------------------------------
Eval num_timesteps=181272, episode_reward=-300.00 +/- 0.00
Episode length: 230.80 +/- 58.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 181272   |
---------------------------------
Eval num_timesteps=183264, episode_reward=-300.00 +/- 0.00
Episode length: 229.40 +/- 26.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 183264   |
---------------------------------
Eval num_timesteps=185256, episode_reward=-300.00 +/- 0.00
Episode length: 211.40 +/- 46.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 185256   |
---------------------------------
Eval num_timesteps=187248, episode_reward=-300.00 +/- 0.00
Episode length: 229.80 +/- 46.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 187248   |
---------------------------------
Eval num_timesteps=189240, episode_reward=-300.00 +/- 0.00
Episode length: 244.80 +/- 64.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 189240   |
---------------------------------
Eval num_timesteps=191232, episode_reward=-300.00 +/- 0.00
Episode length: 243.80 +/- 40.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 191232   |
---------------------------------
Eval num_timesteps=193224, episode_reward=-300.00 +/- 0.00
Episode length: 260.20 +/- 49.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 193224   |
---------------------------------
Eval num_timesteps=195216, episode_reward=-300.00 +/- 0.00
Episode length: 229.00 +/- 79.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 195216   |
---------------------------------
Eval num_timesteps=197208, episode_reward=-300.00 +/- 0.00
Episode length: 309.00 +/- 102.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 309          |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 197208       |
| train/                  |              |
|    approx_kl            | 0.0059443736 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0296       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00474     |
|    std                  | 1.06         |
|    value_loss           | 0.224        |
------------------------------------------
Eval num_timesteps=199200, episode_reward=-300.00 +/- 0.00
Episode length: 288.40 +/- 37.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 199200   |
---------------------------------
Eval num_timesteps=201192, episode_reward=-300.00 +/- 0.00
Episode length: 349.80 +/- 86.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 201192   |
---------------------------------
Eval num_timesteps=203184, episode_reward=-300.00 +/- 0.00
Episode length: 321.40 +/- 60.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 321      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 203184   |
---------------------------------
Eval num_timesteps=205176, episode_reward=-300.00 +/- 0.00
Episode length: 291.20 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 205176   |
---------------------------------
Eval num_timesteps=207168, episode_reward=-300.00 +/- 0.00
Episode length: 304.00 +/- 96.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 207168   |
---------------------------------
Eval num_timesteps=209160, episode_reward=-300.00 +/- 0.00
Episode length: 266.20 +/- 23.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 209160   |
---------------------------------
Eval num_timesteps=211152, episode_reward=-300.00 +/- 0.00
Episode length: 317.00 +/- 65.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 211152   |
---------------------------------
Eval num_timesteps=213144, episode_reward=-300.00 +/- 0.00
Episode length: 285.60 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 213144   |
---------------------------------
Eval num_timesteps=215136, episode_reward=-300.00 +/- 0.00
Episode length: 298.40 +/- 76.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 215136   |
---------------------------------
Eval num_timesteps=217128, episode_reward=-300.00 +/- 0.00
Episode length: 266.00 +/- 27.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 217128   |
---------------------------------
Eval num_timesteps=219120, episode_reward=-300.00 +/- 0.00
Episode length: 284.40 +/- 120.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 219120   |
---------------------------------
Eval num_timesteps=221112, episode_reward=-300.00 +/- 0.00
Episode length: 299.00 +/- 77.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 221112   |
---------------------------------
Eval num_timesteps=223104, episode_reward=-300.00 +/- 0.00
Episode length: 405.40 +/- 53.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 405      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 223104   |
---------------------------------
Eval num_timesteps=225096, episode_reward=-300.00 +/- 0.00
Episode length: 339.20 +/- 122.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 225096   |
---------------------------------
Eval num_timesteps=227088, episode_reward=-300.00 +/- 0.00
Episode length: 247.20 +/- 70.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 227088   |
---------------------------------
Eval num_timesteps=229080, episode_reward=-300.00 +/- 0.00
Episode length: 309.60 +/- 57.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 229080   |
---------------------------------
Eval num_timesteps=231072, episode_reward=-300.00 +/- 0.00
Episode length: 404.20 +/- 115.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 404      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 231072   |
---------------------------------
Eval num_timesteps=233064, episode_reward=-300.00 +/- 0.00
Episode length: 268.20 +/- 119.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 233064   |
---------------------------------
Eval num_timesteps=235056, episode_reward=-300.00 +/- 0.00
Episode length: 330.40 +/- 104.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 235056   |
---------------------------------
Eval num_timesteps=237048, episode_reward=-300.00 +/- 0.00
Episode length: 278.60 +/- 41.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 237048   |
---------------------------------
Eval num_timesteps=239040, episode_reward=-300.00 +/- 0.00
Episode length: 282.40 +/- 84.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 239040   |
---------------------------------
Eval num_timesteps=241032, episode_reward=-300.00 +/- 0.00
Episode length: 305.80 +/- 49.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 241032   |
---------------------------------
Eval num_timesteps=243024, episode_reward=-300.00 +/- 0.00
Episode length: 383.60 +/- 109.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 243024   |
---------------------------------
Eval num_timesteps=245016, episode_reward=-300.00 +/- 0.00
Episode length: 306.60 +/- 52.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 245016   |
---------------------------------
Eval num_timesteps=247008, episode_reward=-300.00 +/- 0.00
Episode length: 518.80 +/- 159.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 519         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 247008      |
| train/                  |             |
|    approx_kl            | 0.008250001 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 0.024       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00799    |
|    std                  | 1.06        |
|    value_loss           | 0.192       |
-----------------------------------------
Eval num_timesteps=249000, episode_reward=-300.00 +/- 0.00
Episode length: 421.00 +/- 101.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=250992, episode_reward=-300.00 +/- 0.00
Episode length: 365.60 +/- 67.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 366      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 250992   |
---------------------------------
Eval num_timesteps=252984, episode_reward=-300.00 +/- 0.00
Episode length: 574.80 +/- 132.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 575      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 252984   |
---------------------------------
Eval num_timesteps=254976, episode_reward=-300.00 +/- 0.00
Episode length: 389.20 +/- 147.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=256968, episode_reward=-300.00 +/- 0.00
Episode length: 591.80 +/- 137.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 256968   |
---------------------------------
Eval num_timesteps=258960, episode_reward=-300.00 +/- 0.00
Episode length: 407.80 +/- 104.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 408      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 258960   |
---------------------------------
Eval num_timesteps=260952, episode_reward=-300.00 +/- 0.00
Episode length: 501.60 +/- 105.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 502      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 260952   |
---------------------------------
Eval num_timesteps=262944, episode_reward=-300.00 +/- 0.00
Episode length: 469.40 +/- 96.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 262944   |
---------------------------------
Eval num_timesteps=264936, episode_reward=-300.00 +/- 0.00
Episode length: 491.20 +/- 236.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 264936   |
---------------------------------
Eval num_timesteps=266928, episode_reward=-300.00 +/- 0.00
Episode length: 487.60 +/- 99.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 266928   |
---------------------------------
Eval num_timesteps=268920, episode_reward=-300.00 +/- 0.00
Episode length: 579.00 +/- 92.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 579      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 268920   |
---------------------------------
Eval num_timesteps=270912, episode_reward=-300.00 +/- 0.00
Episode length: 512.40 +/- 132.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 270912   |
---------------------------------
Eval num_timesteps=272904, episode_reward=-300.00 +/- 0.00
Episode length: 504.20 +/- 91.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 272904   |
---------------------------------
Eval num_timesteps=274896, episode_reward=-300.00 +/- 0.00
Episode length: 433.20 +/- 86.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 274896   |
---------------------------------
Eval num_timesteps=276888, episode_reward=-300.00 +/- 0.00
Episode length: 431.00 +/- 70.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 276888   |
---------------------------------
Eval num_timesteps=278880, episode_reward=-300.00 +/- 0.00
Episode length: 540.60 +/- 190.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 541      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 278880   |
---------------------------------
Eval num_timesteps=280872, episode_reward=-300.00 +/- 0.00
Episode length: 505.80 +/- 112.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 280872   |
---------------------------------
Eval num_timesteps=282864, episode_reward=-300.00 +/- 0.00
Episode length: 486.80 +/- 178.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 282864   |
---------------------------------
Eval num_timesteps=284856, episode_reward=-300.00 +/- 0.00
Episode length: 537.00 +/- 175.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 537      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 284856   |
---------------------------------
Eval num_timesteps=286848, episode_reward=-300.00 +/- 0.00
Episode length: 524.20 +/- 240.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 286848   |
---------------------------------
Eval num_timesteps=288840, episode_reward=-300.00 +/- 0.00
Episode length: 457.40 +/- 228.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 288840   |
---------------------------------
Eval num_timesteps=290832, episode_reward=-300.00 +/- 0.00
Episode length: 347.40 +/- 61.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 290832   |
---------------------------------
Eval num_timesteps=292824, episode_reward=-300.00 +/- 0.00
Episode length: 466.60 +/- 127.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 292824   |
---------------------------------
Eval num_timesteps=294816, episode_reward=-300.00 +/- 0.00
Episode length: 539.80 +/- 145.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 540      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 294816   |
---------------------------------
Eval num_timesteps=296808, episode_reward=-300.00 +/- 0.00
Episode length: 767.80 +/- 110.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 768         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 296808      |
| train/                  |             |
|    approx_kl            | 0.009869476 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | -0.00517    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0084     |
|    std                  | 1.06        |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=298800, episode_reward=-300.00 +/- 0.00
Episode length: 718.20 +/- 136.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 298800   |
---------------------------------
Eval num_timesteps=300792, episode_reward=-300.00 +/- 0.00
Episode length: 758.80 +/- 162.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 300792   |
---------------------------------
Eval num_timesteps=302784, episode_reward=-300.00 +/- 0.00
Episode length: 654.60 +/- 143.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 655      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 302784   |
---------------------------------
Eval num_timesteps=304776, episode_reward=-300.00 +/- 0.00
Episode length: 559.40 +/- 121.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 559      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 304776   |
---------------------------------
Eval num_timesteps=306768, episode_reward=-300.00 +/- 0.00
Episode length: 666.40 +/- 103.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 666      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 306768   |
---------------------------------
Eval num_timesteps=308760, episode_reward=-300.00 +/- 0.00
Episode length: 642.00 +/- 124.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 308760   |
---------------------------------
Eval num_timesteps=310752, episode_reward=-300.00 +/- 0.00
Episode length: 749.20 +/- 110.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 310752   |
---------------------------------
Eval num_timesteps=312744, episode_reward=-300.00 +/- 0.00
Episode length: 667.20 +/- 153.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 312744   |
---------------------------------
Eval num_timesteps=314736, episode_reward=-300.00 +/- 0.00
Episode length: 643.40 +/- 164.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 314736   |
---------------------------------
Eval num_timesteps=316728, episode_reward=-300.00 +/- 0.00
Episode length: 485.20 +/- 158.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 316728   |
---------------------------------
Eval num_timesteps=318720, episode_reward=-300.00 +/- 0.00
Episode length: 698.60 +/- 88.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 699      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 318720   |
---------------------------------
Eval num_timesteps=320712, episode_reward=-300.00 +/- 0.00
Episode length: 574.20 +/- 173.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 574      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 320712   |
---------------------------------
Eval num_timesteps=322704, episode_reward=-300.00 +/- 0.00
Episode length: 682.80 +/- 84.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 683      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 322704   |
---------------------------------
Eval num_timesteps=324696, episode_reward=-300.00 +/- 0.00
Episode length: 669.60 +/- 248.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 324696   |
---------------------------------
Eval num_timesteps=326688, episode_reward=-300.00 +/- 0.00
Episode length: 676.40 +/- 150.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 326688   |
---------------------------------
Eval num_timesteps=328680, episode_reward=-300.00 +/- 0.00
Episode length: 693.40 +/- 129.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 693      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 328680   |
---------------------------------
Eval num_timesteps=330672, episode_reward=-300.00 +/- 0.00
Episode length: 532.00 +/- 201.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 532      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 330672   |
---------------------------------
Eval num_timesteps=332664, episode_reward=-300.00 +/- 0.00
Episode length: 684.00 +/- 151.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 332664   |
---------------------------------
Eval num_timesteps=334656, episode_reward=-300.00 +/- 0.00
Episode length: 743.00 +/- 124.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 334656   |
---------------------------------
Eval num_timesteps=336648, episode_reward=-300.00 +/- 0.00
Episode length: 642.60 +/- 244.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 336648   |
---------------------------------
Eval num_timesteps=338640, episode_reward=-300.00 +/- 0.00
Episode length: 663.80 +/- 152.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 664      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 338640   |
---------------------------------
Eval num_timesteps=340632, episode_reward=-300.00 +/- 0.00
Episode length: 610.20 +/- 98.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 610      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 340632   |
---------------------------------
Eval num_timesteps=342624, episode_reward=-300.00 +/- 0.00
Episode length: 679.20 +/- 43.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 679      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 342624   |
---------------------------------
Eval num_timesteps=344616, episode_reward=-300.00 +/- 0.00
Episode length: 615.40 +/- 150.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 615        |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 344616     |
| train/                  |            |
|    approx_kl            | 0.00864542 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.93      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.001      |
|    loss                 | -0.00838   |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00558   |
|    std                  | 1.07       |
|    value_loss           | 0.208      |
----------------------------------------
Eval num_timesteps=346608, episode_reward=-300.00 +/- 0.00
Episode length: 759.60 +/- 161.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 346608   |
---------------------------------
Eval num_timesteps=348600, episode_reward=-300.00 +/- 0.00
Episode length: 558.40 +/- 127.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 558      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 348600   |
---------------------------------
Eval num_timesteps=350592, episode_reward=-300.00 +/- 0.00
Episode length: 835.00 +/- 144.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 350592   |
---------------------------------
Eval num_timesteps=352584, episode_reward=-300.00 +/- 0.00
Episode length: 738.20 +/- 126.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 352584   |
---------------------------------
Eval num_timesteps=354576, episode_reward=-300.00 +/- 0.00
Episode length: 661.40 +/- 139.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 354576   |
---------------------------------
Eval num_timesteps=356568, episode_reward=-300.00 +/- 0.00
Episode length: 740.40 +/- 54.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 356568   |
---------------------------------
Eval num_timesteps=358560, episode_reward=-300.00 +/- 0.00
Episode length: 618.20 +/- 95.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 358560   |
---------------------------------
Eval num_timesteps=360552, episode_reward=-300.00 +/- 0.00
Episode length: 599.60 +/- 97.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 360552   |
---------------------------------
Eval num_timesteps=362544, episode_reward=-300.00 +/- 0.00
Episode length: 729.40 +/- 155.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 362544   |
---------------------------------
Eval num_timesteps=364536, episode_reward=-300.00 +/- 0.00
Episode length: 773.00 +/- 192.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 773      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 364536   |
---------------------------------
Eval num_timesteps=366528, episode_reward=-300.00 +/- 0.00
Episode length: 724.20 +/- 80.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 366528   |
---------------------------------
Eval num_timesteps=368520, episode_reward=-300.00 +/- 0.00
Episode length: 610.60 +/- 42.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 611      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 368520   |
---------------------------------
Eval num_timesteps=370512, episode_reward=-300.00 +/- 0.00
Episode length: 614.00 +/- 68.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 614      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 370512   |
---------------------------------
Eval num_timesteps=372504, episode_reward=-300.00 +/- 0.00
Episode length: 739.80 +/- 227.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 740      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 372504   |
---------------------------------
Eval num_timesteps=374496, episode_reward=-300.00 +/- 0.00
Episode length: 637.40 +/- 84.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 374496   |
---------------------------------
Eval num_timesteps=376488, episode_reward=-300.00 +/- 0.00
Episode length: 755.40 +/- 151.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 376488   |
---------------------------------
Eval num_timesteps=378480, episode_reward=-300.00 +/- 0.00
Episode length: 776.40 +/- 222.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 776      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 378480   |
---------------------------------
Eval num_timesteps=380472, episode_reward=-300.00 +/- 0.00
Episode length: 712.00 +/- 76.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 380472   |
---------------------------------
Eval num_timesteps=382464, episode_reward=-300.00 +/- 0.00
Episode length: 741.00 +/- 45.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 382464   |
---------------------------------
Eval num_timesteps=384456, episode_reward=-300.00 +/- 0.00
Episode length: 738.60 +/- 176.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 739      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 384456   |
---------------------------------
Eval num_timesteps=386448, episode_reward=-300.00 +/- 0.00
Episode length: 729.20 +/- 183.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 386448   |
---------------------------------
Eval num_timesteps=388440, episode_reward=-300.00 +/- 0.00
Episode length: 592.00 +/- 116.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 388440   |
---------------------------------
Eval num_timesteps=390432, episode_reward=-300.00 +/- 0.00
Episode length: 707.20 +/- 92.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 390432   |
---------------------------------
Eval num_timesteps=392424, episode_reward=-300.00 +/- 0.00
Episode length: 623.00 +/- 106.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 623      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 392424   |
---------------------------------
Eval num_timesteps=394416, episode_reward=-300.00 +/- 0.00
Episode length: 796.60 +/- 120.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 797         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 394416      |
| train/                  |             |
|    approx_kl            | 0.009484685 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0284      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00475    |
|    std                  | 1.08        |
|    value_loss           | 0.2         |
-----------------------------------------
Eval num_timesteps=396408, episode_reward=-300.00 +/- 0.00
Episode length: 674.60 +/- 48.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 396408   |
---------------------------------
Eval num_timesteps=398400, episode_reward=-300.00 +/- 0.00
Episode length: 686.60 +/- 126.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 398400   |
---------------------------------
Eval num_timesteps=400392, episode_reward=-300.00 +/- 0.00
Episode length: 711.60 +/- 87.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 400392   |
---------------------------------
Eval num_timesteps=402384, episode_reward=-300.00 +/- 0.00
Episode length: 612.00 +/- 52.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 612      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 402384   |
---------------------------------
Eval num_timesteps=404376, episode_reward=-300.00 +/- 0.00
Episode length: 619.40 +/- 96.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 619      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 404376   |
---------------------------------
Eval num_timesteps=406368, episode_reward=-300.00 +/- 0.00
Episode length: 737.60 +/- 176.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 406368   |
---------------------------------
Eval num_timesteps=408360, episode_reward=-300.00 +/- 0.00
Episode length: 722.00 +/- 153.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 408360   |
---------------------------------
Eval num_timesteps=410352, episode_reward=-300.00 +/- 0.00
Episode length: 698.00 +/- 144.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 410352   |
---------------------------------
Eval num_timesteps=412344, episode_reward=-300.00 +/- 0.00
Episode length: 664.80 +/- 86.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 665      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 412344   |
---------------------------------
Eval num_timesteps=414336, episode_reward=-300.00 +/- 0.00
Episode length: 731.80 +/- 144.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 414336   |
---------------------------------
Eval num_timesteps=416328, episode_reward=-300.00 +/- 0.00
Episode length: 651.20 +/- 137.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 651      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 416328   |
---------------------------------
Eval num_timesteps=418320, episode_reward=-300.00 +/- 0.00
Episode length: 714.20 +/- 153.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 714      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 418320   |
---------------------------------
Eval num_timesteps=420312, episode_reward=-300.00 +/- 0.00
Episode length: 831.20 +/- 178.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 831      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 420312   |
---------------------------------
Eval num_timesteps=422304, episode_reward=-300.00 +/- 0.00
Episode length: 716.00 +/- 80.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 716      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 422304   |
---------------------------------
Eval num_timesteps=424296, episode_reward=-300.00 +/- 0.00
Episode length: 674.20 +/- 160.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 424296   |
---------------------------------
Eval num_timesteps=426288, episode_reward=-300.00 +/- 0.00
Episode length: 810.80 +/- 262.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 426288   |
---------------------------------
Eval num_timesteps=428280, episode_reward=-300.00 +/- 0.00
Episode length: 603.60 +/- 85.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 428280   |
---------------------------------
Eval num_timesteps=430272, episode_reward=-300.00 +/- 0.00
Episode length: 686.80 +/- 125.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 430272   |
---------------------------------
Eval num_timesteps=432264, episode_reward=-300.00 +/- 0.00
Episode length: 694.00 +/- 221.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 432264   |
---------------------------------
Eval num_timesteps=434256, episode_reward=-300.00 +/- 0.00
Episode length: 663.20 +/- 42.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 663      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 434256   |
---------------------------------
Eval num_timesteps=436248, episode_reward=-300.00 +/- 0.00
Episode length: 731.20 +/- 144.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 731      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 436248   |
---------------------------------
Eval num_timesteps=438240, episode_reward=-300.00 +/- 0.00
Episode length: 764.80 +/- 162.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 438240   |
---------------------------------
Eval num_timesteps=440232, episode_reward=-300.00 +/- 0.00
Episode length: 661.20 +/- 60.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 661      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 440232   |
---------------------------------
Eval num_timesteps=442224, episode_reward=-300.00 +/- 0.00
Episode length: 687.80 +/- 97.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 442224   |
---------------------------------
Eval num_timesteps=444216, episode_reward=-300.00 +/- 0.00
Episode length: 681.20 +/- 125.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 681        |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 444216     |
| train/                  |            |
|    approx_kl            | 0.00872225 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.02      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0324    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.00546   |
|    std                  | 1.09       |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=446208, episode_reward=-300.00 +/- 0.00
Episode length: 902.80 +/- 235.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 903      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 446208   |
---------------------------------
Eval num_timesteps=448200, episode_reward=-300.00 +/- 0.00
Episode length: 814.40 +/- 100.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 448200   |
---------------------------------
Eval num_timesteps=450192, episode_reward=-300.00 +/- 0.00
Episode length: 762.80 +/- 152.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 450192   |
---------------------------------
Eval num_timesteps=452184, episode_reward=-300.00 +/- 0.00
Episode length: 672.40 +/- 119.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 672      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 452184   |
---------------------------------
Eval num_timesteps=454176, episode_reward=-300.00 +/- 0.00
Episode length: 789.80 +/- 54.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 790      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 454176   |
---------------------------------
Eval num_timesteps=456168, episode_reward=-300.00 +/- 0.00
Episode length: 643.20 +/- 106.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 643      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 456168   |
---------------------------------
Eval num_timesteps=458160, episode_reward=-300.00 +/- 0.00
Episode length: 640.00 +/- 66.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 458160   |
---------------------------------
Eval num_timesteps=460152, episode_reward=-300.00 +/- 0.00
Episode length: 679.60 +/- 96.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 460152   |
---------------------------------
Eval num_timesteps=462144, episode_reward=-300.00 +/- 0.00
Episode length: 772.20 +/- 79.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 462144   |
---------------------------------
Eval num_timesteps=464136, episode_reward=-300.00 +/- 0.00
Episode length: 602.80 +/- 112.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 464136   |
---------------------------------
Eval num_timesteps=466128, episode_reward=-300.00 +/- 0.00
Episode length: 720.60 +/- 156.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 466128   |
---------------------------------
Eval num_timesteps=468120, episode_reward=-300.00 +/- 0.00
Episode length: 881.60 +/- 169.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 882      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 468120   |
---------------------------------
Eval num_timesteps=470112, episode_reward=-300.00 +/- 0.00
Episode length: 595.60 +/- 111.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 596      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 470112   |
---------------------------------
Eval num_timesteps=472104, episode_reward=-300.00 +/- 0.00
Episode length: 602.80 +/- 81.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 603      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 472104   |
---------------------------------
Eval num_timesteps=474096, episode_reward=-300.00 +/- 0.00
Episode length: 755.60 +/- 134.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 756      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 474096   |
---------------------------------
Eval num_timesteps=476088, episode_reward=-300.00 +/- 0.00
Episode length: 788.00 +/- 145.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 788      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 476088   |
---------------------------------
Eval num_timesteps=478080, episode_reward=-300.00 +/- 0.00
Episode length: 641.80 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 642      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 478080   |
---------------------------------
Eval num_timesteps=480072, episode_reward=-300.00 +/- 0.00
Episode length: 666.60 +/- 131.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 480072   |
---------------------------------
Eval num_timesteps=482064, episode_reward=-300.00 +/- 0.00
Episode length: 581.60 +/- 72.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 582      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 482064   |
---------------------------------
Eval num_timesteps=484056, episode_reward=-300.00 +/- 0.00
Episode length: 692.00 +/- 118.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 692      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 484056   |
---------------------------------
Eval num_timesteps=486048, episode_reward=-300.00 +/- 0.00
Episode length: 774.20 +/- 96.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 486048   |
---------------------------------
Eval num_timesteps=488040, episode_reward=-300.00 +/- 0.00
Episode length: 635.00 +/- 98.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 488040   |
---------------------------------
Eval num_timesteps=490032, episode_reward=-300.00 +/- 0.00
Episode length: 718.40 +/- 87.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 490032   |
---------------------------------
Eval num_timesteps=492024, episode_reward=-300.00 +/- 0.00
Episode length: 701.20 +/- 121.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 701         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 492024      |
| train/                  |             |
|    approx_kl            | 0.009372122 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0463     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00647    |
|    std                  | 1.1         |
|    value_loss           | 0.0816      |
-----------------------------------------
Eval num_timesteps=494016, episode_reward=-300.00 +/- 0.00
Episode length: 709.00 +/- 151.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 494016   |
---------------------------------
Eval num_timesteps=496008, episode_reward=-300.00 +/- 0.00
Episode length: 746.60 +/- 178.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 747      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 496008   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-300.00 +/- 0.00
Episode length: 698.00 +/- 93.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 698      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=499992, episode_reward=-300.00 +/- 0.00
Episode length: 644.20 +/- 104.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 644      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 499992   |
---------------------------------
Eval num_timesteps=501984, episode_reward=-300.00 +/- 0.00
Episode length: 803.40 +/- 145.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 501984   |
---------------------------------
Eval num_timesteps=503976, episode_reward=-300.00 +/- 0.00
Episode length: 817.00 +/- 142.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 503976   |
---------------------------------
Eval num_timesteps=505968, episode_reward=-300.00 +/- 0.00
Episode length: 669.80 +/- 71.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 505968   |
---------------------------------
Eval num_timesteps=507960, episode_reward=-300.00 +/- 0.00
Episode length: 687.80 +/- 71.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 688      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 507960   |
---------------------------------
Eval num_timesteps=509952, episode_reward=-300.00 +/- 0.00
Episode length: 814.20 +/- 161.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=511944, episode_reward=-300.00 +/- 0.00
Episode length: 720.40 +/- 193.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 511944   |
---------------------------------
Eval num_timesteps=513936, episode_reward=-300.00 +/- 0.00
Episode length: 747.80 +/- 175.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 513936   |
---------------------------------
Eval num_timesteps=515928, episode_reward=-300.00 +/- 0.00
Episode length: 737.80 +/- 144.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 738      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 515928   |
---------------------------------
Eval num_timesteps=517920, episode_reward=-300.00 +/- 0.00
Episode length: 719.40 +/- 72.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 719      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 517920   |
---------------------------------
Eval num_timesteps=519912, episode_reward=-300.00 +/- 0.00
Episode length: 696.00 +/- 160.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 519912   |
---------------------------------
Eval num_timesteps=521904, episode_reward=-300.00 +/- 0.00
Episode length: 765.00 +/- 140.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 765      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 521904   |
---------------------------------
Eval num_timesteps=523896, episode_reward=-300.00 +/- 0.00
Episode length: 716.80 +/- 63.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 523896   |
---------------------------------
Eval num_timesteps=525888, episode_reward=-300.00 +/- 0.00
Episode length: 779.60 +/- 111.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 525888   |
---------------------------------
Eval num_timesteps=527880, episode_reward=-300.00 +/- 0.00
Episode length: 728.40 +/- 173.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 728      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 527880   |
---------------------------------
Eval num_timesteps=529872, episode_reward=-300.00 +/- 0.00
Episode length: 686.00 +/- 193.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 686      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 529872   |
---------------------------------
Eval num_timesteps=531864, episode_reward=-300.00 +/- 0.00
Episode length: 758.60 +/- 108.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 759      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 531864   |
---------------------------------
Eval num_timesteps=533856, episode_reward=-300.00 +/- 0.00
Episode length: 697.40 +/- 98.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 533856   |
---------------------------------
Eval num_timesteps=535848, episode_reward=-300.00 +/- 0.00
Episode length: 681.60 +/- 132.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 535848   |
---------------------------------
Eval num_timesteps=537840, episode_reward=-300.00 +/- 0.00
Episode length: 691.40 +/- 75.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 537840   |
---------------------------------
Eval num_timesteps=539832, episode_reward=-300.00 +/- 0.00
Episode length: 809.60 +/- 300.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 539832   |
---------------------------------
Eval num_timesteps=541824, episode_reward=-300.00 +/- 0.00
Episode length: 833.20 +/- 198.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 833         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 541824      |
| train/                  |             |
|    approx_kl            | 0.010508324 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.08       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0348     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00569    |
|    std                  | 1.11        |
|    value_loss           | 0.0929      |
-----------------------------------------
Eval num_timesteps=543816, episode_reward=-300.00 +/- 0.00
Episode length: 1060.20 +/- 204.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 543816   |
---------------------------------
Eval num_timesteps=545808, episode_reward=-300.00 +/- 0.00
Episode length: 804.00 +/- 58.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 545808   |
---------------------------------
Eval num_timesteps=547800, episode_reward=-300.00 +/- 0.00
Episode length: 929.60 +/- 202.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 547800   |
---------------------------------
Eval num_timesteps=549792, episode_reward=-300.00 +/- 0.00
Episode length: 869.00 +/- 186.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 549792   |
---------------------------------
Eval num_timesteps=551784, episode_reward=-300.00 +/- 0.00
Episode length: 839.40 +/- 128.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 551784   |
---------------------------------
Eval num_timesteps=553776, episode_reward=-300.00 +/- 0.00
Episode length: 607.80 +/- 213.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 608      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 553776   |
---------------------------------
Eval num_timesteps=555768, episode_reward=-300.00 +/- 0.00
Episode length: 845.00 +/- 94.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 845      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 555768   |
---------------------------------
Eval num_timesteps=557760, episode_reward=-300.00 +/- 0.00
Episode length: 601.40 +/- 55.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 601      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 557760   |
---------------------------------
Eval num_timesteps=559752, episode_reward=-300.00 +/- 0.00
Episode length: 1001.20 +/- 324.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 559752   |
---------------------------------
Eval num_timesteps=561744, episode_reward=-300.00 +/- 0.00
Episode length: 1115.40 +/- 223.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 561744   |
---------------------------------
Eval num_timesteps=563736, episode_reward=-300.00 +/- 0.00
Episode length: 733.20 +/- 66.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 733      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 563736   |
---------------------------------
Eval num_timesteps=565728, episode_reward=-300.00 +/- 0.00
Episode length: 840.80 +/- 203.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 565728   |
---------------------------------
Eval num_timesteps=567720, episode_reward=-300.00 +/- 0.00
Episode length: 687.20 +/- 118.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 567720   |
---------------------------------
Eval num_timesteps=569712, episode_reward=-300.00 +/- 0.00
Episode length: 735.40 +/- 169.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 569712   |
---------------------------------
Eval num_timesteps=571704, episode_reward=-300.00 +/- 0.00
Episode length: 850.60 +/- 316.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 851      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 571704   |
---------------------------------
Eval num_timesteps=573696, episode_reward=-300.00 +/- 0.00
Episode length: 646.40 +/- 115.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 573696   |
---------------------------------
Eval num_timesteps=575688, episode_reward=-300.00 +/- 0.00
Episode length: 833.20 +/- 207.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 575688   |
---------------------------------
Eval num_timesteps=577680, episode_reward=-300.00 +/- 0.00
Episode length: 733.60 +/- 66.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 734      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 577680   |
---------------------------------
Eval num_timesteps=579672, episode_reward=-300.00 +/- 0.00
Episode length: 867.00 +/- 389.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 579672   |
---------------------------------
Eval num_timesteps=581664, episode_reward=-300.00 +/- 0.00
Episode length: 660.00 +/- 53.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 581664   |
---------------------------------
Eval num_timesteps=583656, episode_reward=-300.00 +/- 0.00
Episode length: 832.20 +/- 163.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 583656   |
---------------------------------
Eval num_timesteps=585648, episode_reward=-300.00 +/- 0.00
Episode length: 878.60 +/- 316.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 585648   |
---------------------------------
Eval num_timesteps=587640, episode_reward=-300.00 +/- 0.00
Episode length: 706.20 +/- 133.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 587640   |
---------------------------------
Eval num_timesteps=589632, episode_reward=-300.00 +/- 0.00
Episode length: 780.00 +/- 190.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 780      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 589632   |
---------------------------------
Eval num_timesteps=591624, episode_reward=-300.00 +/- 0.00
Episode length: 1167.40 +/- 434.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.17e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 591624       |
| train/                  |              |
|    approx_kl            | 0.0101212775 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0274      |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00628     |
|    std                  | 1.12         |
|    value_loss           | 0.0891       |
------------------------------------------
Eval num_timesteps=593616, episode_reward=-300.00 +/- 0.00
Episode length: 833.20 +/- 187.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 593616   |
---------------------------------
Eval num_timesteps=595608, episode_reward=-300.00 +/- 0.00
Episode length: 976.20 +/- 178.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 595608   |
---------------------------------
Eval num_timesteps=597600, episode_reward=-300.00 +/- 0.00
Episode length: 961.60 +/- 359.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 962      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 597600   |
---------------------------------
Eval num_timesteps=599592, episode_reward=-300.00 +/- 0.00
Episode length: 960.80 +/- 216.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 599592   |
---------------------------------
Eval num_timesteps=601584, episode_reward=-300.00 +/- 0.00
Episode length: 973.00 +/- 365.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 601584   |
---------------------------------
Eval num_timesteps=603576, episode_reward=-300.00 +/- 0.00
Episode length: 886.60 +/- 77.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 887      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 603576   |
---------------------------------
Eval num_timesteps=605568, episode_reward=-300.00 +/- 0.00
Episode length: 749.20 +/- 187.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 749      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 605568   |
---------------------------------
Eval num_timesteps=607560, episode_reward=-300.00 +/- 0.00
Episode length: 706.40 +/- 163.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 706      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 607560   |
---------------------------------
Eval num_timesteps=609552, episode_reward=-300.00 +/- 0.00
Episode length: 760.20 +/- 212.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 760      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 609552   |
---------------------------------
Eval num_timesteps=611544, episode_reward=-300.00 +/- 0.00
Episode length: 825.40 +/- 153.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 611544   |
---------------------------------
Eval num_timesteps=613536, episode_reward=-300.00 +/- 0.00
Episode length: 1032.40 +/- 386.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 613536   |
---------------------------------
Eval num_timesteps=615528, episode_reward=-300.00 +/- 0.00
Episode length: 827.60 +/- 281.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 615528   |
---------------------------------
Eval num_timesteps=617520, episode_reward=-300.00 +/- 0.00
Episode length: 948.00 +/- 185.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 617520   |
---------------------------------
Eval num_timesteps=619512, episode_reward=-300.00 +/- 0.00
Episode length: 927.00 +/- 121.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 619512   |
---------------------------------
Eval num_timesteps=621504, episode_reward=-300.00 +/- 0.00
Episode length: 779.40 +/- 141.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 621504   |
---------------------------------
Eval num_timesteps=623496, episode_reward=-308.82 +/- 17.64
Episode length: 868.80 +/- 429.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 623496   |
---------------------------------
Eval num_timesteps=625488, episode_reward=-300.00 +/- 0.00
Episode length: 717.80 +/- 192.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 718      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 625488   |
---------------------------------
Eval num_timesteps=627480, episode_reward=-366.57 +/- 133.15
Episode length: 933.80 +/- 544.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 934      |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 627480   |
---------------------------------
Eval num_timesteps=629472, episode_reward=-300.00 +/- 0.00
Episode length: 779.00 +/- 181.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 629472   |
---------------------------------
Eval num_timesteps=631464, episode_reward=-300.00 +/- 0.00
Episode length: 811.40 +/- 228.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 631464   |
---------------------------------
Eval num_timesteps=633456, episode_reward=-300.00 +/- 0.00
Episode length: 918.40 +/- 305.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 633456   |
---------------------------------
Eval num_timesteps=635448, episode_reward=-300.00 +/- 0.00
Episode length: 981.40 +/- 71.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 635448   |
---------------------------------
Eval num_timesteps=637440, episode_reward=-300.00 +/- 0.00
Episode length: 990.60 +/- 299.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 991      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 637440   |
---------------------------------
Eval num_timesteps=639432, episode_reward=-300.00 +/- 0.00
Episode length: 620.40 +/- 123.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 620          |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 639432       |
| train/                  |              |
|    approx_kl            | 0.0100541795 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.001        |
|    loss                 | -0.0128      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00542     |
|    std                  | 1.12         |
|    value_loss           | 0.158        |
------------------------------------------
Eval num_timesteps=641424, episode_reward=-300.00 +/- 0.00
Episode length: 894.00 +/- 201.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 894      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 641424   |
---------------------------------
Eval num_timesteps=643416, episode_reward=-300.00 +/- 0.00
Episode length: 726.80 +/- 251.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 727      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 643416   |
---------------------------------
Eval num_timesteps=645408, episode_reward=-300.00 +/- 0.00
Episode length: 1167.40 +/- 672.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 645408   |
---------------------------------
Eval num_timesteps=647400, episode_reward=-300.00 +/- 0.00
Episode length: 873.20 +/- 357.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 873      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 647400   |
---------------------------------
Eval num_timesteps=649392, episode_reward=-300.00 +/- 0.00
Episode length: 1031.80 +/- 420.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 649392   |
---------------------------------
Eval num_timesteps=651384, episode_reward=-300.00 +/- 0.00
Episode length: 837.60 +/- 198.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 651384   |
---------------------------------
Eval num_timesteps=653376, episode_reward=-300.00 +/- 0.00
Episode length: 807.60 +/- 197.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 653376   |
---------------------------------
Eval num_timesteps=655368, episode_reward=-300.00 +/- 0.00
Episode length: 712.00 +/- 181.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 655368   |
---------------------------------
Eval num_timesteps=657360, episode_reward=-300.00 +/- 0.00
Episode length: 880.60 +/- 226.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 657360   |
---------------------------------
Eval num_timesteps=659352, episode_reward=-300.00 +/- 0.00
Episode length: 913.40 +/- 347.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 659352   |
---------------------------------
Eval num_timesteps=661344, episode_reward=-300.00 +/- 0.00
Episode length: 793.20 +/- 131.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 661344   |
---------------------------------
Eval num_timesteps=663336, episode_reward=-300.00 +/- 0.00
Episode length: 774.60 +/- 239.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 663336   |
---------------------------------
Eval num_timesteps=665328, episode_reward=-300.00 +/- 0.00
Episode length: 815.40 +/- 258.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 815      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 665328   |
---------------------------------
Eval num_timesteps=667320, episode_reward=-300.00 +/- 0.00
Episode length: 931.00 +/- 304.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 931      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 667320   |
---------------------------------
Eval num_timesteps=669312, episode_reward=-300.00 +/- 0.00
Episode length: 1026.80 +/- 327.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 669312   |
---------------------------------
Eval num_timesteps=671304, episode_reward=-300.00 +/- 0.00
Episode length: 806.40 +/- 282.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 671304   |
---------------------------------
Eval num_timesteps=673296, episode_reward=-300.00 +/- 0.00
Episode length: 1064.80 +/- 501.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 673296   |
---------------------------------
Eval num_timesteps=675288, episode_reward=-300.00 +/- 0.00
Episode length: 1074.80 +/- 458.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 675288   |
---------------------------------
Eval num_timesteps=677280, episode_reward=-300.00 +/- 0.00
Episode length: 800.20 +/- 302.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 677280   |
---------------------------------
Eval num_timesteps=679272, episode_reward=-300.00 +/- 0.00
Episode length: 757.40 +/- 154.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 679272   |
---------------------------------
Eval num_timesteps=681264, episode_reward=-300.00 +/- 0.00
Episode length: 1014.00 +/- 300.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 681264   |
---------------------------------
Eval num_timesteps=683256, episode_reward=-300.00 +/- 0.00
Episode length: 720.00 +/- 93.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 720      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 683256   |
---------------------------------
Eval num_timesteps=685248, episode_reward=-300.00 +/- 0.00
Episode length: 911.40 +/- 234.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 911      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 685248   |
---------------------------------
Eval num_timesteps=687240, episode_reward=-300.00 +/- 0.00
Episode length: 1188.20 +/- 576.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 687240   |
---------------------------------
Eval num_timesteps=689232, episode_reward=-300.00 +/- 0.00
Episode length: 758.00 +/- 78.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 758         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 689232      |
| train/                  |             |
|    approx_kl            | 0.009835375 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0258     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00639    |
|    std                  | 1.13        |
|    value_loss           | 0.111       |
-----------------------------------------
Eval num_timesteps=691224, episode_reward=-300.00 +/- 0.00
Episode length: 895.60 +/- 185.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 691224   |
---------------------------------
Eval num_timesteps=693216, episode_reward=-300.00 +/- 0.00
Episode length: 973.00 +/- 206.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 693216   |
---------------------------------
Eval num_timesteps=695208, episode_reward=-300.00 +/- 0.00
Episode length: 982.00 +/- 177.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 695208   |
---------------------------------
Eval num_timesteps=697200, episode_reward=-300.00 +/- 0.00
Episode length: 977.80 +/- 406.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 978      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 697200   |
---------------------------------
Eval num_timesteps=699192, episode_reward=-300.00 +/- 0.00
Episode length: 814.20 +/- 275.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 699192   |
---------------------------------
Eval num_timesteps=701184, episode_reward=-300.00 +/- 0.00
Episode length: 900.40 +/- 320.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 900      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 701184   |
---------------------------------
Eval num_timesteps=703176, episode_reward=-300.00 +/- 0.00
Episode length: 811.60 +/- 212.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 703176   |
---------------------------------
Eval num_timesteps=705168, episode_reward=-300.00 +/- 0.00
Episode length: 825.00 +/- 179.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 705168   |
---------------------------------
Eval num_timesteps=707160, episode_reward=-300.00 +/- 0.00
Episode length: 821.40 +/- 255.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 821      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 707160   |
---------------------------------
Eval num_timesteps=709152, episode_reward=-300.00 +/- 0.00
Episode length: 1048.20 +/- 379.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 709152   |
---------------------------------
Eval num_timesteps=711144, episode_reward=-300.00 +/- 0.00
Episode length: 885.60 +/- 231.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 886      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 711144   |
---------------------------------
Eval num_timesteps=713136, episode_reward=-300.00 +/- 0.00
Episode length: 902.00 +/- 396.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 713136   |
---------------------------------
Eval num_timesteps=715128, episode_reward=-300.00 +/- 0.00
Episode length: 976.00 +/- 199.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 715128   |
---------------------------------
Eval num_timesteps=717120, episode_reward=-300.00 +/- 0.00
Episode length: 980.40 +/- 184.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 717120   |
---------------------------------
Eval num_timesteps=719112, episode_reward=-300.00 +/- 0.00
Episode length: 790.60 +/- 201.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 719112   |
---------------------------------
Eval num_timesteps=721104, episode_reward=-300.00 +/- 0.00
Episode length: 806.60 +/- 234.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 721104   |
---------------------------------
Eval num_timesteps=723096, episode_reward=-300.00 +/- 0.00
Episode length: 906.60 +/- 105.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 723096   |
---------------------------------
Eval num_timesteps=725088, episode_reward=-300.00 +/- 0.00
Episode length: 857.40 +/- 158.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 725088   |
---------------------------------
Eval num_timesteps=727080, episode_reward=-300.00 +/- 0.00
Episode length: 874.00 +/- 144.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 874      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 727080   |
---------------------------------
Eval num_timesteps=729072, episode_reward=-300.00 +/- 0.00
Episode length: 1114.00 +/- 523.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 729072   |
---------------------------------
Eval num_timesteps=731064, episode_reward=-300.00 +/- 0.00
Episode length: 768.40 +/- 262.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 731064   |
---------------------------------
Eval num_timesteps=733056, episode_reward=-300.00 +/- 0.00
Episode length: 1069.80 +/- 423.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 733056   |
---------------------------------
Eval num_timesteps=735048, episode_reward=-300.00 +/- 0.00
Episode length: 1176.40 +/- 419.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 735048   |
---------------------------------
Eval num_timesteps=737040, episode_reward=-300.00 +/- 0.00
Episode length: 824.40 +/- 219.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 737040   |
---------------------------------
Eval num_timesteps=739032, episode_reward=-300.00 +/- 0.00
Episode length: 1113.60 +/- 193.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.11e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 739032      |
| train/                  |             |
|    approx_kl            | 0.010301044 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0521     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0075     |
|    std                  | 1.14        |
|    value_loss           | 0.0738      |
-----------------------------------------
Eval num_timesteps=741024, episode_reward=-300.00 +/- 0.00
Episode length: 988.60 +/- 362.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 989      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 741024   |
---------------------------------
Eval num_timesteps=743016, episode_reward=-300.00 +/- 0.00
Episode length: 1009.80 +/- 215.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 743016   |
---------------------------------
Eval num_timesteps=745008, episode_reward=-300.00 +/- 0.00
Episode length: 1181.00 +/- 358.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 745008   |
---------------------------------
Eval num_timesteps=747000, episode_reward=-300.00 +/- 0.00
Episode length: 1465.20 +/- 534.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.47e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=748992, episode_reward=-300.00 +/- 0.00
Episode length: 848.80 +/- 157.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 748992   |
---------------------------------
Eval num_timesteps=750984, episode_reward=-300.00 +/- 0.00
Episode length: 1393.60 +/- 222.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 750984   |
---------------------------------
Eval num_timesteps=752976, episode_reward=-300.00 +/- 0.00
Episode length: 1202.40 +/- 305.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 752976   |
---------------------------------
Eval num_timesteps=754968, episode_reward=-300.00 +/- 0.00
Episode length: 1042.80 +/- 253.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 754968   |
---------------------------------
Eval num_timesteps=756960, episode_reward=-300.00 +/- 0.00
Episode length: 1097.40 +/- 434.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 756960   |
---------------------------------
Eval num_timesteps=758952, episode_reward=-300.00 +/- 0.00
Episode length: 1425.00 +/- 462.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.42e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 758952   |
---------------------------------
Eval num_timesteps=760944, episode_reward=-300.00 +/- 0.00
Episode length: 1137.40 +/- 245.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 760944   |
---------------------------------
Eval num_timesteps=762936, episode_reward=-300.00 +/- 0.00
Episode length: 1368.40 +/- 279.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.37e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 762936   |
---------------------------------
Eval num_timesteps=764928, episode_reward=-300.00 +/- 0.00
Episode length: 1038.40 +/- 266.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 764928   |
---------------------------------
Eval num_timesteps=766920, episode_reward=-300.00 +/- 0.00
Episode length: 1263.60 +/- 279.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 766920   |
---------------------------------
Eval num_timesteps=768912, episode_reward=-300.00 +/- 0.00
Episode length: 1065.00 +/- 239.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 768912   |
---------------------------------
Eval num_timesteps=770904, episode_reward=-300.00 +/- 0.00
Episode length: 1043.40 +/- 214.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 770904   |
---------------------------------
Eval num_timesteps=772896, episode_reward=-300.00 +/- 0.00
Episode length: 991.60 +/- 401.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 772896   |
---------------------------------
Eval num_timesteps=774888, episode_reward=-300.00 +/- 0.00
Episode length: 784.60 +/- 70.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 785      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 774888   |
---------------------------------
Eval num_timesteps=776880, episode_reward=-300.00 +/- 0.00
Episode length: 1183.00 +/- 417.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 776880   |
---------------------------------
Eval num_timesteps=778872, episode_reward=-300.00 +/- 0.00
Episode length: 1094.60 +/- 262.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 778872   |
---------------------------------
Eval num_timesteps=780864, episode_reward=-300.00 +/- 0.00
Episode length: 818.80 +/- 121.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 780864   |
---------------------------------
Eval num_timesteps=782856, episode_reward=-300.00 +/- 0.00
Episode length: 1003.80 +/- 255.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 782856   |
---------------------------------
Eval num_timesteps=784848, episode_reward=-300.00 +/- 0.00
Episode length: 1014.20 +/- 277.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 784848   |
---------------------------------
Eval num_timesteps=786840, episode_reward=-300.00 +/- 0.00
Episode length: 1282.60 +/- 645.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.28e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 786840      |
| train/                  |             |
|    approx_kl            | 0.012750919 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0313     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00511    |
|    std                  | 1.15        |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=788832, episode_reward=-300.00 +/- 0.00
Episode length: 1410.80 +/- 431.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 788832   |
---------------------------------
Eval num_timesteps=790824, episode_reward=-300.00 +/- 0.00
Episode length: 1171.40 +/- 217.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 790824   |
---------------------------------
Eval num_timesteps=792816, episode_reward=-300.00 +/- 0.00
Episode length: 1087.60 +/- 282.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 792816   |
---------------------------------
Eval num_timesteps=794808, episode_reward=-300.00 +/- 0.00
Episode length: 1091.60 +/- 257.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 794808   |
---------------------------------
Eval num_timesteps=796800, episode_reward=-300.00 +/- 0.00
Episode length: 1138.20 +/- 421.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 796800   |
---------------------------------
Eval num_timesteps=798792, episode_reward=-300.00 +/- 0.00
Episode length: 1546.80 +/- 620.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.55e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 798792   |
---------------------------------
Eval num_timesteps=800784, episode_reward=-300.00 +/- 0.00
Episode length: 1240.40 +/- 564.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 800784   |
---------------------------------
Eval num_timesteps=802776, episode_reward=-300.00 +/- 0.00
Episode length: 956.60 +/- 308.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 802776   |
---------------------------------
Eval num_timesteps=804768, episode_reward=-300.00 +/- 0.00
Episode length: 1119.40 +/- 241.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 804768   |
---------------------------------
Eval num_timesteps=806760, episode_reward=-300.00 +/- 0.00
Episode length: 908.80 +/- 186.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 806760   |
---------------------------------
Eval num_timesteps=808752, episode_reward=-300.00 +/- 0.00
Episode length: 1193.80 +/- 451.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 808752   |
---------------------------------
Eval num_timesteps=810744, episode_reward=-300.00 +/- 0.00
Episode length: 1115.80 +/- 326.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 810744   |
---------------------------------
Eval num_timesteps=812736, episode_reward=-300.00 +/- 0.00
Episode length: 1219.60 +/- 414.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 812736   |
---------------------------------
Eval num_timesteps=814728, episode_reward=-300.00 +/- 0.00
Episode length: 1076.00 +/- 399.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 814728   |
---------------------------------
Eval num_timesteps=816720, episode_reward=-300.00 +/- 0.00
Episode length: 979.60 +/- 170.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 816720   |
---------------------------------
Eval num_timesteps=818712, episode_reward=-300.00 +/- 0.00
Episode length: 1143.60 +/- 323.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 818712   |
---------------------------------
Eval num_timesteps=820704, episode_reward=-300.00 +/- 0.00
Episode length: 906.60 +/- 89.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 907      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 820704   |
---------------------------------
Eval num_timesteps=822696, episode_reward=-300.00 +/- 0.00
Episode length: 1072.00 +/- 326.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 822696   |
---------------------------------
Eval num_timesteps=824688, episode_reward=-300.00 +/- 0.00
Episode length: 901.80 +/- 119.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 824688   |
---------------------------------
Eval num_timesteps=826680, episode_reward=-300.00 +/- 0.00
Episode length: 1329.00 +/- 384.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 826680   |
---------------------------------
Eval num_timesteps=828672, episode_reward=-300.00 +/- 0.00
Episode length: 1355.60 +/- 338.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 828672   |
---------------------------------
Eval num_timesteps=830664, episode_reward=-300.00 +/- 0.00
Episode length: 1063.20 +/- 295.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 830664   |
---------------------------------
Eval num_timesteps=832656, episode_reward=-300.00 +/- 0.00
Episode length: 1389.40 +/- 379.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 832656   |
---------------------------------
Eval num_timesteps=834648, episode_reward=-300.00 +/- 0.00
Episode length: 1427.20 +/- 448.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.43e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 834648   |
---------------------------------
Eval num_timesteps=836640, episode_reward=-300.00 +/- 0.00
Episode length: 1135.20 +/- 601.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.14e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 836640      |
| train/                  |             |
|    approx_kl            | 0.010746509 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0143     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00597    |
|    std                  | 1.16        |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=838632, episode_reward=-300.00 +/- 0.00
Episode length: 837.40 +/- 232.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 838632   |
---------------------------------
Eval num_timesteps=840624, episode_reward=-79.99 +/- 440.02
Episode length: 1024.40 +/- 314.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -80      |
| time/              |          |
|    total_timesteps | 840624   |
---------------------------------
New best mean reward!
Eval num_timesteps=842616, episode_reward=-300.00 +/- 0.00
Episode length: 1359.00 +/- 291.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 842616   |
---------------------------------
Eval num_timesteps=844608, episode_reward=-300.00 +/- 0.00
Episode length: 939.80 +/- 227.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 940      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 844608   |
---------------------------------
Eval num_timesteps=846600, episode_reward=-300.00 +/- 0.00
Episode length: 1411.40 +/- 540.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 846600   |
---------------------------------
Eval num_timesteps=848592, episode_reward=-300.00 +/- 0.00
Episode length: 1298.00 +/- 516.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 848592   |
---------------------------------
Eval num_timesteps=850584, episode_reward=-300.00 +/- 0.00
Episode length: 1015.20 +/- 294.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 850584   |
---------------------------------
Eval num_timesteps=852576, episode_reward=-300.00 +/- 0.00
Episode length: 1307.40 +/- 851.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.31e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 852576   |
---------------------------------
Eval num_timesteps=854568, episode_reward=-300.00 +/- 0.00
Episode length: 1330.20 +/- 346.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 854568   |
---------------------------------
Eval num_timesteps=856560, episode_reward=-300.00 +/- 0.00
Episode length: 1149.00 +/- 476.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 856560   |
---------------------------------
Eval num_timesteps=858552, episode_reward=-300.00 +/- 0.00
Episode length: 1315.00 +/- 439.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 858552   |
---------------------------------
Eval num_timesteps=860544, episode_reward=-300.00 +/- 0.00
Episode length: 897.60 +/- 216.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 860544   |
---------------------------------
Eval num_timesteps=862536, episode_reward=-300.00 +/- 0.00
Episode length: 1079.40 +/- 450.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 862536   |
---------------------------------
Eval num_timesteps=864528, episode_reward=-300.00 +/- 0.00
Episode length: 1077.80 +/- 298.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 864528   |
---------------------------------
Eval num_timesteps=866520, episode_reward=-300.00 +/- 0.00
Episode length: 1073.40 +/- 344.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 866520   |
---------------------------------
Eval num_timesteps=868512, episode_reward=-300.00 +/- 0.00
Episode length: 1200.20 +/- 431.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 868512   |
---------------------------------
Eval num_timesteps=870504, episode_reward=-300.00 +/- 0.00
Episode length: 934.40 +/- 202.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 934      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 870504   |
---------------------------------
Eval num_timesteps=872496, episode_reward=-300.00 +/- 0.00
Episode length: 1145.40 +/- 389.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 872496   |
---------------------------------
Eval num_timesteps=874488, episode_reward=-300.00 +/- 0.00
Episode length: 1081.20 +/- 293.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 874488   |
---------------------------------
Eval num_timesteps=876480, episode_reward=-300.00 +/- 0.00
Episode length: 1308.60 +/- 298.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.31e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 876480   |
---------------------------------
Eval num_timesteps=878472, episode_reward=-300.00 +/- 0.00
Episode length: 1173.20 +/- 574.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 878472   |
---------------------------------
Eval num_timesteps=880464, episode_reward=-300.00 +/- 0.00
Episode length: 1267.80 +/- 434.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 880464   |
---------------------------------
Eval num_timesteps=882456, episode_reward=-300.00 +/- 0.00
Episode length: 1102.60 +/- 324.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 882456   |
---------------------------------
Eval num_timesteps=884448, episode_reward=-300.00 +/- 0.00
Episode length: 1180.00 +/- 490.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 884448   |
---------------------------------
Eval num_timesteps=886440, episode_reward=-300.00 +/- 0.00
Episode length: 844.40 +/- 229.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 844         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 886440      |
| train/                  |             |
|    approx_kl            | 0.010624061 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0509     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00665    |
|    std                  | 1.17        |
|    value_loss           | 0.0918      |
-----------------------------------------
Eval num_timesteps=888432, episode_reward=-300.00 +/- 0.00
Episode length: 763.80 +/- 191.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 888432   |
---------------------------------
Eval num_timesteps=890424, episode_reward=-300.00 +/- 0.00
Episode length: 1235.00 +/- 306.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 890424   |
---------------------------------
Eval num_timesteps=892416, episode_reward=-300.00 +/- 0.00
Episode length: 988.40 +/- 159.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 892416   |
---------------------------------
Eval num_timesteps=894408, episode_reward=-300.00 +/- 0.00
Episode length: 1390.80 +/- 537.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 894408   |
---------------------------------
Eval num_timesteps=896400, episode_reward=-300.00 +/- 0.00
Episode length: 1393.20 +/- 620.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 896400   |
---------------------------------
Eval num_timesteps=898392, episode_reward=-300.00 +/- 0.00
Episode length: 1038.00 +/- 404.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 898392   |
---------------------------------
Eval num_timesteps=900384, episode_reward=-300.00 +/- 0.00
Episode length: 1002.80 +/- 318.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 900384   |
---------------------------------
Eval num_timesteps=902376, episode_reward=-300.00 +/- 0.00
Episode length: 1143.80 +/- 307.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 902376   |
---------------------------------
Eval num_timesteps=904368, episode_reward=-300.00 +/- 0.00
Episode length: 1131.80 +/- 365.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 904368   |
---------------------------------
Eval num_timesteps=906360, episode_reward=-300.00 +/- 0.00
Episode length: 1230.00 +/- 685.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 906360   |
---------------------------------
Eval num_timesteps=908352, episode_reward=-300.00 +/- 0.00
Episode length: 956.80 +/- 291.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 908352   |
---------------------------------
Eval num_timesteps=910344, episode_reward=-300.00 +/- 0.00
Episode length: 924.60 +/- 471.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 925      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 910344   |
---------------------------------
Eval num_timesteps=912336, episode_reward=-300.00 +/- 0.00
Episode length: 975.40 +/- 343.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 975      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 912336   |
---------------------------------
Eval num_timesteps=914328, episode_reward=-300.00 +/- 0.00
Episode length: 1160.60 +/- 319.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 914328   |
---------------------------------
Eval num_timesteps=916320, episode_reward=-300.00 +/- 0.00
Episode length: 825.20 +/- 101.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 916320   |
---------------------------------
Eval num_timesteps=918312, episode_reward=-300.00 +/- 0.00
Episode length: 766.00 +/- 267.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 766      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 918312   |
---------------------------------
Eval num_timesteps=920304, episode_reward=-300.00 +/- 0.00
Episode length: 802.20 +/- 156.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 920304   |
---------------------------------
Eval num_timesteps=922296, episode_reward=-300.00 +/- 0.00
Episode length: 964.20 +/- 504.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 964      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 922296   |
---------------------------------
Eval num_timesteps=924288, episode_reward=-300.00 +/- 0.00
Episode length: 1126.00 +/- 189.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 924288   |
---------------------------------
Eval num_timesteps=926280, episode_reward=-300.00 +/- 0.00
Episode length: 1231.40 +/- 261.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 926280   |
---------------------------------
Eval num_timesteps=928272, episode_reward=-300.00 +/- 0.00
Episode length: 878.20 +/- 306.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 878      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 928272   |
---------------------------------
Eval num_timesteps=930264, episode_reward=-300.00 +/- 0.00
Episode length: 835.80 +/- 151.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 930264   |
---------------------------------
Eval num_timesteps=932256, episode_reward=-300.00 +/- 0.00
Episode length: 1289.40 +/- 460.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 932256   |
---------------------------------
Eval num_timesteps=934248, episode_reward=-300.00 +/- 0.00
Episode length: 1022.60 +/- 180.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 934248     |
| train/                  |            |
|    approx_kl            | 0.01129515 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.36      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0296    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00638   |
|    std                  | 1.19       |
|    value_loss           | 0.163      |
----------------------------------------
Eval num_timesteps=936240, episode_reward=-300.00 +/- 0.00
Episode length: 923.20 +/- 179.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 936240   |
---------------------------------
Eval num_timesteps=938232, episode_reward=-344.03 +/- 88.06
Episode length: 1284.40 +/- 349.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.28e+03 |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 938232   |
---------------------------------
Eval num_timesteps=940224, episode_reward=-300.00 +/- 0.00
Episode length: 1015.80 +/- 200.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 940224   |
---------------------------------
Eval num_timesteps=942216, episode_reward=-300.00 +/- 0.00
Episode length: 960.60 +/- 72.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 961      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 942216   |
---------------------------------
Eval num_timesteps=944208, episode_reward=-300.00 +/- 0.00
Episode length: 1079.00 +/- 252.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 944208   |
---------------------------------
Eval num_timesteps=946200, episode_reward=-300.00 +/- 0.00
Episode length: 1157.60 +/- 407.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 946200   |
---------------------------------
Eval num_timesteps=948192, episode_reward=-300.00 +/- 0.00
Episode length: 1291.20 +/- 415.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 948192   |
---------------------------------
Eval num_timesteps=950184, episode_reward=-300.00 +/- 0.00
Episode length: 1196.80 +/- 546.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 950184   |
---------------------------------
Eval num_timesteps=952176, episode_reward=-300.00 +/- 0.00
Episode length: 952.80 +/- 81.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 953      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 952176   |
---------------------------------
Eval num_timesteps=954168, episode_reward=-300.00 +/- 0.00
Episode length: 1067.60 +/- 212.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 954168   |
---------------------------------
Eval num_timesteps=956160, episode_reward=-300.00 +/- 0.00
Episode length: 866.40 +/- 196.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 866      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 956160   |
---------------------------------
Eval num_timesteps=958152, episode_reward=-300.00 +/- 0.00
Episode length: 1295.20 +/- 722.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 958152   |
---------------------------------
Eval num_timesteps=960144, episode_reward=-300.00 +/- 0.00
Episode length: 1133.00 +/- 284.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 960144   |
---------------------------------
Eval num_timesteps=962136, episode_reward=-300.00 +/- 0.00
Episode length: 1098.00 +/- 205.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 962136   |
---------------------------------
Eval num_timesteps=964128, episode_reward=-300.00 +/- 0.00
Episode length: 980.60 +/- 133.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 964128   |
---------------------------------
Eval num_timesteps=966120, episode_reward=-300.00 +/- 0.00
Episode length: 910.40 +/- 214.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 966120   |
---------------------------------
Eval num_timesteps=968112, episode_reward=-300.00 +/- 0.00
Episode length: 1046.60 +/- 88.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 968112   |
---------------------------------
Eval num_timesteps=970104, episode_reward=-300.00 +/- 0.00
Episode length: 1245.80 +/- 378.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 970104   |
---------------------------------
Eval num_timesteps=972096, episode_reward=-300.00 +/- 0.00
Episode length: 1161.40 +/- 172.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 972096   |
---------------------------------
Eval num_timesteps=974088, episode_reward=-300.00 +/- 0.00
Episode length: 1186.00 +/- 375.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 974088   |
---------------------------------
Eval num_timesteps=976080, episode_reward=-300.00 +/- 0.00
Episode length: 1318.80 +/- 284.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 976080   |
---------------------------------
Eval num_timesteps=978072, episode_reward=-300.00 +/- 0.00
Episode length: 1216.60 +/- 777.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 978072   |
---------------------------------
Eval num_timesteps=980064, episode_reward=-300.00 +/- 0.00
Episode length: 1062.40 +/- 297.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 980064   |
---------------------------------
Eval num_timesteps=982056, episode_reward=-300.00 +/- 0.00
Episode length: 1273.40 +/- 426.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 982056   |
---------------------------------
Eval num_timesteps=984048, episode_reward=-300.00 +/- 0.00
Episode length: 1100.00 +/- 385.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.1e+03     |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 984048      |
| train/                  |             |
|    approx_kl            | 0.011109911 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0392     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00718    |
|    std                  | 1.2         |
|    value_loss           | 0.161       |
-----------------------------------------
Eval num_timesteps=986040, episode_reward=-300.00 +/- 0.00
Episode length: 956.80 +/- 203.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 957      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 986040   |
---------------------------------
Eval num_timesteps=988032, episode_reward=-300.00 +/- 0.00
Episode length: 1050.00 +/- 487.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 988032   |
---------------------------------
Eval num_timesteps=990024, episode_reward=-300.00 +/- 0.00
Episode length: 1165.20 +/- 421.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 990024   |
---------------------------------
Eval num_timesteps=992016, episode_reward=-300.00 +/- 0.00
Episode length: 1225.60 +/- 385.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 992016   |
---------------------------------
Eval num_timesteps=994008, episode_reward=-300.00 +/- 0.00
Episode length: 1036.40 +/- 317.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 994008   |
---------------------------------
Eval num_timesteps=996000, episode_reward=-300.00 +/- 0.00
Episode length: 937.80 +/- 208.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=997992, episode_reward=-300.00 +/- 0.00
Episode length: 943.00 +/- 279.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 943      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 997992   |
---------------------------------
Eval num_timesteps=999984, episode_reward=-300.00 +/- 0.00
Episode length: 933.60 +/- 131.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 934      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 999984   |
---------------------------------
Eval num_timesteps=1001976, episode_reward=-300.00 +/- 0.00
Episode length: 1059.80 +/- 225.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1001976  |
---------------------------------
Eval num_timesteps=1003968, episode_reward=-300.00 +/- 0.00
Episode length: 1024.20 +/- 300.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1003968  |
---------------------------------
Eval num_timesteps=1005960, episode_reward=-300.00 +/- 0.00
Episode length: 1091.00 +/- 180.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1005960  |
---------------------------------
Eval num_timesteps=1007952, episode_reward=-300.00 +/- 0.00
Episode length: 905.80 +/- 164.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 906      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1007952  |
---------------------------------
Eval num_timesteps=1009944, episode_reward=-300.00 +/- 0.00
Episode length: 1218.00 +/- 299.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1009944  |
---------------------------------
Eval num_timesteps=1011936, episode_reward=-300.00 +/- 0.00
Episode length: 909.60 +/- 195.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1011936  |
---------------------------------
Eval num_timesteps=1013928, episode_reward=-300.00 +/- 0.00
Episode length: 948.20 +/- 197.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1013928  |
---------------------------------
Eval num_timesteps=1015920, episode_reward=-374.42 +/- 148.84
Episode length: 1109.20 +/- 385.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 1015920  |
---------------------------------
Eval num_timesteps=1017912, episode_reward=-300.00 +/- 0.00
Episode length: 1241.00 +/- 249.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1017912  |
---------------------------------
Eval num_timesteps=1019904, episode_reward=-300.00 +/- 0.00
Episode length: 872.20 +/- 175.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1019904  |
---------------------------------
Eval num_timesteps=1021896, episode_reward=-300.00 +/- 0.00
Episode length: 1051.80 +/- 368.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1021896  |
---------------------------------
Eval num_timesteps=1023888, episode_reward=-300.00 +/- 0.00
Episode length: 1115.80 +/- 523.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1023888  |
---------------------------------
Eval num_timesteps=1025880, episode_reward=-300.00 +/- 0.00
Episode length: 1271.80 +/- 493.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1025880  |
---------------------------------
Eval num_timesteps=1027872, episode_reward=-300.00 +/- 0.00
Episode length: 1072.40 +/- 222.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1027872  |
---------------------------------
Eval num_timesteps=1029864, episode_reward=-300.00 +/- 0.00
Episode length: 930.20 +/- 223.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 930      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1029864  |
---------------------------------
Eval num_timesteps=1031856, episode_reward=-300.00 +/- 0.00
Episode length: 1077.20 +/- 535.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1031856  |
---------------------------------
Eval num_timesteps=1033848, episode_reward=-300.00 +/- 0.00
Episode length: 1049.60 +/- 230.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.05e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1033848     |
| train/                  |             |
|    approx_kl            | 0.010518253 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.001       |
|    loss                 | -0.00632    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00701    |
|    std                  | 1.21        |
|    value_loss           | 0.186       |
-----------------------------------------
Eval num_timesteps=1035840, episode_reward=-300.00 +/- 0.00
Episode length: 897.80 +/- 124.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1035840  |
---------------------------------
Eval num_timesteps=1037832, episode_reward=-300.00 +/- 0.00
Episode length: 1163.00 +/- 564.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1037832  |
---------------------------------
Eval num_timesteps=1039824, episode_reward=-300.00 +/- 0.00
Episode length: 836.80 +/- 144.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1039824  |
---------------------------------
Eval num_timesteps=1041816, episode_reward=-300.00 +/- 0.00
Episode length: 973.20 +/- 132.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1041816  |
---------------------------------
Eval num_timesteps=1043808, episode_reward=-300.00 +/- 0.00
Episode length: 1058.40 +/- 175.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1043808  |
---------------------------------
Eval num_timesteps=1045800, episode_reward=-300.00 +/- 0.00
Episode length: 948.40 +/- 299.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 948      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1045800  |
---------------------------------
Eval num_timesteps=1047792, episode_reward=-300.00 +/- 0.00
Episode length: 1183.40 +/- 562.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1047792  |
---------------------------------
Eval num_timesteps=1049784, episode_reward=-300.00 +/- 0.00
Episode length: 973.00 +/- 140.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 973      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1049784  |
---------------------------------
Eval num_timesteps=1051776, episode_reward=-300.00 +/- 0.00
Episode length: 941.20 +/- 129.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 941      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1051776  |
---------------------------------
Eval num_timesteps=1053768, episode_reward=-300.00 +/- 0.00
Episode length: 1045.00 +/- 185.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1053768  |
---------------------------------
Eval num_timesteps=1055760, episode_reward=-300.00 +/- 0.00
Episode length: 1100.20 +/- 324.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1055760  |
---------------------------------
Eval num_timesteps=1057752, episode_reward=-300.00 +/- 0.00
Episode length: 1096.40 +/- 354.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1057752  |
---------------------------------
Eval num_timesteps=1059744, episode_reward=-300.00 +/- 0.00
Episode length: 959.20 +/- 119.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 959      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1059744  |
---------------------------------
Eval num_timesteps=1061736, episode_reward=-300.00 +/- 0.00
Episode length: 896.20 +/- 203.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1061736  |
---------------------------------
Eval num_timesteps=1063728, episode_reward=-300.00 +/- 0.00
Episode length: 990.60 +/- 96.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 991      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1063728  |
---------------------------------
Eval num_timesteps=1065720, episode_reward=-300.00 +/- 0.00
Episode length: 1030.20 +/- 149.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1065720  |
---------------------------------
Eval num_timesteps=1067712, episode_reward=-300.00 +/- 0.00
Episode length: 1081.00 +/- 232.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1067712  |
---------------------------------
Eval num_timesteps=1069704, episode_reward=-300.00 +/- 0.00
Episode length: 1263.20 +/- 589.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1069704  |
---------------------------------
Eval num_timesteps=1071696, episode_reward=-300.00 +/- 0.00
Episode length: 902.20 +/- 121.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 902      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1071696  |
---------------------------------
Eval num_timesteps=1073688, episode_reward=-300.00 +/- 0.00
Episode length: 826.00 +/- 110.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1073688  |
---------------------------------
Eval num_timesteps=1075680, episode_reward=-300.00 +/- 0.00
Episode length: 1017.60 +/- 406.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1075680  |
---------------------------------
Eval num_timesteps=1077672, episode_reward=-300.00 +/- 0.00
Episode length: 1170.80 +/- 502.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1077672  |
---------------------------------
Eval num_timesteps=1079664, episode_reward=-300.00 +/- 0.00
Episode length: 1289.40 +/- 472.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1079664  |
---------------------------------
Eval num_timesteps=1081656, episode_reward=-300.00 +/- 0.00
Episode length: 1467.40 +/- 523.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.47e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1081656     |
| train/                  |             |
|    approx_kl            | 0.011206776 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0287     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00729    |
|    std                  | 1.23        |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=1083648, episode_reward=-300.00 +/- 0.00
Episode length: 895.60 +/- 252.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 896      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1083648  |
---------------------------------
Eval num_timesteps=1085640, episode_reward=-300.00 +/- 0.00
Episode length: 836.40 +/- 112.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1085640  |
---------------------------------
Eval num_timesteps=1087632, episode_reward=-300.00 +/- 0.00
Episode length: 1060.40 +/- 322.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1087632  |
---------------------------------
Eval num_timesteps=1089624, episode_reward=-300.00 +/- 0.00
Episode length: 1068.40 +/- 208.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1089624  |
---------------------------------
Eval num_timesteps=1091616, episode_reward=-300.00 +/- 0.00
Episode length: 828.80 +/- 316.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1091616  |
---------------------------------
Eval num_timesteps=1093608, episode_reward=-300.00 +/- 0.00
Episode length: 897.60 +/- 115.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 898      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1093608  |
---------------------------------
Eval num_timesteps=1095600, episode_reward=-300.00 +/- 0.00
Episode length: 838.00 +/- 43.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 838      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1095600  |
---------------------------------
Eval num_timesteps=1097592, episode_reward=-300.00 +/- 0.00
Episode length: 918.40 +/- 206.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 918      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1097592  |
---------------------------------
Eval num_timesteps=1099584, episode_reward=-300.00 +/- 0.00
Episode length: 892.40 +/- 290.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1099584  |
---------------------------------
Eval num_timesteps=1101576, episode_reward=-300.00 +/- 0.00
Episode length: 769.60 +/- 125.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1101576  |
---------------------------------
Eval num_timesteps=1103568, episode_reward=-300.00 +/- 0.00
Episode length: 1004.40 +/- 353.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1103568  |
---------------------------------
Eval num_timesteps=1105560, episode_reward=-300.00 +/- 0.00
Episode length: 1222.80 +/- 453.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1105560  |
---------------------------------
Eval num_timesteps=1107552, episode_reward=-300.00 +/- 0.00
Episode length: 1100.80 +/- 525.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1107552  |
---------------------------------
Eval num_timesteps=1109544, episode_reward=-300.00 +/- 0.00
Episode length: 826.60 +/- 153.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1109544  |
---------------------------------
Eval num_timesteps=1111536, episode_reward=-360.93 +/- 121.86
Episode length: 1102.60 +/- 259.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -361     |
| time/              |          |
|    total_timesteps | 1111536  |
---------------------------------
Eval num_timesteps=1113528, episode_reward=-300.00 +/- 0.00
Episode length: 1019.20 +/- 339.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1113528  |
---------------------------------
Eval num_timesteps=1115520, episode_reward=-300.00 +/- 0.00
Episode length: 1193.80 +/- 273.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1115520  |
---------------------------------
Eval num_timesteps=1117512, episode_reward=-300.00 +/- 0.00
Episode length: 1094.80 +/- 703.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1117512  |
---------------------------------
Eval num_timesteps=1119504, episode_reward=-300.00 +/- 0.00
Episode length: 932.80 +/- 212.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1119504  |
---------------------------------
Eval num_timesteps=1121496, episode_reward=-300.00 +/- 0.00
Episode length: 878.80 +/- 329.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 879      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1121496  |
---------------------------------
Eval num_timesteps=1123488, episode_reward=-300.00 +/- 0.00
Episode length: 1051.40 +/- 202.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1123488  |
---------------------------------
Eval num_timesteps=1125480, episode_reward=-300.00 +/- 0.00
Episode length: 924.20 +/- 263.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 924      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1125480  |
---------------------------------
Eval num_timesteps=1127472, episode_reward=-300.00 +/- 0.00
Episode length: 1032.00 +/- 212.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1127472  |
---------------------------------
Eval num_timesteps=1129464, episode_reward=-300.00 +/- 0.00
Episode length: 1087.00 +/- 277.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1129464  |
---------------------------------
Eval num_timesteps=1131456, episode_reward=-379.43 +/- 158.85
Episode length: 1050.20 +/- 235.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.05e+03    |
|    mean_reward          | -379        |
| time/                   |             |
|    total_timesteps      | 1131456     |
| train/                  |             |
|    approx_kl            | 0.012316168 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.51       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0441     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00802    |
|    std                  | 1.23        |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=1133448, episode_reward=-300.00 +/- 0.00
Episode length: 1245.00 +/- 483.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1133448  |
---------------------------------
Eval num_timesteps=1135440, episode_reward=-300.00 +/- 0.00
Episode length: 849.20 +/- 169.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1135440  |
---------------------------------
Eval num_timesteps=1137432, episode_reward=-300.00 +/- 0.00
Episode length: 1288.20 +/- 590.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1137432  |
---------------------------------
Eval num_timesteps=1139424, episode_reward=-300.00 +/- 0.00
Episode length: 1109.40 +/- 303.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1139424  |
---------------------------------
Eval num_timesteps=1141416, episode_reward=-300.00 +/- 0.00
Episode length: 830.40 +/- 157.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1141416  |
---------------------------------
Eval num_timesteps=1143408, episode_reward=-300.00 +/- 0.00
Episode length: 1132.00 +/- 280.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1143408  |
---------------------------------
Eval num_timesteps=1145400, episode_reward=-300.00 +/- 0.00
Episode length: 991.80 +/- 169.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1145400  |
---------------------------------
Eval num_timesteps=1147392, episode_reward=-300.00 +/- 0.00
Episode length: 909.40 +/- 362.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 909      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1147392  |
---------------------------------
Eval num_timesteps=1149384, episode_reward=-300.00 +/- 0.00
Episode length: 909.60 +/- 351.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1149384  |
---------------------------------
Eval num_timesteps=1151376, episode_reward=-300.00 +/- 0.00
Episode length: 1011.60 +/- 159.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1151376  |
---------------------------------
Eval num_timesteps=1153368, episode_reward=-300.00 +/- 0.00
Episode length: 826.40 +/- 142.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 826      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1153368  |
---------------------------------
Eval num_timesteps=1155360, episode_reward=-300.00 +/- 0.00
Episode length: 892.00 +/- 384.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 892      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1155360  |
---------------------------------
Eval num_timesteps=1157352, episode_reward=-300.00 +/- 0.00
Episode length: 977.40 +/- 234.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 977      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1157352  |
---------------------------------
Eval num_timesteps=1159344, episode_reward=-300.00 +/- 0.00
Episode length: 1157.00 +/- 130.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1159344  |
---------------------------------
Eval num_timesteps=1161336, episode_reward=-300.00 +/- 0.00
Episode length: 986.40 +/- 122.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 986      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1161336  |
---------------------------------
Eval num_timesteps=1163328, episode_reward=-300.00 +/- 0.00
Episode length: 850.20 +/- 251.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 850      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1163328  |
---------------------------------
Eval num_timesteps=1165320, episode_reward=-300.00 +/- 0.00
Episode length: 1130.40 +/- 340.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1165320  |
---------------------------------
Eval num_timesteps=1167312, episode_reward=-300.00 +/- 0.00
Episode length: 922.40 +/- 105.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1167312  |
---------------------------------
Eval num_timesteps=1169304, episode_reward=-300.00 +/- 0.00
Episode length: 1072.80 +/- 236.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1169304  |
---------------------------------
Eval num_timesteps=1171296, episode_reward=-300.00 +/- 0.00
Episode length: 1155.40 +/- 218.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1171296  |
---------------------------------
Eval num_timesteps=1173288, episode_reward=-300.00 +/- 0.00
Episode length: 1018.40 +/- 62.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1173288  |
---------------------------------
Eval num_timesteps=1175280, episode_reward=-300.00 +/- 0.00
Episode length: 843.40 +/- 192.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 843      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1175280  |
---------------------------------
Eval num_timesteps=1177272, episode_reward=-300.00 +/- 0.00
Episode length: 783.40 +/- 154.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1177272  |
---------------------------------
Eval num_timesteps=1179264, episode_reward=-300.00 +/- 0.00
Episode length: 1010.00 +/- 324.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1179264  |
---------------------------------
Eval num_timesteps=1181256, episode_reward=-300.00 +/- 0.00
Episode length: 1068.00 +/- 373.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.07e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1181256     |
| train/                  |             |
|    approx_kl            | 0.013013947 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.55       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0359     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00816    |
|    std                  | 1.24        |
|    value_loss           | 0.136       |
-----------------------------------------
Eval num_timesteps=1183248, episode_reward=-300.00 +/- 0.00
Episode length: 1062.60 +/- 169.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1183248  |
---------------------------------
Eval num_timesteps=1185240, episode_reward=-300.00 +/- 0.00
Episode length: 1063.00 +/- 278.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1185240  |
---------------------------------
Eval num_timesteps=1187232, episode_reward=-377.16 +/- 154.33
Episode length: 1261.80 +/- 652.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 1187232  |
---------------------------------
Eval num_timesteps=1189224, episode_reward=-300.00 +/- 0.00
Episode length: 1253.80 +/- 245.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1189224  |
---------------------------------
Eval num_timesteps=1191216, episode_reward=-300.00 +/- 0.00
Episode length: 1038.40 +/- 366.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1191216  |
---------------------------------
Eval num_timesteps=1193208, episode_reward=-300.00 +/- 0.00
Episode length: 1031.80 +/- 423.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1193208  |
---------------------------------
Eval num_timesteps=1195200, episode_reward=-300.00 +/- 0.00
Episode length: 966.00 +/- 187.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 966      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1195200  |
---------------------------------
Eval num_timesteps=1197192, episode_reward=-300.00 +/- 0.00
Episode length: 946.20 +/- 187.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1197192  |
---------------------------------
Eval num_timesteps=1199184, episode_reward=-300.00 +/- 0.00
Episode length: 867.40 +/- 162.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1199184  |
---------------------------------
Eval num_timesteps=1201176, episode_reward=-300.00 +/- 0.00
Episode length: 1172.20 +/- 335.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1201176  |
---------------------------------
Eval num_timesteps=1203168, episode_reward=-300.00 +/- 0.00
Episode length: 1020.20 +/- 100.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1203168  |
---------------------------------
Eval num_timesteps=1205160, episode_reward=-300.00 +/- 0.00
Episode length: 1012.00 +/- 262.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1205160  |
---------------------------------
Eval num_timesteps=1207152, episode_reward=-300.00 +/- 0.00
Episode length: 1058.80 +/- 266.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1207152  |
---------------------------------
Eval num_timesteps=1209144, episode_reward=-300.00 +/- 0.00
Episode length: 1077.40 +/- 223.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1209144  |
---------------------------------
Eval num_timesteps=1211136, episode_reward=-300.00 +/- 0.00
Episode length: 1136.00 +/- 211.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1211136  |
---------------------------------
Eval num_timesteps=1213128, episode_reward=-300.00 +/- 0.00
Episode length: 1215.00 +/- 210.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1213128  |
---------------------------------
Eval num_timesteps=1215120, episode_reward=-300.00 +/- 0.00
Episode length: 1126.00 +/- 446.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1215120  |
---------------------------------
Eval num_timesteps=1217112, episode_reward=-300.00 +/- 0.00
Episode length: 937.40 +/- 214.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 937      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1217112  |
---------------------------------
Eval num_timesteps=1219104, episode_reward=-300.00 +/- 0.00
Episode length: 1014.60 +/- 301.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1219104  |
---------------------------------
Eval num_timesteps=1221096, episode_reward=-300.00 +/- 0.00
Episode length: 1112.80 +/- 245.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1221096  |
---------------------------------
Eval num_timesteps=1223088, episode_reward=-300.00 +/- 0.00
Episode length: 1190.40 +/- 426.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1223088  |
---------------------------------
Eval num_timesteps=1225080, episode_reward=-300.00 +/- 0.00
Episode length: 1128.80 +/- 443.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1225080  |
---------------------------------
Eval num_timesteps=1227072, episode_reward=-300.00 +/- 0.00
Episode length: 926.40 +/- 375.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1227072  |
---------------------------------
Eval num_timesteps=1229064, episode_reward=-300.00 +/- 0.00
Episode length: 1028.80 +/- 318.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.03e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1229064     |
| train/                  |             |
|    approx_kl            | 0.012862303 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.58       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0419     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00784    |
|    std                  | 1.26        |
|    value_loss           | 0.138       |
-----------------------------------------
Eval num_timesteps=1231056, episode_reward=-300.00 +/- 0.00
Episode length: 834.60 +/- 176.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1231056  |
---------------------------------
Eval num_timesteps=1233048, episode_reward=-300.00 +/- 0.00
Episode length: 1296.40 +/- 597.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1233048  |
---------------------------------
Eval num_timesteps=1235040, episode_reward=-300.00 +/- 0.00
Episode length: 949.60 +/- 252.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 950      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1235040  |
---------------------------------
Eval num_timesteps=1237032, episode_reward=-300.00 +/- 0.00
Episode length: 854.40 +/- 260.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 854      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1237032  |
---------------------------------
Eval num_timesteps=1239024, episode_reward=-300.00 +/- 0.00
Episode length: 824.40 +/- 317.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1239024  |
---------------------------------
Eval num_timesteps=1241016, episode_reward=-300.00 +/- 0.00
Episode length: 806.00 +/- 180.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1241016  |
---------------------------------
Eval num_timesteps=1243008, episode_reward=-300.00 +/- 0.00
Episode length: 767.80 +/- 135.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1243008  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=-300.00 +/- 0.00
Episode length: 1326.00 +/- 419.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
Eval num_timesteps=1246992, episode_reward=-300.00 +/- 0.00
Episode length: 1060.40 +/- 720.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1246992  |
---------------------------------
Eval num_timesteps=1248984, episode_reward=-282.35 +/- 35.29
Episode length: 1230.20 +/- 326.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 1248984  |
---------------------------------
Eval num_timesteps=1250976, episode_reward=-300.00 +/- 0.00
Episode length: 904.60 +/- 320.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 905      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1250976  |
---------------------------------
Eval num_timesteps=1252968, episode_reward=-300.00 +/- 0.00
Episode length: 591.60 +/- 151.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 592      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1252968  |
---------------------------------
Eval num_timesteps=1254960, episode_reward=-300.00 +/- 0.00
Episode length: 691.00 +/- 124.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 691      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1254960  |
---------------------------------
Eval num_timesteps=1256952, episode_reward=-300.00 +/- 0.00
Episode length: 681.60 +/- 174.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 682      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1256952  |
---------------------------------
Eval num_timesteps=1258944, episode_reward=-300.00 +/- 0.00
Episode length: 668.00 +/- 70.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 668      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1258944  |
---------------------------------
Eval num_timesteps=1260936, episode_reward=-300.00 +/- 0.00
Episode length: 1169.80 +/- 275.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1260936  |
---------------------------------
Eval num_timesteps=1262928, episode_reward=-300.00 +/- 0.00
Episode length: 857.20 +/- 207.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1262928  |
---------------------------------
Eval num_timesteps=1264920, episode_reward=-300.00 +/- 0.00
Episode length: 944.40 +/- 273.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 944      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1264920  |
---------------------------------
Eval num_timesteps=1266912, episode_reward=-300.00 +/- 0.00
Episode length: 1203.80 +/- 585.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1266912  |
---------------------------------
Eval num_timesteps=1268904, episode_reward=-300.00 +/- 0.00
Episode length: 1067.20 +/- 247.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1268904  |
---------------------------------
Eval num_timesteps=1270896, episode_reward=-300.00 +/- 0.00
Episode length: 712.00 +/- 143.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 712      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1270896  |
---------------------------------
Eval num_timesteps=1272888, episode_reward=-300.00 +/- 0.00
Episode length: 674.60 +/- 153.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 675      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1272888  |
---------------------------------
Eval num_timesteps=1274880, episode_reward=-300.00 +/- 0.00
Episode length: 774.00 +/- 194.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 774      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1274880  |
---------------------------------
Eval num_timesteps=1276872, episode_reward=-300.00 +/- 0.00
Episode length: 828.20 +/- 266.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 828      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1276872  |
---------------------------------
Eval num_timesteps=1278864, episode_reward=-300.00 +/- 0.00
Episode length: 794.40 +/- 155.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 794         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1278864     |
| train/                  |             |
|    approx_kl            | 0.013217695 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.63       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.001       |
|    loss                 | -0.039      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00906    |
|    std                  | 1.27        |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=1280856, episode_reward=-300.00 +/- 0.00
Episode length: 703.20 +/- 166.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 703      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1280856  |
---------------------------------
Eval num_timesteps=1282848, episode_reward=-300.00 +/- 0.00
Episode length: 783.40 +/- 188.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 783      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1282848  |
---------------------------------
Eval num_timesteps=1284840, episode_reward=-300.00 +/- 0.00
Episode length: 626.00 +/- 84.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1284840  |
---------------------------------
Eval num_timesteps=1286832, episode_reward=-300.00 +/- 0.00
Episode length: 951.40 +/- 312.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 951      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1286832  |
---------------------------------
Eval num_timesteps=1288824, episode_reward=-300.00 +/- 0.00
Episode length: 1011.80 +/- 305.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1288824  |
---------------------------------
Eval num_timesteps=1290816, episode_reward=-300.00 +/- 0.00
Episode length: 768.20 +/- 218.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 768      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1290816  |
---------------------------------
Eval num_timesteps=1292808, episode_reward=-300.00 +/- 0.00
Episode length: 841.20 +/- 274.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 841      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1292808  |
---------------------------------
Eval num_timesteps=1294800, episode_reward=-300.00 +/- 0.00
Episode length: 742.80 +/- 198.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1294800  |
---------------------------------
Eval num_timesteps=1296792, episode_reward=-300.00 +/- 0.00
Episode length: 869.00 +/- 215.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 869      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1296792  |
---------------------------------
Eval num_timesteps=1298784, episode_reward=-300.00 +/- 0.00
Episode length: 965.20 +/- 192.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 965      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1298784  |
---------------------------------
Eval num_timesteps=1300776, episode_reward=-300.00 +/- 0.00
Episode length: 634.80 +/- 162.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1300776  |
---------------------------------
Eval num_timesteps=1302768, episode_reward=-300.00 +/- 0.00
Episode length: 708.00 +/- 318.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 708      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1302768  |
---------------------------------
Eval num_timesteps=1304760, episode_reward=-300.00 +/- 0.00
Episode length: 724.20 +/- 163.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1304760  |
---------------------------------
Eval num_timesteps=1306752, episode_reward=-300.00 +/- 0.00
Episode length: 828.80 +/- 277.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1306752  |
---------------------------------
Eval num_timesteps=1308744, episode_reward=-300.00 +/- 0.00
Episode length: 743.20 +/- 140.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1308744  |
---------------------------------
Eval num_timesteps=1310736, episode_reward=-300.00 +/- 0.00
Episode length: 748.00 +/- 119.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1310736  |
---------------------------------
Eval num_timesteps=1312728, episode_reward=-300.00 +/- 0.00
Episode length: 847.20 +/- 93.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 847      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1312728  |
---------------------------------
Eval num_timesteps=1314720, episode_reward=-300.00 +/- 0.00
Episode length: 919.80 +/- 255.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1314720  |
---------------------------------
Eval num_timesteps=1316712, episode_reward=-300.00 +/- 0.00
Episode length: 852.20 +/- 253.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1316712  |
---------------------------------
Eval num_timesteps=1318704, episode_reward=-300.00 +/- 0.00
Episode length: 997.60 +/- 256.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 998      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1318704  |
---------------------------------
Eval num_timesteps=1320696, episode_reward=-300.00 +/- 0.00
Episode length: 702.00 +/- 90.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 702      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1320696  |
---------------------------------
Eval num_timesteps=1322688, episode_reward=-300.00 +/- 0.00
Episode length: 895.20 +/- 261.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 895      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1322688  |
---------------------------------
Eval num_timesteps=1324680, episode_reward=-300.00 +/- 0.00
Episode length: 915.80 +/- 318.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 916      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1324680  |
---------------------------------
Eval num_timesteps=1326672, episode_reward=-300.00 +/- 0.00
Episode length: 637.60 +/- 64.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 638      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1326672  |
---------------------------------
Eval num_timesteps=1328664, episode_reward=-300.00 +/- 0.00
Episode length: 992.20 +/- 644.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 992         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1328664     |
| train/                  |             |
|    approx_kl            | 0.009708756 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0313      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00916    |
|    std                  | 1.28        |
|    value_loss           | 0.211       |
-----------------------------------------
Eval num_timesteps=1330656, episode_reward=-300.00 +/- 0.00
Episode length: 861.20 +/- 162.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1330656  |
---------------------------------
Eval num_timesteps=1332648, episode_reward=-300.00 +/- 0.00
Episode length: 757.40 +/- 88.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1332648  |
---------------------------------
Eval num_timesteps=1334640, episode_reward=-300.00 +/- 0.00
Episode length: 701.00 +/- 155.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 701      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1334640  |
---------------------------------
Eval num_timesteps=1336632, episode_reward=-300.00 +/- 0.00
Episode length: 630.80 +/- 186.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1336632  |
---------------------------------
Eval num_timesteps=1338624, episode_reward=-300.00 +/- 0.00
Episode length: 684.40 +/- 134.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 684      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1338624  |
---------------------------------
Eval num_timesteps=1340616, episode_reward=-300.00 +/- 0.00
Episode length: 922.60 +/- 327.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1340616  |
---------------------------------
Eval num_timesteps=1342608, episode_reward=-300.00 +/- 0.00
Episode length: 782.20 +/- 81.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 782      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1342608  |
---------------------------------
Eval num_timesteps=1344600, episode_reward=-300.00 +/- 0.00
Episode length: 751.60 +/- 148.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 752      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1344600  |
---------------------------------
Eval num_timesteps=1346592, episode_reward=-300.00 +/- 0.00
Episode length: 599.00 +/- 85.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 599      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1346592  |
---------------------------------
Eval num_timesteps=1348584, episode_reward=-300.00 +/- 0.00
Episode length: 789.40 +/- 133.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 789      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1348584  |
---------------------------------
Eval num_timesteps=1350576, episode_reward=-300.00 +/- 0.00
Episode length: 880.80 +/- 193.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 881      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1350576  |
---------------------------------
Eval num_timesteps=1352568, episode_reward=-300.00 +/- 0.00
Episode length: 567.80 +/- 142.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 568      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1352568  |
---------------------------------
Eval num_timesteps=1354560, episode_reward=-300.00 +/- 0.00
Episode length: 1031.60 +/- 306.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1354560  |
---------------------------------
Eval num_timesteps=1356552, episode_reward=-300.00 +/- 0.00
Episode length: 715.40 +/- 229.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 715      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1356552  |
---------------------------------
Eval num_timesteps=1358544, episode_reward=-300.00 +/- 0.00
Episode length: 707.20 +/- 132.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 707      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1358544  |
---------------------------------
Eval num_timesteps=1360536, episode_reward=-300.00 +/- 0.00
Episode length: 746.00 +/- 79.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1360536  |
---------------------------------
Eval num_timesteps=1362528, episode_reward=-300.00 +/- 0.00
Episode length: 775.00 +/- 146.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 775      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1362528  |
---------------------------------
Eval num_timesteps=1364520, episode_reward=-300.00 +/- 0.00
Episode length: 646.40 +/- 54.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1364520  |
---------------------------------
Eval num_timesteps=1366512, episode_reward=-300.00 +/- 0.00
Episode length: 922.00 +/- 425.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 922      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1366512  |
---------------------------------
Eval num_timesteps=1368504, episode_reward=-300.00 +/- 0.00
Episode length: 741.80 +/- 241.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 742      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1368504  |
---------------------------------
Eval num_timesteps=1370496, episode_reward=-300.00 +/- 0.00
Episode length: 721.60 +/- 116.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 722      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1370496  |
---------------------------------
Eval num_timesteps=1372488, episode_reward=-300.00 +/- 0.00
Episode length: 926.60 +/- 255.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 927      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1372488  |
---------------------------------
Eval num_timesteps=1374480, episode_reward=-300.00 +/- 0.00
Episode length: 862.00 +/- 273.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1374480  |
---------------------------------
Eval num_timesteps=1376472, episode_reward=-300.00 +/- 0.00
Episode length: 684.00 +/- 59.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 684       |
|    mean_reward          | -300      |
| time/                   |           |
|    total_timesteps      | 1376472   |
| train/                  |           |
|    approx_kl            | 0.0114076 |
|    clip_fraction        | 0.135     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.68     |
|    explained_variance   | 0.768     |
|    learning_rate        | 0.001     |
|    loss                 | -0.00422  |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.00624  |
|    std                  | 1.29      |
|    value_loss           | 0.273     |
---------------------------------------
Eval num_timesteps=1378464, episode_reward=-300.00 +/- 0.00
Episode length: 987.80 +/- 288.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1378464  |
---------------------------------
Eval num_timesteps=1380456, episode_reward=-300.00 +/- 0.00
Episode length: 747.60 +/- 81.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 748      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1380456  |
---------------------------------
Eval num_timesteps=1382448, episode_reward=-300.00 +/- 0.00
Episode length: 792.60 +/- 116.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 793      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1382448  |
---------------------------------
Eval num_timesteps=1384440, episode_reward=-300.00 +/- 0.00
Episode length: 840.00 +/- 289.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 840      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1384440  |
---------------------------------
Eval num_timesteps=1386432, episode_reward=-300.00 +/- 0.00
Episode length: 777.80 +/- 71.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1386432  |
---------------------------------
Eval num_timesteps=1388424, episode_reward=-300.00 +/- 0.00
Episode length: 703.60 +/- 180.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 704      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1388424  |
---------------------------------
Eval num_timesteps=1390416, episode_reward=-300.00 +/- 0.00
Episode length: 787.40 +/- 177.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 787      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1390416  |
---------------------------------
Eval num_timesteps=1392408, episode_reward=-300.00 +/- 0.00
Episode length: 992.40 +/- 185.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1392408  |
---------------------------------
Eval num_timesteps=1394400, episode_reward=-300.00 +/- 0.00
Episode length: 796.60 +/- 131.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1394400  |
---------------------------------
Eval num_timesteps=1396392, episode_reward=-300.00 +/- 0.00
Episode length: 721.40 +/- 66.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1396392  |
---------------------------------
Eval num_timesteps=1398384, episode_reward=-300.00 +/- 0.00
Episode length: 831.80 +/- 217.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 832      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1398384  |
---------------------------------
Eval num_timesteps=1400376, episode_reward=-300.00 +/- 0.00
Episode length: 925.60 +/- 161.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1400376  |
---------------------------------
Eval num_timesteps=1402368, episode_reward=-300.00 +/- 0.00
Episode length: 720.60 +/- 100.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 721      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1402368  |
---------------------------------
Eval num_timesteps=1404360, episode_reward=-300.00 +/- 0.00
Episode length: 604.00 +/- 48.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 604      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1404360  |
---------------------------------
Eval num_timesteps=1406352, episode_reward=-300.00 +/- 0.00
Episode length: 874.60 +/- 270.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 875      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1406352  |
---------------------------------
Eval num_timesteps=1408344, episode_reward=-300.00 +/- 0.00
Episode length: 756.80 +/- 106.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 757      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1408344  |
---------------------------------
Eval num_timesteps=1410336, episode_reward=-300.00 +/- 0.00
Episode length: 1045.00 +/- 359.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1410336  |
---------------------------------
Eval num_timesteps=1412328, episode_reward=-300.00 +/- 0.00
Episode length: 754.60 +/- 322.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1412328  |
---------------------------------
Eval num_timesteps=1414320, episode_reward=-300.00 +/- 0.00
Episode length: 942.40 +/- 271.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 942      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1414320  |
---------------------------------
Eval num_timesteps=1416312, episode_reward=-300.00 +/- 0.00
Episode length: 825.20 +/- 171.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 825      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1416312  |
---------------------------------
Eval num_timesteps=1418304, episode_reward=-300.00 +/- 0.00
Episode length: 1110.20 +/- 499.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1418304  |
---------------------------------
Eval num_timesteps=1420296, episode_reward=-300.00 +/- 0.00
Episode length: 693.80 +/- 117.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 694      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1420296  |
---------------------------------
Eval num_timesteps=1422288, episode_reward=-300.00 +/- 0.00
Episode length: 697.40 +/- 113.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 697      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1422288  |
---------------------------------
Eval num_timesteps=1424280, episode_reward=-300.00 +/- 0.00
Episode length: 783.80 +/- 77.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1424280  |
---------------------------------
Eval num_timesteps=1426272, episode_reward=-300.00 +/- 0.00
Episode length: 1000.40 +/- 575.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1426272     |
| train/                  |             |
|    approx_kl            | 0.013385688 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.74       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0151      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00655    |
|    std                  | 1.31        |
|    value_loss           | 0.312       |
-----------------------------------------
Eval num_timesteps=1428264, episode_reward=-300.00 +/- 0.00
Episode length: 762.00 +/- 150.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 762      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1428264  |
---------------------------------
Eval num_timesteps=1430256, episode_reward=-300.00 +/- 0.00
Episode length: 728.80 +/- 83.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 729      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1430256  |
---------------------------------
Eval num_timesteps=1432248, episode_reward=-300.00 +/- 0.00
Episode length: 635.00 +/- 95.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 635      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1432248  |
---------------------------------
Eval num_timesteps=1434240, episode_reward=-300.00 +/- 0.00
Episode length: 976.00 +/- 251.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 976      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1434240  |
---------------------------------
Eval num_timesteps=1436232, episode_reward=-300.00 +/- 0.00
Episode length: 867.40 +/- 336.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 867      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1436232  |
---------------------------------
Eval num_timesteps=1438224, episode_reward=-300.00 +/- 0.00
Episode length: 751.20 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 751      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1438224  |
---------------------------------
Eval num_timesteps=1440216, episode_reward=-300.00 +/- 0.00
Episode length: 802.40 +/- 92.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 802      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1440216  |
---------------------------------
Eval num_timesteps=1442208, episode_reward=-300.00 +/- 0.00
Episode length: 617.00 +/- 25.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 617      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1442208  |
---------------------------------
Eval num_timesteps=1444200, episode_reward=-300.00 +/- 0.00
Episode length: 736.60 +/- 98.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 737      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1444200  |
---------------------------------
Eval num_timesteps=1446192, episode_reward=-300.00 +/- 0.00
Episode length: 807.80 +/- 88.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1446192  |
---------------------------------
Eval num_timesteps=1448184, episode_reward=-300.00 +/- 0.00
Episode length: 808.40 +/- 167.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1448184  |
---------------------------------
Eval num_timesteps=1450176, episode_reward=-300.00 +/- 0.00
Episode length: 680.20 +/- 32.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 680      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1450176  |
---------------------------------
Eval num_timesteps=1452168, episode_reward=-300.00 +/- 0.00
Episode length: 800.80 +/- 109.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1452168  |
---------------------------------
Eval num_timesteps=1454160, episode_reward=-300.00 +/- 0.00
Episode length: 764.40 +/- 145.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 764      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1454160  |
---------------------------------
Eval num_timesteps=1456152, episode_reward=-300.00 +/- 0.00
Episode length: 797.80 +/- 224.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 798      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1456152  |
---------------------------------
Eval num_timesteps=1458144, episode_reward=-300.00 +/- 0.00
Episode length: 735.00 +/- 135.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1458144  |
---------------------------------
Eval num_timesteps=1460136, episode_reward=-300.00 +/- 0.00
Episode length: 926.00 +/- 462.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 926      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1460136  |
---------------------------------
Eval num_timesteps=1462128, episode_reward=-300.00 +/- 0.00
Episode length: 668.60 +/- 210.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 669      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1462128  |
---------------------------------
Eval num_timesteps=1464120, episode_reward=-300.00 +/- 0.00
Episode length: 716.80 +/- 40.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 717      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1464120  |
---------------------------------
Eval num_timesteps=1466112, episode_reward=-300.00 +/- 0.00
Episode length: 626.60 +/- 131.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 627      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1466112  |
---------------------------------
Eval num_timesteps=1468104, episode_reward=-300.00 +/- 0.00
Episode length: 781.00 +/- 95.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 781      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1468104  |
---------------------------------
Eval num_timesteps=1470096, episode_reward=-300.00 +/- 0.00
Episode length: 814.00 +/- 118.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1470096  |
---------------------------------
Eval num_timesteps=1472088, episode_reward=-300.00 +/- 0.00
Episode length: 824.00 +/- 275.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1472088  |
---------------------------------
Eval num_timesteps=1474080, episode_reward=-300.00 +/- 0.00
Episode length: 724.00 +/- 108.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 724      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1474080  |
---------------------------------
Eval num_timesteps=1476072, episode_reward=-300.00 +/- 0.00
Episode length: 784.20 +/- 62.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 784         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1476072     |
| train/                  |             |
|    approx_kl            | 0.012741397 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.78       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0303     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00736    |
|    std                  | 1.32        |
|    value_loss           | 0.221       |
-----------------------------------------
Eval num_timesteps=1478064, episode_reward=-300.00 +/- 0.00
Episode length: 686.60 +/- 109.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 687      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1478064  |
---------------------------------
Eval num_timesteps=1480056, episode_reward=-300.00 +/- 0.00
Episode length: 732.00 +/- 279.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1480056  |
---------------------------------
Eval num_timesteps=1482048, episode_reward=-300.00 +/- 0.00
Episode length: 742.60 +/- 23.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 743      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1482048  |
---------------------------------
Eval num_timesteps=1484040, episode_reward=-300.00 +/- 0.00
Episode length: 855.40 +/- 111.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 855      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1484040  |
---------------------------------
Eval num_timesteps=1486032, episode_reward=-300.00 +/- 0.00
Episode length: 1166.60 +/- 465.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1486032  |
---------------------------------
Eval num_timesteps=1488024, episode_reward=-300.00 +/- 0.00
Episode length: 901.20 +/- 91.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 901      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1488024  |
---------------------------------
Eval num_timesteps=1490016, episode_reward=-300.00 +/- 0.00
Episode length: 725.40 +/- 158.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1490016  |
---------------------------------
Eval num_timesteps=1492008, episode_reward=-300.00 +/- 0.00
Episode length: 792.00 +/- 376.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 792      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1492008  |
---------------------------------
Eval num_timesteps=1494000, episode_reward=-300.00 +/- 0.00
Episode length: 858.80 +/- 197.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1495992, episode_reward=-300.00 +/- 0.00
Episode length: 858.80 +/- 209.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1495992  |
---------------------------------
Eval num_timesteps=1497984, episode_reward=-300.00 +/- 0.00
Episode length: 1030.40 +/- 501.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1497984  |
---------------------------------
Eval num_timesteps=1499976, episode_reward=-300.00 +/- 0.00
Episode length: 674.40 +/- 188.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 674      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1499976  |
---------------------------------
Eval num_timesteps=1501968, episode_reward=-300.00 +/- 0.00
Episode length: 624.40 +/- 92.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 624      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1501968  |
---------------------------------
Eval num_timesteps=1503960, episode_reward=-300.00 +/- 0.00
Episode length: 938.20 +/- 152.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 938      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1503960  |
---------------------------------
Eval num_timesteps=1505952, episode_reward=-300.00 +/- 0.00
Episode length: 654.00 +/- 83.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 654      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1505952  |
---------------------------------
Eval num_timesteps=1507944, episode_reward=-300.00 +/- 0.00
Episode length: 882.60 +/- 300.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 883      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1507944  |
---------------------------------
Eval num_timesteps=1509936, episode_reward=-300.00 +/- 0.00
Episode length: 769.80 +/- 196.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 770      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1509936  |
---------------------------------
Eval num_timesteps=1511928, episode_reward=-300.00 +/- 0.00
Episode length: 731.80 +/- 124.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 732      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1511928  |
---------------------------------
Eval num_timesteps=1513920, episode_reward=-300.00 +/- 0.00
Episode length: 745.80 +/- 163.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1513920  |
---------------------------------
Eval num_timesteps=1515912, episode_reward=-300.00 +/- 0.00
Episode length: 798.80 +/- 334.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 799      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1515912  |
---------------------------------
Eval num_timesteps=1517904, episode_reward=-300.00 +/- 0.00
Episode length: 785.60 +/- 115.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 786      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1517904  |
---------------------------------
Eval num_timesteps=1519896, episode_reward=-300.00 +/- 0.00
Episode length: 936.00 +/- 288.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 936      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1519896  |
---------------------------------
Eval num_timesteps=1521888, episode_reward=-300.00 +/- 0.00
Episode length: 1230.00 +/- 831.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1521888  |
---------------------------------
Eval num_timesteps=1523880, episode_reward=-300.00 +/- 0.00
Episode length: 861.60 +/- 184.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 862         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1523880     |
| train/                  |             |
|    approx_kl            | 0.013685799 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.8        |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0224     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0088     |
|    std                  | 1.33        |
|    value_loss           | 0.189       |
-----------------------------------------
Eval num_timesteps=1525872, episode_reward=-300.00 +/- 0.00
Episode length: 622.00 +/- 115.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 622      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1525872  |
---------------------------------
Eval num_timesteps=1527864, episode_reward=-300.00 +/- 0.00
Episode length: 1194.80 +/- 355.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1527864  |
---------------------------------
Eval num_timesteps=1529856, episode_reward=-300.00 +/- 0.00
Episode length: 903.60 +/- 289.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 904      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1529856  |
---------------------------------
Eval num_timesteps=1531848, episode_reward=-300.00 +/- 0.00
Episode length: 919.40 +/- 294.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 919      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1531848  |
---------------------------------
Eval num_timesteps=1533840, episode_reward=-300.00 +/- 0.00
Episode length: 1032.80 +/- 292.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1533840  |
---------------------------------
Eval num_timesteps=1535832, episode_reward=-300.00 +/- 0.00
Episode length: 823.00 +/- 311.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 823      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1535832  |
---------------------------------
Eval num_timesteps=1537824, episode_reward=-300.00 +/- 0.00
Episode length: 763.00 +/- 254.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 763      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1537824  |
---------------------------------
Eval num_timesteps=1539816, episode_reward=-300.00 +/- 0.00
Episode length: 833.20 +/- 264.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1539816  |
---------------------------------
Eval num_timesteps=1541808, episode_reward=-300.00 +/- 0.00
Episode length: 766.60 +/- 145.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 767      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1541808  |
---------------------------------
Eval num_timesteps=1543800, episode_reward=-300.00 +/- 0.00
Episode length: 741.00 +/- 136.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 741      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1543800  |
---------------------------------
Eval num_timesteps=1545792, episode_reward=-300.00 +/- 0.00
Episode length: 1089.60 +/- 265.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.09e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1545792  |
---------------------------------
Eval num_timesteps=1547784, episode_reward=-300.00 +/- 0.00
Episode length: 832.60 +/- 194.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 833      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1547784  |
---------------------------------
Eval num_timesteps=1549776, episode_reward=-300.00 +/- 0.00
Episode length: 772.40 +/- 148.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 772      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1549776  |
---------------------------------
Eval num_timesteps=1551768, episode_reward=-300.00 +/- 0.00
Episode length: 847.80 +/- 190.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 848      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1551768  |
---------------------------------
Eval num_timesteps=1553760, episode_reward=-300.00 +/- 0.00
Episode length: 844.00 +/- 269.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 844      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1553760  |
---------------------------------
Eval num_timesteps=1555752, episode_reward=-300.00 +/- 0.00
Episode length: 933.00 +/- 313.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 933      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1555752  |
---------------------------------
Eval num_timesteps=1557744, episode_reward=-300.00 +/- 0.00
Episode length: 851.60 +/- 224.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 852      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1557744  |
---------------------------------
Eval num_timesteps=1559736, episode_reward=-300.00 +/- 0.00
Episode length: 839.00 +/- 205.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 839      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1559736  |
---------------------------------
Eval num_timesteps=1561728, episode_reward=-300.00 +/- 0.00
Episode length: 887.60 +/- 359.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 888      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1561728  |
---------------------------------
Eval num_timesteps=1563720, episode_reward=-300.00 +/- 0.00
Episode length: 708.80 +/- 82.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 709      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1563720  |
---------------------------------
Eval num_timesteps=1565712, episode_reward=-300.00 +/- 0.00
Episode length: 869.80 +/- 305.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 870      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1565712  |
---------------------------------
Eval num_timesteps=1567704, episode_reward=-300.00 +/- 0.00
Episode length: 613.20 +/- 39.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 613      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1567704  |
---------------------------------
Eval num_timesteps=1569696, episode_reward=-300.00 +/- 0.00
Episode length: 1108.60 +/- 217.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1569696  |
---------------------------------
Eval num_timesteps=1571688, episode_reward=-300.00 +/- 0.00
Episode length: 696.40 +/- 94.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1571688  |
---------------------------------
Eval num_timesteps=1573680, episode_reward=-300.00 +/- 0.00
Episode length: 853.40 +/- 195.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 853         |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1573680     |
| train/                  |             |
|    approx_kl            | 0.013263348 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.85       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0278     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00992    |
|    std                  | 1.34        |
|    value_loss           | 0.203       |
-----------------------------------------
Eval num_timesteps=1575672, episode_reward=-300.00 +/- 0.00
Episode length: 922.80 +/- 360.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 923      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1575672  |
---------------------------------
Eval num_timesteps=1577664, episode_reward=-300.00 +/- 0.00
Episode length: 1008.80 +/- 161.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1577664  |
---------------------------------
Eval num_timesteps=1579656, episode_reward=-300.00 +/- 0.00
Episode length: 800.40 +/- 246.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 800      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1579656  |
---------------------------------
Eval num_timesteps=1581648, episode_reward=-300.00 +/- 0.00
Episode length: 980.40 +/- 162.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 980      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1581648  |
---------------------------------
Eval num_timesteps=1583640, episode_reward=-300.00 +/- 0.00
Episode length: 1058.60 +/- 224.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1583640  |
---------------------------------
Eval num_timesteps=1585632, episode_reward=-300.00 +/- 0.00
Episode length: 1052.00 +/- 153.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1585632  |
---------------------------------
Eval num_timesteps=1587624, episode_reward=-300.00 +/- 0.00
Episode length: 1215.80 +/- 311.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1587624  |
---------------------------------
Eval num_timesteps=1589616, episode_reward=-300.00 +/- 0.00
Episode length: 1117.60 +/- 411.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1589616  |
---------------------------------
Eval num_timesteps=1591608, episode_reward=-300.00 +/- 0.00
Episode length: 1114.40 +/- 189.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1591608  |
---------------------------------
Eval num_timesteps=1593600, episode_reward=-300.00 +/- 0.00
Episode length: 743.60 +/- 278.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 744      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1593600  |
---------------------------------
Eval num_timesteps=1595592, episode_reward=-300.00 +/- 0.00
Episode length: 1171.60 +/- 343.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1595592  |
---------------------------------
Eval num_timesteps=1597584, episode_reward=-300.00 +/- 0.00
Episode length: 991.60 +/- 215.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 992      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1597584  |
---------------------------------
Eval num_timesteps=1599576, episode_reward=-300.00 +/- 0.00
Episode length: 836.20 +/- 193.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 836      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1599576  |
---------------------------------
Eval num_timesteps=1601568, episode_reward=-300.00 +/- 0.00
Episode length: 1046.60 +/- 99.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1601568  |
---------------------------------
Eval num_timesteps=1603560, episode_reward=-300.00 +/- 0.00
Episode length: 1167.20 +/- 253.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1603560  |
---------------------------------
Eval num_timesteps=1605552, episode_reward=-300.00 +/- 0.00
Episode length: 857.00 +/- 248.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 857      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1605552  |
---------------------------------
Eval num_timesteps=1607544, episode_reward=-300.00 +/- 0.00
Episode length: 1079.80 +/- 329.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1607544  |
---------------------------------
Eval num_timesteps=1609536, episode_reward=-300.00 +/- 0.00
Episode length: 1082.80 +/- 409.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1609536  |
---------------------------------
Eval num_timesteps=1611528, episode_reward=-300.00 +/- 0.00
Episode length: 1014.60 +/- 240.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1611528  |
---------------------------------
Eval num_timesteps=1613520, episode_reward=-300.00 +/- 0.00
Episode length: 859.20 +/- 361.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 859      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1613520  |
---------------------------------
Eval num_timesteps=1615512, episode_reward=-300.00 +/- 0.00
Episode length: 1264.80 +/- 383.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1615512  |
---------------------------------
Eval num_timesteps=1617504, episode_reward=-300.00 +/- 0.00
Episode length: 946.20 +/- 372.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 946      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1617504  |
---------------------------------
Eval num_timesteps=1619496, episode_reward=-300.00 +/- 0.00
Episode length: 758.00 +/- 234.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 758      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1619496  |
---------------------------------
Eval num_timesteps=1621488, episode_reward=-300.00 +/- 0.00
Episode length: 1122.80 +/- 520.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1621488  |
---------------------------------
Eval num_timesteps=1623480, episode_reward=-300.00 +/- 0.00
Episode length: 1133.60 +/- 363.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1623480     |
| train/                  |             |
|    approx_kl            | 0.017115355 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.89       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0454     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00659    |
|    std                  | 1.36        |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=1625472, episode_reward=-300.00 +/- 0.00
Episode length: 1212.40 +/- 241.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1625472  |
---------------------------------
Eval num_timesteps=1627464, episode_reward=-300.00 +/- 0.00
Episode length: 967.20 +/- 208.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 967      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1627464  |
---------------------------------
Eval num_timesteps=1629456, episode_reward=-300.00 +/- 0.00
Episode length: 982.40 +/- 325.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 982      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1629456  |
---------------------------------
Eval num_timesteps=1631448, episode_reward=-359.86 +/- 119.71
Episode length: 1143.80 +/- 445.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1631448  |
---------------------------------
Eval num_timesteps=1633440, episode_reward=-300.00 +/- 0.00
Episode length: 1129.40 +/- 248.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1633440  |
---------------------------------
Eval num_timesteps=1635432, episode_reward=-300.00 +/- 0.00
Episode length: 1161.60 +/- 361.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1635432  |
---------------------------------
Eval num_timesteps=1637424, episode_reward=-300.00 +/- 0.00
Episode length: 1033.00 +/- 334.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1637424  |
---------------------------------
Eval num_timesteps=1639416, episode_reward=-300.00 +/- 0.00
Episode length: 1409.60 +/- 383.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1639416  |
---------------------------------
Eval num_timesteps=1641408, episode_reward=-300.00 +/- 0.00
Episode length: 1418.20 +/- 312.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.42e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1641408  |
---------------------------------
Eval num_timesteps=1643400, episode_reward=-300.00 +/- 0.00
Episode length: 965.40 +/- 250.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 965      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1643400  |
---------------------------------
Eval num_timesteps=1645392, episode_reward=-300.00 +/- 0.00
Episode length: 1341.60 +/- 659.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1645392  |
---------------------------------
Eval num_timesteps=1647384, episode_reward=-300.00 +/- 0.00
Episode length: 1156.20 +/- 142.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1647384  |
---------------------------------
Eval num_timesteps=1649376, episode_reward=-300.00 +/- 0.00
Episode length: 1210.60 +/- 164.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1649376  |
---------------------------------
Eval num_timesteps=1651368, episode_reward=-300.00 +/- 0.00
Episode length: 1206.60 +/- 386.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1651368  |
---------------------------------
Eval num_timesteps=1653360, episode_reward=-300.00 +/- 0.00
Episode length: 1444.60 +/- 465.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.44e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1653360  |
---------------------------------
Eval num_timesteps=1655352, episode_reward=-300.00 +/- 0.00
Episode length: 979.20 +/- 76.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 979      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1655352  |
---------------------------------
Eval num_timesteps=1657344, episode_reward=-300.00 +/- 0.00
Episode length: 1072.00 +/- 512.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1657344  |
---------------------------------
Eval num_timesteps=1659336, episode_reward=-300.00 +/- 0.00
Episode length: 1049.60 +/- 339.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1659336  |
---------------------------------
Eval num_timesteps=1661328, episode_reward=-300.00 +/- 0.00
Episode length: 1220.60 +/- 138.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1661328  |
---------------------------------
Eval num_timesteps=1663320, episode_reward=-300.00 +/- 0.00
Episode length: 1134.40 +/- 154.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1663320  |
---------------------------------
Eval num_timesteps=1665312, episode_reward=-300.00 +/- 0.00
Episode length: 1141.60 +/- 230.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1665312  |
---------------------------------
Eval num_timesteps=1667304, episode_reward=-300.00 +/- 0.00
Episode length: 1047.20 +/- 340.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1667304  |
---------------------------------
Eval num_timesteps=1669296, episode_reward=-300.00 +/- 0.00
Episode length: 1003.80 +/- 165.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1669296  |
---------------------------------
Eval num_timesteps=1671288, episode_reward=-300.00 +/- 0.00
Episode length: 1207.80 +/- 161.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.21e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1671288     |
| train/                  |             |
|    approx_kl            | 0.015957747 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0614     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00946    |
|    std                  | 1.36        |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=1673280, episode_reward=-300.00 +/- 0.00
Episode length: 1021.20 +/- 301.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1673280  |
---------------------------------
Eval num_timesteps=1675272, episode_reward=-300.00 +/- 0.00
Episode length: 1172.60 +/- 276.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.17e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1675272  |
---------------------------------
Eval num_timesteps=1677264, episode_reward=-300.00 +/- 0.00
Episode length: 1328.00 +/- 337.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1677264  |
---------------------------------
Eval num_timesteps=1679256, episode_reward=-300.00 +/- 0.00
Episode length: 1331.20 +/- 328.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1679256  |
---------------------------------
Eval num_timesteps=1681248, episode_reward=-300.00 +/- 0.00
Episode length: 1470.40 +/- 393.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.47e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1681248  |
---------------------------------
Eval num_timesteps=1683240, episode_reward=-300.00 +/- 0.00
Episode length: 796.60 +/- 187.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 797      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1683240  |
---------------------------------
Eval num_timesteps=1685232, episode_reward=-300.00 +/- 0.00
Episode length: 1096.80 +/- 101.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1685232  |
---------------------------------
Eval num_timesteps=1687224, episode_reward=-300.00 +/- 0.00
Episode length: 1229.60 +/- 84.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1687224  |
---------------------------------
Eval num_timesteps=1689216, episode_reward=-300.00 +/- 0.00
Episode length: 1081.80 +/- 163.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1689216  |
---------------------------------
Eval num_timesteps=1691208, episode_reward=-300.00 +/- 0.00
Episode length: 1122.80 +/- 169.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1691208  |
---------------------------------
Eval num_timesteps=1693200, episode_reward=-300.00 +/- 0.00
Episode length: 1422.80 +/- 279.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.42e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1693200  |
---------------------------------
Eval num_timesteps=1695192, episode_reward=-300.00 +/- 0.00
Episode length: 1112.00 +/- 353.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1695192  |
---------------------------------
Eval num_timesteps=1697184, episode_reward=-300.00 +/- 0.00
Episode length: 1163.40 +/- 405.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1697184  |
---------------------------------
Eval num_timesteps=1699176, episode_reward=-300.00 +/- 0.00
Episode length: 1070.20 +/- 191.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1699176  |
---------------------------------
Eval num_timesteps=1701168, episode_reward=-300.00 +/- 0.00
Episode length: 1157.00 +/- 303.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1701168  |
---------------------------------
Eval num_timesteps=1703160, episode_reward=-300.00 +/- 0.00
Episode length: 1531.40 +/- 564.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.53e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1703160  |
---------------------------------
Eval num_timesteps=1705152, episode_reward=-300.00 +/- 0.00
Episode length: 1187.60 +/- 316.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1705152  |
---------------------------------
Eval num_timesteps=1707144, episode_reward=-300.00 +/- 0.00
Episode length: 1133.00 +/- 276.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1707144  |
---------------------------------
Eval num_timesteps=1709136, episode_reward=-300.00 +/- 0.00
Episode length: 1249.40 +/- 104.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1709136  |
---------------------------------
Eval num_timesteps=1711128, episode_reward=-300.00 +/- 0.00
Episode length: 1508.00 +/- 276.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1711128  |
---------------------------------
Eval num_timesteps=1713120, episode_reward=-300.00 +/- 0.00
Episode length: 1344.00 +/- 349.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1713120  |
---------------------------------
Eval num_timesteps=1715112, episode_reward=-300.00 +/- 0.00
Episode length: 1095.00 +/- 240.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1715112  |
---------------------------------
Eval num_timesteps=1717104, episode_reward=-300.00 +/- 0.00
Episode length: 1046.40 +/- 179.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1717104  |
---------------------------------
Eval num_timesteps=1719096, episode_reward=-300.00 +/- 0.00
Episode length: 1049.40 +/- 227.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1719096  |
---------------------------------
Eval num_timesteps=1721088, episode_reward=-300.00 +/- 0.00
Episode length: 1339.80 +/- 449.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.34e+03   |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 1721088    |
| train/                  |            |
|    approx_kl            | 0.01728311 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.92      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0759    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.00981   |
|    std                  | 1.36       |
|    value_loss           | 0.0825     |
----------------------------------------
Eval num_timesteps=1723080, episode_reward=-300.00 +/- 0.00
Episode length: 1159.40 +/- 149.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1723080  |
---------------------------------
Eval num_timesteps=1725072, episode_reward=-337.85 +/- 75.70
Episode length: 1365.80 +/- 508.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.37e+03 |
|    mean_reward     | -338     |
| time/              |          |
|    total_timesteps | 1725072  |
---------------------------------
Eval num_timesteps=1727064, episode_reward=-300.00 +/- 0.00
Episode length: 1322.40 +/- 497.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1727064  |
---------------------------------
Eval num_timesteps=1729056, episode_reward=-300.00 +/- 0.00
Episode length: 1458.20 +/- 332.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.46e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1729056  |
---------------------------------
Eval num_timesteps=1731048, episode_reward=-300.00 +/- 0.00
Episode length: 1195.20 +/- 192.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1731048  |
---------------------------------
Eval num_timesteps=1733040, episode_reward=-300.00 +/- 0.00
Episode length: 1069.60 +/- 113.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.07e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1733040  |
---------------------------------
Eval num_timesteps=1735032, episode_reward=-300.00 +/- 0.00
Episode length: 1279.80 +/- 97.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.28e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1735032  |
---------------------------------
Eval num_timesteps=1737024, episode_reward=-300.00 +/- 0.00
Episode length: 1100.20 +/- 182.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1737024  |
---------------------------------
Eval num_timesteps=1739016, episode_reward=-300.00 +/- 0.00
Episode length: 1114.20 +/- 351.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.11e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1739016  |
---------------------------------
Eval num_timesteps=1741008, episode_reward=-300.00 +/- 0.00
Episode length: 1129.80 +/- 346.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.13e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1741008  |
---------------------------------
Eval num_timesteps=1743000, episode_reward=-300.00 +/- 0.00
Episode length: 1262.80 +/- 373.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.26e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1743000  |
---------------------------------
Eval num_timesteps=1744992, episode_reward=-300.00 +/- 0.00
Episode length: 1152.60 +/- 356.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1744992  |
---------------------------------
Eval num_timesteps=1746984, episode_reward=-300.00 +/- 0.00
Episode length: 1409.80 +/- 665.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1746984  |
---------------------------------
Eval num_timesteps=1748976, episode_reward=-300.00 +/- 0.00
Episode length: 1652.40 +/- 571.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.65e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1748976  |
---------------------------------
Eval num_timesteps=1750968, episode_reward=-300.00 +/- 0.00
Episode length: 1205.00 +/- 391.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.20e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1750968  |
---------------------------------
Eval num_timesteps=1752960, episode_reward=-300.00 +/- 0.00
Episode length: 1123.00 +/- 216.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1752960  |
---------------------------------
Eval num_timesteps=1754952, episode_reward=-300.00 +/- 0.00
Episode length: 1356.60 +/- 207.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1754952  |
---------------------------------
Eval num_timesteps=1756944, episode_reward=-300.00 +/- 0.00
Episode length: 1226.60 +/- 147.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1756944  |
---------------------------------
Eval num_timesteps=1758936, episode_reward=-300.00 +/- 0.00
Episode length: 1097.80 +/- 226.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1758936  |
---------------------------------
Eval num_timesteps=1760928, episode_reward=-300.00 +/- 0.00
Episode length: 1208.00 +/- 571.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1760928  |
---------------------------------
Eval num_timesteps=1762920, episode_reward=-300.00 +/- 0.00
Episode length: 1285.20 +/- 234.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1762920  |
---------------------------------
Eval num_timesteps=1764912, episode_reward=-300.00 +/- 0.00
Episode length: 1213.60 +/- 392.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1764912  |
---------------------------------
Eval num_timesteps=1766904, episode_reward=-300.00 +/- 0.00
Episode length: 1291.40 +/- 187.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1766904  |
---------------------------------
Eval num_timesteps=1768896, episode_reward=-300.00 +/- 0.00
Episode length: 1632.40 +/- 448.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.63e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1768896  |
---------------------------------
Eval num_timesteps=1770888, episode_reward=-300.00 +/- 0.00
Episode length: 1315.80 +/- 192.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.32e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1770888     |
| train/                  |             |
|    approx_kl            | 0.016557531 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0657     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0114     |
|    std                  | 1.36        |
|    value_loss           | 0.064       |
-----------------------------------------
Eval num_timesteps=1772880, episode_reward=-300.00 +/- 0.00
Episode length: 1268.00 +/- 228.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1772880  |
---------------------------------
Eval num_timesteps=1774872, episode_reward=-300.00 +/- 0.00
Episode length: 1388.80 +/- 333.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1774872  |
---------------------------------
Eval num_timesteps=1776864, episode_reward=-300.00 +/- 0.00
Episode length: 1115.80 +/- 212.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1776864  |
---------------------------------
Eval num_timesteps=1778856, episode_reward=-300.00 +/- 0.00
Episode length: 1248.80 +/- 116.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1778856  |
---------------------------------
Eval num_timesteps=1780848, episode_reward=-300.00 +/- 0.00
Episode length: 1218.20 +/- 306.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1780848  |
---------------------------------
Eval num_timesteps=1782840, episode_reward=-300.00 +/- 0.00
Episode length: 1202.60 +/- 115.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1782840  |
---------------------------------
Eval num_timesteps=1784832, episode_reward=-300.00 +/- 0.00
Episode length: 1337.20 +/- 336.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1784832  |
---------------------------------
Eval num_timesteps=1786824, episode_reward=-300.00 +/- 0.00
Episode length: 1104.40 +/- 180.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1786824  |
---------------------------------
Eval num_timesteps=1788816, episode_reward=-300.00 +/- 0.00
Episode length: 1240.80 +/- 219.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1788816  |
---------------------------------
Eval num_timesteps=1790808, episode_reward=-300.00 +/- 0.00
Episode length: 1179.00 +/- 166.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1790808  |
---------------------------------
Eval num_timesteps=1792800, episode_reward=-300.00 +/- 0.00
Episode length: 1280.40 +/- 325.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.28e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1792800  |
---------------------------------
Eval num_timesteps=1794792, episode_reward=-300.00 +/- 0.00
Episode length: 1219.60 +/- 295.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1794792  |
---------------------------------
Eval num_timesteps=1796784, episode_reward=-300.00 +/- 0.00
Episode length: 1508.40 +/- 271.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1796784  |
---------------------------------
Eval num_timesteps=1798776, episode_reward=-300.00 +/- 0.00
Episode length: 1122.20 +/- 236.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1798776  |
---------------------------------
Eval num_timesteps=1800768, episode_reward=-300.00 +/- 0.00
Episode length: 1490.60 +/- 493.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.49e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1800768  |
---------------------------------
Eval num_timesteps=1802760, episode_reward=-300.00 +/- 0.00
Episode length: 1485.40 +/- 392.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.49e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1802760  |
---------------------------------
Eval num_timesteps=1804752, episode_reward=-300.00 +/- 0.00
Episode length: 1188.60 +/- 319.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.19e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1804752  |
---------------------------------
Eval num_timesteps=1806744, episode_reward=-300.00 +/- 0.00
Episode length: 1330.60 +/- 303.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1806744  |
---------------------------------
Eval num_timesteps=1808736, episode_reward=-300.00 +/- 0.00
Episode length: 1269.40 +/- 106.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1808736  |
---------------------------------
Eval num_timesteps=1810728, episode_reward=-300.00 +/- 0.00
Episode length: 1327.00 +/- 439.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1810728  |
---------------------------------
Eval num_timesteps=1812720, episode_reward=-300.00 +/- 0.00
Episode length: 1329.00 +/- 181.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1812720  |
---------------------------------
Eval num_timesteps=1814712, episode_reward=-300.00 +/- 0.00
Episode length: 1222.60 +/- 135.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1814712  |
---------------------------------
Eval num_timesteps=1816704, episode_reward=-300.00 +/- 0.00
Episode length: 1475.60 +/- 290.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.48e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1816704  |
---------------------------------
Eval num_timesteps=1818696, episode_reward=-300.00 +/- 0.00
Episode length: 1258.60 +/- 350.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.26e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 1818696     |
| train/                  |             |
|    approx_kl            | 0.015114662 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0766     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0112     |
|    std                  | 1.36        |
|    value_loss           | 0.0446      |
-----------------------------------------
Eval num_timesteps=1820688, episode_reward=-300.00 +/- 0.00
Episode length: 919.80 +/- 224.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 920      |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1820688  |
---------------------------------
Eval num_timesteps=1822680, episode_reward=-300.00 +/- 0.00
Episode length: 1330.20 +/- 587.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1822680  |
---------------------------------
Eval num_timesteps=1824672, episode_reward=-300.00 +/- 0.00
Episode length: 1323.40 +/- 299.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1824672  |
---------------------------------
Eval num_timesteps=1826664, episode_reward=-300.00 +/- 0.00
Episode length: 1204.80 +/- 312.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.2e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1826664  |
---------------------------------
Eval num_timesteps=1828656, episode_reward=-300.00 +/- 0.00
Episode length: 1310.60 +/- 223.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.31e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1828656  |
---------------------------------
Eval num_timesteps=1830648, episode_reward=-300.00 +/- 0.00
Episode length: 1467.80 +/- 449.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.47e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1830648  |
---------------------------------
Eval num_timesteps=1832640, episode_reward=-300.00 +/- 0.00
Episode length: 1183.20 +/- 259.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.18e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1832640  |
---------------------------------
Eval num_timesteps=1834632, episode_reward=-300.00 +/- 0.00
Episode length: 1375.40 +/- 465.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.38e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1834632  |
---------------------------------
Eval num_timesteps=1836624, episode_reward=-300.00 +/- 0.00
Episode length: 1432.80 +/- 264.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.43e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1836624  |
---------------------------------
Eval num_timesteps=1838616, episode_reward=-300.00 +/- 0.00
Episode length: 1528.00 +/- 388.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.53e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1838616  |
---------------------------------
Eval num_timesteps=1840608, episode_reward=-300.00 +/- 0.00
Episode length: 1601.40 +/- 687.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1840608  |
---------------------------------
Eval num_timesteps=1842600, episode_reward=-300.00 +/- 0.00
Episode length: 1664.00 +/- 441.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.66e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1842600  |
---------------------------------
Eval num_timesteps=1844592, episode_reward=-300.00 +/- 0.00
Episode length: 1320.00 +/- 458.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1844592  |
---------------------------------
Eval num_timesteps=1846584, episode_reward=-300.00 +/- 0.00
Episode length: 1524.60 +/- 93.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.52e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1846584  |
---------------------------------
Eval num_timesteps=1848576, episode_reward=-300.00 +/- 0.00
Episode length: 1291.20 +/- 127.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1848576  |
---------------------------------
Traceback (most recent call last):
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] The pipe has been ended
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 567, in <module>
    sim.run_full(args)
  File "C:\Files\Egyetem\Szakdolgozat\RL\Sol\Model\pybullet_drone_simulator.py", line 351, in run_full
    model.learn(total_timesteps=int(5e6),
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 277, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 200, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 219, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\callbacks.py", line 460, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\evaluation.py", line 94, in evaluate_policy
    new_observations, rewards, dones, infos = env.step(actions)
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 206, in step
    return self.step_wait()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\vec_normalize.py", line 181, in step_wait
    obs, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\site-packages\stable_baselines3\common\vec_env\subproc_vec_env.py", line 129, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "C:\Users\xx4qw\anaconda3\envs\CondaDrone\lib\multiprocessing\connection.py", line 321, in _recv_bytes
    raise EOFError
EOFError